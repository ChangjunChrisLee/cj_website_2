[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Changjun LEE",
    "section": "",
    "text": "I’m a data scientist and humanist who believes caffeine is the nectar of the gods. With a strong faith in a higher power, I’m on a quest to find my purpose in this vast universe. My research interests span technology, particularly AI and emerging media, people, and culture. I am dedicated to exploring how society interacts with and is transformed by these advancements.\nI lead the Culture & Technology Convergence Lab, where we focus on technology, management, and policy to enhance joyful and healthy living. In my free time, you’ll often find me sipping coffee, discovering new gadgets, media, and content, or embarking on spontaneous trips while pondering the meaning of life.\nAbout me more.. here"
  },
  {
    "objectID": "teaching/stat101/index.html",
    "href": "teaching/stat101/index.html",
    "title": "Statistics for Data Science",
    "section": "",
    "text": "Introduction\nWelcome to this course on STAT101 (a.k.a. statistical and data sciences via R)! In this course, we will learn the fundamentals of statistical inference and data analysis using the R programming language. Our textbook, “ModernDive”, will be our primary resource for learning the material.\nThe course is designed to be accessible to students with little to no prior experience in statistics or R. We will start by introducing you to the basics of R and data visualization using the ggplot2 package. We will then cover the fundamentals of data summarization, probability, distributions, and statistical inference.\nThroughout the course, you will have the opportunity to work with real-world datasets and apply the techniques you learn to answer interesting questions. You will also have the chance to develop your data analysis skills using R and the tidyverse packages (dplyr, ggplot2, tidyr).\nBy the end of the course, you should have a solid understanding of the fundamental concepts and tools of statistical inference and data analysis. You should also be comfortable using R to manipulate data, create visualizations, and perform statistical tests.\nI hope you will find this course engaging and rewarding, and that you will leave with a deeper understanding of the power and importance of statistical and data analysis in our modern world.\n\n\n\nSyllabus\n\nWeek 1: Introduction to data and R (Chapter 1)\nWeek 2: Visualization with ggplot2 (Chapter 2)\nWeek 3: Summarizing data with dplyr (Chapter 3)\nWeek 4: Probability (Chapter 4)\nWeek 5: Distributions (Chapter 5)\nWeek 6: Foundations for statistical inference: sampling distributions and the central limit theorem (Chapter 6)\nWeek 7: Mid-term exam\nWeek 8: Introduction to inference (Chapter 7)\nWeek 9: Confidence intervals (Chapter 8)\nWeek 10: Hypothesis testing (Chapter 9)\nWeek 11: Inference for numerical data (Chapter 10)\nWeek 12: Inference for categorical data (Chapter 11)\nWeek 13: Simple linear regression (Chapter 12)\nWeek 14: Multiple linear regression (Chapter 13)\nWeek 15: Final exam\n\n\n\n\n\nTextbooks for the course\n\nStatistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\n“ModernDive: An Introduction to Statistical and Data Sciences via R” by Chester Ismay and Albert Y. Kim is a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language. The book covers a wide range of topics, including data visualization, data summarization, probability and distributions, statistical inference, hypothesis testing, and linear regression, and emphasizes the importance of data visualization, reproducibility, and statistical thinking. With its use of the tidyverse suite of packages, numerous exercises and examples, and guidance on how to use R and RStudio, “ModernDive” is a valuable resource for students and practitioners who want to develop their data analysis skills."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/15_week.html",
    "href": "teaching/ml101/weekly/posts/15_week.html",
    "title": "Project presentation",
    "section": "",
    "text": "최종 프로젝트 발표\n\nDate: 14 June (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)\n\nFinal Report Submission\n\n제출 링크: https://forms.gle/TmQfALks1cHwQ6P3A\n파일을 올려야 하기 때문에 한양대 계정으로 로그인 해야함.\n파일 이름: 오전_1조.zip (zip으로 압축해서 업로드)\n\n동료평가 제출\n\n14일 자정까지 꼭 제출해주세요\n제출 링크: https://forms.gle/8iXa4sbjGtEVa56q8"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/15_week.html#for-the-domestic-students",
    "href": "teaching/ml101/weekly/posts/15_week.html#for-the-domestic-students",
    "title": "Project presentation",
    "section": "",
    "text": "최종 프로젝트 발표\n\nDate: 14 June (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)\n\nFinal Report Submission\n\n제출 링크: https://forms.gle/TmQfALks1cHwQ6P3A\n파일을 올려야 하기 때문에 한양대 계정으로 로그인 해야함.\n파일 이름: 오전_1조.zip (zip으로 압축해서 업로드)\n\n동료평가 제출\n\n14일 자정까지 꼭 제출해주세요\n제출 링크: https://forms.gle/8iXa4sbjGtEVa56q8"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/15_week.html#for-the-exchange-students",
    "href": "teaching/ml101/weekly/posts/15_week.html#for-the-exchange-students",
    "title": "Project presentation",
    "section": "For the exchange students",
    "text": "For the exchange students\n\nPlease submit your final output by this day (14 June).\nSubmission link: https://forms.gle/TmQfALks1cHwQ6P3A\n\nFile name: your_name.zip (make it zip)\nTo upload a file, you must log in with your Hanyang University account."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/13_week.html",
    "href": "teaching/ml101/weekly/posts/13_week.html",
    "title": "Natural Language Process",
    "section": "",
    "text": "Weekly design\n\n\nClass\n\nRecommended books for the text mining in R\n\nText Mining with R (written by Julia Silge & David Robinson)\nR로 하는 텍스트마이닝 (written by 안도현 교수님)\nDo it 쉽게 배우는 R 텍스트마이닝 (written by 김영우)\n\n\nCode for hands-on practice\n\nNLP_1\nNLP_2\nNLP_3\nNLP_4\n\n\nRecorded lecture"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/11_week.html",
    "href": "teaching/ml101/weekly/posts/11_week.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #9\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nClass\nApriori Algorithm Implementation in R using ‘arules’ library Association mining is usually done on transactions data from a retail market or from an online e-commerce store. Since most transactions data is large, the apriori algorithm makes it easier to find these patterns or rules quickly. Association Rules are widely used to analyze retail basket or transaction data, and are intended to identify strong rules discovered in transaction data using measures of interestingness, based on the concept of strong rules.\nApriori uses a “bottom up” approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found.\n\nDownload the grocery dataset\n[Grocery data]\nImport Required libraries and data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(plyr)\n\n------------------------------------------------------------------------------\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n------------------------------------------------------------------------------\n\nAttaching package: 'plyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\ngroceries &lt;- read.csv(\"content/Groceries_dataset.csv\")\nhead(groceries, 20)\n\n   Member_number       Date           itemDescription\n1           1808 21-07-2015            tropical fruit\n2           2552 05-01-2015                whole milk\n3           2300 19-09-2015                 pip fruit\n4           1187 12-12-2015          other vegetables\n5           3037 01-02-2015                whole milk\n6           4941 14-02-2015                rolls/buns\n7           4501 08-05-2015          other vegetables\n8           3803 23-12-2015                pot plants\n9           2762 20-03-2015                whole milk\n10          4119 12-02-2015            tropical fruit\n11          1340 24-02-2015              citrus fruit\n12          2193 14-04-2015                      beef\n13          1997 21-07-2015               frankfurter\n14          4546 03-09-2015                   chicken\n15          4736 21-07-2015                    butter\n16          1959 30-03-2015     fruit/vegetable juice\n17          1974 03-05-2015 packaged fruit/vegetables\n18          2421 02-09-2015                 chocolate\n19          1513 03-08-2015             specialty bar\n20          1905 07-07-2015          other vegetables\n\n\nData Cleaning and Exploration\nChecking NA values\n\nglimpse(groceries)\n\nRows: 38,765\nColumns: 3\n$ Member_number   &lt;int&gt; 1808, 2552, 2300, 1187, 3037, 4941, 4501, 3803, 2762, …\n$ Date            &lt;chr&gt; \"21-07-2015\", \"05-01-2015\", \"19-09-2015\", \"12-12-2015\"…\n$ itemDescription &lt;chr&gt; \"tropical fruit\", \"whole milk\", \"pip fruit\", \"other ve…\n\nsummary(groceries)\n\n Member_number      Date           itemDescription   \n Min.   :1000   Length:38765       Length:38765      \n 1st Qu.:2002   Class :character   Class :character  \n Median :3005   Mode  :character   Mode  :character  \n Mean   :3004                                        \n 3rd Qu.:4007                                        \n Max.   :5000                                        \n\nsum(is.na(groceries))\n\n[1] 0\n\n\nGroup all the items that were bought together by the same customer on the same date\n\nitemList &lt;- ddply(groceries, \n                  c(\"Member_number\",\"Date\"), \n                  function(df1) paste(df1$itemDescription, collapse = \",\")\n                  )\n                  \nhead(itemList,15)\n\n   Member_number       Date                                            V1\n1           1000 15-03-2015 sausage,whole milk,semi-finished bread,yogurt\n2           1000 24-06-2014                 whole milk,pastry,salty snack\n3           1000 24-07-2015                   canned beer,misc. beverages\n4           1000 25-11-2015                      sausage,hygiene articles\n5           1000 27-05-2015                       soda,pickled vegetables\n6           1001 02-05-2015                              frankfurter,curd\n7           1001 07-02-2014                 sausage,whole milk,rolls/buns\n8           1001 12-12-2014                               whole milk,soda\n9           1001 14-04-2015                              beef,white bread\n10          1001 20-01-2015           frankfurter,soda,whipped/sour cream\n11          1002 09-02-2014            frozen vegetables,other vegetables\n12          1002 26-04-2014                             butter,whole milk\n13          1002 26-04-2015                          tropical fruit,sugar\n14          1002 30-08-2015               butter milk,specialty chocolate\n15          1003 10-02-2015                            sausage,rolls/buns\n\n\nRemove member number and date\n\nitemList %&gt;% \n  select(V1) %&gt;% \n  setNames(c(\"itemList\")) %&gt;% \n  head\n\n                                       itemList\n1 sausage,whole milk,semi-finished bread,yogurt\n2                 whole milk,pastry,salty snack\n3                   canned beer,misc. beverages\n4                      sausage,hygiene articles\n5                       soda,pickled vegetables\n6                              frankfurter,curd\n\nitemList &lt;- itemList %&gt;% \n  select(V1) %&gt;% \n  setNames(c(\"itemList\")) \n\nwrite.csv(itemList,\"ItemList.csv\", quote = FALSE, row.names = TRUE)\n\nConvert CSV file to Basket Format\n\nlibrary(arules)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'arules'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\nlibrary(arulesViz)\n\n\n# read the transactional dataset from a CSV file and convert it into a transaction object\ntxn = read.transactions(file = \"ItemList.csv\", \n                         rm.duplicates = TRUE, # remove duplicate transactions\n                         format = \"basket\", # dataset is in basket format (each row represents a single transaction)\n                         sep = \",\", # CSV file is comma-separated\n                         cols = 1) # transaction IDs are stored in the first column of the CSV file\n\ndistribution of transactions with duplicates:\nitems\n  1   2   3   4 \n662  39   5   1 \n\nprint(txn)\n\ntransactions in sparse format with\n 14964 transactions (rows) and\n 168 items (columns)\n\n\n\n\nThe first line of output shows the distribution of transactions by item. In this case, there are four items (items 1, 2, 3, and 4), and the numbers indicate how many transactions in the dataset contain each item. For example, there are 662 transactions that contain item 1, 39 transactions that contain item 2, 5 transactions that contain item 3, and 1 transaction that contains item 4. This information is useful for understanding the frequency of different items in the dataset and identifying which items are most commonly associated with each other.\nThe second line of output shows the total number of transactions in the dataset and the number of unique items that appear in those transactions. Specifically, there are 14964 transactions (rows) and 168 unique items (columns). The transactions are in sparse format, meaning that the majority of the entries in the transaction matrix are zero (i.e., most transactions do not contain most of the items). This format is used to save memory when working with large datasets that have many items.\n\n\nMost Frequent Products\n\nitemFrequencyPlot(txn, topN = 20)\n\n\n\n\nApriori Algorithm The apriori() generates the most relevent set of rules from a given transaction data. It also shows the support, confidence and lift of those rules. These three measure can be used to decide the relative strength of the rules. So what do these terms mean?\nLets consider the rule {X → Y} in order to compute these metrics.\n\\[\nSupport(X,Y) = \\frac{frq(X,Y)}{N}\n\\]\n\\[\nConfidence(X → Y) = \\frac{frq(X,Y)}{frq(X)}\n\\]\n\\[\nLift(X → Y) = \\frac{Confidence(X → Y)}{Support(Y)}\n\\]\n\n\nbasket_rules &lt;- apriori(txn, \n                        parameter = list(\n                          minlen = 2, # Minimum number of items in a rule (in this case, 2)\n                          sup = 0.001, # Minimum support threshold (a rule must be present in at least 0.1% of transactions)\n                          conf = 0.05, # Minimum confidence threshold (rules must have at least 5% confidence)\n                          target = \"rules\" # Specifies that we want to generate association rules\n                        ))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.05    0.1    1 none FALSE            TRUE       5   0.001      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 14 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[168 item(s), 14964 transaction(s)] done [0.00s].\nsorting and recoding items ... [149 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nwriting ... [450 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\nThe apriori function takes several parameters, including the transaction dataset (txn) and a list of parameters (parameter) that control the behavior of the algorithm. The minlen parameter sets the minimum number of items in a rule to 2, which means that the algorithm will only consider rules that involve at least 2 items. The sup parameter sets the minimum support threshold to 0.001, which means that a rule must be present in at least 0.1% of transactions in order to be considered significant. The conf parameter sets the minimum confidence threshold to 0.05, which means that a rule must have at least 5% confidence (i.e., be correct at least 5% of the time) to be considered significant. Finally, the target parameter specifies that we want to generate association rules rather than just frequent itemsets.\n\n\nTotal rules generated\n\nprint(length(basket_rules))\n\n[1] 450\n\nsummary(basket_rules)\n\nset of 450 rules\n\nrule length distribution (lhs + rhs):sizes\n  2   3 \n423  27 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    2.00    2.00    2.06    2.00    3.00 \n\nsummary of quality measures:\n    support           confidence         coverage             lift       \n Min.   :0.001002   Min.   :0.05000   Min.   :0.005346   Min.   :0.5195  \n 1st Qu.:0.001270   1st Qu.:0.06397   1st Qu.:0.015972   1st Qu.:0.7673  \n Median :0.001938   Median :0.08108   Median :0.023590   Median :0.8350  \n Mean   :0.002760   Mean   :0.08759   Mean   :0.033723   Mean   :0.8859  \n 3rd Qu.:0.003341   3rd Qu.:0.10482   3rd Qu.:0.043705   3rd Qu.:0.9601  \n Max.   :0.014836   Max.   :0.25581   Max.   :0.157912   Max.   :2.1831  \n     count      \n Min.   : 15.0  \n 1st Qu.: 19.0  \n Median : 29.0  \n Mean   : 41.3  \n 3rd Qu.: 50.0  \n Max.   :222.0  \n\nmining info:\n data ntransactions support confidence\n  txn         14964   0.001       0.05\n                                                                                          call\n apriori(data = txn, parameter = list(minlen = 2, sup = 0.001, conf = 0.05, target = \"rules\"))\n\n\nInspecting the basket rules\n\ninspect(basket_rules[1:20])\n\n     lhs                            rhs                support     confidence\n[1]  {frozen fish}               =&gt; {whole milk}       0.001069233 0.1568627 \n[2]  {seasonal products}         =&gt; {rolls/buns}       0.001002406 0.1415094 \n[3]  {pot plants}                =&gt; {other vegetables} 0.001002406 0.1282051 \n[4]  {pot plants}                =&gt; {whole milk}       0.001002406 0.1282051 \n[5]  {pasta}                     =&gt; {whole milk}       0.001069233 0.1322314 \n[6]  {pickled vegetables}        =&gt; {whole milk}       0.001002406 0.1119403 \n[7]  {packaged fruit/vegetables} =&gt; {rolls/buns}       0.001202887 0.1417323 \n[8]  {detergent}                 =&gt; {yogurt}           0.001069233 0.1240310 \n[9]  {detergent}                 =&gt; {rolls/buns}       0.001002406 0.1162791 \n[10] {detergent}                 =&gt; {whole milk}       0.001403368 0.1627907 \n[11] {semi-finished bread}       =&gt; {other vegetables} 0.001002406 0.1056338 \n[12] {semi-finished bread}       =&gt; {whole milk}       0.001670676 0.1760563 \n[13] {red/blush wine}            =&gt; {rolls/buns}       0.001336541 0.1273885 \n[14] {red/blush wine}            =&gt; {other vegetables} 0.001136060 0.1082803 \n[15] {flour}                     =&gt; {tropical fruit}   0.001069233 0.1095890 \n[16] {flour}                     =&gt; {whole milk}       0.001336541 0.1369863 \n[17] {herbs}                     =&gt; {yogurt}           0.001136060 0.1075949 \n[18] {herbs}                     =&gt; {whole milk}       0.001136060 0.1075949 \n[19] {processed cheese}          =&gt; {root vegetables}  0.001069233 0.1052632 \n[20] {processed cheese}          =&gt; {rolls/buns}       0.001470195 0.1447368 \n     coverage    lift      count\n[1]  0.006816359 0.9933534 16   \n[2]  0.007083667 1.2864807 15   \n[3]  0.007818765 1.0500611 15   \n[4]  0.007818765 0.8118754 15   \n[5]  0.008086073 0.8373723 16   \n[6]  0.008954825 0.7088763 15   \n[7]  0.008487036 1.2885066 18   \n[8]  0.008620690 1.4443580 16   \n[9]  0.008620690 1.0571081 15   \n[10] 0.008620690 1.0308929 21   \n[11] 0.009489441 0.8651911 15   \n[12] 0.009489441 1.1148993 25   \n[13] 0.010491847 1.1581057 20   \n[14] 0.010491847 0.8868668 17   \n[15] 0.009756750 1.6172489 16   \n[16] 0.009756750 0.8674833 20   \n[17] 0.010558674 1.2529577 17   \n[18] 0.010558674 0.6813587 17   \n[19] 0.010157712 1.5131200 16   \n[20] 0.010157712 1.3158214 22   \n\n\nVisualizing the Association Rules\n\nplot(basket_rules, jitter = 0)\n\n\n\n\n\nplot(basket_rules, method = \"grouped\", control = list(k = 5))\n\n\n\n\nGraph of first 20 rules\n\nplot(basket_rules[1:20], method=\"graph\")\n\n\n\n\nGraph of first 50 rules\n\nplot(basket_rules[1:50], method=\"graph\")\n\n\n\n\nParallel coordinates plot\n\nplot(basket_rules[1:20], method=\"paracoord\")\n\n\n\n\nChanging hyperparameters:\n\nbasket_rules2 &lt;- apriori(txn, \n                         parameter = list(minlen=3, \n                                          sup = 0.001, \n                                          conf = 0.1,\n                                          target=\"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.1    0.1    1 none FALSE            TRUE       5   0.001      3\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 14 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[168 item(s), 14964 transaction(s)] done [0.00s].\nsorting and recoding items ... [149 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nwriting ... [17 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\nprint(length(basket_rules2))\n\n[1] 17\n\n\n\nsummary(basket_rules2)\n\nset of 17 rules\n\nrule length distribution (lhs + rhs):sizes\n 3 \n17 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      3       3       3       3       3       3 \n\nsummary of quality measures:\n    support           confidence        coverage             lift       \n Min.   :0.001002   Min.   :0.1018   Min.   :0.005346   Min.   :0.7214  \n 1st Qu.:0.001136   1st Qu.:0.1172   1st Qu.:0.008086   1st Qu.:0.8897  \n Median :0.001136   Median :0.1269   Median :0.008955   Median :1.1081  \n Mean   :0.001207   Mean   :0.1437   Mean   :0.008821   Mean   :1.1794  \n 3rd Qu.:0.001337   3rd Qu.:0.1642   3rd Qu.:0.010559   3rd Qu.:1.2297  \n Max.   :0.001470   Max.   :0.2558   Max.   :0.011160   Max.   :2.1831  \n     count      \n Min.   :15.00  \n 1st Qu.:17.00  \n Median :17.00  \n Mean   :18.06  \n 3rd Qu.:20.00  \n Max.   :22.00  \n\nmining info:\n data ntransactions support confidence\n  txn         14964   0.001        0.1\n                                                                                         call\n apriori(data = txn, parameter = list(minlen = 3, sup = 0.001, conf = 0.1, target = \"rules\"))\n\n\n\ninspect(basket_rules2)\n\n     lhs                               rhs                support    \n[1]  {sausage, yogurt}              =&gt; {whole milk}       0.001470195\n[2]  {sausage, whole milk}          =&gt; {yogurt}           0.001470195\n[3]  {whole milk, yogurt}           =&gt; {sausage}          0.001470195\n[4]  {sausage, soda}                =&gt; {whole milk}       0.001069233\n[5]  {sausage, whole milk}          =&gt; {soda}             0.001069233\n[6]  {rolls/buns, sausage}          =&gt; {whole milk}       0.001136060\n[7]  {sausage, whole milk}          =&gt; {rolls/buns}       0.001136060\n[8]  {rolls/buns, yogurt}           =&gt; {whole milk}       0.001336541\n[9]  {whole milk, yogurt}           =&gt; {rolls/buns}       0.001336541\n[10] {other vegetables, yogurt}     =&gt; {whole milk}       0.001136060\n[11] {whole milk, yogurt}           =&gt; {other vegetables} 0.001136060\n[12] {rolls/buns, soda}             =&gt; {other vegetables} 0.001136060\n[13] {other vegetables, soda}       =&gt; {rolls/buns}       0.001136060\n[14] {other vegetables, rolls/buns} =&gt; {soda}             0.001136060\n[15] {rolls/buns, soda}             =&gt; {whole milk}       0.001002406\n[16] {other vegetables, soda}       =&gt; {whole milk}       0.001136060\n[17] {other vegetables, rolls/buns} =&gt; {whole milk}       0.001202887\n     confidence coverage    lift      count\n[1]  0.2558140  0.005747126 1.6199746 22   \n[2]  0.1641791  0.008954825 1.9118880 22   \n[3]  0.1317365  0.011160118 2.1830624 22   \n[4]  0.1797753  0.005947608 1.1384500 16   \n[5]  0.1194030  0.008954825 1.2296946 16   \n[6]  0.2125000  0.005346164 1.3456835 17   \n[7]  0.1268657  0.008954825 1.1533523 17   \n[8]  0.1709402  0.007818765 1.0825005 20   \n[9]  0.1197605  0.011160118 1.0887581 20   \n[10] 0.1404959  0.008086073 0.8897081 17   \n[11] 0.1017964  0.011160118 0.8337610 17   \n[12] 0.1404959  0.008086073 1.1507281 17   \n[13] 0.1172414  0.009689922 1.0658566 17   \n[14] 0.1075949  0.010558674 1.1080872 17   \n[15] 0.1239669  0.008086073 0.7850365 15   \n[16] 0.1172414  0.009689922 0.7424460 17   \n[17] 0.1139241  0.010558674 0.7214386 18   \n\n\n\nplot(basket_rules2, method=\"graph\")\n\n\n\n\n\nplot(basket_rules2, method=\"paracoord\")"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/09_week.html",
    "href": "teaching/ml101/weekly/posts/09_week.html",
    "title": "Regression",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #7\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nAbout QZ #1\n\n\n\nClass\n\nMotivation\nIn many real-world applications, the relationship between the dependent variable and independent variables is not always linear. Non-linear regression is a versatile tool that can be used to model complex relationships between variables, allowing for a more accurate representation of the underlying processes.\n\n\nTheory\nNon-linear regression seeks to find the best-fit curve or surface through the data points by minimizing the sum of the squared residuals, which represent the difference between the observed and predicted values. The general form of a non-linear regression model can be written as:\n\\[\ny = f(x, β) + ε\n\\]\nwhere\n\ny is the dependent variable,\nx is the independent variable,\nβ represents the vector of parameters to be estimated,\nf(x, β) is the non-linear function, and\nε is the error term.\n\n\nGeneralized Linear Model (GLM)\nGLM stands for Generalized Linear Model in R. It is a flexible extension of the ordinary linear regression that allows for response variables with error distribution models other than the normal distribution, such as the binomial or Poisson distributions. The GLM is used to model the relationship between a response variable and one or more predictor variables by combining a linear predictor function with a specified probability distribution for the response variable.\nThe glm() function in R is used to fit generalized linear models, and its general syntax is:\nglm(formula, data, family)\nwhere:\n\nformula: A symbolic description of the model to be fitted, such as y ~ x1 + x2.\ndata: A data frame containing the variables in the model.\nfamily: A description of the error distribution and link function to be used in the model. Common choices include binomial, poisson, and gaussian. The link function, which can be specified using the link argument within the family function, determines how the expected value of the response variable is related to the linear predictor function. Examples of link functions are Logit and Probit.\n\n\n\nThe GLM can be applied to various types of regression problems, including linear regression, logistic regression, and Poisson regression, by specifying the appropriate distribution family and link function. This versatility makes the GLM a powerful and widely used tool for modeling relationships between variables in various fields.\n\n\nThen, what is the difference btw GLM & LM? See the link below.\nThe Difference Between glm and lm in R\n\n\nLogit Model (A representative model in GLM)\n\nLogistic regression, specifically the logit model, is a popular technique for handling non-linear dependent variables, allowing us to predict the probability of an event occurring given a set of input variables.\n\n\\[\nP(Y=1) = \\frac{1}{(1 + exp(-z))}\n\\] where z is a linear function of the predictor variables: \\[\nz = β_0 + β_1X_1 + β_2X_2 + ... + β_kX_k\n\\]\nThe logit transformation, which is the log-odds of the probability, is given by:\n\\[\nlogit(P(Y=1)) = \\log{\\frac {P(Y=1)}{P(Y=0)}} = z\n\\] The coefficients \\((β_0, β_1, ... β_k)\\) are estimated using Maximum Likelihood Estimation (MLE), which seeks to maximize the likelihood of observing the data given the logistic model.\nLet’s use R to fit a logit model to a simple dataset. First, we will check if the required library is installed, and if not, install and load it:\n\n# install.packages(\"glm2\")\nlibrary(glm2)\n\nNext, let’s create a synthetic dataset for our example:\n\nset.seed(42)\nx1 &lt;- runif(100, 0, 10)\nx2 &lt;- runif(100, 0, 10)\nz &lt;- 0.5 + 0.7 * x1 - 0.3 * x2\np &lt;- 1 / (1 + exp(-z))\ny &lt;- ifelse(p &gt; 0.5, 1, 0)\ndata &lt;- data.frame(x1, x2, y)\ndata\n\n             x1         x2 y\n1   9.148060435 6.26245345 1\n2   9.370754133 2.17157698 1\n3   2.861395348 2.16567311 1\n4   8.304476261 3.88945029 1\n5   6.417455189 9.42455692 1\n6   5.190959491 9.62608014 1\n7   7.365883146 7.39855279 1\n8   1.346665972 7.33245906 0\n9   6.569922904 5.35761290 1\n10  7.050647840 0.02272966 1\n11  4.577417762 6.08937453 1\n12  7.191122517 8.36801559 1\n13  9.346722472 7.51522563 1\n14  2.554288243 4.52731573 1\n15  4.622928225 5.35789994 1\n16  9.400145228 5.37376695 1\n17  9.782264284 0.01380844 1\n18  1.174873617 3.55665954 1\n19  4.749970816 6.12133090 1\n20  5.603327462 8.28942131 1\n21  9.040313873 3.56721999 1\n22  1.387101677 4.10635126 1\n23  9.888917289 5.73475899 1\n24  9.466682326 5.89678304 1\n25  0.824375581 7.19657292 0\n26  5.142117843 3.94973045 1\n27  3.902034671 9.19203929 1\n28  9.057381309 9.62570294 1\n29  4.469696281 2.33523526 1\n30  8.360042600 7.24497600 1\n31  7.375956178 9.03634525 1\n32  8.110551413 6.03474085 1\n33  3.881082828 6.31507299 1\n34  6.851697294 9.37385850 1\n35  0.039483388 8.50482751 0\n36  8.329160803 5.79820899 1\n37  0.073341469 8.21403924 0\n38  2.076589728 1.13718609 1\n39  9.066014078 7.64507759 1\n40  6.117786434 6.23613457 1\n41  3.795592405 1.48446607 1\n42  4.357715850 0.80264467 1\n43  0.374310329 4.64069551 0\n44  9.735399138 7.79368161 1\n45  4.317512489 7.33527960 1\n46  9.575765966 8.17230444 1\n47  8.877549055 1.70162481 1\n48  6.399787695 9.44720326 1\n49  9.709666104 2.93623841 1\n50  6.188382073 1.49072052 1\n51  3.334272113 7.19378591 1\n52  3.467482482 3.24085952 1\n53  3.984854114 7.78809499 1\n54  7.846927757 3.94441002 1\n55  0.389364911 6.78592868 0\n56  7.487953862 7.75825043 1\n57  6.772768302 1.87869044 1\n58  1.712643304 0.29085819 1\n59  2.610879638 1.35713797 1\n60  5.144129347 6.80164178 1\n61  6.756072745 9.34822954 1\n62  9.828171979 5.50494084 1\n63  7.595442676 6.01766235 1\n64  5.664884241 1.96994488 1\n65  8.496897186 5.35236611 1\n66  1.894739354 1.79555739 1\n67  2.712866147 4.51886494 1\n68  8.281584852 3.17053352 1\n69  6.932048204 1.16174670 1\n70  2.405447396 1.86102157 1\n71  0.429887960 7.29730097 0\n72  1.404790941 4.11872071 1\n73  2.163854151 4.14049682 1\n74  4.793985642 4.80310129 1\n75  1.974103423 4.27494466 1\n76  7.193558377 1.36490360 1\n77  0.078847387 8.24679406 0\n78  3.754899646 5.92304243 1\n79  5.144077083 7.94396978 1\n80  0.015705542 7.69032426 0\n81  5.816040025 9.18056417 1\n82  1.579052082 8.62629777 0\n83  3.590283059 3.16975238 1\n84  6.456318784 2.59260576 1\n85  7.758233626 7.42266452 1\n86  5.636468416 7.47361117 1\n87  2.337033986 9.17904034 0\n88  0.899805163 7.93191209 0\n89  0.856120649 1.33329618 1\n90  3.052183695 2.87749752 1\n91  6.674265147 1.94676144 1\n92  0.002388966 7.84109383 0\n93  2.085699569 1.28872162 1\n94  9.330341273 1.29089284 1\n95  9.256447486 0.72253111 1\n96  7.340943010 0.53129483 1\n97  3.330719834 5.31874436 1\n98  5.150633298 1.12308242 1\n99  7.439746463 7.43187720 1\n100 6.191592400 7.31315477 1\n\n\nHere, we have generated 100 data points with two predictor variables, x1 and x2, and a binary outcome variable, y.\nNow, let’s fit the logit model using the glm() function:\n\nmodel &lt;- glm(y ~ x1 + x2, data = data, family = binomial(link = \"logit\"))\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nTo view the estimated coefficients, we can use the summary() function:\n\nsummary(model)\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    39.06   32579.11   0.001    0.999\nx1             31.28   11604.28   0.003    0.998\nx2            -15.17    8369.54  -0.002    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7.7277e+01  on 99  degrees of freedom\nResidual deviance: 1.3375e-08  on 97  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n\n\nTo make predictions on new data, we can use the predict() function:\n\nnew_data &lt;- data.frame(x1 = c(5, 7), x2 = c(3, 9))\nnew_data\n\n  x1 x2\n1  5  3\n2  7  9\n\npredicted_prob &lt;- predict(model, newdata = new_data, \n                          type = \"response\")\npredicted_prob\n\n1 2 \n1 1 \n\npredicted_class &lt;- ifelse(predicted_prob &gt; 0.5, 1, 0)\npredicted_class\n\n1 2 \n1 1 \n\n\n\n\n\n\nSecond practice with another dataset\nLet’s use haberman dataset\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nhaberman&lt;-read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\", header=F)\nnames(haberman)&lt;-c(\"age\", \"op_year\", \"no_nodes\", \"survival\")\n\nglimpse(haberman)\n\nRows: 306\nColumns: 4\n$ age      &lt;int&gt; 30, 30, 30, 31, 31, 33, 33, 34, 34, 34, 34, 34, 34, 34, 35, 3…\n$ op_year  &lt;int&gt; 64, 62, 65, 59, 65, 58, 60, 59, 66, 58, 60, 61, 67, 60, 64, 6…\n$ no_nodes &lt;int&gt; 1, 3, 0, 2, 4, 10, 0, 0, 9, 30, 1, 10, 7, 0, 13, 0, 1, 0, 0, …\n$ survival &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nThe Haberman dataset, also known as the Haberman’s Survival dataset, is a dataset containing cases from a study conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who underwent surgery for breast cancer. The dataset is often used for classification and data analysis tasks in machine learning and statistics.\nThe Haberman dataset contains 306 instances (rows) and 4 attributes (columns). The attributes are:\n\nAge: The patient’s age at the time of the operation, represented as an integer.\nYear: The year of the operation, represented as an integer from 58 (1958) to 69 (1969).\nNodes: The number of positive axillary nodes detected, represented as an integer. A positive axillary node is a lymph node containing cancer cells. A higher number of positive axillary nodes generally indicates a more advanced stage of cancer.\nStatus: The survival status of the patient, represented as an integer. A value of 1 indicates that the patient survived for 5 years or longer after the surgery, while a value of 2 indicates that the patient died within 5 years of the surgery.\nResponse var: Survival in 5 years\n\nThe goal of analyzing the Haberman dataset is usually to predict a patient’s survival status based on the other three attributes (age, year, and nodes). This is typically treated as a binary classification problem, with survival status as the dependent variable and the other attributes as independent variables. Various machine learning algorithms, including logistic regression, support vector machines, and decision trees, can be applied to this dataset for predictive modeling and analysis.\n\ntable(haberman$survival)\n\n\n  1   2 \n225  81 \n\nprop.table(table(haberman$survival))\n\n\n        1         2 \n0.7352941 0.2647059 \n\n\nAdding a Binary Survival Indicator to the Haberman Dataset Using mutate and ifelse\n\nhaberman %&gt;% \n  mutate(n_survival=ifelse(survival==2,1,0)) %&gt;% \n  head\n\n  age op_year no_nodes survival n_survival\n1  30      64        1        1          0\n2  30      62        3        1          0\n3  30      65        0        1          0\n4  31      59        2        1          0\n5  31      65        4        1          0\n6  33      58       10        1          0\n\n\n\nhaberman %&gt;% \n  mutate(n_survival=ifelse(survival==2,1,0)) %&gt;% \n  select(-survival) -&gt; haberman\nsummary(haberman)\n\n      age           op_year         no_nodes        n_survival    \n Min.   :30.00   Min.   :58.00   Min.   : 0.000   Min.   :0.0000  \n 1st Qu.:44.00   1st Qu.:60.00   1st Qu.: 0.000   1st Qu.:0.0000  \n Median :52.00   Median :63.00   Median : 1.000   Median :0.0000  \n Mean   :52.46   Mean   :62.85   Mean   : 4.026   Mean   :0.2647  \n 3rd Qu.:60.75   3rd Qu.:65.75   3rd Qu.: 4.000   3rd Qu.:1.0000  \n Max.   :83.00   Max.   :69.00   Max.   :52.000   Max.   :1.0000  \n\n\nVisualize the density of age, op_year, and no_nodes\n\npar(mfrow=c(1,3))\nplot(density(haberman$age))\nplot(density(haberman$op_year))\nplot(density(haberman$no_nodes))\n\n\n\n\nMake them box_plot as well\n\npar(mfrow=c(1,3))\nboxplot(haberman$age)\nboxplot(haberman$op_year)\nboxplot(haberman$no_nodes)\n\n\n\n\nCheck correlation between vars in the data\n\ncorr &lt;- round(cor(haberman), 2)\ncorr\n\n             age op_year no_nodes n_survival\nage         1.00    0.09    -0.06       0.07\nop_year     0.09    1.00     0.00       0.00\nno_nodes   -0.06    0.00     1.00       0.29\nn_survival  0.07    0.00     0.29       1.00\n\n\nMake it cor_plot\n\nlibrary(ggcorrplot)\nggcorrplot(corr, method = \"circle\")\n\n\n\n\nSee the relationship between Xs & Y\n\npar(mfrow=c(2,2))\nplot(haberman$age, haberman$n_survival)\nplot(haberman$op_year, haberman$n_survival)\nplot(haberman$no_nodes, haberman$n_survival)\n\n\n\n\nAge & Survival\n\nhaberman %&gt;% \n  ggplot(aes(x=age, y=n_survival)) + \n  geom_jitter(aes(col=factor(n_survival)), \n              height=0.1, width=0.1)\n\n\n\n\nOp_year & Survival\n\nhaberman %&gt;% \n  ggplot(aes(x=op_year, y=n_survival)) + \n  geom_jitter(aes(col=factor(n_survival)), \n              height=0.1, width=0.1)\n\n\n\n\nno_nodes & survival\n\nhaberman %&gt;% \n  ggplot(aes(x=no_nodes, y=n_survival)) + \n  geom_jitter(aes(col=factor(n_survival)), \n              height=0.1, width=0.1)\n\n\n\n\nFit the data to the simple linear model\n\nlinear.model&lt;-glm(\"n_survival~.\", \n                  data=haberman)\nsummary(linear.model)\n\n\nCall:\nglm(formula = \"n_survival~.\", data = haberman)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.103030   0.476429   0.216    0.829    \nage          0.003577   0.002259   1.583    0.114    \nop_year     -0.001563   0.007496  -0.209    0.835    \nno_nodes     0.017963   0.003381   5.313  2.1e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.179504)\n\n    Null deviance: 59.559  on 305  degrees of freedom\nResidual deviance: 54.210  on 302  degrees of freedom\nAIC: 348.79\n\nNumber of Fisher Scoring iterations: 2\n\n\nFit the data to the generalized linear model\n\nlogit.model&lt;-glm(\"n_survival~.\", \n                 data=haberman, \n                 family=\"binomial\")\nsummary(logit.model)\n\n\nCall:\nglm(formula = \"n_survival~.\", family = \"binomial\", data = haberman)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.861625   2.675197  -0.696    0.487    \nage          0.019899   0.012735   1.563    0.118    \nop_year     -0.009784   0.042013  -0.233    0.816    \nno_nodes     0.088442   0.019849   4.456 8.36e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 353.69  on 305  degrees of freedom\nResidual deviance: 328.26  on 302  degrees of freedom\nAIC: 336.26\n\nNumber of Fisher Scoring iterations: 4\n\n\nOdds ratio\n\nexp(logit.model$coefficients)\n\n(Intercept)         age     op_year    no_nodes \n  0.1554198   1.0200987   0.9902638   1.0924714 \n\nexp(cbind(OR = coef(logit.model), confint(logit.model)))\n\nWaiting for profiling to be done...\n\n\n                   OR       2.5 %    97.5 %\n(Intercept) 0.1554198 0.000794884 29.367796\nage         1.0200987 0.995033131  1.046136\nop_year     0.9902638 0.911586214  1.075310\nno_nodes    1.0924714 1.052631312  1.137984\n\n\nPrediction\n\nnewdata&lt;-data.frame(age=c(10,20,30), \n                    op_year=c(40,50,60), \n                    no_nodes=c(1,3,5))\nnewdata\n\n  age op_year no_nodes\n1  10      40        1\n2  20      50        3\n3  30      60        5\n\npredict(linear.model, newdata)\n\n         1          2          3 \n0.09422042 0.15027781 0.20633519 \n\n\nType of prediction is a predicted probability (type=“response”).\n\npredict(logit.model, newdata, type = \"response\")\n\n        1         2         3 \n0.1228683 0.1561044 0.1963186 \n\npred_prob &lt;- predict(logit.model, newdata, type = \"response\")\n\npredicted_class &lt;- ifelse(pred_prob &gt; 0.5, 1, 0)\npredicted_class\n\n1 2 3 \n0 0 0"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/07_week.html",
    "href": "teaching/ml101/weekly/posts/07_week.html",
    "title": "Regression",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #6\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nClass\n\n\nMotivation\nLinear regression is a foundational technique in statistical analysis and machine learning that helps us understand and quantify relationships between variables. As social scientists, we often aim to analyze the effect of certain factors on an outcome of interest. Linear regression provides us with a way to model these relationships, quantify their effects, and make predictions based on our findings. By mastering linear regression, social scientists can gain valuable insights into various phenomena, test hypotheses, and make data-driven decisions.\n\n\n\nUsage and Importance\nLinear regression is widely used in social science research for several reasons:\n\nSimplicity: Linear regression is relatively easy to understand and implement, making it an accessible method for researchers across disciplines. Despite its simplicity, it can often provide valuable insights and predictions.\nInterpretability: The coefficients obtained from linear regression have a clear interpretation, allowing researchers to understand the effect of each independent variable on the dependent variable.\nBasis for Advanced Techniques: Linear regression serves as a foundation for more advanced statistical and machine learning techniques. Gaining a deep understanding of linear regression helps social scientists better understand and apply these more advanced methods.\n\n\n\nReal-world Applications\nLinear regression has a wide range of applications in social science research. Some examples include:\n\nEconomics: Linear regression can be used to study the impact of various factors on economic indicators, such as GDP growth, unemployment rate, and inflation.\nPolitical Science: Researchers can use linear regression to analyze the effects of political factors on election outcomes, public opinion, or policy decisions.\nSociology: Linear regression can help us understand the relationship between social variables, such as education level, income, and various social outcomes like crime rates, health status, and life satisfaction.\nPsychology: Researchers can use linear regression to study the effects of different psychological factors on human behavior, mental health, and well-being.\nEducation: Linear regression can be used to analyze the impact of various factors on educational outcomes, such as standardized test scores, graduation rates, and college enrollment.\n\nOverall, linear regression is a versatile and powerful tool for social scientists, enabling them to gain insights into the relationships between variables and make evidence-based predictions.\n\n\n\nTheory\nSimple Linear Regression\n\nSimple linear regression is a statistical method that helps us understand the relationship between one dependent variable (y) and one independent variable (x). It models the relationship as a linear function.\n\n\nEquation:\n\\[\ny = β_1 + β_2x + ε\n\\]\n\n\\(y\\) : dependent variable (outcome)\n\\(x\\) : independent variable (predictor)\n\\(β_1\\) : intercept (value of y when x = 0)\n\\(β_2\\) : slope (change in y for a one-unit increase in x)\n\\(ε\\) : error term (difference between the predicted and observed values of y)\n\n\nMultiple Linear Regression\n\nMultiple linear regression is an extension of simple linear regression that allows us to model the relationship between one dependent variable (y) and multiple independent variables (x₁, x₂, …, xₙ). It is useful when we want to analyze the impact of several predictors on an outcome variable.\n\nEquation\n\\[\ny = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε\n\\]\n\n\\(y\\) : dependent variable (outcome)\n\\(x₁, x₂, …, xₙ\\) : independent variables (predictors)\n\\(β₀\\) : intercept (value of y when all x’s are 0)\n\\(β₁, β₂, …, βₙ\\) : coefficients (change in y for a one-unit increase in the corresponding x)\n\\(ε\\) : error term (difference between the predicted and observed values of y)\n\n\nAssumptions of Linear Regression\n\nLinearity: The relationship between the dependent variable and the independent variables is linear.\nIndependence: The observations in the dataset are independent of each other.\nHomoscedasticity: The variance of the error term is constant for all values of the independent variables.\nNormality: The error term follows a normal distribution.\nNo multicollinearity: The independent variables are not highly correlated with each other.\n\n\nCoefficient Estimation: Least Squares (LS) Method\n\nMinimize the sum of the squared differences between the observed and predicted values of the dependent variable.\n\n\nFormula:\n\\[\nβ = (X'X)^{-1}X'y\n\\]where X is the matrix of independent variables, y is the dependent variable, and β is the vector of coefficients.\n\nModel Evaluation Metrics\n\nR-squared (Coefficient of Determination): Proportion of the variance in the dependent variable that can be explained by the independent variables. Ranges from 0 to 1.\n\\[\nR^2 = 1- \\frac{SSE}{SST}\n\\]\n\\[\nSSE = \\sum(y_i - \\hat{y_i})^2\n\\]\n\\[\nSST=\\sum(y_i - \\bar{y_i})^2\n\\]\n\nwhere SSE is the sum of squared errors and SST is the sum of squared total\n\nAdjusted R-squared: R-squared adjusted for the number of predictors in the model. Useful for comparing models with different numbers of predictors.\n\\[\nAdj.R^2=1-\\frac{(1-R^2)(N-1)}{N-p-1}\n\\]\n\nwhere \\(R^2\\) is sample R-squared, \\(N\\) is Total Sample Size, and \\(p\\) is the number of independent variables\n\nRoot Mean Squared Error (RMSE): The square root of the average squared differences between the observed and predicted values of the dependent variable. A measure of the model’s prediction accuracy.\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}{(y_i-\\hat{y_i})^2}}{N}}\n\\]\n\nwhere N is the number of data points (observations)\n\n\n\n\n\nPop-up Quizzes\n\nWhat is the main objective of simple linear regression?\n\nTo predict the value of a dependent variable based on the values of multiple independent variables.\nTo predict the value of a dependent variable based on the value of one independent variable.\nTo predict the values of independent variables based on the value of a dependent variable.\nTo find the correlation between two independent variables.\n\nIn a multiple linear regression model, what happens when two or more independent variables are highly correlated with each other?\n\nThe model becomes more accurate.\nThe model becomes more interpretable.\nMulticollinearity occurs, which can affect the stability and interpretation of the coefficients.\nThe model’s R-squared value decreases significantly.\n\nWhich of the following is NOT an assumption of linear regression?\n\nLinearity\nIndependence\nHomoscedasticity\nExponential distribution of the error term\n\nIn a simple linear regression model with the equation y = β₀ + β₁x + ε, what does β₁ represent?\n\nThe intercept of the model, or the value of y when x = 0.\nThe slope of the model, or the change in y for a one-unit increase in x.\nThe error term, or the difference between the predicted and observed values of y.\nThe coefficient of determination, or the proportion of the variance in y explained by x.\n\nWhich of the following metrics can be used to evaluate the performance of a linear regression model?\n\nR-squared\nAdjusted R-squared\nRoot Mean Squared Error (RMSE)\nAll of the above\n\nAnswers: bcdbd\n\n\n\n\nHands-on Practice\nFor this hands-on practice, we will use the mtcars dataset, which is built into R. The dataset contains information about various car models, including miles per gallon (mpg), number of cylinders (cyl), horsepower (hp), and weight (wt). The goal is to predict miles per gallon based on the number of cylinders, horsepower, and weight using linear regression.\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the 'mtcars' dataset\ndata(mtcars)\n\n# View the first few rows of the dataset\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nExploratory Data Analysis\n\n# Summary statistics\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n# Visualize relationships between variables using scatterplots\npairs(mtcars[, c(\"mpg\", \"cyl\", \"hp\", \"wt\")])\n\n\n\n\nSimple Linear Regression in R (Predicting mpg based on weight)\n\n# Fit a simple linear regression model\nsimple_model &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Model summary and interpretation\nsummary(simple_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Model diagnostics (residuals vs. fitted values)\nplot(simple_model, which = 1)\n\n\n\n\nHypothesis Testing and Statistical Significance in Linear Regression\nT-statistics and p-values are essential concepts in statistical hypothesis testing and linear regression analysis.\n\nT-statistics\n\nA t-statistic is a measure of how many standard deviations a regression coefficient is from zero. It is used to test the null hypothesis that there is no relationship between the independent and dependent variables (i.e., the coefficient is zero). A higher t-statistic value indicates a stronger relationship between the variables.\n\nThe t-statistic for a regression coefficient can be calculated as:\n\\[\nt = \\frac{\\beta - H₀}{se(\\beta)}\n\\]\n\nwhere \\(t\\) is the t-statistic, \\(\\beta\\) is the estimated regression coefficient, \\(H₀\\) is the null hypothesis value (usually 0), and \\(se(\\beta)\\) is the standard error of the estimated coefficient.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nt-통계량은 클 수록 좋음 -&gt; 분자는 클 수록, 분모는 작을 수록 좋음\n분자가 크려면: 회귀 계수 (Beta의 추정값)이 커야함\n분모가 작으려면: 회귀 계수의 표준 오차가 작아야함\n회귀 계수의 표준 오차가 작으려면: 표준오차(Beta) = MSE / (X의 표준편차 * 표본수) 이므로 MSE가 작아야 하고 표본수가 커야함.\n종합하면, 회귀 계수가 크고, MSE가 작고, 표본 수가 커질 수록 t-통계량이 커진다\n\n\n\n\n\nP-values\n\nA p-value is the probability of obtaining a test statistic as extreme as the observed value under the null hypothesis. It helps us determine the statistical significance of a regression coefficient. In general, a smaller p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, suggesting that the coefficient is significantly different from zero.\n\nTo calculate the p-value for a t-statistic, we use the cumulative distribution function (CDF) of the t-distribution with n - k degrees of freedom, where n is the number of observations and k is the number of estimated coefficients (including the intercept).\n\\[\nP(T &gt; |t|) = 1 - CDF(t, df = n - k)\n\\]\n\n\nMultiple Linear Regression in R (Adding number of cylinders and horsepower as predictors)\n\n# Fit a multiple linear regression model\nmultiple_model &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\n# Model summary and interpretation\nsummary(multiple_model)\n\n\nCall:\nlm(formula = mpg ~ cyl + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 38.75179    1.78686  21.687  &lt; 2e-16 ***\ncyl         -0.94162    0.55092  -1.709 0.098480 .  \nhp          -0.01804    0.01188  -1.519 0.140015    \nwt          -3.16697    0.74058  -4.276 0.000199 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.512 on 28 degrees of freedom\nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8263 \nF-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n\n# Model diagnostics (residuals vs. fitted values)\nplot(multiple_model, which = 1)\n\n\n\n\nModel Evaluation and Comparison\n\n# Calculate R-squared and adjusted R-squared for both models\nsimple_r_squared &lt;- summary(simple_model)$r.squared\nsimple_adj_r_squared &lt;- summary(simple_model)$adj.r.squared\n\nmultiple_r_squared &lt;- summary(multiple_model)$r.squared\nmultiple_adj_r_squared &lt;- summary(multiple_model)$adj.r.squared\n\n# Compare R-squared and adjusted R-squared values\ncat(\"Simple Model - R-squared:\", simple_r_squared, \"Adjusted R-squared:\", simple_adj_r_squared, \"\\n\")\n\nSimple Model - R-squared: 0.7528328 Adjusted R-squared: 0.7445939 \n\ncat(\"Multiple Model - R-squared:\", multiple_r_squared, \"Adjusted R-squared:\", multiple_adj_r_squared, \"\\n\")\n\nMultiple Model - R-squared: 0.84315 Adjusted R-squared: 0.8263446 \n\n\nModel Predictions\n\n# Make predictions using the multiple linear regression model\nnew_data &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  hp = c(100, 150, 200),\n  wt = c(2.5, 3.0, 3.5)\n)\n\npredicted_mpg &lt;- predict(multiple_model, newdata = new_data)\n\n# View predicted mpg values\npredicted_mpg\n\n       1        2        3 \n25.26408 20.89545 16.52683 \n\n\n\nAddressing Multi-collinearity\n\n# Check for multicollinearity using the Variance Inflation Factor (VIF)\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nvif(multiple_model)\n\n     cyl       hp       wt \n4.757456 3.258481 2.580486 \n\n\nVariance Inflation Factor (VIF) is a measure used to detect the presence and severity of multicollinearity in a multiple linear regression model. Multicollinearity occurs when two or more independent variables in the model are highly correlated, which can lead to instability in the estimated regression coefficients and make it difficult to interpret their individual effects on the dependent variable.\n\nIf VIF values are significantly greater than 1 (&gt; 5 or 10), consider removing or combining correlated predictors\n\nVIF for the j-th independent variable can be calculated as:\n\\[\nVIF(j) = \\frac{1}{1 - R²(j)}\n\\]\nHere, \\(R²(j)\\) is the coefficient of determination (R-squared) of the regression model when the j-th independent variable is regressed on all the other independent variables in the model. In other words, \\(R²(j)\\) measures the proportion of variance in the j-th independent variable that can be explained by the other independent variables.\nIf the VIF value for a particular independent variable is close to 1, it means that there is no significant multicollinearity between that variable and the other independent variables. As the VIF value increases, it suggests a higher degree of multicollinearity.\nThe general interpretation of VIF values is as follows:\n\nVIF = 1: No multicollinearity\nVIF between 1 and 5: Moderate multicollinearity\nVIF greater than 5 or 10: High multicollinearity (threshold values may vary depending on the field of study)\n\nIf high multicollinearity is detected, it is often advisable to address the issue by removing or combining correlated predictors, or by using regularization techniques such as Lasso, Ridge, or Elastic Net regression. This can help improve the stability and interpretability of the regression coefficients.\n\nOptional: Regularization techniques (Lasso, Ridge, and Elastic Net)\n\nLasso, Ridge, and Elastic Net are regularization techniques used in linear regression models to address issues like multicollinearity, overfitting, and feature selection. They work by adding a penalty term to the linear regression’s objective function, which helps to shrink the coefficients towards zero and simplify the model. Here’s a brief explanation of each technique along with the relevant equations:\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator)\nLasso regression adds an L1 penalty term to the linear regression’s objective function. The L1 penalty term is the sum of the absolute values of the coefficients. The objective function for Lasso regression is:\n\\[\nObjective = RSS + λ Σ|β_j|\n\\]\nwhere:\n\n\\(RSS\\) is the residual sum of squares.\n\\(β_j\\) represents the j-th coefficient in the model.\n\\(λ\\) (lambda) is the regularization parameter that controls the strength of the L1 penalty. Higher values of λ result in more shrinkage and simpler models.\n\nLasso regression can drive some coefficients to zero, effectively performing feature selection by excluding irrelevant variables from the model.\n\n\n\nRidge Regression\nRidge regression adds an L2 penalty term to the linear regression’s objective function. The L2 penalty term is the sum of the squares of the coefficients. The objective function for Ridge regression is:\n\\[\nObjective = RSS + λ  Σ(β_j)^2\n\\]\nwhere:\n\n\\(RSS\\) is the residual sum of squares.\n\\(β_j\\) represents the j-th coefficient in the model.\n\\(λ\\) (lambda) is the regularization parameter that controls the strength of the L2 penalty. Higher values of λ result in more shrinkage and simpler models.\n\n\nRidge regression doesn’t drive coefficients to zero but can shrink them close to zero, leading to a more stable and interpretable model, especially when multicollinearity is present.\n\n\n\n\nElastic Net Regression\nElastic Net regression combines both L1 and L2 penalty terms, effectively blending Lasso and Ridge regression (진리의 반반). The objective function for Elastic Net regression is:\n\\[\nObjective = RSS + λ [(1 - α)  Σ(β_j)^2 + α  Σ|β_j|]\n\\]\nwhere:\n\n\\(RSS\\) is the residual sum of squares.\n\\(β_j\\) represents the j-th coefficient in the model.\n\\(λ\\) (lambda) is the regularization parameter that controls the overall strength of the penalty.\n\\(α\\) (alpha) is the mixing parameter that determines the balance between L1 (Lasso) and L2 (Ridge) penalties.\n\nα = 1 results in Lasso regression,\nα = 0 results in Ridge regression,\nand values between 0 and 1 produce a mix of both.\n\n\n\nElastic Net regression can be useful when there are many correlated predictors, as it can perform feature selection like Lasso while maintaining the stability and robustness of Ridge regression.\n\n\n\nLet’s learn how to code lasso, ridge, and elastic net regression.\n\n# Load necessary library\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n# Prepare data for regularization\nx &lt;- model.matrix(mpg ~ cyl + hp + wt, data = mtcars)[, -1]\ny &lt;- mtcars$mpg\n\n# Fit Lasso, Ridge, and Elastic Net models\nlasso_model &lt;- glmnet(x, y, alpha = 1)\nridge_model &lt;- glmnet(x, y, alpha = 0)\nelastic_net_model &lt;- glmnet(x, y, alpha = 0.5)\n\n# Cross-validation to find the optimal lambda value\ncv_lasso &lt;- cv.glmnet(x, y, alpha = 1)\ncv_ridge &lt;- cv.glmnet(x, y, alpha = 0)\ncv_elastic_net &lt;- cv.glmnet(x, y, alpha = 0.5)\n\n# Model summary and interpretation\ncat(\"Lasso - Optimal Lambda:\", cv_lasso$lambda.min, \"\\n\")\n\nLasso - Optimal Lambda: 0.0338637 \n\ncat(\"Ridge - Optimal Lambda:\", cv_ridge$lambda.min, \"\\n\")\n\nRidge - Optimal Lambda: 0.7467388 \n\ncat(\"Elastic Net - Optimal Lambda:\", cv_elastic_net$lambda.min, \"\\n\")\n\nElastic Net - Optimal Lambda: 0.3000745 \n\n# Make predictions using Lasso, Ridge, and Elastic Net models:\n# Create new data for predictions\nnew_data &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  hp = c(100, 150, 200),\n  wt = c(2.5, 3.0, 3.5)\n)\n\n# Prepare new data for predictions\nnew_data_x &lt;- model.matrix(~ cyl + hp + wt, data = new_data)[, -1]\n\n# Make predictions\nlasso_predictions &lt;- predict(cv_lasso, new_data_x, s = \"lambda.min\")\nridge_predictions &lt;- predict(cv_ridge, new_data_x, s = \"lambda.min\")\nelastic_net_predictions &lt;- predict(cv_elastic_net, new_data_x, s = \"lambda.min\")\n\n# View predictions\ncat(\"Lasso Predictions:\", lasso_predictions, \"\\n\")\n\nLasso Predictions: 25.23425 20.89185 16.54944 \n\ncat(\"Ridge Predictions:\", ridge_predictions, \"\\n\")\n\nRidge Predictions: 25.06857 20.80336 16.53814 \n\ncat(\"Elastic Net Predictions:\", elastic_net_predictions, \"\\n\")\n\nElastic Net Predictions: 25.08941 20.857 16.62459 \n\n\n\n\nConclusion\nIn this hands-on practice, we used the ‘mtcars’ dataset to predict miles per gallon (mpg) based on the number of cylinders, horsepower, and weight of a car. We started with a simple linear regression model using only weight as a predictor and then moved to a multiple linear regression model with three predictors. We also explored regularization techniques and made predictions using our multiple linear regression model.\nRemember to always perform exploratory data analysis and check the assumptions of linear regression before fitting your models. Also, consider applying regularization techniques when multicollinearity is present or when the model is overfitting the data.\n\n\n\nFor your further study\nIn linear regression, the method of least squares is commonly used to estimate the coefficients of the regression model. However, there is another estimation method called Maximum Likelihood Estimation (MLE) that can be used as an alternative to least squares. In this optional material, we will introduce the concept of MLE, explain how it works, and discuss its advantages and disadvantages compared to least squares.\n\nMaximum Likelihood Estimation\n\nMaximum Likelihood Estimation is a statistical method used to estimate the parameters of a model by finding the values that maximize the likelihood function. The likelihood function measures how likely the observed data is, given the parameters of the model. In the context of linear regression, MLE seeks to find the values of the coefficients that maximize the likelihood of observing the data, assuming that the error terms follow a normal distribution\n\n\nMLE in Linear Regression\nLet’s consider the linear regression model:\n\\[\ny_i = β_0 + β_1 x_i + ε_i\n\\]\nwhere \\(y_i\\) is the dependent variable, \\(x_i\\) is the independent variable, \\(β_0\\) and \\(β_1\\) are the regression coefficients, and \\(ε_i\\) is the error term.\nAssuming that the error terms \\(ε_i\\) are normally distributed with mean 0 and constant variance \\(σ^2\\), the probability density function (PDF) of the normal distribution for a single observation is:\n\\[\nf(y_i | x_i, β_0, β_1, σ^2) = \\frac{1}{σ  \\sqrt{2π}}  exp(\\frac{-(y_i - (β_0 + β_1 x_i))^2}  {2  σ^2})\n\\]\nThe likelihood function is the product of the PDFs for all observations:\n\\[\nL(β_0, β_1, σ^2) = Π f(y_i | x_i, β_0, β_1, σ^2)\n\\]\nTo make the optimization problem easier, we take the natural logarithm of the likelihood function, which is called the log-likelihood function:\n\\[\nlogL(β_0, β_1, σ^2) = Σ log(f(y_i | x_i, β_0, β_1, σ^2))\n\\]\nThe goal of MLE is to find the values of \\(β_0\\), \\(β_1\\), and \\(σ^2\\) that maximize the log-likelihood function.\n\nAdvantages and Disadvantages of MLE\nAdvantages:\n\nMLE provides a general framework that can be applied to a wide range of statistical models, not just linear regression.\nMLE is asymptotically unbiased and efficient, meaning that as the sample size increases, the estimates converge to the true parameter values, and the estimates have the smallest possible variance.\nMLE allows for the estimation of additional parameters, such as the error variance \\(σ^2\\) in linear regression.\n\nDisadvantages:\n\nMLE can be computationally intensive, especially for complex models with many parameters.\nMLE relies on the assumption that the error terms follow a specific distribution (e.g., normal distribution in linear regression). If this assumption is not met, the estimates may be biased or inefficient.\n\nLet’s demonstrate the similarity of the estimates by fitting a linear regression model using both LS and MLE, and then visualize the fitted lines. To do this, we’ll predict miles per gallon (mpg) based on the weight (wt) of a car using the ‘mtcars’ dataset.\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n# Load the 'mtcars' dataset\ndata(mtcars)\n\nFit the linear regression model using LS (lm function):\n\n# Fit the model using LS\nls_model &lt;- lm(mpg ~ wt, data = mtcars)\n\nsummary(ls_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFit the linear regression model using MLE (fit a normal linear model with mle2 function):\n\n# Load necessary libraries\nlibrary(bbmle)\n\nLoading required package: stats4\n\n\n\nAttaching package: 'bbmle'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Define the log-likelihood function for MLE\nloglik_fn &lt;- function(beta0, beta1, sigma) {\n  y &lt;- mtcars$mpg\n  x &lt;- mtcars$wt\n  n &lt;- length(y)\n  \n  mu &lt;- beta0 + beta1 * x\n  epsilon &lt;- y - mu\n  \n  loglik &lt;- -n/2 * log(2 * pi) - n/2 * log(sigma^2) - 1/(2 * sigma^2) * sum(epsilon^2)\n  return(-loglik) # The optimization function will minimize the function, so we need to negate the log-likelihood\n}\n\n\n# Fit the model using MLE\nmle_model &lt;- mle2(loglik_fn, start = list(beta0 = coef(ls_model)[1], \n                                          beta1 = coef(ls_model)[2], \n                                          sigma = 1))\nsummary(mle_model)\n\nMaximum likelihood estimation\n\nCall:\nmle2(minuslogl = loglik_fn, start = list(beta0 = coef(ls_model)[1], \n    beta1 = coef(ls_model)[2], sigma = 1))\n\nCoefficients:\n      Estimate Std. Error z value     Pr(z)    \nbeta0 37.28513    1.81800 20.5088 &lt; 2.2e-16 ***\nbeta1 -5.34447    0.54135 -9.8725 &lt; 2.2e-16 ***\nsigma  2.94916    0.36864  8.0000 1.244e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n-2 log L: 160.0294 \n\n\nVisualize the fitted lines:\n\n# Extract coefficients from the LS and MLE models\nls_coefs &lt;- coef(ls_model)\nmle_coefs &lt;- coef(mle_model)\n\n# Create a scatter plot of mpg vs. wt\nmtcars_plot &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  xlim(c(1.5, 5.5)) +\n  ylim(c(5, 40))\n\n# Add the LS and MLE fitted lines to the plot\nmtcars_plot +\n  geom_abline(aes(intercept = ls_coefs[1], slope = ls_coefs[2], color = \"LS\", linetype = \"LS\"), size = 1, alpha=0.5) +\n  geom_abline(aes(intercept = mle_coefs[1], slope = mle_coefs[2], color = \"MLE\", linetype = \"MLE\"), size = 1) +\n  scale_color_manual(\"Model\", values = c(\"LS\" = \"blue\", \"MLE\" = \"red\")) +\n  scale_linetype_manual(\"Model\", values = c(\"LS\" = \"solid\", \"MLE\" = \"dashed\")) +\n  labs(title = \"Linear Regression: LS vs. MLE\", x = \"Weight\", y = \"Miles per Gallon\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nIn the resulting plot, you’ll notice that the LS and MLE fitted lines are almost indistinguishable, which confirms that the estimates are the same when the error terms follow a normal distribution."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/05_week.html",
    "href": "teaching/ml101/weekly/posts/05_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/05_week.html#footnotes",
    "href": "teaching/ml101/weekly/posts/05_week.html#footnotes",
    "title": "Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA benign tumor is a non-cancerous growth that does not spread to other parts of the body. It is made up of cells that are similar in appearance to normal cells, and it usually grows slowly. While a benign tumor is not usually life-threatening, it can still cause health problems depending on its location and size. In some cases, a benign tumor may need to be removed if it is causing symptoms or is at risk of becoming cancerous. Unlike a malignant tumor, which is cancerous and can spread to other parts of the body, a benign tumor does not invade nearby tissues or organs or spread to other parts of the body.↩︎"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/03_week.html",
    "href": "teaching/ml101/weekly/posts/03_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nLearning Types\n\n\n\nSupervised learning & Decision Tree\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #2\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nClass\n\n\nWhy do we need various ML algorithms?\n\n“Each machine learning algorithm serves a specific purpose, much like how each individual serves a unique role in society. There is no one superior ML algorithm, as each is used for a different purpose.”\n\n\n\n\nDifferent types of data and problems: Different ML algorithms are suited to different types of data and problems. For example, linear regression is well-suited for continuous numerical data, while decision trees are well-suited for categorical data. Similarly, some algorithms are better suited for classification problems (predicting a categorical outcome), while others are better suited for regression problems (predicting a continuous outcome).\nPerformance trade-offs: Different ML algorithms can have different trade-offs in terms of performance and interpretability. For example, some algorithms, like k-nearest neighbors, are simple to understand and interpret, but may not perform as well as more complex algorithms like neural networks. On the other hand, some algorithms, like deep learning, can achieve very high performance, but may be more difficult to interpret.\nModel assumptions: Different ML algorithms make different assumptions about the relationship between the predictors and the outcome. For example, linear regression assumes a linear relationship, while decision trees do not make this assumption. Selecting the appropriate algorithm depends on the nature of the problem and the data, as well as the goals of the analysis.\nComputational resources: Some ML algorithms are more computationally intensive than others, and may require more resources (such as memory and processing power) to fit and use. In some cases, a more complex algorithm may be necessary to achieve the desired level of performance, but in other cases, a simpler algorithm may be sufficient.\nModel interpretability: Some ML algorithms are more interpretable than others, which can be important in certain applications. For example, in medical applications, it may be important to understand why a model is making a certain prediction, while in other applications, it may be more important to simply achieve high accuracy.\n\nThe choice of ML algorithm depends on the data, the problem, and the goals of the analysis, and it is important to consider these factors when selecting an algorithm.\n\n\n\nDecision Tree\n\nGenerates a set of rules that allow for accurate predictions using one explanatory variable at a time, resulting in a structure similar to an inverted tree: decision tree\n한번에 하나씩의 설명 변수를 사용하여 정확한 예측이 가능한 규칙들의 집합을 생성.\n\n\n\n\nTerminologies\n\nNode: A point in the decision tree where a decision or split is made.\nParent node: A node that has other nodes connected to it, called child nodes.\nChild node: A node that is connected to a parent node and splits off in a different direction in the tree.\nSplit criterion: The criterion used to determine the split at each node in the decision tree. This can be based on various measures such as information gain, Gini impurity, or chi-squared statistic.\nRoot node: The top node in the decision tree that represents the entire dataset.\nLeaf node: A node in the decision tree that represents a final prediction or classification, as it does not have any child nodes connected to it.\n\n\nThen, “Why is it called a ‘decision’ tree?”\n\nThis is because it presents results in the form of easily understandable rules,\nreduces the need for pre-processing data such as normalization or imputation,\nand considers both numeric and categorical variables as its independent variables (features).\n\n\n\nEssential ideas of decision tree\n\nRecursive Partitioning: The process of dividing the input variable space into two parts, with the goal of increasing the purity of each area after separation rather than before separation\nPruning the Tree: The process of consolidating areas that have been separated into overly detailed regions in order to prevent overfitting.”\n\n\n\n\nImpurity index 1: Gini index\n\n\n\\[\nI(A)=1-\\sum_{k = 1}^{c} p_k^2\n\\]\n\n\\(I(A)\\) is Gini index of A area in which there is c number of classes\n\\(P_k\\) is number of observations in k class\n\n\n\n\n\\[\nI(A)=1-\\sum_{k = 1}^{c} p_k^2\n=1-(\\frac{6}{16})^2-(\\frac{10}{16})^2 = 0.4688\n\\]\n\n\\(I(A)\\) is 0 when there are only one category in area c\n\\(I(A)\\) is 0.5 when there are half of one and half of another category in area c\n\n\n\n\nGini index when there are two or more areas\n\\[\nI(A)=\\sum_{i = 1}^{d} (R_i(1-\\sum_{k = 1}^{c} p_{ik}^2))\n\\]\n\nAfter splitting the area A, \\(I(A)\\) is Gini Index for two or more areas\n\\(R_i\\) is a ratio of i area in A\n\n\n\n\n\\[\nI(A)=\n\\frac{8}{16}\n\\times\n(1-(\\frac{7}{8})^2 -(\\frac{1}{8})^2)+\n\\frac{8}{16}\n\\times\n(1-(\\frac{3}{8})^2 -(\\frac{5}{8})^2)=0.3438\n\\]\n\nAfter splitting, the information gain is \\(0.4668 - 0.3438=0.1250\\)\n\n\n\n\nImpurity index 2: Deviance\n\\[\nD_i = -2 \\sum_{k}n_{ik} log(P_{ik})\n\\]\n\n\\(i\\) is node index\n\\(k\\) is class index\n\\(P_{ik}\\) is a probability of class k in node i\n\n\n\\[\nD_i = -2 \\times (10 \\times log (\\frac{10}{16})  + 6 \\times\n(\\frac{6}{16}))=21.17\n\\]\n\n\n\\[\nD_1 =\n-2 \\times (7 \\times log (\\frac{7}{8})  +\n1 \\times log(\\frac{1}{8}))=6.03\n\\]\n\\[\nD_2 =\n-2 \\times (3 \\times log (\\frac{3}{8})  +\n5 \\times log(\\frac{5}{8}))=10.59\n\\]\n\\[\nD_1 + D_2 = 16.62\n\\]\n\n\nAfter splitting, the information gain is \\(21.17 - 16.62=4.55\\)\n\n\n\n\nOver-fitting issue\n\nOver-fitting is a common issue in machine learning algorithms, including decision trees. It occurs when a model learns the training data too well and captures not only the underlying patterns but also the noise present in the data. As a result, the model becomes too complex and performs poorly on new, unseen data, even though it may have high accuracy on the training data.\nIn the context of decision trees, over-fitting can occur when the tree becomes too deep, with many levels and branches. A deep tree may split the training data into very specific and detailed regions, resulting in a model that is too tailored to the training data and not generalizable to new data.\n\n \n\nTo address over-fitting in decision trees, several techniques can be employed:\n\n\nPruning: This involves trimming the tree by removing branches that do not significantly improve the model’s performance. Pruning can be done in a top-down manner (pre-pruning), where the tree is stopped from growing further once a certain depth or node count is reached, or in a bottom-up manner (post-pruning), where the tree is fully grown and then branches are removed based on certain criteria.\nLimiting tree depth: By setting a maximum depth for the tree, the algorithm is forced to make fewer splits and create a simpler model, reducing the likelihood of over-fitting.\nUsing minimum node size: Setting a minimum number of samples required to create a split or leaf node can prevent the tree from making splits that are too specific to the training data.\nCross-validation: This technique involves splitting the data into multiple training and validation sets and averaging the model’s performance across these sets. Cross-validation can help in choosing the optimal tree depth and other hyper-parameters that lead to the best generalization performance.\n\n\n\n\nPruning\n \n\nA technique used to reduce over-fitting in decision tree models by removing branches that do not significantly improve model performance.\n\n\nGoal: To simplify the tree, improving generalizability and reducing complexity, while maintaining prediction accuracy.\nTwo main types of pruning:\n\nPre-pruning (Top-down): Stops tree growth once certain criteria are met, such as maximum depth or minimum node size.\nPost-pruning (Bottom-up): Allows the tree to fully grow, then iteratively removes branches based on specific criteria, such as error rate or information gain.\n\nCriteria for pruning: Can be based on various measures, such as error rate, information gain, or Gini impurity, and the cost of complexity (CC)\nCross-validation: Often used in conjunction with pruning to determine the optimal level of pruning, by comparing model performance on multiple training and validation sets.\nBenefits: Pruning can lead to better generalization performance, reduced over-fitting, and more interpretable models.\n\n\n\n\nCost of complexity\n\\[\nCC(T)=Err(T)+\\alpha L(T)\n\\]\n\n\\(CC(T)\\) is cost of complexity\n\\(Err(T)\\) is error rate of Tree with test dataset\n\\(L(T)\\) is the number of last nodes of Tree\n\\(\\alpha\\) is weight of combining Err(T) and L(T): depends on researcher’s decision.\n\n\n\nCost of complexity Example #1\n\n\n\nTree 1: Err(T) = 10%\nTree 2: Err(T) = 15%\nChoose Tree A is better as the same number of last nodes\n\n\n\n\nCost of complexity Example #2\n\n\n\nTree A: Err(T) = 15%\nTree B: Err(T) = 15%\nChoose Tree A is better as the same Err(T) but low number of last nodes\n\n\n\n\n\nPractice with data\n\n\nhead(iris, 10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\nUsing ‘rpart’ to train the model with the ‘iris’ dataset\n\nThe target (response) variable should be ‘factor’ in R\nOr, use rpart(…, method=‘class’) option\nOtherwise, rpart do regression instead of classification\nThen, let’s train the model with the data ‘iris’\n\n\nlibrary(rpart)\nr = rpart(Species ~ ., data = iris)\nprint(r)\n\nn= 150 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  \n  2) Petal.Length&lt; 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  \n    6) Petal.Width&lt; 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *\n    7) Petal.Width&gt;=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *\n\n\n\nTree visualization\n\n\npar(mfrow = c(1,1), xpd = NA)\nplot(r)\ntext(r, use.n = T)\n\n\n\n\n\nThe question of the root node (node 0): [Petal.Length&lt;2.45?]\n\nYes of 50 to the left,\nNo of 100 to the right among 150 sample\n50 on the left all belong to ‘setosa’ so STOP 🡪 Make a leaf and record setosa(50/0/0)\n\nThe child node on the right side of the root: [Petal.Width&lt;1.75?]\n\nYes of 54 to the left,\nand No of 46 to the right among the 100\n\nOn the left 54, there are 49 versicolor and 5 virginica 🡪 To avoid overfitting the algorithm STOP and record versicolor(0/49/5).\nOn the right 46, there are 1 versicolor and 45 virginica 🡪 To avoid overfitting the algorithm STOP and record virginica(0/1/45).\n\n\n\nPrediction\n\nUse the function ‘prediction’\ntype=‘class’ option prints the class (the default is type=‘prob’ which prints probability of each class)\n\n\np = predict(r, iris, type = 'class')\nhead(p, 10)\n\n     1      2      3      4      5      6      7      8      9     10 \nsetosa setosa setosa setosa setosa setosa setosa setosa setosa setosa \nLevels: setosa versicolor virginica\n\n\n\nConfusion matrix\n\nShow the correct and the wrong classifications in detail\nSee below for example, among 50 versicolor, 49 are correct but one is wrong to virginica\n\n\ntable(p, iris$Species)\n\n\np            setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         49         5\n  virginica       0          1        45\n\n\n\n\nNew data is from train data but a little bit of change is given.\n\nnewd = data.frame(Sepal.Length = c(5.11, 7.01, 6.32),\n                  Sepal.Width = c(3.51, 3.2, 3.31),\n                  Petal.Length = c(1.4, 4.71, 6.02),\n                  Petal.Width = c(0.19, 1.4, 2.49))\nnewd\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1         5.11        3.51         1.40        0.19\n2         7.01        3.20         4.71        1.40\n3         6.32        3.31         6.02        2.49\n\npredict(r, newdata = newd)\n\n  setosa versicolor  virginica\n1      1 0.00000000 0.00000000\n2      0 0.90740741 0.09259259\n3      0 0.02173913 0.97826087\n\n\n\nLet’s read the result with the ‘function ’summary’\n\nsummary(r)\n\nCall:\nrpart(formula = Species ~ ., data = iris)\n  n= 150 \n\n    CP nsplit rel error xerror       xstd\n1 0.50      0      1.00   1.22 0.04772141\n2 0.44      1      0.50   0.84 0.06079474\n3 0.01      2      0.06   0.10 0.03055050\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          34           31           21           14 \n\nNode number 1: 150 observations,    complexity param=0.5\n  predicted class=setosa      expected loss=0.6666667  P(node) =1\n    class counts:    50    50    50\n   probabilities: 0.333 0.333 0.333 \n  left son=2 (50 obs) right son=3 (100 obs)\n  Primary splits:\n      Petal.Length &lt; 2.45 to the left,  improve=50.00000, (0 missing)\n      Petal.Width  &lt; 0.8  to the left,  improve=50.00000, (0 missing)\n      Sepal.Length &lt; 5.45 to the left,  improve=34.16405, (0 missing)\n      Sepal.Width  &lt; 3.35 to the right, improve=19.03851, (0 missing)\n  Surrogate splits:\n      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)\n      Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)\n      Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)\n\nNode number 2: 50 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3333333\n    class counts:    50     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 100 observations,    complexity param=0.44\n  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667\n    class counts:     0    50    50\n   probabilities: 0.000 0.500 0.500 \n  left son=6 (54 obs) right son=7 (46 obs)\n  Primary splits:\n      Petal.Width  &lt; 1.75 to the left,  improve=38.969400, (0 missing)\n      Petal.Length &lt; 4.75 to the left,  improve=37.353540, (0 missing)\n      Sepal.Length &lt; 6.15 to the left,  improve=10.686870, (0 missing)\n      Sepal.Width  &lt; 2.45 to the left,  improve= 3.555556, (0 missing)\n  Surrogate splits:\n      Petal.Length &lt; 4.75 to the left,  agree=0.91, adj=0.804, (0 split)\n      Sepal.Length &lt; 6.15 to the left,  agree=0.73, adj=0.413, (0 split)\n      Sepal.Width  &lt; 2.95 to the left,  agree=0.67, adj=0.283, (0 split)\n\nNode number 6: 54 observations\n  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36\n    class counts:     0    49     5\n   probabilities: 0.000 0.907 0.093 \n\nNode number 7: 46 observations\n  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667\n    class counts:     0     1    45\n   probabilities: 0.000 0.022 0.978 \n\n\n\nVariable importance shows the order of the explainable (independent) variables which contributes to predicting Y\nWhen the model is trained, it chooses the features that are important at the same time\nThat’s why the first visualization tree used only two variables: Petal.Width and Length\n\n\nThe effective visualization helps to read the results of decision tree\n\nlibrary(rpart.plot)\nrpart.plot(r)\n\n\n\n\nYou can change a style of the graph\n\nrpart.plot(r, type = 4)\n\n\n\n\n\n\n\nPros & Cons of DT\n\nCon: the performance is not that good\nPros\n\nEasy interpretability\n(예를 들어, “꽃받침 길이가 2.54보다 크고, 꽃받침 너비가 1.75보다 작아 versicolor로 분류”했다는 해석을 내놓을 수 있음)\nFast prediction (몇 번의 비교 연산으로 분류)\nEnsemble techniques make DT the great again (e.g. Random Forest)\nDT accepts categorical variables well (e.g. gender, pclass)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/01_week.html",
    "href": "teaching/ml101/weekly/posts/01_week.html",
    "title": "Course Intro",
    "section": "",
    "text": "Weekly design\n\n\nWeek 1 Class\n\nSlide\n\n\n\nInstall gadgets\n\nInstall R, R Studio, & Rtools\n\nR\n\nFor window https://cran.r-project.org/bin/windows/base/\nFor mac https://cran.r-project.org/bin/macosx/\n\nR Studio\n\nhttps://posit.co/download/rstudio-desktop/\n\nRtools (only for window user)\n\nhttps://cran.r-project.org/bin/windows/Rtools/\nTo install R packages containing C/C++ language for window users (no need for Linux and Mac users)\n\n\nThings you need to know\n\nDon’t use OneDrive.\n\nUse Github instead\nMany people get an error when installing becausedf OneDrive\n\nSet Windows user name to English\n\nIf Korean characters are mixed in the installation path, there is a high probability of error occurrence\n\n\nInstallation Order\n\nStep 1 ‑ Download the file\n\nDownload R, Rtools, Rstudio installation files\n\nStep 2 - Install R\n\nUnified installation path: All will be installed in the C:/R folder\nRun in administrator mode when running the R installation file\nAfter installing R, grant write permission to the R folder, Right-click and turn off read only\n\nStep 3 ‑ Install Rtools\n\nAdministrator mode execution installation and folder setting as C:\\R\\rtools40\nCreate environment variable RTOOLS40_HOME after installation: Value - C:\\R\\rtools40\\\nAdd %RTOOLS40_HOME%\\usr\\bin\\ to the Path variable.\n\nStep 4 - Install Rstudio\n\nRight-click and run as administrator - installation path C:\\R\\Rstudio\nCheck rtools connection with Sys.which(“make”) command after installation"
  },
  {
    "objectID": "teaching/ml101/notice/index.html",
    "href": "teaching/ml101/notice/index.html",
    "title": "Notice",
    "section": "",
    "text": "Pre-class\n\nPrior to attending the offline (or online streaming Zoom class), students are expected to view the recorded lecture delivered by the lecturer.\n\nThey are encouraged to take an active role in their learning by studying the material independently.\nThe video covers the fundamental concepts of the Machine Learning algorithm, including its underlying theory when necessary.\nTo assess their level of understanding, students are required to submit discussion posts. This serves as a means for evaluating their comprehension of the material.\n\n\nIn-class\n\nDuring the class, the lecturer will summarize the pre-class lecture and provide additional clarification on the concepts covered.\nTo reinforce their understanding, students will engage in hands-on practice with more advanced code. This will provide them with the opportunity to apply their knowledge and develop their coding skills.\n\nThe official in-class time is AM 10:00 ~ 12:00"
  },
  {
    "objectID": "teaching/ml101/notice/index.html#course-design",
    "href": "teaching/ml101/notice/index.html#course-design",
    "title": "Notice",
    "section": "",
    "text": "Pre-class\n\nPrior to attending the offline (or online streaming Zoom class), students are expected to view the recorded lecture delivered by the lecturer.\n\nThey are encouraged to take an active role in their learning by studying the material independently.\nThe video covers the fundamental concepts of the Machine Learning algorithm, including its underlying theory when necessary.\nTo assess their level of understanding, students are required to submit discussion posts. This serves as a means for evaluating their comprehension of the material.\n\n\nIn-class\n\nDuring the class, the lecturer will summarize the pre-class lecture and provide additional clarification on the concepts covered.\nTo reinforce their understanding, students will engage in hands-on practice with more advanced code. This will provide them with the opportunity to apply their knowledge and develop their coding skills.\n\nThe official in-class time is AM 10:00 ~ 12:00"
  },
  {
    "objectID": "teaching/ml101/notice/index.html#textbooks",
    "href": "teaching/ml101/notice/index.html#textbooks",
    "title": "Notice",
    "section": "Textbooks",
    "text": "Textbooks"
  },
  {
    "objectID": "teaching/ml101/notice/index.html#grading",
    "href": "teaching/ml101/notice/index.html#grading",
    "title": "Notice",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nItems\nRatio (%)\n\n\n\n\nAttendance\n0\n\n\nDiscussion Submission\n20\n\n\nQZ #1 & QZ #2\n50\n\n\nIC-PBL Final Score\n30\n\n\nTotal\n100\n\n\n\n\n\nWhen attendance falls below 1/3 in a class, it may result in a grade of F.\n\nThis policy is in place to encourage regular attendance and ensure that all students have the opportunity to fully participate in the learning experience. It is the responsibility of the students to attend classes regularly and stay engaged in their education.\n\nThe Final Score for the ICPBL is calculated by multiplying the ICPBL Team Score with the Individual Peer Review Score.\nIndividual Peer Review Scores are obtained through an anonymous survey conducted within the group.\nIt is worth noting that the team assignment is only available for domestic students. On the other hand, exchange students are required to complete an individual project."
  },
  {
    "objectID": "teaching/ml101/notice/index.html#communication",
    "href": "teaching/ml101/notice/index.html#communication",
    "title": "Notice",
    "section": "Communication",
    "text": "Communication\n\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nTHU_ML101_2023 (목요일반)\n\nhttps://open.kakao.com/o/gQnhw75e\n\nTUE_ML101_2023 (화요일반)\n\nhttps://open.kakao.com/o/gwiEw75e\n\n\n\n\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s90SQocc\npw: hanyang"
  },
  {
    "objectID": "teaching/ml101/icpbl/team_assign.html",
    "href": "teaching/ml101/icpbl/team_assign.html",
    "title": "IC-PBL Team assignment",
    "section": "",
    "text": "Team assignemtn for Thursday class\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(101101)\n# input students\nstudents&lt;-c(\n\"이유정\",\n\"박승연\",\n\"이윤진\",\n\"정재윤\",\n\"김상경\",\n\"정혜림\",\n\"박지원\",\n\"정지윤\",\n\"김재엽\",\n\"김찬우\",\n\"박종현\",\n\"윤지성\")\n\nstudents\n\n [1] \"이유정\" \"박승연\" \"이윤진\" \"정재윤\" \"김상경\" \"정혜림\" \"박지원\" \"정지윤\"\n [9] \"김재엽\" \"김찬우\" \"박종현\" \"윤지성\"\n\n# No of people per a team\nteam_size &lt;- 4\n\n# No of team we need\nno_team &lt;- length(students) %/% team_size\nno_team\n\n[1] 3\n\n# No of remainders\nremainder &lt;- length(students) %% team_size\nremainder\n\n[1] 0\n\n# Create a tibble (student_no, team)\n\nif (remainder == 0){\n  \n  tibble(\n    seat = 1:length(students),\n    team = rep(1:no_team, team_size)\n  ) -&gt; seat_table\n  \n} else {\n  \n  tibble(\n    seat = 1:length(students),\n    team = c(rep(1:no_team, team_size),\n             1:remainder)\n  ) -&gt; seat_table\n  \n}\n\nseat_table\n\n# A tibble: 12 × 2\n    seat  team\n   &lt;int&gt; &lt;int&gt;\n 1     1     1\n 2     2     2\n 3     3     3\n 4     4     1\n 5     5     2\n 6     6     3\n 7     7     1\n 8     8     2\n 9     9     3\n10    10     1\n11    11     2\n12    12     3\n\n# Give random number for the students\n\ntibble(\n  seat     = sample(1:length(students), \n                    length(students), \n                    replace = F),\n  students = students\n) -&gt; given_seat\ngiven_seat\n\n# A tibble: 12 × 2\n    seat students\n   &lt;int&gt; &lt;chr&gt;   \n 1     4 이유정  \n 2     1 박승연  \n 3     5 이윤진  \n 4     6 정재윤  \n 5     8 김상경  \n 6     3 정혜림  \n 7     7 박지원  \n 8    11 정지윤  \n 9    12 김재엽  \n10     9 김찬우  \n11     2 박종현  \n12    10 윤지성  \n\n# Join the team - seat - students\n\nseat_table %&gt;% \n  left_join(given_seat) %&gt;% \n  arrange(team, students) %&gt;% \n  select(-seat) -&gt; team_table\n\nJoining with `by = join_by(seat)`\n\nknitr::kable(team_table)\n\n\n\n\nteam\nstudents\n\n\n\n\n1\n박승연\n\n\n1\n박지원\n\n\n1\n윤지성\n\n\n1\n이유정\n\n\n2\n김상경\n\n\n2\n박종현\n\n\n2\n이윤진\n\n\n2\n정지윤\n\n\n3\n김재엽\n\n\n3\n김찬우\n\n\n3\n정재윤\n\n\n3\n정혜림\n\n\n\n\n\n\n\nTeam assignemtn for Tuesday class\n\nlibrary(tidyverse)\nset.seed(101101)\n\n# input students\n\nstudents&lt;-c(\n\"김정환\",\n\"김숭기\",\n\"김원\",\n\"이정헌\",\n\"노솔\",\n\"최지희\",\n\"조민석\",\n\"김민지\",\n\"김가영\",\n\"박은서\",\n\"임예빈\",\n\"문하윤\")\n\nstudents\n\n [1] \"김정환\" \"김숭기\" \"김원\"   \"이정헌\" \"노솔\"   \"최지희\" \"조민석\" \"김민지\"\n [9] \"김가영\" \"박은서\" \"임예빈\" \"문하윤\"\n\n# No of people per a team\nteam_size &lt;- 4\n\n# No of team we need\nno_team &lt;- length(students) %/% team_size\nno_team\n\n[1] 3\n\n# No of remainders\nremainder &lt;- length(students) %% team_size\nremainder\n\n[1] 0\n\n# Create a tibble (student_no, team)\n\nif (remainder == 0){\n  \n  tibble(\n    seat = 1:length(students),\n    team = rep(1:no_team, team_size)\n  ) -&gt; seat_table\n  \n} else {\n  \n  tibble(\n    seat = 1:length(students),\n    team = c(rep(1:no_team, team_size),\n             1:remainder)\n  ) -&gt; seat_table\n  \n}\n\nseat_table\n\n# A tibble: 12 × 2\n    seat  team\n   &lt;int&gt; &lt;int&gt;\n 1     1     1\n 2     2     2\n 3     3     3\n 4     4     1\n 5     5     2\n 6     6     3\n 7     7     1\n 8     8     2\n 9     9     3\n10    10     1\n11    11     2\n12    12     3\n\n# Give random number for the students\n\ntibble(\n  seat     = sample(1:length(students), \n                    length(students), \n                    replace = F),\n  students = students\n) -&gt; given_seat\ngiven_seat\n\n# A tibble: 12 × 2\n    seat students\n   &lt;int&gt; &lt;chr&gt;   \n 1     4 김정환  \n 2     1 김숭기  \n 3     5 김원    \n 4     6 이정헌  \n 5     8 노솔    \n 6     3 최지희  \n 7     7 조민석  \n 8    11 김민지  \n 9    12 김가영  \n10     9 박은서  \n11     2 임예빈  \n12    10 문하윤  \n\n# Join the team - seat - students\n\nseat_table %&gt;% \n  left_join(given_seat) %&gt;% \n  arrange(team, students) %&gt;% \n  select(-seat) -&gt; team_table\n\nJoining with `by = join_by(seat)`\n\nknitr::kable(team_table)\n\n\n\n\nteam\nstudents\n\n\n\n\n1\n김숭기\n\n\n1\n김정환\n\n\n1\n문하윤\n\n\n1\n조민석\n\n\n2\n김민지\n\n\n2\n김원\n\n\n2\n노솔\n\n\n2\n임예빈\n\n\n3\n김가영\n\n\n3\n박은서\n\n\n3\n이정헌\n\n\n3\n최지희"
  },
  {
    "objectID": "teaching/ml101/icpbl/exchange.html",
    "href": "teaching/ml101/icpbl/exchange.html",
    "title": "Final projects",
    "section": "",
    "text": "There are 10 dataset you can choose\n\nSee introduction on the data\n\nAmong the dataset in the link, choose two (e.g. one for classification and the other for regression)\nDo the process below\n\nEDA\n\nData-preprocessing (wrangling)\nDeal with Missing values\nData visualization\nFactorization\nIf needed, standardization of the variables\n\nModeling\nPredicting\n\nData list\n\nNetflix show\nStudents performance\nMobile Price Classification\nDogs & Cats Images\nTrip Advisor Hotel Reviews\nMelbourne Housing Market\nChurn Modelling\nAmazon Best Seller Books\nMedial Cost Personal DB\nKepler Exoplanet Search Results\n\nFinal output\n\nSubmit a summary of your work, no more than 10 pages\nFile type\n\nIt can be in any form, such as a PDF, a link after uploading it to your blog, or a link after writing it on a Notion page, and so on."
  },
  {
    "objectID": "teaching/ml101/about/index.html",
    "href": "teaching/ml101/about/index.html",
    "title": "About ML101",
    "section": "",
    "text": "Goal\n\n\n\nThis course covers the fundamentals of the field, including supervised and unsupervised learning algorithms, regression, classification, and clustering. The course may also cover topics such as model evaluation, feature selection, and regularization.\nIn a supervised learning setting, students learn about linear regression and logistic regression, as well as more complex algorithms such as Naive Bayes, decision trees, random forests, and kNN. They learn how to train models on a labeled dataset and make predictions on new data.\nIn an unsupervised learning setting, students learn about clustering algorithms such as k-means and Apriori. They learn how to extract meaningful structure from unlabeled data.\nThe course may also cover advanced topics such as natural language processing. Students learn how to implement and use these algorithms in R.\nThroughout the course, students work on practical projects and assignments to apply the concepts they have learned. By the end of the course, students should have a solid understanding of the basics of machine learning and be able to apply these concepts to real-world problems."
  },
  {
    "objectID": "teaching/media_ds/notice/index.html",
    "href": "teaching/media_ds/notice/index.html",
    "title": "Notice",
    "section": "",
    "text": "Pre-class\n\n학생들은 오프라인(또는 온라인 스트리밍 Zoom 수업)에 참석하기 전에 강사가 제공하는 녹화된 강의를 시청합니다.\n\n학생들은 자료를 독립적으로 공부함으로써 학습에 적극적인 역할을 하도록 권장됩니다.\n사전 학습 내용은 R과 머신러닝의 기본 이론과 개념을 다룹니다.\n\n\nIn-class\n\n수업 중에 강사는 수업 전 강의를 요약하고 다루는 개념에 대한 추가 설명을 제공합니다.\n학생들의 이해를 강화하기 위해 학생들은 실습에 참여합니다. 이를 통해 배운 지식을 적용하고 코딩 기술을 개발할 수 있는 기회를 얻을 수 있습니다.\n\nThe official in-class time is PM 14:00 ~ 16:00"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#course-design",
    "href": "teaching/media_ds/notice/index.html#course-design",
    "title": "Notice",
    "section": "",
    "text": "Pre-class\n\n학생들은 오프라인(또는 온라인 스트리밍 Zoom 수업)에 참석하기 전에 강사가 제공하는 녹화된 강의를 시청합니다.\n\n학생들은 자료를 독립적으로 공부함으로써 학습에 적극적인 역할을 하도록 권장됩니다.\n사전 학습 내용은 R과 머신러닝의 기본 이론과 개념을 다룹니다.\n\n\nIn-class\n\n수업 중에 강사는 수업 전 강의를 요약하고 다루는 개념에 대한 추가 설명을 제공합니다.\n학생들의 이해를 강화하기 위해 학생들은 실습에 참여합니다. 이를 통해 배운 지식을 적용하고 코딩 기술을 개발할 수 있는 기회를 얻을 수 있습니다.\n\nThe official in-class time is PM 14:00 ~ 16:00"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#textbooks",
    "href": "teaching/media_ds/notice/index.html#textbooks",
    "title": "Notice",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\n\n\n\n\n\n\n\n아래 페이지 방문하시면 R 기초와 기초 통계에 대한 정리가 너무 잘 되어 있는 사이트가 있습니다. 제 콘텐츠와 병행해서 보시면 많은 도움이 될 것 같아 공유합니다.\nhttps://kilhwan.github.io/rprogramming/"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#grading",
    "href": "teaching/media_ds/notice/index.html#grading",
    "title": "Notice",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nItems\nRatio (%)\n\n\n\n\nAttendance\n0\n\n\nPaper proposal\n100\n\n\nTotal\n100\n\n\n\n\n출석 점수 없지만 출석이 1/3 미만: F"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#communication",
    "href": "teaching/media_ds/notice/index.html#communication",
    "title": "Notice",
    "section": "Communication",
    "text": "Communication\n\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gomSif7e\n\n\n\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s90SQocc\npw: hanyang\n\n\n\n\n\nInstall gadgets\n\nInstall R, R Studio, & Rtools\n\nR\n\nFor window https://cran.r-project.org/bin/windows/base/\nFor mac https://cran.r-project.org/bin/macosx/\n\nR Studio\n\nhttps://posit.co/download/rstudio-desktop/\n\nRtools (only for window user)\n\nhttps://cran.r-project.org/bin/windows/Rtools/\nTo install R packages containing C/C++ language for window users (no need for Linux and Mac users)\n\n\nThings you need to know\n\nDon’t use OneDrive.\n\nUse Github instead\nMany people get an error when installing becausedf OneDrive\n\nSet Windows user name to English\n\nIf Korean characters are mixed in the installation path, there is a high probability of error occurrence\n\n\nInstallation Order\n\nStep 1 ‑ Download the file\n\nDownload R, Rtools, Rstudio installation files\n\nStep 2 - Install R\n\nUnified installation path: All will be installed in the C:/R folder\nRun in administrator mode when running the R installation file\nAfter installing R, grant write permission to the R folder, Right-click and turn off read only\n\nStep 3 ‑ Install Rtools\n\nAdministrator mode execution installation and folder setting as C:\\R\\rtools40\nCreate environment variable RTOOLS40_HOME after installation: Value - C:\\R\\rtools40\\\nAdd %RTOOLS40_HOME%\\usr\\bin\\ to the Path variable.\n\nStep 4 - Install Rstudio\n\nRight-click and run as administrator - installation path C:\\R\\Rstudio\nCheck rtools connection with Sys.which(“make”) command after installation"
  },
  {
    "objectID": "teaching/media_ds/about/titanic_viz.html",
    "href": "teaching/media_ds/about/titanic_viz.html",
    "title": "Titanic data visualiztion practice in R",
    "section": "",
    "text": "The original source is here below\n[https://www.kaggle.com/romabash/titanic-data-visualization]\nDownload titanic.csv\n\n\n\n\n\n# Load packages and Data\nlibrary(readr) # Reading in data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes) # Data visualization\nlibrary(RColorBrewer) # Data visualization\ntitanic &lt;- read_csv(\"data/titanic.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(titanic)\n\n[1] 891\n\nhead(titanic)\n\n# A tibble: 6 × 12\n  PassengerId Survived Pclass Name    Sex     Age SibSp Parch Ticket  Fare Cabin\n        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1           1        0      3 Braund… male     22     1     0 A/5 2…  7.25 &lt;NA&gt; \n2           2        1      1 Cuming… fema…    38     1     0 PC 17… 71.3  C85  \n3           3        1      3 Heikki… fema…    26     0     0 STON/…  7.92 &lt;NA&gt; \n4           4        1      1 Futrel… fema…    35     1     0 113803 53.1  C123 \n5           5        0      3 Allen,… male     35     0     0 373450  8.05 &lt;NA&gt; \n6           6        0      3 Moran,… male     NA     0     0 330877  8.46 &lt;NA&gt; \n# ℹ 1 more variable: Embarked &lt;chr&gt;\n\ntable(titanic$Pclass)\n\n\n  1   2   3 \n216 184 491 \n\n\n\n\n\n\n# Convert Variable into Factors\n# Convert Pclass, Survived and Sex Variables into Factors using the mutate function\n# Keep Age numeric\n\nstr(titanic)\n\nspc_tbl_ [891 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PassengerId: num [1:891] 1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...\n $ Pclass     : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : chr [1:891] \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ...\n $ Sex        : chr [1:891] \"male\" \"female\" \"female\" \"female\" ...\n $ Age        : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : chr [1:891] \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ...\n $ Fare       : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : chr [1:891] NA \"C85\" NA \"C123\" ...\n $ Embarked   : chr [1:891] \"S\" \"C\" \"S\" \"S\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PassengerId = col_double(),\n  ..   Survived = col_double(),\n  ..   Pclass = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   SibSp = col_double(),\n  ..   Parch = col_double(),\n  ..   Ticket = col_character(),\n  ..   Fare = col_double(),\n  ..   Cabin = col_character(),\n  ..   Embarked = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ntitanic &lt;- titanic %&gt;%\n  mutate(Pclass = factor(Pclass), \n         Survived = factor(Survived), \n         Sex = factor(Sex))\n\ntitanic %&gt;% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;fct&gt; male, female, female, female, male, male, male, male, fema…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n# Age has some missing values (NA). Missing values will be ignored for now\n\n\n\n\n\n#######################################\n# 1. Look at the Total Survival Rate\n# Use table function to look at the Survival rates\n# Convert table to a tibble (similar to dataframe) to use later when plotting with ggplot2 to add text to the graph\n# Use rename function to rename default Column names\n\nstr(table(titanic$Survived))\n\n 'table' int [1:2(1d)] 549 342\n - attr(*, \"dimnames\")=List of 1\n  ..$ : chr [1:2] \"0\" \"1\"\n\nas.data.frame(table(titanic$Survived))\n\n  Var1 Freq\n1    0  549\n2    1  342\n\nsurvival &lt;- table(titanic$Survived) %&gt;%\n  as.data.frame() %&gt;%\n  rename(Survived = Var1, Count = Freq)\n\nsurvival\n\n  Survived Count\n1        0   549\n2        1   342\n\ntitanic %&gt;% \n  group_by(Survived) %&gt;% \n  summarize(Count=n())\n\n# A tibble: 2 × 2\n  Survived Count\n  &lt;fct&gt;    &lt;int&gt;\n1 0          549\n2 1          342\n\n# Look at the Total Survival Rate Proportion\n# Use prop.table to get the proportion\n\nsurvival_ratio &lt;- table(titanic$Survived) %&gt;% prop.table %&gt;% \n  as.data.frame() %&gt;%\n  rename(Survived = Var1, Percentage = Freq) %&gt;%\n  mutate(Percentage = round(Percentage, 2)*100)\n\nsurvival_ratio\n\n  Survived Percentage\n1        0         62\n2        1         38\n\n# Plot the Total Survival Rate\n# Using a barplot with theme_few theme from the ggthemes package\n# Add some styling to the plot: Center the Title and color, Edit the Legends\n# Use tibble of survival data in geom_text to add the Count to the plot\n# Use tibble of survival data ratio in geom_label to add the Percentages to the bars\n\n\n\n# survival_ratio %&gt;% \n#   mutate(y=c(300, 200)) -&gt;survival_ratio\n\nsurvival %&gt;% \n  mutate(lab=c(\"NS\", \"S\"))-&gt; survival\n\nsurvival_ratio %&gt;% \n  mutate(cordi=c(300, 190)) -&gt; survival_ratio\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Survived, fill = Survived)) +\n  geom_text(data = survival, \n            aes(x = Survived, y = Count, label = Count), \n            position = position_dodge(width=0.1),\n            vjust=-0.25,\n            fontface = \"bold\") +\n  geom_label(data = survival_ratio, \n             aes(x = Survived, y = Percentage, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 5)) +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Total Survival Rate\") +\n  scale_x_discrete(name= \"Survival Rate\", labels = c(\"Did Not Survive\", \"Survived\")) +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# Group the data by Sex using the group_by function\n# Get the total Count of passengers in each gender group with summarise function\n\ngender &lt;- titanic %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(Count = n())\n\ntable(titanic$Sex) %&gt;% \n  as.data.frame %&gt;% \n  rename(Sex=Var1,Count=Freq)\n\n     Sex Count\n1 female   314\n2   male   577\n\ngender\n\n# A tibble: 2 × 2\n  Sex    Count\n  &lt;fct&gt;  &lt;int&gt;\n1 female   314\n2 male     577\n\n# Look at the Gender Survival Rate Proportion\n# Group by Sex and Survived to get the Count of survived by gender using summarise function\n# Use mutate to add a new Percentage variable\ngender_ratio &lt;- titanic %&gt;%\n  group_by(Sex, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\ngender_ratio\n\n# A tibble: 4 × 4\n# Groups:   Sex [2]\n  Sex    Survived Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 female 0           81         26\n2 female 1          233         74\n3 male   0          468         81\n4 male   1          109         19\n\n# Plot the Gender Survival Rate\n# Using a barplot\n# Represent Gender on the x-axis\n# Use Color to represent Survival on the Plot\n# Add the Count and Percentage using geom_text and geom_label respectively\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Sex, fill = Survived)) +\n  geom_text(data = gender, \n            aes(x = Sex, y = Count, label = Count), \n            # position = position_dodge(width=0.9), \n            vjust=-0.25, \n            fontface = \"bold\") +\n  geom_label(data = gender_ratio, \n             aes(x = Sex, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5)) +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Survival Rate\") +\n  scale_x_discrete(name= \"Gender\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 3. Look at Survival Rate by Ticket Class\n# Group the data by Pclass using the group_by function\n# Get the total Count of passengers in each Pclass with summarise function\n\npclass &lt;- titanic %&gt;%\n  group_by(Pclass) %&gt;%\n  summarise(Count = n())\n\npclass\n\n# A tibble: 3 × 2\n  Pclass Count\n  &lt;fct&gt;  &lt;int&gt;\n1 1        216\n2 2        184\n3 3        491\n\n# Look at the Pclass Survival Rate Proportion\n# Group by Pclass and Survived to get the Count of survived in each Pclass using summarise function\n# Use mutate to add a new Percentage variable\npclass_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass'. You can override using the\n`.groups` argument.\n\npclass_ratio\n\n# A tibble: 6 × 4\n# Groups:   Pclass [3]\n  Pclass Survived Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 1      0           80         37\n2 1      1          136         63\n3 2      0           97         53\n4 2      1           87         47\n5 3      0          372         76\n6 3      1          119         24\n\n# Plot the Gender Survival Rate\n# Using a barplot using black and white theme theme_bw from ggplot2\n# Represent Pclass on the x-axis\n# Use Color to represent Survival on the Plot\n# Add the Count and Percentage using geom_text and geom_label respectively\n\n# pclass %&gt;% \n#   mutate(lab=c(\"First\", \"Business\", \"Economy\"))-&gt;pclass\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Pclass, fill = Survived)) +\n  geom_text(data = pclass, \n            aes(x = Pclass, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust=-0.25, \n            fontface = \"bold\") +\n  geom_label(data = pclass_ratio, \n             aes(x = Pclass, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5)) +\n  theme_solarized(base_size = 12, base_family = \"\", light = TRUE) + \n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Pclass Survival Rate\") +\n  scale_x_discrete(name= \"Pclass\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 4. Look at Gender Proportion in each Class\n# Before looking at the proportion of Males and Females that Survived in each Pclass, let's look at the Gender proportion in each class\npclass_gender &lt;- titanic %&gt;%\n  group_by(Pclass) %&gt;%\n  summarise(Count = n())\n\npclass_gender\n\n# A tibble: 3 × 2\n  Pclass Count\n  &lt;fct&gt;  &lt;int&gt;\n1 1        216\n2 2        184\n3 3        491\n\n# Look at the Pclass Gender Proportion\npclass_gender_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Sex) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass'. You can override using the\n`.groups` argument.\n\npclass_gender_ratio\n\n# A tibble: 6 × 4\n# Groups:   Pclass [3]\n  Pclass Sex    Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1      female    94         44\n2 1      male     122         56\n3 2      female    76         41\n4 2      male     108         59\n5 3      female   144         29\n6 3      male     347         71\n\n# Plot the Pclass Gender Proportion\n# Represent Pclass on the x-axis\n# Use Color to represent Gender on the Plot using the RColorBrewer package\n# Add the Count and Percentage using geom_text and geom_label respectively\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Pclass, fill = Sex)) +\n  geom_text(data = pclass_gender, \n            aes(x = Pclass, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust=-0.25, \n            fontface = \"bold\") +\n  geom_label(data = pclass_gender_ratio, \n             aes(x = Pclass, y = Count, label = paste0(Percentage, \"%\"), group = Sex), \n             position = position_stack(vjust = 0.5)) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Proportion by Ticket Class\") +\n  scale_x_discrete(name= \"Pclass\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_brewer(name = \"Gender\", labels = c(\"Female\", \"Male\"), palette = \"Paired\")\n\n\n\n\n\n\n\n\n# 5. Look at Survival Rate by Gender in each Pclass\n# First look at Pclass Gender Proportion\n# Second look at Survival Rate by Gender in each Pclass\n\n\npclass_gender_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Sex) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass'. You can override using the\n`.groups` argument.\n\npclass_gender_ratio\n\n# A tibble: 6 × 4\n# Groups:   Pclass [3]\n  Pclass Sex    Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1      female    94         44\n2 1      male     122         56\n3 2      female    76         41\n4 2      male     108         59\n5 3      female   144         29\n6 3      male     347         71\n\npclass_gender_survived_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Sex, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass', 'Sex'. You can override using the\n`.groups` argument.\n\npclass_gender_survived_ratio\n\n# A tibble: 12 × 5\n# Groups:   Pclass, Sex [6]\n   Pclass Sex    Survived Count Percentage\n   &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n 1 1      female 0            3          3\n 2 1      female 1           91         97\n 3 1      male   0           77         63\n 4 1      male   1           45         37\n 5 2      female 0            6          8\n 6 2      female 1           70         92\n 7 2      male   0           91         84\n 8 2      male   1           17         16\n 9 3      female 0           72         50\n10 3      female 1           72         50\n11 3      male   0          300         86\n12 3      male   1           47         14\n\n# Plot the Gender Survival Proportion by Pclass\n# Represent Sex on the x-axis\n# Use Color to represent Survival\n# Use faceting to separate by Pclass using facet_wrap\n# Add the Count and Percentage using geom_text and geom_label respectively\n\n\n\n# Using facet_wrap(~ Pclass)\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Sex, fill = Survived)) +\n  facet_wrap(~ Pclass) +\n  geom_text(data = pclass_gender_ratio, \n            aes(x = Sex, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust= -0.5, \n            fontface = \"bold\") +\n  geom_label(data = pclass_gender_survived_ratio, \n             aes(x = Sex, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5))\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Sex, fill = Survived)) +\n  facet_wrap(~ Pclass) +\n  geom_text(data = pclass_gender_ratio, \n            aes(x = Sex, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust= -1.5, \n            fontface = \"bold\") +\n  geom_label(data = pclass_gender_survived_ratio, \n             aes(x = Sex, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5)) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Survival Rate by Pclass\") +\n  scale_x_discrete(name= \"Gender by Pclass \") +\n  scale_y_continuous(name = \"Passenger Count\", limits = c(0,360)) +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n# Using facet_grid(Sex ~ Pclass) to separate Gender and Pclass\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Survived, fill = Survived)) +\n  facet_grid(Sex ~ Pclass) +\n  geom_text(data = pclass_gender_survived_ratio, \n            aes(x = Survived, y = Count, label = paste0(Percentage, \"%\")), \n            position = position_dodge(width=0.9), \n            vjust= -0.5, \n            fontface = \"bold\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Survival Rate by Pclass\") +\n  scale_x_discrete(name= \"Survival Rate\", labels = c(\"No\", \"Yes\")) +\n  scale_y_continuous(name = \"Passenger Count\", limits = c(0,360)) +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 6. Look at Survival Rate by Age\n# First look at the Age distribution\n# Second look at Survival rate by Age\n# Find the Average Age of Passengers\n# Remove Missing Values (177 NA Values)\nmedian(titanic$Age, na.rm = TRUE)\n\n[1] 28\n\n# Plot the Age Distribution\n# Using a Histogram (Continious Data)\n# Using binwidth = 5 years\n# Ignoring 177 observations with missing Age\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age), \n                 binwidth = 5, color = \"#355a63\", \n                 fill = \"#96e4f7\") +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Age Distribution\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 5*c(0:18)) +\n  scale_y_continuous(name = \"Passenger Count\")\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n# Plot the Survival Rate by Age\n# Use automatic fill based on Survived\n# Ignoring 177 observations with missing Age\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age, fill = Survived), binwidth = 5, color = \"#355a63\") +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Survival Rate by Age\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 5*c(0:18)) +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n# 7. Look at Survival Rates by Age when segmented by Gender and Class\n# Look At Survival Rate based on Gender and Class Segmented by Age\n# Females that Did NOT Survive in 1st and 2nd Class (3% and 8%) seem to be randomly distributed by Age\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age, fill = Survived), binwidth = 5, color = \"#355a63\")+\n  facet_grid(Sex ~ Pclass)\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age, fill = Survived), binwidth = 5, color = \"#355a63\") +\n  facet_grid(Sex ~ Pclass) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Survival Rate by Age, Gender and Class\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 10*c(0:8)) +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n# 8. Look at Survival Based on Family Size\n# Add a Varibale for Family Size\n# Combine SibSp and Parch variables together and add 1 (for self)\n# Use the mutate function to add FamilySize to the dataset\ntitanic &lt;- titanic %&gt;%\n  mutate(FamilySize = 1 + SibSp + Parch)\n\n# Look at Survival Rate Based on Family Size\ntitanic %&gt;%\n  group_by(FamilySize, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'FamilySize'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 16 × 4\n# Groups:   FamilySize [9]\n   FamilySize Survived Count Percentage\n        &lt;dbl&gt; &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n 1          1 0          374         70\n 2          1 1          163         30\n 3          2 0           72         45\n 4          2 1           89         55\n 5          3 0           43         42\n 6          3 1           59         58\n 7          4 0            8         28\n 8          4 1           21         72\n 9          5 0           12         80\n10          5 1            3         20\n11          6 0           19         86\n12          6 1            3         14\n13          7 0            8         67\n14          7 1            4         33\n15          8 0            6        100\n16         11 0            7        100\n\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = FamilySize, fill = Survived), binwidth = 1) +\n  facet_grid(Sex ~ Pclass) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Survival Rate by Family Size\") +\n  scale_x_continuous(name = \"Family Size\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 9. Looking At Survival by Gender, Class, Age and FamilySize\n# Segment by Pclass and Sex\n# Represent Age on the x-axis (grouped by 10 years) and FamilySize on the y-axis\n# Represent Survival with color\ntitanic %&gt;%\n  ggplot() +\n  geom_point(aes(x = Age, y = FamilySize, color = Survived), alpha = 0.7) +\n  facet_grid(Sex ~ Pclass) +\n  theme_bw() +\n  theme(plot.title = element_text(size=18, color = \"#054354\")) +\n  ggtitle(\"Survival Rate by Gender, Class, Age, and Family Size\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 10*c(0:8)) +\n  scale_y_continuous(name = \"Family Size\") +\n  scale_color_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\nWarning: Removed 177 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "teaching/media_ds/about/titanic_viz.html#titanic-dataset-visualization-from-kaggle",
    "href": "teaching/media_ds/about/titanic_viz.html#titanic-dataset-visualization-from-kaggle",
    "title": "Titanic data visualiztion practice in R",
    "section": "",
    "text": "The original source is here below\n[https://www.kaggle.com/romabash/titanic-data-visualization]\nDownload titanic.csv\n\n\n\n\n\n# Load packages and Data\nlibrary(readr) # Reading in data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes) # Data visualization\nlibrary(RColorBrewer) # Data visualization\ntitanic &lt;- read_csv(\"data/titanic.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(titanic)\n\n[1] 891\n\nhead(titanic)\n\n# A tibble: 6 × 12\n  PassengerId Survived Pclass Name    Sex     Age SibSp Parch Ticket  Fare Cabin\n        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1           1        0      3 Braund… male     22     1     0 A/5 2…  7.25 &lt;NA&gt; \n2           2        1      1 Cuming… fema…    38     1     0 PC 17… 71.3  C85  \n3           3        1      3 Heikki… fema…    26     0     0 STON/…  7.92 &lt;NA&gt; \n4           4        1      1 Futrel… fema…    35     1     0 113803 53.1  C123 \n5           5        0      3 Allen,… male     35     0     0 373450  8.05 &lt;NA&gt; \n6           6        0      3 Moran,… male     NA     0     0 330877  8.46 &lt;NA&gt; \n# ℹ 1 more variable: Embarked &lt;chr&gt;\n\ntable(titanic$Pclass)\n\n\n  1   2   3 \n216 184 491 \n\n\n\n\n\n\n# Convert Variable into Factors\n# Convert Pclass, Survived and Sex Variables into Factors using the mutate function\n# Keep Age numeric\n\nstr(titanic)\n\nspc_tbl_ [891 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PassengerId: num [1:891] 1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...\n $ Pclass     : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : chr [1:891] \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ...\n $ Sex        : chr [1:891] \"male\" \"female\" \"female\" \"female\" ...\n $ Age        : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : chr [1:891] \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ...\n $ Fare       : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : chr [1:891] NA \"C85\" NA \"C123\" ...\n $ Embarked   : chr [1:891] \"S\" \"C\" \"S\" \"S\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PassengerId = col_double(),\n  ..   Survived = col_double(),\n  ..   Pclass = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   SibSp = col_double(),\n  ..   Parch = col_double(),\n  ..   Ticket = col_character(),\n  ..   Fare = col_double(),\n  ..   Cabin = col_character(),\n  ..   Embarked = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ntitanic &lt;- titanic %&gt;%\n  mutate(Pclass = factor(Pclass), \n         Survived = factor(Survived), \n         Sex = factor(Sex))\n\ntitanic %&gt;% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;fct&gt; male, female, female, female, male, male, male, male, fema…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n# Age has some missing values (NA). Missing values will be ignored for now\n\n\n\n\n\n#######################################\n# 1. Look at the Total Survival Rate\n# Use table function to look at the Survival rates\n# Convert table to a tibble (similar to dataframe) to use later when plotting with ggplot2 to add text to the graph\n# Use rename function to rename default Column names\n\nstr(table(titanic$Survived))\n\n 'table' int [1:2(1d)] 549 342\n - attr(*, \"dimnames\")=List of 1\n  ..$ : chr [1:2] \"0\" \"1\"\n\nas.data.frame(table(titanic$Survived))\n\n  Var1 Freq\n1    0  549\n2    1  342\n\nsurvival &lt;- table(titanic$Survived) %&gt;%\n  as.data.frame() %&gt;%\n  rename(Survived = Var1, Count = Freq)\n\nsurvival\n\n  Survived Count\n1        0   549\n2        1   342\n\ntitanic %&gt;% \n  group_by(Survived) %&gt;% \n  summarize(Count=n())\n\n# A tibble: 2 × 2\n  Survived Count\n  &lt;fct&gt;    &lt;int&gt;\n1 0          549\n2 1          342\n\n# Look at the Total Survival Rate Proportion\n# Use prop.table to get the proportion\n\nsurvival_ratio &lt;- table(titanic$Survived) %&gt;% prop.table %&gt;% \n  as.data.frame() %&gt;%\n  rename(Survived = Var1, Percentage = Freq) %&gt;%\n  mutate(Percentage = round(Percentage, 2)*100)\n\nsurvival_ratio\n\n  Survived Percentage\n1        0         62\n2        1         38\n\n# Plot the Total Survival Rate\n# Using a barplot with theme_few theme from the ggthemes package\n# Add some styling to the plot: Center the Title and color, Edit the Legends\n# Use tibble of survival data in geom_text to add the Count to the plot\n# Use tibble of survival data ratio in geom_label to add the Percentages to the bars\n\n\n\n# survival_ratio %&gt;% \n#   mutate(y=c(300, 200)) -&gt;survival_ratio\n\nsurvival %&gt;% \n  mutate(lab=c(\"NS\", \"S\"))-&gt; survival\n\nsurvival_ratio %&gt;% \n  mutate(cordi=c(300, 190)) -&gt; survival_ratio\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Survived, fill = Survived)) +\n  geom_text(data = survival, \n            aes(x = Survived, y = Count, label = Count), \n            position = position_dodge(width=0.1),\n            vjust=-0.25,\n            fontface = \"bold\") +\n  geom_label(data = survival_ratio, \n             aes(x = Survived, y = Percentage, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 5)) +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Total Survival Rate\") +\n  scale_x_discrete(name= \"Survival Rate\", labels = c(\"Did Not Survive\", \"Survived\")) +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# Group the data by Sex using the group_by function\n# Get the total Count of passengers in each gender group with summarise function\n\ngender &lt;- titanic %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(Count = n())\n\ntable(titanic$Sex) %&gt;% \n  as.data.frame %&gt;% \n  rename(Sex=Var1,Count=Freq)\n\n     Sex Count\n1 female   314\n2   male   577\n\ngender\n\n# A tibble: 2 × 2\n  Sex    Count\n  &lt;fct&gt;  &lt;int&gt;\n1 female   314\n2 male     577\n\n# Look at the Gender Survival Rate Proportion\n# Group by Sex and Survived to get the Count of survived by gender using summarise function\n# Use mutate to add a new Percentage variable\ngender_ratio &lt;- titanic %&gt;%\n  group_by(Sex, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\ngender_ratio\n\n# A tibble: 4 × 4\n# Groups:   Sex [2]\n  Sex    Survived Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 female 0           81         26\n2 female 1          233         74\n3 male   0          468         81\n4 male   1          109         19\n\n# Plot the Gender Survival Rate\n# Using a barplot\n# Represent Gender on the x-axis\n# Use Color to represent Survival on the Plot\n# Add the Count and Percentage using geom_text and geom_label respectively\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Sex, fill = Survived)) +\n  geom_text(data = gender, \n            aes(x = Sex, y = Count, label = Count), \n            # position = position_dodge(width=0.9), \n            vjust=-0.25, \n            fontface = \"bold\") +\n  geom_label(data = gender_ratio, \n             aes(x = Sex, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5)) +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Survival Rate\") +\n  scale_x_discrete(name= \"Gender\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 3. Look at Survival Rate by Ticket Class\n# Group the data by Pclass using the group_by function\n# Get the total Count of passengers in each Pclass with summarise function\n\npclass &lt;- titanic %&gt;%\n  group_by(Pclass) %&gt;%\n  summarise(Count = n())\n\npclass\n\n# A tibble: 3 × 2\n  Pclass Count\n  &lt;fct&gt;  &lt;int&gt;\n1 1        216\n2 2        184\n3 3        491\n\n# Look at the Pclass Survival Rate Proportion\n# Group by Pclass and Survived to get the Count of survived in each Pclass using summarise function\n# Use mutate to add a new Percentage variable\npclass_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass'. You can override using the\n`.groups` argument.\n\npclass_ratio\n\n# A tibble: 6 × 4\n# Groups:   Pclass [3]\n  Pclass Survived Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 1      0           80         37\n2 1      1          136         63\n3 2      0           97         53\n4 2      1           87         47\n5 3      0          372         76\n6 3      1          119         24\n\n# Plot the Gender Survival Rate\n# Using a barplot using black and white theme theme_bw from ggplot2\n# Represent Pclass on the x-axis\n# Use Color to represent Survival on the Plot\n# Add the Count and Percentage using geom_text and geom_label respectively\n\n# pclass %&gt;% \n#   mutate(lab=c(\"First\", \"Business\", \"Economy\"))-&gt;pclass\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Pclass, fill = Survived)) +\n  geom_text(data = pclass, \n            aes(x = Pclass, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust=-0.25, \n            fontface = \"bold\") +\n  geom_label(data = pclass_ratio, \n             aes(x = Pclass, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5)) +\n  theme_solarized(base_size = 12, base_family = \"\", light = TRUE) + \n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Pclass Survival Rate\") +\n  scale_x_discrete(name= \"Pclass\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 4. Look at Gender Proportion in each Class\n# Before looking at the proportion of Males and Females that Survived in each Pclass, let's look at the Gender proportion in each class\npclass_gender &lt;- titanic %&gt;%\n  group_by(Pclass) %&gt;%\n  summarise(Count = n())\n\npclass_gender\n\n# A tibble: 3 × 2\n  Pclass Count\n  &lt;fct&gt;  &lt;int&gt;\n1 1        216\n2 2        184\n3 3        491\n\n# Look at the Pclass Gender Proportion\npclass_gender_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Sex) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass'. You can override using the\n`.groups` argument.\n\npclass_gender_ratio\n\n# A tibble: 6 × 4\n# Groups:   Pclass [3]\n  Pclass Sex    Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1      female    94         44\n2 1      male     122         56\n3 2      female    76         41\n4 2      male     108         59\n5 3      female   144         29\n6 3      male     347         71\n\n# Plot the Pclass Gender Proportion\n# Represent Pclass on the x-axis\n# Use Color to represent Gender on the Plot using the RColorBrewer package\n# Add the Count and Percentage using geom_text and geom_label respectively\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Pclass, fill = Sex)) +\n  geom_text(data = pclass_gender, \n            aes(x = Pclass, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust=-0.25, \n            fontface = \"bold\") +\n  geom_label(data = pclass_gender_ratio, \n             aes(x = Pclass, y = Count, label = paste0(Percentage, \"%\"), group = Sex), \n             position = position_stack(vjust = 0.5)) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Proportion by Ticket Class\") +\n  scale_x_discrete(name= \"Pclass\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_brewer(name = \"Gender\", labels = c(\"Female\", \"Male\"), palette = \"Paired\")\n\n\n\n\n\n\n\n\n# 5. Look at Survival Rate by Gender in each Pclass\n# First look at Pclass Gender Proportion\n# Second look at Survival Rate by Gender in each Pclass\n\n\npclass_gender_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Sex) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass'. You can override using the\n`.groups` argument.\n\npclass_gender_ratio\n\n# A tibble: 6 × 4\n# Groups:   Pclass [3]\n  Pclass Sex    Count Percentage\n  &lt;fct&gt;  &lt;fct&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1      female    94         44\n2 1      male     122         56\n3 2      female    76         41\n4 2      male     108         59\n5 3      female   144         29\n6 3      male     347         71\n\npclass_gender_survived_ratio &lt;- titanic %&gt;%\n  group_by(Pclass, Sex, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'Pclass', 'Sex'. You can override using the\n`.groups` argument.\n\npclass_gender_survived_ratio\n\n# A tibble: 12 × 5\n# Groups:   Pclass, Sex [6]\n   Pclass Sex    Survived Count Percentage\n   &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n 1 1      female 0            3          3\n 2 1      female 1           91         97\n 3 1      male   0           77         63\n 4 1      male   1           45         37\n 5 2      female 0            6          8\n 6 2      female 1           70         92\n 7 2      male   0           91         84\n 8 2      male   1           17         16\n 9 3      female 0           72         50\n10 3      female 1           72         50\n11 3      male   0          300         86\n12 3      male   1           47         14\n\n# Plot the Gender Survival Proportion by Pclass\n# Represent Sex on the x-axis\n# Use Color to represent Survival\n# Use faceting to separate by Pclass using facet_wrap\n# Add the Count and Percentage using geom_text and geom_label respectively\n\n\n\n# Using facet_wrap(~ Pclass)\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Sex, fill = Survived)) +\n  facet_wrap(~ Pclass) +\n  geom_text(data = pclass_gender_ratio, \n            aes(x = Sex, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust= -0.5, \n            fontface = \"bold\") +\n  geom_label(data = pclass_gender_survived_ratio, \n             aes(x = Sex, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5))\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Sex, fill = Survived)) +\n  facet_wrap(~ Pclass) +\n  geom_text(data = pclass_gender_ratio, \n            aes(x = Sex, y = Count, label = Count), \n            position = position_dodge(width=0.9), \n            vjust= -1.5, \n            fontface = \"bold\") +\n  geom_label(data = pclass_gender_survived_ratio, \n             aes(x = Sex, y = Count, label = paste0(Percentage, \"%\"), group = Survived), \n             position = position_stack(vjust = 0.5)) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Survival Rate by Pclass\") +\n  scale_x_discrete(name= \"Gender by Pclass \") +\n  scale_y_continuous(name = \"Passenger Count\", limits = c(0,360)) +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n# Using facet_grid(Sex ~ Pclass) to separate Gender and Pclass\n\ntitanic %&gt;%\n  ggplot() +\n  geom_bar(aes(x = Survived, fill = Survived)) +\n  facet_grid(Sex ~ Pclass) +\n  geom_text(data = pclass_gender_survived_ratio, \n            aes(x = Survived, y = Count, label = paste0(Percentage, \"%\")), \n            position = position_dodge(width=0.9), \n            vjust= -0.5, \n            fontface = \"bold\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Gender Survival Rate by Pclass\") +\n  scale_x_discrete(name= \"Survival Rate\", labels = c(\"No\", \"Yes\")) +\n  scale_y_continuous(name = \"Passenger Count\", limits = c(0,360)) +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 6. Look at Survival Rate by Age\n# First look at the Age distribution\n# Second look at Survival rate by Age\n# Find the Average Age of Passengers\n# Remove Missing Values (177 NA Values)\nmedian(titanic$Age, na.rm = TRUE)\n\n[1] 28\n\n# Plot the Age Distribution\n# Using a Histogram (Continious Data)\n# Using binwidth = 5 years\n# Ignoring 177 observations with missing Age\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age), \n                 binwidth = 5, color = \"#355a63\", \n                 fill = \"#96e4f7\") +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Age Distribution\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 5*c(0:18)) +\n  scale_y_continuous(name = \"Passenger Count\")\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n# Plot the Survival Rate by Age\n# Use automatic fill based on Survived\n# Ignoring 177 observations with missing Age\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age, fill = Survived), binwidth = 5, color = \"#355a63\") +\n  theme_few() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Survival Rate by Age\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 5*c(0:18)) +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n# 7. Look at Survival Rates by Age when segmented by Gender and Class\n# Look At Survival Rate based on Gender and Class Segmented by Age\n# Females that Did NOT Survive in 1st and 2nd Class (3% and 8%) seem to be randomly distributed by Age\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age, fill = Survived), binwidth = 5, color = \"#355a63\")+\n  facet_grid(Sex ~ Pclass)\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = Age, fill = Survived), binwidth = 5, color = \"#355a63\") +\n  facet_grid(Sex ~ Pclass) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Survival Rate by Age, Gender and Class\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 10*c(0:8)) +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n# 8. Look at Survival Based on Family Size\n# Add a Varibale for Family Size\n# Combine SibSp and Parch variables together and add 1 (for self)\n# Use the mutate function to add FamilySize to the dataset\ntitanic &lt;- titanic %&gt;%\n  mutate(FamilySize = 1 + SibSp + Parch)\n\n# Look at Survival Rate Based on Family Size\ntitanic %&gt;%\n  group_by(FamilySize, Survived) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Percentage = round(Count/sum(Count)*100))\n\n`summarise()` has grouped output by 'FamilySize'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 16 × 4\n# Groups:   FamilySize [9]\n   FamilySize Survived Count Percentage\n        &lt;dbl&gt; &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;\n 1          1 0          374         70\n 2          1 1          163         30\n 3          2 0           72         45\n 4          2 1           89         55\n 5          3 0           43         42\n 6          3 1           59         58\n 7          4 0            8         28\n 8          4 1           21         72\n 9          5 0           12         80\n10          5 1            3         20\n11          6 0           19         86\n12          6 1            3         14\n13          7 0            8         67\n14          7 1            4         33\n15          8 0            6        100\n16         11 0            7        100\n\ntitanic %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = FamilySize, fill = Survived), binwidth = 1) +\n  facet_grid(Sex ~ Pclass) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, size=18, color = \"#054354\")) +\n  ggtitle(\"Titanic Survival Rate by Family Size\") +\n  scale_x_continuous(name = \"Family Size\") +\n  scale_y_continuous(name = \"Passenger Count\") +\n  scale_fill_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\n\n\n\n\n\n\n\n# 9. Looking At Survival by Gender, Class, Age and FamilySize\n# Segment by Pclass and Sex\n# Represent Age on the x-axis (grouped by 10 years) and FamilySize on the y-axis\n# Represent Survival with color\ntitanic %&gt;%\n  ggplot() +\n  geom_point(aes(x = Age, y = FamilySize, color = Survived), alpha = 0.7) +\n  facet_grid(Sex ~ Pclass) +\n  theme_bw() +\n  theme(plot.title = element_text(size=18, color = \"#054354\")) +\n  ggtitle(\"Survival Rate by Gender, Class, Age, and Family Size\") +\n  scale_x_continuous(name= \"Passenger Age\", breaks = 10*c(0:8)) +\n  scale_y_continuous(name = \"Family Size\") +\n  scale_color_discrete(name = \"Outcome\", labels = c(\"Did Not Survive\", \"Survived\"))\n\nWarning: Removed 177 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "teaching/media_ds/about/preclass.html",
    "href": "teaching/media_ds/about/preclass.html",
    "title": "Pre-class R code",
    "section": "",
    "text": "Download data\n[data_for_class.zip]\n\n\n################################################\n# 1. Syntax \n################################################\n\n# 1. Iris Data\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# visualization\nplot(iris)\n\n\n\nplot(iris$Petal.Width, iris$Petal.Length, col=iris$Species)\n\n\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nplot(iris$Sepal.Length, iris$Sepal.Width)\n\n\n\n# 2. Tip data\n\ntips=read.csv('http://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\nstr(tips) \n\n'data.frame':   244 obs. of  7 variables:\n $ total_bill: num  17 10.3 21 23.7 24.6 ...\n $ tip       : num  1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ...\n $ sex       : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ smoker    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ day       : chr  \"Sun\" \"Sun\" \"Sun\" \"Sun\" ...\n $ time      : chr  \"Dinner\" \"Dinner\" \"Dinner\" \"Dinner\" ...\n $ size      : int  2 3 3 2 4 4 2 4 2 2 ...\n\nhead(tips, 7)\n\n  total_bill  tip    sex smoker day   time size\n1      16.99 1.01 Female     No Sun Dinner    2\n2      10.34 1.66   Male     No Sun Dinner    3\n3      21.01 3.50   Male     No Sun Dinner    3\n4      23.68 3.31   Male     No Sun Dinner    2\n5      24.59 3.61 Female     No Sun Dinner    4\n6      25.29 4.71   Male     No Sun Dinner    4\n7       8.77 2.00   Male     No Sun Dinner    2\n\ntail(tips, 7)\n\n    total_bill  tip    sex smoker  day   time size\n238      32.83 1.17   Male    Yes  Sat Dinner    2\n239      35.83 4.67 Female     No  Sat Dinner    3\n240      29.03 5.92   Male     No  Sat Dinner    3\n241      27.18 2.00 Female    Yes  Sat Dinner    2\n242      22.67 2.00   Male    Yes  Sat Dinner    2\n243      17.82 1.75   Male     No  Sat Dinner    2\n244      18.78 3.00 Female     No Thur Dinner    2\n\nsummary(tips)\n\n   total_bill         tip             sex               smoker         \n Min.   : 3.07   Min.   : 1.000   Length:244         Length:244        \n 1st Qu.:13.35   1st Qu.: 2.000   Class :character   Class :character  \n Median :17.80   Median : 2.900   Mode  :character   Mode  :character  \n Mean   :19.79   Mean   : 2.998                                        \n 3rd Qu.:24.13   3rd Qu.: 3.562                                        \n Max.   :50.81   Max.   :10.000                                        \n     day                time                size     \n Length:244         Length:244         Min.   :1.00  \n Class :character   Class :character   1st Qu.:2.00  \n Mode  :character   Mode  :character   Median :2.00  \n                                       Mean   :2.57  \n                                       3rd Qu.:3.00  \n                                       Max.   :6.00  \n\nhist(tips$total_bill)\n\n# visualization\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n\n\nhist(tips$size)\n\n\n\ntips %&gt;% ggplot(aes(size)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + geom_point()\n\n\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + geom_point(aes(col=day))\n\n\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + geom_point(aes(col=day, pch=sex), size=5)\n\n\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + \n  geom_point(aes(col=day)) +\n  geom_line()\n\n\n\ntips %&gt;% ggplot(aes(total_bill, tip, col=day, pch=sex)) + geom_point(size=3)\n\n\n\n\n\n# 03 Data type #\n\nx = 5\ny = 2\nx/y\n\n[1] 2.5\n\nxi = 1 + 2i\nyi = 1 - 2i\nxi+yi\n\n[1] 2+0i\n\nstr = \"Hello, World!\"\nstr\n\n[1] \"Hello, World!\"\n\nblood.type = factor(c('A', 'B', 'O', 'AB'))\nblood.type\n\n[1] A  B  O  AB\nLevels: A AB B O\n\nT\n\n[1] TRUE\n\nF\n\n[1] FALSE\n\nxinf = Inf\nyinf = -Inf\nxinf/yinf\n\n[1] NaN\n\nx = 1       # x에 단순히 1을 넣은 경우 x는 숫자형\nx\n\n[1] 1\n\nis.integer(x)\n\n[1] FALSE\n\nx = 1L      # x에 1L을 입력한 경우 x는 정수형\nx\n\n[1] 1\n\nis.integer(x)\n\n[1] TRUE\n\nx = as.integer(1)   # x에 1을 as.integer 함수로 변환하여 입력한 경우 x는 정수형\nx\n\n[1] 1\n\nis.integer(x)\n\n[1] TRUE\n\n\n\n# 05 벡터 #\n1:7         # 1부터 7까지 1씩 증가시켜 요소가 7개인 벡터 생성\n\n[1] 1 2 3 4 5 6 7\n\n7:1         # 7부터 1까지 1씩 감소시켜 요소가 7개인 벡터 생성\n\n[1] 7 6 5 4 3 2 1\n\nvector(length = 5)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nc(1:5)      # 1~5 요소로 구성된 벡터 생성. 1:5와 동일\n\n[1] 1 2 3 4 5\n\nc(1, 2, 3, c(4:6))  # 1~3 요소와 4~6 요소를 결합한 1~6 요소로 구성된 벡터 생성\n\n[1] 1 2 3 4 5 6\n\nx = c(1, 2, 3)  # 1~3 요소로 구성된 벡터를 x에 저장\nx       # x 출력\n\n[1] 1 2 3\n\ny = c()         # y를 빈 벡터로 생성\ny = c(y, c(1:3))    # 기존 y 벡터에 c(1:3) 벡터를 추가해 생성\ny       # y 출력\n\n[1] 1 2 3\n\nseq(from = 1, to = 10, by = 2)  # 1부터 10까지 2씩 증가하는 벡터 생성\n\n[1] 1 3 5 7 9\n\nseq(1, 10, by = 2)          # 1부터 10까지 2씩 증가하는 벡터 생성\n\n[1] 1 3 5 7 9\n\nseq(0, 1, by = 0.1)             # 0부터 1까지 0.1씩 증가하는 요소가 11개인 벡터 생성\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nseq(0, 1, length.out = 11)      # 0부터 1까지 요소가 11개인 벡터 생성\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nrep(c(1:3), times = 2)      # (1, 2, 3) 벡터를 2번 반복한 벡터 생성\n\n[1] 1 2 3 1 2 3\n\nrep(c(1:3), each = 2)       # (1, 2, 3) 벡터의 개별 요소를 2번 반복한 벡터 생성\n\n[1] 1 1 2 2 3 3\n\nx = c(2, 4, 6, 8, 10)\nlength(x)       # x 벡터의 길이(크기)를 구함\n\n[1] 5\n\nx[1]        # x 벡터의 1번 요소 값을 구함\n\n[1] 2\n\n# x[1, 2, 3]        # x 벡터의 1, 2, 3번 요소를 구할 때 이렇게 입력하면 오류\nx[c(1, 2, 3)]   # x 벡터의 1, 2, 3번 요소를 구할 때는 벡터로 묶어야 함\n\n[1] 2 4 6\n\nx[-c(1, 2, 3)]  # x 벡터에서 1, 2, 3번 요소를 제외한 값 출력\n\n[1]  8 10\n\nx[c(1:3)]       # x 벡터에서 1번부터 3번 요소를 출력\n\n[1] 2 4 6\n\nx = c(1, 2, 3, 4)\ny = c(5, 6, 7, 8)\nz = c(3, 4)\nw = c(5, 6, 7)\nx+2         # x 벡터의 개별 요소에 2를 각각 더함\n\n[1] 3 4 5 6\n\nx + y       # x 벡터와 y 벡터의 크기가 동일하므로 각 요소별로 더함\n\n[1]  6  8 10 12\n\nx + z       # x 벡터가 z 벡터 크기의 정수배인 경우엔 작은 쪽 벡터 요소를 순환하며 더함\n\n[1] 4 6 6 8\n\nx + w       # x와 w의 크기가 정수배가 아니므로 연산 오류\n\nWarning in x + w: longer object length is not a multiple of shorter object\nlength\n\n\n[1]  6  8 10  9\n\nx &gt;5        # x 벡터의 요소 값이 5보다 큰지 확인\n\n[1] FALSE FALSE FALSE FALSE\n\nall(x&gt;5)        # x 벡터의 요소 값이 모두 5보다 큰지 확인\n\n[1] FALSE\n\nany(x&gt;5)        # x 벡터의 요소 값 중 일부가 5보다 큰지 확인\n\n[1] FALSE\n\nx = 1:10\nhead(x)         # 데이터의 앞 6개 요소를 추출\n\n[1] 1 2 3 4 5 6\n\ntail(x)         # 데이터의 뒤 6개 요소를 추출\n\n[1]  5  6  7  8  9 10\n\nhead(x, 3)  # 데이터의 앞 3개 요소를 추출\n\n[1] 1 2 3\n\ntail(x, 3)      # 데이터의 뒤 3개 요소를 추출\n\n[1]  8  9 10\n\nx = c(1, 2, 3)\ny = c(3, 4, 5)\nz = c(3, 1, 2)\nunion(x, y)     # 합집합\n\n[1] 1 2 3 4 5\n\nintersect(x, y)     # 교집합\n\n[1] 3\n\nsetdiff(x, y)   # 차집합(x에서 y와 동일한 요소 제외)\n\n[1] 1 2\n\nsetdiff(y, x)   # 차집합(y에서 x와 동일 요소 제외)\n\n[1] 4 5\n\nsetequal(x, y)  # x와 y에 동일한 요소가 있는지 비교\n\n[1] FALSE\n\nsetequal(x, z)  # x와 z에 동일한 요소가 있는지 비교\n\n[1] TRUE\n\n\n\n# 06 행렬 #\n# N차원 배열 생성\nx = array(1:5, c(2, 4)) # 1~5 값을 2× 4 행렬에 할당\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    2\n[2,]    2    4    1    3\n\nx[1, ] # 1행 요소 값 출력\n\n[1] 1 3 5 2\n\nx[, 2] # 2열 요소 값 출력\n\n[1] 3 4\n\ndimnamex = list(c(\"1st\", \"2nd\"), c(\"1st\", \"2nd\", \"3rd\", \"4th\")) # 행과 열 이름 설정\nx = array(1:5, c(2, 4), dimnames = dimnamex)\nx\n\n    1st 2nd 3rd 4th\n1st   1   3   5   2\n2nd   2   4   1   3\n\nx[\"1st\", ]\n\n1st 2nd 3rd 4th \n  1   3   5   2 \n\nx[, \"4th\"]\n\n1st 2nd \n  2   3 \n\n# 2차원 배열 생성\nx = 1:12\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\nmatrix(x, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(x, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# 벡터를 묶어 배열 생성\nv1 = c(1, 2, 3, 4)\nv2 = c(5, 6, 7, 8)\nv3 = c(9, 10, 11, 12)\ncbind(v1, v2, v3) # 열 단위로 묶어 배열 생성\n\n     v1 v2 v3\n[1,]  1  5  9\n[2,]  2  6 10\n[3,]  3  7 11\n[4,]  4  8 12\n\nrbind(v1, v2, v3) # 행 단위로 묶어 배열 생성\n\n   [,1] [,2] [,3] [,4]\nv1    1    2    3    4\nv2    5    6    7    8\nv3    9   10   11   12\n\n# [표 3-7]의 연산자를 활용한 다양한 행렬 연산\n# 2×2 행렬 2개를 각각 x, y에 저장\nx = array(1:4, dim = c(2, 2))\ny = array(5:8, dim = c(2, 2))\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\ny\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nx + y\n\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n\nx - y\n\n     [,1] [,2]\n[1,]   -4   -4\n[2,]   -4   -4\n\nx * y # 각 열별 곱셈\n\n     [,1] [,2]\n[1,]    5   21\n[2,]   12   32\n\nx %*% y # 수학적인 행렬 곱셈\n\n     [,1] [,2]\n[1,]   23   31\n[2,]   34   46\n\nt(x) # x의 전치 행렬\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsolve(x) # x의 역행렬\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\ndet(x) # x의 행렬식\n\n[1] -2\n\nx = array(1:12, c(3, 4))\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\napply(x, 1, mean) # 가운데 값이 1이면 함수를 행별로 적용\n\n[1] 5.5 6.5 7.5\n\napply(x, 2, mean) # 가운데 값이 2이면 함수를 열별로 적용\n\n[1]  2  5  8 11\n\nx = array(1:12, c(3, 4))\ndim(x)\n\n[1] 3 4\n\nx = array(1:12, c(3, 4))\nsample(x) # 배열 요소를 임의로 섞어 추출\n\n [1]  2 10 11  1  9  5  4  6  3  7 12  8\n\nsample(x, 10) # 배열 요소 중 10개를 골라 추출\n\n [1] 12  6  2  4 11  5  9  3  1 10\n\nsample(x, 10, prob = c(1:12)/24) # 각 요소별 추출 확률을 달리할 수 있음\n\n [1]  5  9 10  8 12 11  7  2  3  6\n\nsample(10) # 단순히 숫자만 사용하여 샘플을 만들 수 있음\n\n [1]  3  6  9 10  1  7  2  4  5  8\n\n\n\n# 07 데이터 프레임 #\nname = c(\"철수\", \"춘향\", \"길동\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients = data.frame(name, age, gender, blood.type)\npatients\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\n# 다음과 같이 한 행으로 작성할 수도 있음\npatients1 = data.frame(name = c(\"철수\", \"춘향\", \"길동\"), age = c(22, 20, 25), gender = factor(c(\"M\", \"F\", \"M\")), blood.type = factor(c(\"A\", \"O\", \"B\")))\npatients1\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\npatients$name # name 속성 값 출력\n\n[1] \"철수\" \"춘향\" \"길동\"\n\npatients[1, ] # 1행 값 출력\n\n  name age gender blood.type\n1 철수  22      M          A\n\npatients[, 2] # 2열 값 출력\n\n[1] 22 20 25\n\npatients[3, 1] # 3행 1열 값 출력\n\n[1] \"길동\"\n\npatients[patients$name==\"철수\", ] # 환자 중 철수에 대한 정보 추출\n\n  name age gender blood.type\n1 철수  22      M          A\n\npatients[patients$name==\"철수\", c(\"name\", \"age\")] # 철수 이름과 나이 정보만 추출\n\n  name age\n1 철수  22\n\nhead(cars) # cars 데이터 셋 확인. head 함수의 기본 기능은 앞 6개 데이터를 추출함\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# speed\nattach(cars) # attach 함수를 통해 cars의 각 속성을 변수로 이용하게 함\n# speed # speed라는 변수명을 직접 이용할 수 있음.\ndetach(cars) # detach 함수를 통해 cars의 각 속성을 변수로 사용하는 것을 해제함\n\n\n# 데이터 속성을 이용해 함수 적용\nmean(cars$speed)\n\n[1] 15.4\n\nmax(cars$speed)\n\n[1] 25\n\n# with 함수를 이용해 함수 적용\nwith(cars, mean(speed))\n\n[1] 15.4\n\nwith(cars, max(speed))\n\n[1] 25\n\n# 속도가 20 초과인 데이터만 추출\nsubset(cars, speed &gt; 20)\n\n   speed dist\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\n# 속도가 20 초과인 dist 데이터만 추출, 여러 열 선택은 c( ) 안을 ,로 구분\nsubset(cars, speed &gt; 20, select = c(dist))\n\n   dist\n44   66\n45   54\n46   70\n47   92\n48   93\n49  120\n50   85\n\n# 속도가 20 초과인 데이터 중 dist를 제외한 데이터만 추출\nsubset(cars, speed &gt; 20, select = -c(dist))\n\n   speed\n44    22\n45    23\n46    24\n47    24\n48    24\n49    24\n50    25\n\nhead(airquality) # airquality 데이터에는 NA가 포함되어 있음\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nhead(na.omit(airquality)) # NA가 포함된 값을 제외하여 추출함\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n7    23     299  8.6   65     5   7\n8    19      99 13.8   59     5   8\n\n# merge(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by, all = FALSE, all.x = all, all.y = all, sort = TRUE, suffixes = c(\".x\",\".y\"), incomparables = NULL, ...)\n\nname = c(\"철수\", \"춘향\", \"길동\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients1 = data.frame(name, age, gender)\npatients1\n\n  name age gender\n1 철수  22      M\n2 춘향  20      F\n3 길동  25      M\n\npatients2 = data.frame(name, blood.type)\npatients2\n\n  name blood.type\n1 철수          A\n2 춘향          O\n3 길동          B\n\npatients = merge(patients1, patients2, by = \"name\")\npatients\n\n  name age gender blood.type\n1 길동  25      M          B\n2 철수  22      M          A\n3 춘향  20      F          O\n\n# 이름이 같은 열 변수가 없다면, merge 함수의 by.x와 by.y에 합칠 때\n# 사용할 열의 속성명을 각각 기입해주어야 함\nname1 = c(\"철수\", \"춘향\", \"길동\")\nname2 = c(\"민수\", \"춘향\", \"길동\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients1 = data.frame(name1, age, gender)\npatients1\n\n  name1 age gender\n1  철수  22      M\n2  춘향  20      F\n3  길동  25      M\n\npatients2 = data.frame(name2, blood.type)\npatients2\n\n  name2 blood.type\n1  민수          A\n2  춘향          O\n3  길동          B\n\npatients = merge(patients1, patients2, by.x = \"name1\", by.y = \"name2\")\npatients\n\n  name1 age gender blood.type\n1  길동  25      M          B\n2  춘향  20      F          O\n\npatients = merge(patients1, patients2, by.x = \"name1\", by.y = \"name2\", all = TRUE)\npatients\n\n  name1 age gender blood.type\n1  길동  25      M          B\n2  민수  NA   &lt;NA&gt;          A\n3  철수  22      M       &lt;NA&gt;\n4  춘향  20      F          O\n\nx = array(1:12, c(3, 4))\nis.data.frame(x) # 현재 x는 데이터 프레임이 아님\n\n[1] FALSE\n\nas.data.frame(x)\n\n  V1 V2 V3 V4\n1  1  4  7 10\n2  2  5  8 11\n3  3  6  9 12\n\n# is.data.frame 함수를 호출하는 것만으로 x가 데이터 프레임으로 바뀌지 않음\nis.data.frame(x)\n\n[1] FALSE\n\n# as.data.frame 함수로 x를 데이터 프레임 형식으로 변환\nx = as.data.frame(x)\nx\n\n  V1 V2 V3 V4\n1  1  4  7 10\n2  2  5  8 11\n3  3  6  9 12\n\n# x가 데이터 프레임 형식으로 변환되었음을 확인\nis.data.frame(x)\n\n[1] TRUE\n\n# 데이터 프레임으로 변환 시 자동 지정되는 열 이름을 names 함수로 재지정함\nnames(x) = c(\"1st\", \"2nd\", \"3rd\", \"4th\")\nx\n\n  1st 2nd 3rd 4th\n1   1   4   7  10\n2   2   5   8  11\n3   3   6   9  12\n\n\n\n# 08 리스트 #\npatients = data.frame(name = c(\"철수\", \"춘향\", \"길동\"), age = c(22, 20, 25), gender = factor(c(\"M\", \"F\", \"M\")), blood.type = factor(c(\"A\", \"O\", \"B\")))\nno.patients = data.frame(day = c(1:6), no = c(50, 60, 55, 52, 65, 58))\n\n\n# 데이터를 단순 추가\nlistPatients = list(patients, no.patients) \nlistPatients\n\n[[1]]\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\n[[2]]\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# 각 데이터에 이름을 부여하면서 추가 \nlistPatients = list(patients=patients, no.patients = no.patients) \nlistPatients\n\n$patients\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\n$no.patients\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\nlistPatients$patients       # 요소명 입력\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\nlistPatients[[1]]               # 인덱스 입력\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\nlistPatients[[\"patients\"]]          # 요소명을 \"\"에 입력\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\nlistPatients[[\"no.patients\"]]       # 요소명을 \"\"에 입력\n\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# no.patients 요소의 평균을 구해줌\nlapply(listPatients$no.patients, mean) \n\n$day\n[1] 3.5\n\n$no\n[1] 56.66667\n\n# patients 요소의 평균을 구해줌. 숫자 형태가 아닌 것은 평균이 구해지지 않음\nlapply(listPatients$patients, mean) \n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\n\n$name\n[1] NA\n\n$age\n[1] 22.33333\n\n$gender\n[1] NA\n\n$blood.type\n[1] NA\n\nsapply(listPatients$no.patients, mean) \n\n     day       no \n 3.50000 56.66667 \n\n# sapply()의 simplify 옵션을 F로 하면 lapply() 결과와 동일한 결과를 반환함\nsapply(listPatients$no.patients, mean, simplify = F) \n\n$day\n[1] 3.5\n\n$no\n[1] 56.66667\n\n\n\n# 01 파일 읽고 쓰기 #\n\n# 파일 마지막 행에서 [Enter]를 누르지 않은 경우\nstudents = read.table(\"data_2/students1.txt\", header = T, fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\n# 파일 마지막 행에서 [Enter]를 누른 경우\nstudents = read.table(\"data_2/students2.txt\",  header = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \n\n# 읽은 파일의 구조 확인\nstr(students) \n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 파일을 있는 형태 그대로 읽음\nstudents = read.table(\"data_2/students1.txt\", header = T, as.is = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 파일을 읽을 때 문장을 요인으로 인식하지 않도록 설정\nstudents = read.table(\"data_2/students1.txt\", header = T, stringsAsFactors = F, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 구분 기호는 쉼표(,), 첫 행은 header로 인식하여 파일을 있는 그대로 읽어들이면 \n# NA로 인해 math 요소가 문장으로 인식됨\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : chr  \" 100\" \" 80\" \" 90\" \" NA\" ...\n\n# \"NA\" 문장을 결측값 NA로 처리하라고 해도 처리가 안됨. 정확한 문장은 NA 앞에 빈 칸이 있어야 하기 때문\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, na.strings = \"NA\", fileEncoding = \"CP949\", encoding = \"UTF-8\")  \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : chr  \" 100\" \" 80\" \" 90\" \" NA\" ...\n\n# \"NA\"로 정확하게 입력하자 결측값 NA로 처리되면서 math 요소가 모두 숫자로 인식됨\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, na.strings = \" NA\", fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 NA 100\n\n# strip.white에서 빈칸을 제거하면 na.string의 기본값이 \"NA\"로 설정되어 math 요소가 모두 숫자로 인식됨.\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, strip.white = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 NA 100\n\n# 첫 행이 header이므로 header 옵션을 지정할 필요가 없음\nstudents = read.csv(\"data_2/students.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstudents\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     100   80\n3 박정원     90      95   90\n4 이상훈    100      85   95\n5 최건우     85     100  100\n\n# 읽은 파일의 구조 확인\nstr(students) \n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# name 속성을 요인에서 문장으로 변경\nstudents$name = as.character(students$name) \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 파일을 읽을 때 문장을 요인으로 인식하지 않도록 설정함\nstudents = read.csv(\"data_2/students.csv\", stringsAsFactors = FALSE, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 문장에 큰따옴표가 표시됨.\nwrite.table(students, file = \"data_2/output.txt\") \n\n# 문장에 큰따옴표되지 않음.\nwrite.table(students, file = \"data_2/output.txt\", quote = F) \n\n\n# 02 데이터 정제를 위한 조건문과 반복문 #\n\ntest = c(15, 20, 30, NA, 45)    # 벡터인 경우\ntest[test&lt;40]   # 값이 40 미만인 요소 추출\n\n[1] 15 20 30 NA\n\ntest[test%%3!= 0]   # 값이 3으로 나누어 떨어지지 않는 요소 추출\n\n[1] 20 NA\n\ntest[is.na(test)]   # NA인 요소 추출\n\n[1] NA\n\ntest[!is.na(test)]          # NA가 아닌 요소 추출\n\n[1] 15 20 30 45\n\ntest[test%%2==0 & !is.na(test)] # 2의 배수면서 NA가 아닌 요소 추출\n\n[1] 20 30\n\ncharacters = data.frame(name = c(\"길동\", \"춘향\", \"철수\"), \n                        age = c(30, 16, 21), \n                        gender = factor(c(\"M\", \"F\",\"M\")))  \n# 데이터 프레임인 경우\n\ncharacters\n\n  name age gender\n1 길동  30      M\n2 춘향  16      F\n3 철수  21      M\n\ncharacters[characters$gender ==\"F\",1]  # 성별이 여성인 행 추출\n\n[1] \"춘향\"\n\nlibrary(dplyr)\n\ncharacters %&gt;% filter(gender==\"F\") %&gt;% select(name)\n\n  name\n1 춘향\n\ncharacters[characters$age&lt;30 & characters$gender ==\"M\",] \n\n  name age gender\n3 철수  21      M\n\n# 30살 미만의 남성 행 추출                    \ncharacters %&gt;% filter(age&lt;30 & gender==\"M\")\n\n  name age gender\n1 철수  21      M\n\nx = 5\nif(x %% 2 ==0) {\n  print('x는 짝수')    # 조건식이 참일 때 수행\n}   else {\n  print('x는 홀수')    # 조건식이 거짓일 때 수행\n}\n\n[1] \"x는 홀수\"\n\nx = 8\nif(x&gt;0) {\n  print('x is a positive value.')   # x가 0보다 크면 출력\n} else if(x&lt;0) {\n  print('x is a negative value.')   # 위 조건을 만족하지 않고 x가 0보다 작으면 출력\n} else {\n  print('x is zero.')       # 위 조건을 모두 만족하지 않으면 출력\n}\n\n[1] \"x is a positive value.\"\n\nx = c(-5:5)\noptions(digits = 3)     # 숫자 표현 시 유효자릿수를 3자리로 설정\nsqrt(x)\n\nWarning in sqrt(x): NaNs produced\n\n\n [1]  NaN  NaN  NaN  NaN  NaN 0.00 1.00 1.41 1.73 2.00 2.24\n\nsqrt(ifelse(x&gt;=0, x, NA))   # NaN이 발생하지 않게 음수면 NA로 표시\n\n [1]   NA   NA   NA   NA   NA 0.00 1.00 1.41 1.73 2.00 2.24\n\nstudents = read.csv(\"data_2/students2.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\nstudents         # 데이터에 100 초과 값과 음수 값이 포함되어 있음.\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     120   80\n3 박정원     90      95   90\n4 이상훈    100      85 -100\n5 최건우     85     100  100\n\nstudents[, 2] = ifelse(students[, 2]&gt;= 0 & students[, 2]&lt;= 100, \n                       students[, 2], NA)\nstudents[, 3] = ifelse(students[, 3]&gt;= 0 & students[, 3]&lt;= 100, \n                       students[, 3], NA)\nstudents[, 4] = ifelse(students[, 4]&gt;= 0 & students[, 4]&lt;= 100, \n                       students[, 4], NA)\nstudents         # ifelse 문으로 2~4열 값 중 0~100 외의 값은 NA로 처리함.\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\n# repeat 문을 이용해 1부터 10까지 숫자 증가시키기\ni = 1                # i의 시작값은 1\nrepeat {\n  if(i&gt;10) {         # i가 10을 넘으면 반복을 중단(break)함\n    break\n  } else {\n    print(i)\n    i = i+1           # i를 1 증가시킴.\n  }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\n# while 문을 이용해 1부터 10까지 숫자 증가시키기\ni = 1 # i의 시작값은 1임.\nwhile(i &lt; 10){ # i가 10 이하인 동안에 반복함\n  print(i)\n  i = i+1 # i를 1 증가시킴.\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n# while 문을 이용해 구구단 2단 만들기\ni = 1\nwhile(i&lt;10) {\n  print(paste(2, \"X\", i, \"=\", 2*i))\n  i = i+1\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n\n# for 문을 이용한 1부터 10까지 숫자 증가시키기\nfor(i in 1:10) {\n  print(i)\n}  \n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n# for 문을 이용해 구구단 2단 만들기\nfor(i in 1:9) {\n  print(paste(2, \"X\", i, \"=\", 2*i))\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n\n# for 문을 이용해 구구단 2~9단 만들기\nfor(i in 2:9) {\n  for(j in 1:9) {\n    print(paste(i, \"X\", j, \"=\", i*j))\n  }\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n[1] \"3 X 1 = 3\"\n[1] \"3 X 2 = 6\"\n[1] \"3 X 3 = 9\"\n[1] \"3 X 4 = 12\"\n[1] \"3 X 5 = 15\"\n[1] \"3 X 6 = 18\"\n[1] \"3 X 7 = 21\"\n[1] \"3 X 8 = 24\"\n[1] \"3 X 9 = 27\"\n[1] \"4 X 1 = 4\"\n[1] \"4 X 2 = 8\"\n[1] \"4 X 3 = 12\"\n[1] \"4 X 4 = 16\"\n[1] \"4 X 5 = 20\"\n[1] \"4 X 6 = 24\"\n[1] \"4 X 7 = 28\"\n[1] \"4 X 8 = 32\"\n[1] \"4 X 9 = 36\"\n[1] \"5 X 1 = 5\"\n[1] \"5 X 2 = 10\"\n[1] \"5 X 3 = 15\"\n[1] \"5 X 4 = 20\"\n[1] \"5 X 5 = 25\"\n[1] \"5 X 6 = 30\"\n[1] \"5 X 7 = 35\"\n[1] \"5 X 8 = 40\"\n[1] \"5 X 9 = 45\"\n[1] \"6 X 1 = 6\"\n[1] \"6 X 2 = 12\"\n[1] \"6 X 3 = 18\"\n[1] \"6 X 4 = 24\"\n[1] \"6 X 5 = 30\"\n[1] \"6 X 6 = 36\"\n[1] \"6 X 7 = 42\"\n[1] \"6 X 8 = 48\"\n[1] \"6 X 9 = 54\"\n[1] \"7 X 1 = 7\"\n[1] \"7 X 2 = 14\"\n[1] \"7 X 3 = 21\"\n[1] \"7 X 4 = 28\"\n[1] \"7 X 5 = 35\"\n[1] \"7 X 6 = 42\"\n[1] \"7 X 7 = 49\"\n[1] \"7 X 8 = 56\"\n[1] \"7 X 9 = 63\"\n[1] \"8 X 1 = 8\"\n[1] \"8 X 2 = 16\"\n[1] \"8 X 3 = 24\"\n[1] \"8 X 4 = 32\"\n[1] \"8 X 5 = 40\"\n[1] \"8 X 6 = 48\"\n[1] \"8 X 7 = 56\"\n[1] \"8 X 8 = 64\"\n[1] \"8 X 9 = 72\"\n[1] \"9 X 1 = 9\"\n[1] \"9 X 2 = 18\"\n[1] \"9 X 3 = 27\"\n[1] \"9 X 4 = 36\"\n[1] \"9 X 5 = 45\"\n[1] \"9 X 6 = 54\"\n[1] \"9 X 7 = 63\"\n[1] \"9 X 8 = 72\"\n[1] \"9 X 9 = 81\"\n\n# 1부터 10까지의 수 중 짝수만 출력하기\nfor(i in 1:10) {\n  if(i%%2 == 0) {\n    print(i)\n  }\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n# 1부터 10까지의 수 중 소수 출력하기\nfor(i in 1:10) {\n  check = 0\n  for(j in 1:i) {\n    if(i%%j ==0) {\n      check = check+1\n    }\n  }\n  if(check ==2) { \n    print(i)\n  }\n}\n\n[1] 2\n[1] 3\n[1] 5\n[1] 7\n\nstudents = read.csv(\"data_2/students2.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\nstudents        # 데이터에 100 초과 값과 음수 값이 포함되어 있음\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     120   80\n3 박정원     90      95   90\n4 이상훈    100      85 -100\n5 최건우     85     100  100\n\nfor(i in 2:4) {\n  students[, i] = ifelse(students[, i]&gt;= 0 & students[, i]&lt;= 100, \n                         students[, i], NA)\n}\n\n\nstudents        # ifelse 문으로 2~4열 값 중 0~100 외의 값은 NA로 처리함\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\n# 03 사용자 정의 함수 : 원하는 기능 묶기 # \nx=5\nfa = 1  # 계승값을 저장할 변수\nwhile(x&gt;1) {  # x가 1보다 큰 동안 반복\n  \n  fa = fa*x   # x 값을 fa에 곱한 후 fa에 다시 저장\n  x = x-1  # x 값을 1 감소\n  x\n}  \nfa\n\n[1] 120\n\nfact = function(x) {   # 함수의 이름은 fact, 입력은 x\n  fa = 1  # 계승값을 저장할 변수\n  while(x&gt;1) {  # x가 1보다 큰 동안 반복\n    fa = fa*x   # x 값을 fa에 곱한 후 fa에 다시 저장\n    x = x-1  # x 값을 1 감소\n  }  \n  return(fa)   # 최종 계산된 fa 반환\n}\nfact(5)   # 5!을 계산한 결과 출력\n\n[1] 120\n\nmy.is.na&lt;-function(x) { # table(is.na()) 함수를 하나로 묶은 my.is.na 함수를 만듦\n  table(is.na(x))\n}\n\nmy.is.na(airquality)    # 이 결과는 table(is.na(airquality))와 같음.\n\n\nFALSE  TRUE \n  874    44 \n\ntable(is.na(airquality))\n\n\nFALSE  TRUE \n  874    44 \n\n\n\n# 04 데이터 정제 예제 1 : 결측값 처리 # \n\n# is.na 함수를 이용해 결측값 처리하기\nstr(airquality) # airquality 데이터의 구조를 살펴봄.\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n# airquality 데이터에서 NA인 것은 TRUE, 아니면 FALSE로 나타냄. 데이터가 많아 head 함수로 추려냄.\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nhead(is.na(airquality)) \n\n     Ozone Solar.R  Wind  Temp Month   Day\n[1,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[5,]  TRUE    TRUE FALSE FALSE FALSE FALSE\n[6,] FALSE    TRUE FALSE FALSE FALSE FALSE\n\ntable(is.na(airquality))    # NA가 총 44개 있음.\n\n\nFALSE  TRUE \n  874    44 \n\nsum(is.na(airquality))  # NA가 총 44개 있음.\n\n[1] 44\n\ntable(is.na(airquality$Temp))   # Temp에는 NA가 없음을 확인함.\n\n\nFALSE \n  153 \n\ntable(is.na(airquality$Ozone))  # Ozone에는 NA가 37개 발견됨.\n\n\nFALSE  TRUE \n  116    37 \n\nmean(airquality$Temp)       # NA가 없는 Temp는 평균이 구해짐.\n\n[1] 77.9\n\nmean(airquality$Ozone)      # NA가 있는 Ozone은 평균이 NA로 나옴.\n\n[1] NA\n\nair_narm = airquality[!is.na(airquality$Ozone), ] # Ozone 속성에서 NA가 없는 값만 추출함. \nair_narm\n\n    Ozone Solar.R Wind Temp Month Day\n1      41     190  7.4   67     5   1\n2      36     118  8.0   72     5   2\n3      12     149 12.6   74     5   3\n4      18     313 11.5   62     5   4\n6      28      NA 14.9   66     5   6\n7      23     299  8.6   65     5   7\n8      19      99 13.8   59     5   8\n9       8      19 20.1   61     5   9\n11      7      NA  6.9   74     5  11\n12     16     256  9.7   69     5  12\n13     11     290  9.2   66     5  13\n14     14     274 10.9   68     5  14\n15     18      65 13.2   58     5  15\n16     14     334 11.5   64     5  16\n17     34     307 12.0   66     5  17\n18      6      78 18.4   57     5  18\n19     30     322 11.5   68     5  19\n20     11      44  9.7   62     5  20\n21      1       8  9.7   59     5  21\n22     11     320 16.6   73     5  22\n23      4      25  9.7   61     5  23\n24     32      92 12.0   61     5  24\n28     23      13 12.0   67     5  28\n29     45     252 14.9   81     5  29\n30    115     223  5.7   79     5  30\n31     37     279  7.4   76     5  31\n38     29     127  9.7   82     6   7\n40     71     291 13.8   90     6   9\n41     39     323 11.5   87     6  10\n44     23     148  8.0   82     6  13\n47     21     191 14.9   77     6  16\n48     37     284 20.7   72     6  17\n49     20      37  9.2   65     6  18\n50     12     120 11.5   73     6  19\n51     13     137 10.3   76     6  20\n62    135     269  4.1   84     7   1\n63     49     248  9.2   85     7   2\n64     32     236  9.2   81     7   3\n66     64     175  4.6   83     7   5\n67     40     314 10.9   83     7   6\n68     77     276  5.1   88     7   7\n69     97     267  6.3   92     7   8\n70     97     272  5.7   92     7   9\n71     85     175  7.4   89     7  10\n73     10     264 14.3   73     7  12\n74     27     175 14.9   81     7  13\n76      7      48 14.3   80     7  15\n77     48     260  6.9   81     7  16\n78     35     274 10.3   82     7  17\n79     61     285  6.3   84     7  18\n80     79     187  5.1   87     7  19\n81     63     220 11.5   85     7  20\n82     16       7  6.9   74     7  21\n85     80     294  8.6   86     7  24\n86    108     223  8.0   85     7  25\n87     20      81  8.6   82     7  26\n88     52      82 12.0   86     7  27\n89     82     213  7.4   88     7  28\n90     50     275  7.4   86     7  29\n91     64     253  7.4   83     7  30\n92     59     254  9.2   81     7  31\n93     39      83  6.9   81     8   1\n94      9      24 13.8   81     8   2\n95     16      77  7.4   82     8   3\n96     78      NA  6.9   86     8   4\n97     35      NA  7.4   85     8   5\n98     66      NA  4.6   87     8   6\n99    122     255  4.0   89     8   7\n100    89     229 10.3   90     8   8\n101   110     207  8.0   90     8   9\n104    44     192 11.5   86     8  12\n105    28     273 11.5   82     8  13\n106    65     157  9.7   80     8  14\n108    22      71 10.3   77     8  16\n109    59      51  6.3   79     8  17\n110    23     115  7.4   76     8  18\n111    31     244 10.9   78     8  19\n112    44     190 10.3   78     8  20\n113    21     259 15.5   77     8  21\n114     9      36 14.3   72     8  22\n116    45     212  9.7   79     8  24\n117   168     238  3.4   81     8  25\n118    73     215  8.0   86     8  26\n120    76     203  9.7   97     8  28\n121   118     225  2.3   94     8  29\n122    84     237  6.3   96     8  30\n123    85     188  6.3   94     8  31\n124    96     167  6.9   91     9   1\n125    78     197  5.1   92     9   2\n126    73     183  2.8   93     9   3\n127    91     189  4.6   93     9   4\n128    47      95  7.4   87     9   5\n129    32      92 15.5   84     9   6\n130    20     252 10.9   80     9   7\n131    23     220 10.3   78     9   8\n132    21     230 10.9   75     9   9\n133    24     259  9.7   73     9  10\n134    44     236 14.9   81     9  11\n135    21     259 15.5   76     9  12\n136    28     238  6.3   77     9  13\n137     9      24 10.9   71     9  14\n138    13     112 11.5   71     9  15\n139    46     237  6.9   78     9  16\n140    18     224 13.8   67     9  17\n141    13      27 10.3   76     9  18\n142    24     238 10.3   68     9  19\n143    16     201  8.0   82     9  20\n144    13     238 12.6   64     9  21\n145    23      14  9.2   71     9  22\n146    36     139 10.3   81     9  23\n147     7      49 10.3   69     9  24\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\nmean(air_narm$Ozone)    # 결측값이 제거된 데이터에서는 mean 함수가 정상적으로 동작함.\n\n[1] 42.1\n\n# na.omit 함수를 이용해 결측값 처리하기\nair_narm1 = na.omit(airquality)\nmean(air_narm1$Ozone)\n\n[1] 42.1\n\n# 함수 속성인 na.rm을 이용해 결측값 처리하기\nmean(airquality$Ozone, na.rm = T)\n\n[1] 42.1\n\nmean(airquality$Ozone, na.rm = F)\n\n[1] NA\n\ntable(is.na(airquality))\n\n\nFALSE  TRUE \n  874    44 \n\ntable(is.na(airquality$Ozone))\n\n\nFALSE  TRUE \n  116    37 \n\ntable(is.na(airquality$Solar.R))\n\n\nFALSE  TRUE \n  146     7 \n\nair_narm = airquality[!is.na(airquality$Ozone) & !is.na(airquality$Solar.R), ]\nmean(air_narm$Ozone)\n\n[1] 42.1\n\n\n\n# 05 데이터 정제 예제 2 : 이상값 처리 # \n\n# 이상값이 포함된 환자 데이터\npatients = data.frame(name = c(\"환자1\", \"환자2\", \"환자3\", \"환자4\", \"환자5\"), age = c(22, 20, 25, 30, 27), gender=factor(c(\"M\", \"F\", \"M\", \"K\", \"F\")), blood.type = factor(c(\"A\", \"O\", \"B\", \"AB\", \"C\")))\npatients\n\n   name age gender blood.type\n1 환자1  22      M          A\n2 환자2  20      F          O\n3 환자3  25      M          B\n4 환자4  30      K         AB\n5 환자5  27      F          C\n\n# 성별에서 이상값 제거\npatients_outrm = patients[patients$gender==\"M\"|patients$gender==\"F\", ]\npatients_outrm  \n\n   name age gender blood.type\n1 환자1  22      M          A\n2 환자2  20      F          O\n3 환자3  25      M          B\n5 환자5  27      F          C\n\n# 성별과 혈액형에서 이상값 제거\npatients_outrm1 = patients[(patients$gender == \"M\"|patients$gender == \"F\") & \n                             (patients$blood.type == \"A\"|\n                                patients$blood.type == \"B\"|\n                                patients$blood.type == \"O\"|\n                                patients$blood.type == \"AB\"), ]\npatients_outrm1  \n\n   name age gender blood.type\n1 환자1  22      M          A\n2 환자2  20      F          O\n3 환자3  25      M          B\n\n# 이상값이 포함된 환자 데이터\npatients = data.frame(name = c(\"환자1\", \"환자2\", \"환자3\", \"환자4\", \"환자5\"), \n                      age = c(22, 20, 25, 30, 27), \n                      gender = c(1, 2, 1, 3, 2), \n                      blood.type = c(1, 3, 2, 4, 5))\npatients    \n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n4 환자4  30      3          4\n5 환자5  27      2          5\n\n# 성별에 있는 이상값을 결측값으로 변경\npatients$gender = ifelse((patients$gender&lt;1|patients$gender&gt;2), NA, patients$gender)\npatients    \n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n4 환자4  30     NA          4\n5 환자5  27      2          5\n\n# 형액형에 있는 이상값도 결측값으로 변경\npatients$blood.type = ifelse((patients$blood.type&lt;1|patients$blood.type&gt;4), NA, \n                             patients$blood.type)\npatients\n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n4 환자4  30     NA          4\n5 환자5  27      2         NA\n\n# 결측값을 모두 제거\npatients[!is.na(patients$gender)&!is.na(patients$blood.type), ]\n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n\nboxplot(airquality[, c(1:4)])    # Ozone, Solar.R, Wind, Temp에 대한 boxplot\n\n\n\nboxplot(airquality[, 1])$stats   # Ozone의 boxplot 통계값 계산\n\n\n\n\n      [,1]\n[1,]   1.0\n[2,]  18.0\n[3,]  31.5\n[4,]  63.5\n[5,] 122.0\n\nair = airquality                 # 임시 저장 변수로 airquality 데이터 복사\ntable(is.na(air$Ozone))          # Ozone의 현재 NA 개수 확인\n\n\nFALSE  TRUE \n  116    37 \n\n# 이상값을 NA로 변경\nair$Ozone = ifelse(air$Ozone&lt;1|air$Ozone&gt;122, NA, air$Ozone) \ntable(is.na(air$Ozone)) # 이상값 처리 후 NA 개수 확인(2개 증가)\n\n\nFALSE  TRUE \n  114    39 \n\n# NA 제거\nair_narm = air[!is.na(air$Ozone), ] \nmean(air_narm$Ozone) # 이상값 두 개 제거로 is.na 함수를 이용한 결과보다 값이 줄어듦\n\n[1] 40.2\n\n\n\n# 02 베이스 R을 이용한 데이터 가공 # \n\nlibrary(gapminder) \nlibrary(dplyr)\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.8, 30.3, 32.0, 34.0, 36.1, 38.4, 39.9, 40.8, 41.7, 41.8, …\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779, 821, 853, 836, 740, 786, 978, 852, 649, 635, 727, 975, …\n\ngapminder[, c(\"country\", \"lifeExp\")]\n\n# A tibble: 1,704 × 2\n   country     lifeExp\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 Afghanistan    28.8\n 2 Afghanistan    30.3\n 3 Afghanistan    32.0\n 4 Afghanistan    34.0\n 5 Afghanistan    36.1\n 6 Afghanistan    38.4\n 7 Afghanistan    39.9\n 8 Afghanistan    40.8\n 9 Afghanistan    41.7\n10 Afghanistan    41.8\n# ℹ 1,694 more rows\n\ngapminder[, c(\"country\", \"lifeExp\", \"year\")]\n\n# A tibble: 1,704 × 3\n   country     lifeExp  year\n   &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 Afghanistan    28.8  1952\n 2 Afghanistan    30.3  1957\n 3 Afghanistan    32.0  1962\n 4 Afghanistan    34.0  1967\n 5 Afghanistan    36.1  1972\n 6 Afghanistan    38.4  1977\n 7 Afghanistan    39.9  1982\n 8 Afghanistan    40.8  1987\n 9 Afghanistan    41.7  1992\n10 Afghanistan    41.8  1997\n# ℹ 1,694 more rows\n\ngapminder[1:15, ]\n\n# A tibble: 15 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n11 Afghanistan Asia       2002    42.1 25268405      727.\n12 Afghanistan Asia       2007    43.8 31889923      975.\n13 Albania     Europe     1952    55.2  1282697     1601.\n14 Albania     Europe     1957    59.3  1476505     1942.\n15 Albania     Europe     1962    64.8  1728137     2313.\n\nlibrary(dplyr)\ngapminder %&gt;% filter(country==\"Croatia\") %&gt;% select(year, gdpPercap) %&gt;% plot\n\n\n\ngapminder[gapminder$country == \"Croatia\", ]\n\n# A tibble: 12 × 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Croatia Europe     1952    61.2 3882229     3119.\n 2 Croatia Europe     1957    64.8 3991242     4338.\n 3 Croatia Europe     1962    67.1 4076557     5478.\n 4 Croatia Europe     1967    68.5 4174366     6960.\n 5 Croatia Europe     1972    69.6 4225310     9164.\n 6 Croatia Europe     1977    70.6 4318673    11305.\n 7 Croatia Europe     1982    70.5 4413368    13222.\n 8 Croatia Europe     1987    71.5 4484310    13823.\n 9 Croatia Europe     1992    72.5 4494013     8448.\n10 Croatia Europe     1997    73.7 4444595     9876.\n11 Croatia Europe     2002    74.9 4481020    11628.\n12 Croatia Europe     2007    75.7 4493312    14619.\n\ngapminder[gapminder$country == \"Korea, Rep.\", ]\n\n# A tibble: 12 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Korea, Rep. Asia       1952    47.5 20947571     1031.\n 2 Korea, Rep. Asia       1957    52.7 22611552     1488.\n 3 Korea, Rep. Asia       1962    55.3 26420307     1536.\n 4 Korea, Rep. Asia       1967    57.7 30131000     2029.\n 5 Korea, Rep. Asia       1972    62.6 33505000     3031.\n 6 Korea, Rep. Asia       1977    64.8 36436000     4657.\n 7 Korea, Rep. Asia       1982    67.1 39326000     5623.\n 8 Korea, Rep. Asia       1987    69.8 41622000     8533.\n 9 Korea, Rep. Asia       1992    72.2 43805450    12104.\n10 Korea, Rep. Asia       1997    74.6 46173816    15994.\n11 Korea, Rep. Asia       2002    77.0 47969150    19234.\n12 Korea, Rep. Asia       2007    78.6 49044790    23348.\n\n\"Korea, Rep.\"\n\n[1] \"Korea, Rep.\"\n\nlevels(gapminder$country)\n\n  [1] \"Afghanistan\"              \"Albania\"                 \n  [3] \"Algeria\"                  \"Angola\"                  \n  [5] \"Argentina\"                \"Australia\"               \n  [7] \"Austria\"                  \"Bahrain\"                 \n  [9] \"Bangladesh\"               \"Belgium\"                 \n [11] \"Benin\"                    \"Bolivia\"                 \n [13] \"Bosnia and Herzegovina\"   \"Botswana\"                \n [15] \"Brazil\"                   \"Bulgaria\"                \n [17] \"Burkina Faso\"             \"Burundi\"                 \n [19] \"Cambodia\"                 \"Cameroon\"                \n [21] \"Canada\"                   \"Central African Republic\"\n [23] \"Chad\"                     \"Chile\"                   \n [25] \"China\"                    \"Colombia\"                \n [27] \"Comoros\"                  \"Congo, Dem. Rep.\"        \n [29] \"Congo, Rep.\"              \"Costa Rica\"              \n [31] \"Cote d'Ivoire\"            \"Croatia\"                 \n [33] \"Cuba\"                     \"Czech Republic\"          \n [35] \"Denmark\"                  \"Djibouti\"                \n [37] \"Dominican Republic\"       \"Ecuador\"                 \n [39] \"Egypt\"                    \"El Salvador\"             \n [41] \"Equatorial Guinea\"        \"Eritrea\"                 \n [43] \"Ethiopia\"                 \"Finland\"                 \n [45] \"France\"                   \"Gabon\"                   \n [47] \"Gambia\"                   \"Germany\"                 \n [49] \"Ghana\"                    \"Greece\"                  \n [51] \"Guatemala\"                \"Guinea\"                  \n [53] \"Guinea-Bissau\"            \"Haiti\"                   \n [55] \"Honduras\"                 \"Hong Kong, China\"        \n [57] \"Hungary\"                  \"Iceland\"                 \n [59] \"India\"                    \"Indonesia\"               \n [61] \"Iran\"                     \"Iraq\"                    \n [63] \"Ireland\"                  \"Israel\"                  \n [65] \"Italy\"                    \"Jamaica\"                 \n [67] \"Japan\"                    \"Jordan\"                  \n [69] \"Kenya\"                    \"Korea, Dem. Rep.\"        \n [71] \"Korea, Rep.\"              \"Kuwait\"                  \n [73] \"Lebanon\"                  \"Lesotho\"                 \n [75] \"Liberia\"                  \"Libya\"                   \n [77] \"Madagascar\"               \"Malawi\"                  \n [79] \"Malaysia\"                 \"Mali\"                    \n [81] \"Mauritania\"               \"Mauritius\"               \n [83] \"Mexico\"                   \"Mongolia\"                \n [85] \"Montenegro\"               \"Morocco\"                 \n [87] \"Mozambique\"               \"Myanmar\"                 \n [89] \"Namibia\"                  \"Nepal\"                   \n [91] \"Netherlands\"              \"New Zealand\"             \n [93] \"Nicaragua\"                \"Niger\"                   \n [95] \"Nigeria\"                  \"Norway\"                  \n [97] \"Oman\"                     \"Pakistan\"                \n [99] \"Panama\"                   \"Paraguay\"                \n[101] \"Peru\"                     \"Philippines\"             \n[103] \"Poland\"                   \"Portugal\"                \n[105] \"Puerto Rico\"              \"Reunion\"                 \n[107] \"Romania\"                  \"Rwanda\"                  \n[109] \"Sao Tome and Principe\"    \"Saudi Arabia\"            \n[111] \"Senegal\"                  \"Serbia\"                  \n[113] \"Sierra Leone\"             \"Singapore\"               \n[115] \"Slovak Republic\"          \"Slovenia\"                \n[117] \"Somalia\"                  \"South Africa\"            \n[119] \"Spain\"                    \"Sri Lanka\"               \n[121] \"Sudan\"                    \"Swaziland\"               \n[123] \"Sweden\"                   \"Switzerland\"             \n[125] \"Syria\"                    \"Taiwan\"                  \n[127] \"Tanzania\"                 \"Thailand\"                \n[129] \"Togo\"                     \"Trinidad and Tobago\"     \n[131] \"Tunisia\"                  \"Turkey\"                  \n[133] \"Uganda\"                   \"United Kingdom\"          \n[135] \"United States\"            \"Uruguay\"                 \n[137] \"Venezuela\"                \"Vietnam\"                 \n[139] \"West Bank and Gaza\"       \"Yemen, Rep.\"             \n[141] \"Zambia\"                   \"Zimbabwe\"                \n\ngapminder[gapminder$country == \"Croatia\", \"pop\"]\n\n# A tibble: 12 × 1\n       pop\n     &lt;int&gt;\n 1 3882229\n 2 3991242\n 3 4076557\n 4 4174366\n 5 4225310\n 6 4318673\n 7 4413368\n 8 4484310\n 9 4494013\n10 4444595\n11 4481020\n12 4493312\n\ngapminder[gapminder$country == \"Croatia\", c(\"lifeExp\",\"pop\")]\n\n# A tibble: 12 × 2\n   lifeExp     pop\n     &lt;dbl&gt;   &lt;int&gt;\n 1    61.2 3882229\n 2    64.8 3991242\n 3    67.1 4076557\n 4    68.5 4174366\n 5    69.6 4225310\n 6    70.6 4318673\n 7    70.5 4413368\n 8    71.5 4484310\n 9    72.5 4494013\n10    73.7 4444595\n11    74.9 4481020\n12    75.7 4493312\n\ngapminder[gapminder$country == \"Croatia\" & #Croatia extraction\n            gapminder$year &gt; 1990, #1990 after\n          c(\"lifeExp\",\"pop\")] # those variables \n\n# A tibble: 4 × 2\n  lifeExp     pop\n    &lt;dbl&gt;   &lt;int&gt;\n1    72.5 4494013\n2    73.7 4444595\n3    74.9 4481020\n4    75.7 4493312\n\napply(gapminder[gapminder$country == \"Croatia\", \n                c(\"lifeExp\",\"pop\")], \n      2, mean)\n\n lifeExp      pop \n7.01e+01 4.29e+06 \n\napply(gapminder[gapminder$country == \"Korea, Rep.\", \n                c(\"lifeExp\",\"pop\")], \n      2, mean)\n\n lifeExp      pop \n      65 36499386 \n\n# 03 dplyr 라이브러리를 이용한 데이터 가공 # \nselect(gapminder, country, year, lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\nfilter(gapminder, country == \"Croatia\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Croatia Europe     1952    61.2 3882229     3119.\n 2 Croatia Europe     1957    64.8 3991242     4338.\n 3 Croatia Europe     1962    67.1 4076557     5478.\n 4 Croatia Europe     1967    68.5 4174366     6960.\n 5 Croatia Europe     1972    69.6 4225310     9164.\n 6 Croatia Europe     1977    70.6 4318673    11305.\n 7 Croatia Europe     1982    70.5 4413368    13222.\n 8 Croatia Europe     1987    71.5 4484310    13823.\n 9 Croatia Europe     1992    72.5 4494013     8448.\n10 Croatia Europe     1997    73.7 4444595     9876.\n11 Croatia Europe     2002    74.9 4481020    11628.\n12 Croatia Europe     2007    75.7 4493312    14619.\n\nsummarise(gapminder, pop_avg = mean(pop))\n\n# A tibble: 1 × 1\n    pop_avg\n      &lt;dbl&gt;\n1 29601212.\n\nsummarise(group_by(gapminder, continent), pop_avg = mean(pop))\n\n# A tibble: 5 × 2\n  continent   pop_avg\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Africa     9916003.\n2 Americas  24504795.\n3 Asia      77038722.\n4 Europe    17169765.\n5 Oceania    8874672.\n\nsummarise(group_by(gapminder, continent, country), pop_avg = mean(pop))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 142 × 3\n# Groups:   continent [5]\n   continent country                    pop_avg\n   &lt;fct&gt;     &lt;fct&gt;                        &lt;dbl&gt;\n 1 Africa    Algeria                  19875406.\n 2 Africa    Angola                    7309390.\n 3 Africa    Benin                     4017497.\n 4 Africa    Botswana                   971186.\n 5 Africa    Burkina Faso              7548677.\n 6 Africa    Burundi                   4651608.\n 7 Africa    Cameroon                  9816648.\n 8 Africa    Central African Republic  2560963 \n 9 Africa    Chad                      5329256.\n10 Africa    Comoros                    361684.\n# ℹ 132 more rows\n\ngapminder %&gt;% \n  group_by(continent, country) %&gt;% \n  summarise(pop_avg = mean(pop))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 142 × 3\n# Groups:   continent [5]\n   continent country                    pop_avg\n   &lt;fct&gt;     &lt;fct&gt;                        &lt;dbl&gt;\n 1 Africa    Algeria                  19875406.\n 2 Africa    Angola                    7309390.\n 3 Africa    Benin                     4017497.\n 4 Africa    Botswana                   971186.\n 5 Africa    Burkina Faso              7548677.\n 6 Africa    Burundi                   4651608.\n 7 Africa    Cameroon                  9816648.\n 8 Africa    Central African Republic  2560963 \n 9 Africa    Chad                      5329256.\n10 Africa    Comoros                    361684.\n# ℹ 132 more rows\n\ntemp1 = filter(gapminder, country == \"Croatia\")      \ntemp2 = select(temp1, country, year, lifeExp)  \ntemp3 = apply(temp2[ , c(\"lifeExp\")], 2, mean)\ntemp3\n\nlifeExp \n   70.1 \n\ngapminder %&gt;% \n  filter(country == \"Croatia\") %&gt;% \n  select(country, year, lifeExp) %&gt;% \n  summarise(lifeExp_avg = mean(lifeExp))\n\n# A tibble: 1 × 1\n  lifeExp_avg\n        &lt;dbl&gt;\n1        70.1\n\n\n\n# 04 데이터 가공의 실제 # \navocado &lt;- read.csv(\"data_2/avocado.csv\", header=TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\nstr(avocado)\n\n'data.frame':   18249 obs. of  14 variables:\n $ X           : int  0 1 2 3 4 5 6 7 8 9 ...\n $ Date        : chr  \"2015-12-27\" \"2015-12-20\" \"2015-12-13\" \"2015-12-06\" ...\n $ AveragePrice: num  1.33 1.35 0.93 1.08 1.28 1.26 0.99 0.98 1.02 1.07 ...\n $ Total.Volume: num  64237 54877 118220 78992 51040 ...\n $ X4046       : num  1037 674 795 1132 941 ...\n $ X4225       : num  54455 44639 109150 71976 43838 ...\n $ X4770       : num  48.2 58.3 130.5 72.6 75.8 ...\n $ Total.Bags  : num  8697 9506 8145 5811 6184 ...\n $ Small.Bags  : num  8604 9408 8042 5677 5986 ...\n $ Large.Bags  : num  93.2 97.5 103.1 133.8 197.7 ...\n $ XLarge.Bags : num  0 0 0 0 0 0 0 0 0 0 ...\n $ type        : chr  \"conventional\" \"conventional\" \"conventional\" \"conventional\" ...\n $ year        : int  2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 ...\n $ region      : chr  \"Albany\" \"Albany\" \"Albany\" \"Albany\" ...\n\n(x_avg = avocado %&gt;% group_by(region) %&gt;% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice)))\n\n# A tibble: 54 × 3\n   region                 V_avg P_avg\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n 1 Albany                47538.  1.56\n 2 Atlanta              262145.  1.34\n 3 BaltimoreWashington  398562.  1.53\n 4 Boise                 42643.  1.35\n 5 Boston               287793.  1.53\n 6 BuffaloRochester      67936.  1.52\n 7 California          3044324.  1.40\n 8 Charlotte            105194.  1.61\n 9 Chicago              395569.  1.56\n10 CincinnatiDayton     131722.  1.21\n# ℹ 44 more rows\n\n(x_avg = avocado %&gt;% group_by(region, year) %&gt;% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice)))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 216 × 4\n# Groups:   region [54]\n   region               year   V_avg P_avg\n   &lt;chr&gt;               &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Albany               2015  38749.  1.54\n 2 Albany               2016  50619.  1.53\n 3 Albany               2017  49355.  1.64\n 4 Albany               2018  64249.  1.44\n 5 Atlanta              2015 223382.  1.38\n 6 Atlanta              2016 272374.  1.21\n 7 Atlanta              2017 271841.  1.43\n 8 Atlanta              2018 342976.  1.29\n 9 BaltimoreWashington  2015 390823.  1.37\n10 BaltimoreWashington  2016 393210.  1.59\n# ℹ 206 more rows\n\nx_avg = avocado %&gt;% group_by(region, year, type) %&gt;% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice))\n\n`summarise()` has grouped output by 'region', 'year'. You can override using\nthe `.groups` argument.\n\navocado %&gt;% \n  group_by(region, year, type) %&gt;% \n  summarize(V_avg = mean(Total.Volume), \n            P_avg = mean(AveragePrice)) -&gt; x_avg\n\n`summarise()` has grouped output by 'region', 'year'. You can override using\nthe `.groups` argument.\n\nx_avg %&gt;% filter(region != \"TotalUS\") %&gt;% ggplot(aes(year, V_avg, col = type)) + geom_line() + facet_wrap(~region)\n\n\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\narrange(x_avg, desc(V_avg))\n\n# A tibble: 432 × 5\n# Groups:   region, year [216]\n   region        year type             V_avg P_avg\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 TotalUS       2018 conventional 42125533. 1.06 \n 2 TotalUS       2016 conventional 34043450. 1.05 \n 3 TotalUS       2017 conventional 33995658. 1.22 \n 4 TotalUS       2015 conventional 31224729. 1.01 \n 5 SouthCentral  2018 conventional  7465557. 0.806\n 6 West          2018 conventional  7451445. 0.981\n 7 California    2018 conventional  6786962. 1.08 \n 8 West          2016 conventional  6404892. 0.916\n 9 West          2017 conventional  6279482. 1.10 \n10 California    2016 conventional  6105539. 1.05 \n# ℹ 422 more rows\n\nx_avg1 = x_avg %&gt;% filter(region != \"TotalUS\")\n\n\nwine &lt;- read.table(\"data_2/wine.data.txt\", header = TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\nhead(wine)\n\n  X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04 X3.92\n1  1   13.2  1.78  2.14  11.2  100 2.65  2.76 0.26  1.28  4.38  1.05  3.40\n2  1   13.2  2.36  2.67  18.6  101 2.80  3.24 0.30  2.81  5.68  1.03  3.17\n3  1   14.4  1.95  2.50  16.8  113 3.85  3.49 0.24  2.18  7.80  0.86  3.45\n4  1   13.2  2.59  2.87  21.0  118 2.80  2.69 0.39  1.82  4.32  1.04  2.93\n5  1   14.2  1.76  2.45  15.2  112 3.27  3.39 0.34  1.97  6.75  1.05  2.85\n6  1   14.4  1.87  2.45  14.6   96 2.50  2.52 0.30  1.98  5.25  1.02  3.58\n  X1065\n1  1050\n2  1185\n3  1480\n4   735\n5  1450\n6  1290\n\nn = readLines(\"data_2/wine.name.txt\")\nn\n\n [1] \"1) Alcohol\"                      \"2) Malic acid\"                  \n [3] \"3) Ash\"                          \"4) Alcalinity of ash\"           \n [5] \"5) Magnesium\"                    \"6) Total phenols\"               \n [7] \"7) Flavanoids\"                   \"8) Nonflavanoid phenols\"        \n [9] \"9) Proanthocyanins\"              \"10)Color intensity\"             \n[11] \"11)Hue\"                          \"12)OD280/OD315 of diluted wines\"\n[13] \"13)Proline\"                     \n\nnames(wine)[2:14] &lt;- substr(n, 4, nchar(n))\nnames(wine)\n\n [1] \"X1\"                           \"Alcohol\"                     \n [3] \"Malic acid\"                   \"Ash\"                         \n [5] \"Alcalinity of ash\"            \"Magnesium\"                   \n [7] \"Total phenols\"                \"Flavanoids\"                  \n [9] \"Nonflavanoid phenols\"         \"Proanthocyanins\"             \n[11] \"Color intensity\"              \"Hue\"                         \n[13] \"OD280/OD315 of diluted wines\" \"Proline\"                     \n\ntrain_set = sample_frac(wine, 0.6)\nstr(train_set)\n\n'data.frame':   106 obs. of  14 variables:\n $ X1                          : int  1 2 3 3 3 1 2 2 2 3 ...\n $ Alcohol                     : num  13.8 11.6 12.5 13.2 12.9 ...\n $ Malic acid                  : num  1.53 1.99 1.24 3.3 4.61 1.97 2.13 1.52 2.83 2.67 ...\n $ Ash                         : num  2.7 2.28 2.25 2.28 2.48 2.68 2.78 2.2 2.22 2.48 ...\n $ Alcalinity of ash           : num  19.5 18 17.5 18.5 21.5 16.8 28.5 19 18 22 ...\n $ Magnesium                   : int  132 98 85 98 86 102 92 162 88 112 ...\n $ Total phenols               : num  2.95 3.02 2 1.8 1.7 3 2.13 2.5 2.45 1.48 ...\n $ Flavanoids                  : num  2.74 2.26 0.58 0.83 0.65 3.23 2.24 2.27 2.25 1.36 ...\n $ Nonflavanoid phenols        : num  0.5 0.17 0.6 0.61 0.47 0.31 0.58 0.32 0.25 0.24 ...\n $ Proanthocyanins             : num  1.35 1.35 1.25 1.87 0.86 1.66 1.76 3.28 1.99 1.26 ...\n $ Color intensity             : num  5.4 3.25 5.45 10.52 7.65 ...\n $ Hue                         : num  1.25 1.16 0.75 0.56 0.54 1.07 0.97 1.16 1.15 0.48 ...\n $ OD280/OD315 of diluted wines: num  3 2.96 1.51 1.51 1.86 2.84 2.44 2.63 3.3 1.47 ...\n $ Proline                     : int  1235 345 650 675 625 1270 466 937 290 480 ...\n\ntest_set = setdiff(wine, train_set)\nstr(test_set)\n\n'data.frame':   71 obs. of  14 variables:\n $ X1                          : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Alcohol                     : num  14.4 14.1 13.9 14.1 14.1 ...\n $ Malic acid                  : num  1.95 2.15 1.35 2.16 1.48 1.81 1.92 1.57 1.59 1.63 ...\n $ Ash                         : num  2.5 2.61 2.27 2.3 2.32 2.7 2.72 2.62 2.48 2.28 ...\n $ Alcalinity of ash           : num  16.8 17.6 16 18 16.8 17.2 20 20 16.5 16 ...\n $ Magnesium                   : int  113 121 98 105 95 112 120 115 108 126 ...\n $ Total phenols               : num  3.85 2.6 2.98 2.95 2.2 2.85 2.8 2.95 3.3 3 ...\n $ Flavanoids                  : num  3.49 2.51 3.15 3.32 2.43 2.91 3.14 3.4 3.93 3.17 ...\n $ Nonflavanoid phenols        : num  0.24 0.31 0.22 0.22 0.26 0.3 0.33 0.4 0.32 0.24 ...\n $ Proanthocyanins             : num  2.18 1.25 1.85 2.38 1.57 1.46 1.97 1.72 1.86 2.1 ...\n $ Color intensity             : num  7.8 5.05 7.22 5.75 5 7.3 6.2 6.6 8.7 5.65 ...\n $ Hue                         : num  0.86 1.06 1.01 1.25 1.17 1.28 1.07 1.13 1.23 1.09 ...\n $ OD280/OD315 of diluted wines: num  3.45 3.58 3.55 3.17 2.82 2.88 2.65 2.57 2.82 3.71 ...\n $ Proline                     : int  1480 1295 1045 1510 1280 1310 1280 1130 1680 780 ...\n\nelec_gen = read.csv(\"data_2/electricity_generation_per_person.csv\", header = TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\nnames(elec_gen)\n\n [1] \"country\" \"X1985\"   \"X1986\"   \"X1987\"   \"X1988\"   \"X1989\"   \"X1990\"  \n [8] \"X1991\"   \"X1992\"   \"X1993\"   \"X1994\"   \"X1995\"   \"X1996\"   \"X1997\"  \n[15] \"X1998\"   \"X1999\"   \"X2000\"   \"X2001\"   \"X2002\"   \"X2003\"   \"X2004\"  \n[22] \"X2005\"   \"X2006\"   \"X2007\"   \"X2008\"   \"X2009\"   \"X2010\"   \"X2011\"  \n[29] \"X2012\"   \"X2013\"   \"X2014\"   \"X2015\"   \"X2016\"  \n\nnames(elec_gen) = substr(names(elec_gen), 2, nchar(names(elec_gen)))\nnames(elec_gen)[1]&lt;-\"country\"\n\nnames(elec_gen)\n\n [1] \"country\" \"1985\"    \"1986\"    \"1987\"    \"1988\"    \"1989\"    \"1990\"   \n [8] \"1991\"    \"1992\"    \"1993\"    \"1994\"    \"1995\"    \"1996\"    \"1997\"   \n[15] \"1998\"    \"1999\"    \"2000\"    \"2001\"    \"2002\"    \"2003\"    \"2004\"   \n[22] \"2005\"    \"2006\"    \"2007\"    \"2008\"    \"2009\"    \"2010\"    \"2011\"   \n[29] \"2012\"    \"2013\"    \"2014\"    \"2015\"    \"2016\"   \n\nelec_use = read.csv(\"data_2/electricity_use_per_person.csv\", header = TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\nnames(elec_use)[2:56] = substr(names(elec_use)[2:56], 2, nchar(names(elec_use)[2:56]))\n\n# install.packages(\"tidyr\")\nlibrary(tidyr)\nelec_gen_df = gather(elec_gen, -country, key = \"year\", value = \"ElectricityGeneration\")\nelec_use_df = gather(elec_use, -country, key = \"year\", value = \"ElectricityUse\")\n\nelec_gen_use = merge(elec_gen_df, elec_use_df)\n\n\n# Data Visualization\n\n# 평균\napply(anscombe, 1, mean)\n\n [1]  8.65  7.45 10.47  8.57  9.36 10.49  6.34  7.03  9.71  6.93  5.75\n\napply(anscombe, 2, mean)\n\n x1  x2  x3  x4  y1  y2  y3  y4 \n9.0 9.0 9.0 9.0 7.5 7.5 7.5 7.5 \n\n# 분산\napply(anscombe, 2, var)\n\n   x1    x2    x3    x4    y1    y2    y3    y4 \n11.00 11.00 11.00 11.00  4.13  4.13  4.12  4.12 \n\n# 상관관계(상관계수)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.816\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.816\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.816\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.817\n\nlibrary(gapminder)\nlibrary(dplyr)\ny &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(c_pop = sum(pop)) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nhead(y, 20)\n\n# A tibble: 20 × 3\n# Groups:   year [4]\n    year continent      c_pop\n   &lt;int&gt; &lt;fct&gt;          &lt;dbl&gt;\n 1  1952 Africa     237640501\n 2  1952 Americas   345152446\n 3  1952 Asia      1395357351\n 4  1952 Europe     418120846\n 5  1952 Oceania     10686006\n 6  1957 Africa     264837738\n 7  1957 Americas   386953916\n 8  1957 Asia      1562780599\n 9  1957 Europe     437890351\n10  1957 Oceania     11941976\n11  1962 Africa     296516865\n12  1962 Americas   433270254\n13  1962 Asia      1696357182\n14  1962 Europe     460355155\n15  1962 Oceania     13283518\n16  1967 Africa     335289489\n17  1967 Americas   480746623\n18  1967 Asia      1905662900\n19  1967 Europe     481178958\n20  1967 Oceania     14600414\n\nplot(y$year, y$c_pop)\n\n\n\nplot(y$year, y$c_pop, col = y$continent)\n\n\n\nplot(y$year, y$c_pop, col = y$continent, pch = c(1:5))\nplot(y$year, y$c_pop, col = y$continent, pch = c(1:length(levels(y$continent))))\n\n# 범례 개수를 숫자로 지정\nlegend(\"topright\", legend = levels((y$continent)), pch = c(1:5), col = c(1:5))\n\n# 범례 개수를 데이터 개수에 맞게 지정\nlegend(\"bottomleft\", legend = levels((y$continent)), pch = c(1:length(levels(y$continent))), col = c(1:length(levels(y$continent))))\n\n\n\n# 02 시각화의 기본 기능 #\nplot(gapminder$gdpPercap, gapminder$lifeExp, col = gapminder$continent)\nlegend(\"bottomright\", legend = levels((gapminder$continent)), \n       pch = c(1:length(levels(gapminder$continent))), \n       col = c(1:length(levels(y$continent))))\n\n\n\nplot(log10(gapminder$gdpPercap), gapminder$lifeExp, col = gapminder$continent)\nlegend(\"bottomright\", legend  = levels((gapminder$continent)), pch = c(1:length(levels(gapminder$continent))), col = c(1:length(levels(y$continent))))\n\n\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n# gapminder %&gt;% ggplot(,aes())\n\nggplot(gapminder, aes(x =  gdpPercap, y = lifeExp, col = continent)) + \n  geom_point() + \n  scale_x_log10()\n\n\n\nggplot(gapminder, aes(x =  gdpPercap, y = lifeExp, col = continent, size = pop)) + \n  geom_point() + \n  scale_x_log10()\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent, size = pop)) + \n  geom_point(alpha = 0.5) + \n  scale_x_log10()\n\n\n\ntable(gapminder$year)\n\n\n1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 \n 142  142  142  142  142  142  142  142  142  142  142  142 \n\ngapminder %&gt;% filter(year==1977) %&gt;% \n  ggplot(., aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) + \n  geom_point(alpha=0.5) + \n  scale_x_log10() \n\n\n\ngapminder %&gt;% filter(year==2007) %&gt;% \n  ggplot(., aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) + \n  geom_point(alpha=0.5) + \n  scale_x_log10() \n\n\n\nggplot(gapminder, aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) + \n  geom_point(alpha=0.5) + \n  scale_x_log10() + \n  facet_wrap(~year)\n\n\n\ngapminder %&gt;% \n  filter(year == 1952 & continent ==\"Asia\") %&gt;% \n  ggplot(aes(reorder(country, pop), pop)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip()\n\n\n\ngapminder %&gt;% filter(year==1952 & continent== \"Asia\") %&gt;% ggplot(aes(reorder(country, pop), pop)) + geom_bar(stat  = \"identity\") + scale_y_log10() + coord_flip()\n\n\n\ngapminder %&gt;% \n  filter(country == \"Korea, Rep.\") %&gt;% \n  ggplot(aes(year, lifeExp, col = country)) + \n  geom_point() + \n  geom_line()\n\n\n\ngapminder %&gt;% \n  filter(country == \"Korea, Rep.\") %&gt;% \n  ggplot(aes(year, lifeExp, col = country)) + \n  # geom_point() + \n  geom_line()\n\n\n\ngapminder %&gt;% \n  ggplot(aes(x = year, y = lifeExp, col = continent)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nx = filter(gapminder, year == 1952)\nhist(x$lifeExp, main = \"Histogram of lifeExp in 1952\")\n\n\n\nx %&gt;% ggplot(aes(lifeExp)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nx %&gt;% ggplot(aes(continent, lifeExp)) + geom_boxplot()\n\n\n\nplot(log10(gapminder$gdpPercap), gapminder$lifeExp)\n\n\n\n\n\n# 03 Visualization Tool\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# type = \"p\"는 점 플롯, main = \"cars\"는 그래프의 제목\nplot(cars, type  = \"p\", main  = \"cars\")\n\n\n\nplot(cars, type = \"l\", main = \"cars\")       # type =\"l\"은 선을 사용한 플롯\n\n\n\nplot(cars, type=\"b\", main=\"cars\")   # type =\"b\"는 점과 선을 모두 사용한 플롯\n\n\n\nplot(cars, type = \"h\", main = \"cars\")  # type =\"h\"는 히스토그램과 같은 막대 그래프\n\n\n\nx = gapminder %&gt;% filter(year == 1952 & continent == \"Asia\") %&gt;% mutate(gdp = gdpPercap*pop) %&gt;% select(country, gdp) %&gt;% arrange(desc(gdp)) %&gt;% head()\npie(x$gdp, x$country)\n\n\n\nbarplot(x$gdp, names.arg = x$country)\n\n\n\nx = gapminder %&gt;% filter(year == 2007 & continent == \"Asia\") %&gt;% mutate(gdp  = gdpPercap*pop) %&gt;% select(country, gdp) %&gt;% arrange(desc(gdp)) %&gt;% head()\npie(x$gdp, x$country)\n\n\n\nbarplot(x$gdp, names.arg = x$country)\n\n\n\nmatplot(iris[, 1:4], type = \"l\")\nlegend(\"topleft\", names(iris)[1:4], lty = c(1, 2, 3, 4), col = c(1, 2, 3, 4))\n\n\n\nhist(cars$speed)\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) + geom_point(alpha = 0.2)\n\n\n\ngapminder %&gt;% filter(lifeExp&gt;70) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n = n_distinct(country)) %&gt;% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\")\n\n\n\ngapminder %&gt;% filter(year == 2007) %&gt;% \n  ggplot(aes(lifeExp, col = continent)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ngapminder %&gt;% filter(year == 2007) %&gt;% \n  ggplot(aes(lifeExp, col = continent)) + \n  geom_histogram(position = \"dodge\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ngapminder %&gt;% \n  filter(year == 2007) %&gt;% \n  ggplot(aes(continent, lifeExp, col = continent)) + \n  geom_boxplot()\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) + \n  geom_point(alpha = 0.2)\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) +\n  geom_point(alpha = 0.2) + scale_x_log10() # 가로축을 로그 스케일로 변환함.\n\n\n\ngapminder %&gt;% \n  filter(continent == \"Africa\") %&gt;% \n  ggplot(aes(country, lifeExp)) + \n  geom_bar(stat  =  \"identity\")                  # [그림 6-35(a)]\n\n\n\ngapminder %&gt;% \n  filter(continent == \"Africa\") %&gt;% \n  ggplot(aes(country, lifeExp)) + \n  geom_bar(stat  =  \"identity\") + \n  coord_flip()    # [그림 6-35(b)] 플롯의 방향을 전환함. \n\n\n\n# install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\ndisplay.brewer.all()\n\n\n\n# [그림 6-37(a)] : 기본 팔레트를 적용한 그래프\ngapminder %&gt;% filter(lifeExp&gt;70) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n  = n_distinct(country)) %&gt;% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\", aes(fill = continent)) \n\n\n\n# [그림 6-37(c)] Blues 팔레트를 적용한 그래프\ngapminder %&gt;% \n  filter(lifeExp&gt;70) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n = n_distinct(country)) %&gt;% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\", aes(fill = continent)) + scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n# [그림 6-37(d)] Oranges 팔레트를 적용한 그래프\ngapminder %&gt;% \n  filter(lifeExp&gt;70) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n =  n_distinct(country)) %&gt;% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\", aes(fill =  continent)) + scale_fill_brewer(palette = \"Oranges\")\n\n\n\n\n\n# reorder(continent, -n)은 continent를 n을 기준으로 내림차 순으로 정렬하라는 의미\ngapminder %&gt;% \n  filter(lifeExp &gt;70) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n  =  n_distinct(country)) %&gt;% \n  ggplot(aes(x = reorder(continent, -n), y =  n)) + \n  geom_bar(stat = \"identity\", aes(fill =  continent)) + \n  scale_fill_brewer(palette  = \"Blues\")\n\n\n\n# 실습!!\ngapminder %&gt;%\n  filter(continent == \"Africa\", year==2007) %&gt;%\n  ggplot(aes(reorder(country, lifeExp), lifeExp, fill=lifeExp)) +\n  geom_bar(stat  =  \"identity\") +\n  coord_flip()\n\n\n\n# \ngapminder %&gt;%\n  filter(continent == \"Africa\", year==2007) %&gt;%\n  ggplot(aes(reorder(country, lifeExp), lifeExp, fill=lifeExp)) +\n  geom_bar(stat  =  \"identity\") +\n  coord_flip()  + \n  scale_fill_distiller(palette = \"Oranges\", direction=1)\n\n\n\n\n\n# 04 시각화를 이용한 데이터 탐색 #\n\ngapminder %&gt;% ggplot(aes(gdpPercap, lifeExp, col = continent)) + geom_point(alpha  =  0.2) + facet_wrap(~year) + scale_x_log10()\n\n\n\ngapminder %&gt;% filter(year == 1952 & gdpPercap &gt; 10000 & continent == \"Asia\") \n\n# A tibble: 1 × 6\n  country continent  year lifeExp    pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;\n1 Kuwait  Asia       1952    55.6 160000   108382.\n\ngapminder %&gt;% filter(country == \"Kuwait\") %&gt;% ggplot(aes(year, gdpPercap)) + geom_point() + geom_line()             # [그림 6-40(a)]\n\n\n\ngapminder %&gt;% filter(country == \"Kuwait\") %&gt;% ggplot(aes(year, pop)) + geom_point() + geom_line()                   # [그림 6-40(b)]\n\n\n\ngapminder %&gt;% filter(country == \"Korea, Rep.\") %&gt;% ggplot(aes(year, gdpPercap)) + geom_point() + geom_line()        # [그림 6-41(a)]\n\n\n\ngapminder %&gt;% filter(country == \"Korea, Rep.\") %&gt;% ggplot(aes(year, pop)) + geom_point() + geom_line()              # [그림 6-41(b)]\n\n\n\ngapminder %&gt;% filter(country == \"Kuwait\" | country == \"Korea, Rep.\") %&gt;% mutate(gdp = gdpPercap*pop) %&gt;% ggplot(aes(year, gdp, col = country)) + geom_point() + geom_line()\n\n\n\n# [그림 6-43(a)] gdpPercap의 변화 비교 \ngapminder %&gt;% filter(country == \"Kuwait\"|country == \"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China\"|country == \"Japan\")  %&gt;% ggplot(aes(year, gdpPercap, col = country)) + geom_point() + geom_line()\n\n\n\n# [그림 6-43(b)] pop의 변화 비교 \ngapminder %&gt;% filter(country == \"Kuwait\"|country==\"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China\"|country == \"Japan\")  %&gt;% ggplot(aes(year, pop, col=country)) + geom_point() + geom_line()\n\n\n\n# [그림 6-43(c)] gdp의 변화 비교 \ngapminder %&gt;% filter(country == \"Kuwait\"|country == \"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China\"|country == \"Japan\")  %&gt;% mutate(gdp=gdpPercap*pop) %&gt;% ggplot(aes(year, gdp, col = country)) + geom_point() + geom_line() + scale_y_log10()"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_4.html",
    "href": "teaching/media_ds/about/NLP_4.html",
    "title": "NLP",
    "section": "",
    "text": "This material is the contents of a seminar held at Sogang University on the subject of Text mining in academic research. [Link]\n\n\nLDA Model\n\n# 전처리하기\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nraw_news_comment &lt;- read_csv(\"data/news_comment_parasite.csv\") %&gt;%\n  mutate(id = row_number())\n\nRows: 4150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): reply, press, title, url\ndttm (1): reg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_news_comment\n\n# A tibble: 4,150 × 6\n   reg_time            reply                             press title url      id\n   &lt;dttm&gt;              &lt;chr&gt;                             &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 2020-02-10 16:59:02 \"정말 우리 집에 좋은 일이 생겨 …  MBC   '기…  http…     1\n 2 2020-02-10 13:32:24 \"와 너무 기쁘다! 이 시국에 정말 … SBS   [영…  http…     2\n 3 2020-02-10 12:30:09 \"우리나라의 영화감독분들 그리고 … 한겨… ‘기…  http…     3\n 4 2020-02-10 13:08:22 \"봉준호 감독과 우리나라 대한민국… 한겨… ‘기…  http…     4\n 5 2020-02-10 16:25:41 \"노벨상 탄느낌이네요\\r\\n축하축하… 한겨… ‘기…  http…     5\n 6 2020-02-10 12:31:45 \"기생충 상 받을때 박수 쳤어요.감… 한겨… ‘기…  http…     6\n 7 2020-02-10 12:31:33 \"대한민국 영화사를 새로 쓰고 계…  한겨… ‘기…  http…     7\n 8 2020-02-11 09:20:52 \"저런게 아카데미상 받으면  '태극… 한겨… ‘기…  http…     8\n 9 2020-02-10 20:53:27 \"다시한번 보여주세요 영화관에서 … 한겨… ‘기…  http…     9\n10 2020-02-10 20:22:41 \"대한민국 BTS와함께  봉준호감독…  한겨… ‘기…  http…    10\n# ℹ 4,140 more rows\n\n\n기본적인 전처리\n\n# 기본적인 전처리\nlibrary(stringr)\nlibrary(textclean)\nnews_comment &lt;- raw_news_comment %&gt;%\n  mutate(reply = str_replace_all(reply, \"[^가-힣]\", \" \"),\n         reply = str_squish(reply)) %&gt;%\n  distinct(reply, .keep_all = T) %&gt;% # 중복 댓글 제거\n  filter(str_count(reply, boundary(\"word\")) &gt;= 3) # 짧은 문서 제거, 3단어 이상 추출\nnews_comment\n\n# A tibble: 3,329 × 6\n   reg_time            reply                             press title url      id\n   &lt;dttm&gt;              &lt;chr&gt;                             &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 2020-02-10 16:59:02 정말 우리 집에 좋은 일이 생겨 기… MBC   '기…  http…     1\n 2 2020-02-10 13:32:24 와 너무 기쁘다 이 시국에 정말 내… SBS   [영…  http…     2\n 3 2020-02-10 12:30:09 우리나라의 영화감독분들 그리고 …  한겨… ‘기…  http…     3\n 4 2020-02-10 13:08:22 봉준호 감독과 우리나라 대한민국 … 한겨… ‘기…  http…     4\n 5 2020-02-10 16:25:41 노벨상 탄느낌이네요 축하축하 합…  한겨… ‘기…  http…     5\n 6 2020-02-10 12:31:45 기생충 상 받을때 박수 쳤어요 감…  한겨… ‘기…  http…     6\n 7 2020-02-10 12:31:33 대한민국 영화사를 새로 쓰고 계시… 한겨… ‘기…  http…     7\n 8 2020-02-11 09:20:52 저런게 아카데미상 받으면 태극기 … 한겨… ‘기…  http…     8\n 9 2020-02-10 20:53:27 다시한번 보여주세요 영화관에서 …  한겨… ‘기…  http…     9\n10 2020-02-10 20:22:41 대한민국 와함께 봉준호감독님까지… 한겨… ‘기…  http…    10\n# ℹ 3,319 more rows\n\n\n\n# 2. 명사 추출하기\nlibrary(tidytext)\nlibrary(KoNLP)\n\nChecking user defined dictionary!\n\n# 명사 추출\ncomment &lt;- news_comment %&gt;%\n  unnest_tokens(input = reply,\n                output = word,\n                token = extractNoun,\n                drop = F) %&gt;%\n  filter(str_count(word) &gt; 1) %&gt;%\n  group_by(id) %&gt;% # 댓글 내 중복 단어 제거\n  distinct(word, .keep_all = T) %&gt;%\n  ungroup() %&gt;%\n  select(id, word) \ncomment\n\n# A tibble: 21,457 × 2\n      id word    \n   &lt;int&gt; &lt;chr&gt;   \n 1     1 우리    \n 2     1 행복    \n 3     2 시국    \n 4     2 감사    \n 5     2 하다    \n 6     2 진심    \n 7     3 우리나라\n 8     3 영화감독\n 9     3 영감    \n10     3 봉감    \n# ℹ 21,447 more rows\n\n\n\n# 3. 빈도 높은 단어 제거하기\ncount_word &lt;- comment %&gt;%\n  add_count(word) %&gt;%\n  filter(n &lt;= 200) %&gt;%\n  select(-n)\ncount_word\n\n# A tibble: 18,210 × 2\n      id word      \n   &lt;int&gt; &lt;chr&gt;     \n 1     1 우리      \n 2     1 행복      \n 3     2 시국      \n 4     2 감사      \n 5     2 하다      \n 6     2 진심      \n 7     3 우리나라  \n 8     3 영화감독  \n 9     3 영감      \n10     3 공동각본쓴\n# ℹ 18,200 more rows\n\n\n\n# 4. 불용어 제거하기, 유의어 처리하기\n# 불용어, 유의어 확인하기\ncount_word %&gt;%\n  count(word, sort = T) %&gt;%\n  print(n = 200)\n\n# A tibble: 6,022 × 2\n    word             n\n    &lt;chr&gt;        &lt;int&gt;\n  1 작품상         200\n  2 자랑           193\n  3 블랙리스트     173\n  4 조국           170\n  5 한국           165\n  6 대박           148\n  7 세계           140\n  8 수상           135\n  9 미국           128\n 10 들이           123\n 11 정치           108\n 12 역사           102\n 13 오스카         101\n 14 우리나라        96\n 15 감독상          93\n 16 진심            93\n 17 좌파            90\n 18 작품            87\n 19 한국영화        87\n 20 사람            86\n 21 배우            85\n 22 박근혜          84\n 23 국민            80\n 24 하다            80\n 25 최고            79\n 26 호감            79\n 27 우리            78\n 28 문화            75\n 29 생각            71\n 30 수상소감        68\n 31 감사            67\n 32 가족            65\n 33 나라            65\n 34 오늘            63\n 35 시상식          61\n 36 문재인          60\n 37 자랑스럽습니    60\n 38 송강호          59\n 39 소름            57\n 40 정권            54\n 41 각본상          53\n 42 감동            53\n 43 댓글            51\n 44 빨갱이          51\n 45 인정            48\n 46 소식            47\n 47 자한            47\n 48 소감            45\n 49 이미경          44\n 50 하나            43\n 51 한국인          43\n 52 대통령          42\n 53 정부            42\n 54 아카데미상      39\n 55 하게            39\n 56 위상            38\n 57 문재            37\n 58 쾌거            37\n 59 감격            36\n 60 순간            36\n 61 외국            36\n 62 전세계          36\n 63 호가            36\n 64 하면            35\n 65 눈물            34\n 66 보수            34\n 67 와우            34\n 68 현실            34\n 69 기사            33\n 70 영광            33\n 71 영화계          33\n 72 경사            32\n 73 사회            31\n 74 한국의          31\n 75 국제            30\n 76 누구            30\n 77 때문            29\n 78 마지막          29\n 79 얘기            29\n 80 인간            29\n 81 자랑스럽        29\n 82 해서            29\n 83 이번            28\n 84 훌륭            28\n 85 그네            27\n 86 기분            27\n 87 로컬            27\n 88 사람들          27\n 89 영화제          27\n 90 정도            27\n 91 뉴스            26\n 92 왜구            26\n 93 하네            26\n 94 자유            25\n 95 추카            25\n 96 기생            24\n 97 반미            24\n 98 영화상          24\n 99 이야기          24\n100 정경            24\n101 해요            24\n102 내용            23\n103 당신            23\n104 세상            23\n105 수준            23\n106 위대            23\n107 이것            23\n108 일본            23\n109 국위선양        22\n110 니들            22\n111 다시            22\n112 중국            22\n113 진정            22\n114 계획            21\n115 국가            21\n116 네이버          21\n117 숟가락          21\n118 쓰레기          21\n119 왕이            21\n120 재미            21\n121 정신            21\n122 존경            21\n123 행복            21\n124 국격            20\n125 문화계          20\n126 예술            20\n127 코로나          20\n128 하기            20\n129 하지            20\n130 가슴            19\n131 강국            19\n132 사건            19\n133 아시아          19\n134 완전            19\n135 우파            19\n136 중요            19\n137 최초            19\n138 부회장          18\n139 사실            18\n140 소리            18\n141 제작            18\n142 각본            17\n143 발전            17\n144 스텝            17\n145 시절            17\n146 실화            17\n147 올해            17\n148 의미            17\n149 자기            17\n150 자신            17\n151 천재            17\n152 토착            17\n153 한거            17\n154 한번            17\n155 해주            17\n156 그것            16\n157 노벨상          16\n158 다들            16\n159 다음            16\n160 모두            16\n161 박수            16\n162 상상            16\n163 시대            16\n164 어디            16\n165 여기            16\n166 오스카상        16\n167 최우수작품상    16\n168 한국어          16\n169 후보            16\n170 고생            15\n171 기대            15\n172 덕분            15\n173 발표            15\n174 상은            15\n175 예상            15\n176 월드컵          15\n177 응원            15\n178 이해            15\n179 조선            15\n180 한류            15\n181 해외            15\n182 까지            14\n183 대한            14\n184 리스            14\n185 모습            14\n186 바이러스        14\n187 생중계          14\n188 여자            14\n189 예전            14\n190 이거            14\n191 이름            14\n192 장면            14\n193 표현            14\n194 하신            14\n195 한국적          14\n196 한마디          14\n197 황금종려상      14\n198 김연아          13\n199 만큼            13\n200 방탄            13\n# ℹ 5,822 more rows\n\n\n이 부분은 상당히 context dependent 하면서 labour intensive 한 작업들..\n\n# 불용어 목록 만들기\nstopword &lt;- c(\"들이\", \"하다\", \"하게\", \"하면\", \"해서\", \"이번\", \"하네\",\n              \"해요\", \"이것\", \"니들\", \"하기\", \"하지\", \"한거\", \"해주\",\n              \"그것\", \"어디\", \"여기\", \"까지\", \"이거\", \"하신\", \"만큼\")\n\n\n# 불용어, 유의어 처리하기\ncount_word &lt;- count_word %&gt;%\n  filter(!word %in% stopword) %&gt;%\n  mutate(word = recode(word,\n                       \"자랑스럽습니\" = \"자랑\",\n                       \"자랑스럽\" = \"자랑\",\n                       \"자한\" = \"자유한국당\",\n                       \"문재\" = \"문재인\",\n                       \"한국의\" = \"한국\",\n                       \"그네\" = \"박근혜\",\n                       \"추카\" = \"축하\",\n                       \"정경\" = \"정경심\",\n                       \"방탄\" = \"방탄소년단\"))\n\n\n# ***불용어 목록을 파일로 만들어 활용하기\n# tibble 구조로 불용어 목록 만들기\nstopword &lt;- tibble(word = c(\"들이\", \"하다\", \"하게\", \"하면\", \"해서\", \"이번\", \"하네\",\n                            \"해요\", \"이것\", \"니들\", \"하기\", \"하지\", \"한거\", \"해주\",\n                            \"그것\", \"어디\", \"여기\", \"까지\", \"이거\", \"하신\", \"만큼\"))\n# 불용어 목록 저장하기\n# readr::write_csv(stopword, \"stopword.csv\")\n\n# 불용어 목록 불러오기\n# stopword &lt;- read_csv(\"stopword.csv\")\n\n아래 둘 중 하나 선택\n\n불용어 제거 by using filter()\n\n\n# 불용어 제거하기 - filter()\ncount_word &lt;- count_word %&gt;%\n  filter(!word %in% stopword$word)\n\n\n불용어 제거 by using anti_join()\n\n\n# 불용어 제거하기 - dplyr::anti_join()\ncount_word &lt;- count_word %&gt;%\n  anti_join(stopword, by = \"word\")\n\n\n# LDA 모델 만들기\n# 1. DTM 만들기\n# 문서별 단어 빈도 구하기\ncount_word_doc &lt;- count_word %&gt;%\n  count(id, word, sort = T)\ncount_word_doc\n\n# A tibble: 17,592 × 3\n      id word           n\n   &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1    35 한국           2\n 2   206 자랑           2\n 3   566 자랑           2\n 4   578 자랑           2\n 5   598 자랑           2\n 6  1173 한국           2\n 7  1599 한국           2\n 8  1762 한국           2\n 9  2240 한국           2\n10  2307 방탄소년단     2\n# ℹ 17,582 more rows\n\n\n\n\nDTM : long form to wide\n\n# DTM 만들기\n# install.packages(\"tm\")\ndtm_comment &lt;- count_word_doc %&gt;%\n  cast_dtm(document = id, term = word, value = n)\ndtm_comment\n\n&lt;&lt;DocumentTermMatrix (documents: 3203, terms: 5995)&gt;&gt;\nNon-/sparse entries: 17592/19184393\nSparsity           : 100%\nMaximal term length: 35\nWeighting          : term frequency (tf)\n\n\nLDA Model: LDA() No. of topics: k\n\n# 2. LDA 모델 만들기\n# install.packages(\"topicmodels\")\n\n# 토픽 모델 만들기\nlibrary(topicmodels)\nlda_model &lt;- LDA(dtm_comment,\n                 k = 8,\n                 method = \"Gibbs\",\n                 control = list(seed = 1234))\n\n\n# 모델 내용 확인\nglimpse(lda_model)\n\nFormal class 'LDA_Gibbs' [package \"topicmodels\"] with 16 slots\n  ..@ seedwords      : NULL\n  ..@ z              : int [1:17604] 8 8 4 3 7 4 3 1 1 1 ...\n  ..@ alpha          : num 6.25\n  ..@ call           : language LDA(x = dtm_comment, k = 8, method = \"Gibbs\", control = list(seed = 1234))\n  ..@ Dim            : int [1:2] 3203 5995\n  ..@ control        :Formal class 'LDA_Gibbscontrol' [package \"topicmodels\"] with 14 slots\n  ..@ k              : int 8\n  ..@ terms          : chr [1:5995] \"한국\" \"자랑\" \"방탄소년단\" \"박근혜\" ...\n  ..@ documents      : chr [1:3203] \"35\" \"206\" \"566\" \"578\" ...\n  ..@ beta           : num [1:8, 1:5995] -7.81 -10.22 -10.25 -5.83 -10.25 ...\n  ..@ gamma          : num [1:3203, 1:8] 0.151 0.15 0.11 0.114 0.11 ...\n  ..@ wordassignments:List of 5\n  .. ..$ i   : int [1:17592] 1 1 1 1 1 1 1 1 1 1 ...\n  .. ..$ j   : int [1:17592] 1 98 99 100 101 102 103 104 105 106 ...\n  .. ..$ v   : num [1:17592] 8 4 3 7 4 3 7 2 8 6 ...\n  .. ..$ nrow: int 3203\n  .. ..$ ncol: int 5995\n  .. ..- attr(*, \"class\")= chr \"simple_triplet_matrix\"\n  ..@ loglikelihood  : num -126429\n  ..@ iter           : int 2000\n  ..@ logLiks        : num(0) \n  ..@ n              : int 17604\n\n\n\n\n토픽별 주요 단어\n\n# 토픽별 단어 확률, beta 추출하기\nterm_topic &lt;- tidy(lda_model, matrix = \"beta\")\nterm_topic\n\n# A tibble: 47,960 × 3\n   topic term       beta\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1     1 한국  0.000405 \n 2     2 한국  0.0000364\n 3     3 한국  0.0000353\n 4     4 한국  0.00295  \n 5     5 한국  0.0000353\n 6     6 한국  0.0000356\n 7     7 한국  0.00661  \n 8     8 한국  0.0593   \n 9     1 자랑  0.0181   \n10     2 자랑  0.00440  \n# ℹ 47,950 more rows\n\n\n\n# 토픽별 단어 수\nterm_topic %&gt;%\n  count(topic)\n\n# A tibble: 8 × 2\n  topic     n\n  &lt;int&gt; &lt;int&gt;\n1     1  5995\n2     2  5995\n3     3  5995\n4     4  5995\n5     5  5995\n6     6  5995\n7     7  5995\n8     8  5995\n\n\n\n# 토픽 1의 beta 합계\nterm_topic %&gt;%\n  filter(topic == 1) %&gt;%\n  summarise(sum_beta = sum(beta))\n\n# A tibble: 1 × 1\n  sum_beta\n     &lt;dbl&gt;\n1        1\n\n\n\n# 특정 단어의 토픽별 확률 살펴보기\nterm_topic %&gt;%\n  filter(term == \"작품상\")\n\n# A tibble: 8 × 3\n  topic term        beta\n  &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1     1 작품상 0.0000368\n2     2 작품상 0.000763 \n3     3 작품상 0.0000353\n4     4 작품상 0.0000364\n5     5 작품상 0.0000353\n6     6 작품상 0.0695   \n7     7 작품상 0.000727 \n8     8 작품상 0.000388 \n\n\n\n# 특정 토픽에서 beta가 높은 단어 살펴보기\nterm_topic %&gt;%\n  filter(topic == 6) %&gt;%\n  arrange(-beta)\n\n# A tibble: 5,995 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     6 작품상   0.0695 \n 2     6 감독상   0.0318 \n 3     6 한국영화 0.0228 \n 4     6 수상     0.0214 \n 5     6 각본상   0.0154 \n 6     6 나라     0.0143 \n 7     6 호감     0.0136 \n 8     6 감격     0.0129 \n 9     6 순간     0.0125 \n10     6 눈물     0.00788\n# ℹ 5,985 more rows\n\n\n\n# 모든 토픽의 주요 단어 살펴보기\nterms(lda_model, 20) %&gt;%\n  data.frame()\n\n    Topic.1 Topic.2 Topic.3    Topic.4    Topic.5  Topic.6    Topic.7\n1      작품    대박    조국       역사       자랑   작품상 블랙리스트\n2      진심  시상식  문재인   우리나라       우리   감독상     박근혜\n3      정치    오늘    가족       세계       최고 한국영화       사람\n4      자랑    국민    문화     오스카       감사     수상     송강호\n5  수상소감    소름  대통령       수상       생각   각본상     이미경\n6      댓글    정치    자랑     빨갱이       소식     나라 자유한국당\n7      외국    배우    때문     영화계   국위선양     호감       정권\n8      경사    계획    인정 아카데미상       감동     감격       소감\n9      훌륭    축하    정부       인간       하나     순간       보수\n10     좌파    위상    강국       얘기 방탄소년단     눈물       인정\n11     왜구    최고    호감       로컬     영화상   전세계     마지막\n12     배우    한번    와우       내용   한국영화     진정       기생\n13     예술    쾌거    사건       좌파       정도   노벨상       하나\n14   전세계    생각    국격       정신       와우     소식     네이버\n15   아시아    중국    고생       의미       조선     기사       한국\n16     호감    다음    덕분       생각       존경     문화     이야기\n17     토착    기분    기대       상상       후보     국제     부회장\n18     발전    왕이    정말       국민       우한     각본     쓰레기\n19   사람들    세상    해도       나라       시대     다들       좌파\n20     수준    자랑    눈물       정부       행복     발표       영광\n        Topic.8\n1          한국\n2          미국\n3        한국인\n4          세계\n5          좌파\n6          배우\n7          감동\n8          누구\n9          사회\n10         자유\n11         현실\n12         영광\n13         위대\n14       영화제\n15         이해\n16         자신\n17 최우수작품상\n18         예상\n19   황금종려상\n20         이유\n\n\n\n# 토픽별 주요 단어 시각화하기\n# 1. 토픽별로 beta가 가장 높은 단어 추출하기\n# 토픽별 beta 상위 10개 단어 추출\ntop_term_topic &lt;- term_topic %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10)\ntop_term_topic\n\n# A tibble: 83 × 3\n# Groups:   topic [8]\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 작품     0.0299 \n 2     1 진심     0.0240 \n 3     1 정치     0.0192 \n 4     1 자랑     0.0181 \n 5     1 수상소감 0.0166 \n 6     1 댓글     0.0151 \n 7     1 외국     0.0122 \n 8     1 경사     0.0107 \n 9     1 훌륭     0.00998\n10     1 좌파     0.00814\n# ℹ 73 more rows\n\n\n\n# 2. 막대 그래프 만들기\n# install.packages(\"scales\") # restart 알림 발생 시, '아니요' 선택\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nggplot(top_term_topic,\n       aes(x = reorder_within(term, beta, topic),\n           y = beta,\n           fill = factor(topic))) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 4) +\n  coord_flip() +\n  scale_x_reordered() +\n  scale_y_continuous(n.breaks = 4,\n                     labels = number_format(accuracy = .01)) +\n  labs(x = NULL) \n\n\n\n\n\n\n문서를 토픽별로 분류\n\n# 문서별 토픽 확률 gamma 추출하기\ndoc_topic &lt;- tidy(lda_model, matrix = \"gamma\")\ndoc_topic\n\n# A tibble: 25,624 × 3\n   document topic  gamma\n   &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n 1 35           1 0.151 \n 2 206          1 0.15  \n 3 566          1 0.110 \n 4 578          1 0.114 \n 5 598          1 0.110 \n 6 1173         1 0.110 \n 7 1599         1 0.114 \n 8 1762         1 0.0962\n 9 2240         1 0.125 \n10 2307         1 0.135 \n# ℹ 25,614 more rows\n\n\n\n# gamma 살펴보기\ndoc_topic %&gt;%\n  count(topic)\n\n# A tibble: 8 × 2\n  topic     n\n  &lt;int&gt; &lt;int&gt;\n1     1  3203\n2     2  3203\n3     3  3203\n4     4  3203\n5     5  3203\n6     6  3203\n7     7  3203\n8     8  3203\n\n\n\n# 문서 1의 gamma 합계\ndoc_topic %&gt;%\n  filter(document == 1) %&gt;%\n  summarise(sum_gamma = sum(gamma))\n\n# A tibble: 1 × 1\n  sum_gamma\n      &lt;dbl&gt;\n1         1\n\n\n\n# 문서별 확률이 가장 높은 토픽으로 분류하기\n# 1. 문서별로 확률이 가장 높은 토픽 추출하기\ndoc_class &lt;- doc_topic %&gt;%\n  group_by(document) %&gt;%\n  slice_max(gamma, n = 1)\ndoc_class\n\n# A tibble: 5,328 × 3\n# Groups:   document [3,203]\n   document topic gamma\n   &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1 1            5 0.159\n 2 10           8 0.168\n 3 100          5 0.153\n 4 1000         7 0.15 \n 5 1001         1 0.137\n 6 1001         3 0.137\n 7 1001         7 0.137\n 8 1002         3 0.137\n 9 1002         7 0.137\n10 1002         8 0.137\n# ℹ 5,318 more rows\n\n\n\n# 2. 원문에 확률이 가장 높은 토픽 번호 부여하기\n# integer로 변환\ndoc_class$document &lt;- as.integer(doc_class$document)\n\n\n# 원문에 토픽 번호 부여\nnews_comment_topic &lt;- raw_news_comment %&gt;%\n  left_join(doc_class, by = c(\"id\" = \"document\"))\n\n\n# 결합 확인\nnews_comment_topic %&gt;%\n  select(id, topic)\n\n# A tibble: 6,275 × 2\n      id topic\n   &lt;int&gt; &lt;int&gt;\n 1     1     5\n 2     2     1\n 3     2     5\n 4     2     6\n 5     3     2\n 6     3     8\n 7     4     4\n 8     5     5\n 9     5     6\n10     6     3\n# ℹ 6,265 more rows\n\n\n\n# 3. 토픽별 문서 수 살펴보기\nnews_comment_topic %&gt;%\n  count(topic)\n\n# A tibble: 9 × 2\n  topic     n\n  &lt;int&gt; &lt;int&gt;\n1     1   660\n2     2   704\n3     3   663\n4     4   609\n5     5   708\n6     6   690\n7     7   649\n8     8   645\n9    NA   947\n\n\n\n# topic이 NA인 문서 제거\nnews_comment_topic &lt;- news_comment_topic %&gt;%\n  na.omit()\n\n\nnews_comment_topic %&gt;%\n  count(topic)\n\n# A tibble: 8 × 2\n  topic     n\n  &lt;int&gt; &lt;int&gt;\n1     1   660\n2     2   704\n3     3   663\n4     4   609\n5     5   708\n6     6   690\n7     7   649\n8     8   645\n\n\n\n# 토픽별 문서 수와 단어 시각화하기\n# 1. 토픽별 주요 단어 목록 만들기\ntop_terms &lt;- term_topic %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 6, with_ties = F) %&gt;%\n  summarise(term = paste(term, collapse = \", \"))\ntop_terms\n\n# A tibble: 8 × 2\n  topic term                                                \n  &lt;int&gt; &lt;chr&gt;                                               \n1     1 작품, 진심, 정치, 자랑, 수상소감, 댓글              \n2     2 대박, 시상식, 오늘, 국민, 소름, 정치                \n3     3 조국, 문재인, 가족, 문화, 대통령, 자랑              \n4     4 역사, 우리나라, 세계, 오스카, 수상, 빨갱이          \n5     5 자랑, 우리, 최고, 감사, 생각, 소식                  \n6     6 작품상, 감독상, 한국영화, 수상, 각본상, 나라        \n7     7 블랙리스트, 박근혜, 사람, 송강호, 이미경, 자유한국당\n8     8 한국, 미국, 한국인, 세계, 좌파, 배우                \n\n\n\n# 2. 토픽별 문서 빈도 구하기\ncount_topic &lt;- news_comment_topic %&gt;%\n  count(topic)\ncount_topic\n\n# A tibble: 8 × 2\n  topic     n\n  &lt;int&gt; &lt;int&gt;\n1     1   660\n2     2   704\n3     3   663\n4     4   609\n5     5   708\n6     6   690\n7     7   649\n8     8   645\n\n\n\n# 3. 문서 빈도에 주요 단어 결합하기\ncount_topic_word &lt;- count_topic %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic_name = paste(\"Topic\", topic))\ncount_topic_word\n\n# A tibble: 8 × 4\n  topic     n term                                                 topic_name\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                                                &lt;chr&gt;     \n1     1   660 작품, 진심, 정치, 자랑, 수상소감, 댓글               Topic 1   \n2     2   704 대박, 시상식, 오늘, 국민, 소름, 정치                 Topic 2   \n3     3   663 조국, 문재인, 가족, 문화, 대통령, 자랑               Topic 3   \n4     4   609 역사, 우리나라, 세계, 오스카, 수상, 빨갱이           Topic 4   \n5     5   708 자랑, 우리, 최고, 감사, 생각, 소식                   Topic 5   \n6     6   690 작품상, 감독상, 한국영화, 수상, 각본상, 나라         Topic 6   \n7     7   649 블랙리스트, 박근혜, 사람, 송강호, 이미경, 자유한국당 Topic 7   \n8     8   645 한국, 미국, 한국인, 세계, 좌파, 배우                 Topic 8   \n\n\n\n# 4. 토픽별 문서 수와 주요 단어로 막대 그래프 만들기\nggplot(count_topic_word,\n       aes(x = reorder(topic_name, n),\n           y = n,\n           fill = topic_name)) +\n  geom_col(show.legend = F) +\n  coord_flip() +\n  geom_text(aes(label = n) , # 문서 빈도 표시\n            hjust = -0.2) + # 막대 밖에 표시\n  geom_text(aes(label = term), # 주요 단어 표시\n            hjust = 1.03, # 막대 안에 표시\n            col = \"white\", # 색깔\n            fontface = \"bold\", # 두껍게\n            family = \"nanumgothic\") + # 폰트\n  scale_y_continuous(expand = c(0, 0), # y축-막대 간격 줄이기\n                     limits = c(0, 820)) + # y축 범위\n  labs(x = NULL)\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n토픽 이름 짓기\n\n# 토픽별 주요 문서 살펴보고 토픽 이름 짓기\n# 1. 원문을 읽기 편하게 전처리하기, gamma가 높은 순으로 정렬하기\ncomment_topic &lt;- news_comment_topic %&gt;%\n  mutate(reply = str_squish(replace_html(reply))) %&gt;%\n  arrange(-gamma)\n\ncomment_topic %&gt;%\n  select(gamma, reply)\n\n# A tibble: 5,328 × 2\n   gamma reply                                                                  \n   &lt;dbl&gt; &lt;chr&gt;                                                                  \n 1 0.264 \"도서관서 여자화장실서 나오는 남자사서보고 카메라있는지없는지 확인했다…\n 2 0.260 \"봉준호 송강호 블랙리스트 올리고 CJ 이미경 대표는 박근혜가 보기싫다는 …\n 3 0.239 \"보수정권때 블랙리스트에 오른 봉준호 송강호가 보기싫다는 박근혜의 말한…\n 4 0.238 \"도서관서 여자화장실서 나오는 남자사서보고 카메라있는지없는지 확인했다…\n 5 0.235 \"당초 \\\"1917\\\"과 \\\"기생충\\\"의 접전을[초기엔 1917이 훨씬 우세]예상했지… \n 6 0.234 \"박그네 밑에서 블랙리스트 있었는데 ㅋㅋㅋㅋㅋㅋㅋ 이미경이는 박근혜가 …\n 7 0.226 \"위대한 박정희 삼성이 대한민국을 세계에 우뚝 세워 놨기에 가능한 일....…\n 8 0.225 \"기생충 영화보고 좌빨이 얼마나 기생충인지 못느낀 사람 제정신이야? 좌빨…\n 9 0.225 \"봉준호 감독과 송강호 배우는 이명박그네 정권 시절 문화계 블랙리스트 였…\n10 0.224 \"나중에 기생충 정부 영화 한편 나오겠네. 남자 주연 문xx ,여자주연 김정x…\n# ℹ 5,318 more rows\n\n\n\n# 2. 주요 단어가 사용된 문서 살펴보기\n# 토픽 1 내용 살펴보기\ncomment_topic %&gt;%\n  filter(topic == 1 & str_detect(reply, \"작품\")) %&gt;%\n  head(50) %&gt;%\n  pull(reply)\n\n [1] \"봉감독의 'local'이라는 말에 발끈했나요? 미국 아카데미의 놀라운 변화입니다. 기생충이란 영화의 작품적 우수성 뿐만 아니라 '봉준호'라는 개인의 네임벨류와 인간적 매력과 천재성이 이번 아카데미 수상에 큰 역할을 한 것 같네요. 그의 각종 수상소감을 보면 면면히 드러나네요~~\"                                                                                           \n [2] \"이 작품을 기준으로 앞으로도 계속 쓰여질 것입니다. 진심으로 축하드립니다^^\"                                                                                                                                                                                                                                                                                         \n [3] \"이런 위대한 작품과 감독을 블랙리스트에 올려 대중에게서 뺏어 묻어버릴려고 했던 쥐닥정권 그걸 찬양하는 소시오패스일베충 벌래에게 무한한 저주가 함께하길 기원합니다.^^\"                                                                                                                                                                                               \n [4] \"폐쇄적이라는 평가를 받아온 오스카가 외국영화에 작품상을 주는걸 보니 또다른 의미의 권위가 느껴진다\"                                                                                                                                                                                                                                                                 \n [5] \"봉준호감독 대단하다 열등감이있는외모 안된다는 편견 자신과의 싸움 결국 그럭게 말하는자 들은 뭐하고사는지?우스꽝스럽다고 비꼬고 놀렸을만한 모습이지만 그들은 이런상 한번이라도 받을수있는 자격이있는지 암튼 대다하고 작품활동열심히 하셔서 멋진사람으로 기억되길,.,,외모비하하는 찌질이들은 아무재능없는 소인배가되고 결국 계속 악플 올렸다간 따돌림이나 당하겠지...\"\n [6] \"이런 감독을 박그네 토착왜구 정부에서는 좌파 블랙리스트에 올렸었지 ㅋㅋㅋ 근데 정작 미국에선 작품상 ㅋㅋㅋ\"                                                                                                                                                                                                                                                         \n [7] \"축하합니다!! 자랑스럽고 멋져요^^ 앞으로도 멋진 작품 부탁드립니다!\"                                                                                                                                                                                                                                                                                                 \n [8] \"정말 대단하다는 말밖에는~ 진심으로 축하드립니다. 앞으로도 좋은 작품 많이 만들어주세요~^^\"                                                                                                                                                                                                                                                                          \n [9] \"한류의 또다른 이정표를 봉준호가 해내는구가 오스카상 특히 작품상은 비영어권 국가가 수상하기 어려운 상당히 보수적인데 대단하다 썩은 정치로 한숨쉬는 국민들에게 또 다른 희망과 자부심을 심어줘서 고맙다\"                                                                                                                                                              \n[10] \"작품이라고 할 수 없는 습작 정도의 물건을 상을 줬으니, 그동안 오스카 상이 얼마나 추접한 상인지 증명하는 것임.\"                                                                                                                                                                                                                                                      \n[11] \"기생충을 재밌게는봤지만,작품성이 그리대단한줄은 몰랐네요,축하합니다\"                                                                                                                                                                                                                                                                                               \n[12] \"완전감동이네요..제2의기생충같은 작품 많이많이 만들어주세요..\"                                                                                                                                                                                                                                                                                                      \n[13] \"봉감독님 너무 축하합니다! 앞으로도 훌륭한 작품들 기대합니다♥\"                                                                                                                                                                                                                                                                                                      \n[14] \"작품도 뛰어났지만 국가 위상이 그만큼 높아진 것. 촛불시민혁명 국민들이 비폭력적으로 정권교체 하는데 성공하고 문 대통령이 남북정상회담 멋지게 해내셨고 그 과정을 거치면서 대한민국이 서구 사회에 유명해진 것. 분위기가 뒷받침 해주지 않으면 아무리 작품이 좋아도 이런 성과는 못냄.\"                                                                                  \n[15] \"각본상받았다고 하길래 그게 끝인줄 알았더니 시작에 불과했었네. 한국영화의 쾌거로구나. 헐리웃영화가 아닌 작품이 작품상을 받은게 최초라는데 역사를 썼다. 와우~\"                                                                                                                                                                                                       \n[16] \"작품..감독..배우는 말할 것도 없고 샤론최님까지 통역을 잘 해주셔서 외국사람들으 현지호응도..작품 이해도가 높아져서.. 분위기 잘 잡아주셨나보네요.\"                                                                                                                                                                                                                   \n[17] \"개인적으론 봉감독 작품중에 여전히 살인의 추억이 가장 인상깊지만... 기생충은 전세계적으로 화두가 된 주제선정이 탁월했고 풀어내는 모양새도 능수능란하게 과하지도 덜하지도 않게 딱 좋았다. 여유있게 유머를 가지고 오스카를 향한 긴 레이스에서 정치[?]도 잘했다.\"                                                                                                      \n[18] \"백인우월주의가 팽배한 그 나라에서 조차 인정할 수 밖에 없을만큼 대단한 작품이었다는 반증인거지. 그리고 저 영화 보고 기분 더러웠다는 건 제대로 느낀거 아닐까? 기생충 자체가 사회풍자가 들어있으니까. 원래 너무 현실적인게 가장 불편함. 극구 부정하고 싶거든.\"                                                                                                        \n[19] \"각본상 준거보니 작품상은 1917 아니면 조커다\"                                                                                                                                                                                                                                                                                                                       \n[20] \"축하드려요. 당신은 우리의 자랑입니다. 좋은 작품 앞으로도 기대할게요^^ 짝짝짝\"                                                                                                                                                                                                                                                                                      \n[21] \"오스카 작품상 탔다 비영어권처음이다\"                                                                                                                                                                                                                                                                                                                               \n[22] \"축하합니다~^^. 작품상까지 받은 작품인데 주연상이 없음이 좀 아이러니지만....\"                                                                                                                                                                                                                                                                                       \n[23] \"김기덕 감독님 영화는 이렇게 해야됩니다. 맨날 어둡고 보기 힘든 영화만 만들지 마시고 이런 작품도 만들어 주세요! 봉 감독님 진심 축하드립니다.\"                                                                                                                                                                                                                        \n[24] \"솔직히 작품상은 조커가 타는 게 맞는데 동양인 쿼터 준다고 많이 배려해 준 듯.\"                                                                                                                                                                                                                                                                                       \n[25] \"솔직히 눈물난다ㅡ내가 내 인생영화로 꼽았던 작품! 개봉첫날 두번 봤다 ㅜ.ㅜ\"                                                                                                                                                                                                                                                                                         \n[26] \"와...햐늘을 찌르는 작품성을 바탕으로 아카데미는 로컬영화제라고 인터뷰하던 그대의 고귀한 자태는정말 멋잇엇어요....자랑스럽습니다\"                                                                                                                                                                                                                                   \n[27] \"남들은 남감독 작품이 해외서 상을 타니 입에 침이 마르도록 찬양을 한다. 왜 저렇게 남자일을 자기일처럼 좋아할까? 저 자리에 여성은 없고 향후 몇 십년 동안 없을 수도 있단 생각에 난 웃음도 나질 않는다. ㅉ\"                                                                                                                                                             \n[28] \"우리나라 영화제..10년도 넘은 배우한테..신인신ㅇ 주고 공동수상 남발하고..감독상 받으면 작품상은 포기해야 하는데..외국의 냉철함 너무 보기 좋네\"                                                                                                                                                                                                                      \n[29] \"축하합니다. 어깨가 으쓱해집니다. 작품상까지 수상하시길 응원합니다.\"                                                                                                                                                                                                                                                                                                \n[30] \"꿈 아니냐?? 작품상??? 전 세계에서 한해 제일 잘만들면서 대중성도 있는 작품에 주는 그상 맞냐? World no.1 ㄷ ㄷ ㄷ ㄷ ㄷ 진짜 개소름\"                                                                                                                                                                                                                                 \n[31] \"근데 나는 봉준호 송강호도 대단하지만 이선균이 진심 대단함. 이선균 필모는 넘사네 진짜. 작품도 다양하고.\"                                                                                                                                                                                                                                                            \n[32] \"정말 자랑스럽습니다! 눈물흘리며 같이 환호했습니다! 앞으로도 역사에 남을 작품 많이 남겨주세요!\"                                                                                                                                                                                                                                                                     \n[33] \"다른 작품들도 진짜 쟁쟁했는데.... 이건 대한민국 영화의 새로운 역사입니다!!! 봉 감독님 진짜 축하드려요!!! 그리고 수구꼴통 토착왜구 새끼들아!!! 제발 이런걸 정치적인거랑 엮지마라!!!!! 늬들의 행태 너무나 역겹다!!!!!!\"                                                                                                                                              \n[34] \"아카데미에서 큰상받은건 축하하고 대한민국인으로서 자랑스럽긴한데, 영알못은 기생충의 어떤점이 작품성이 있는지 잘 모르겠다.\"                                                                                                                                                                                                                                         \n[35] \"일하다가 눈물날뻔..^^ 감격스럽네요 자랑스럽네요 봉감독님의 더 많은 작품, 훌륭한 작품 기대합니다~~^^\"                                                                                                                                                                                                                                                               \n[36] \"얼마전 괴물 다시보는데도 진짜 봉감독님 작품 너무 재밌어요! 수상 축하드려요!\"                                                                                                                                                                                                                                                                                       \n[37] \"작품성과 흥행성을 모두 거머쥔 시대상을 딱 반영하는 상이다. 오스카도 반영했을듯 하네ㅋ\"                                                                                                                                                                                                                                                                             \n[38] \"자본주의 민주주의 아메리카가 인정한 작품. 기생충이 짱개의 유사사회주의 찬양한것처럼 달창 거지기생충들 그냥 좋댄다 ㅋㅋㅋㅋㅋ\"                                                                                                                                                                                                                                      \n[39] \"작품상은 가능성이 희박하다는 기사를 봤었는데... 기적이네요.... 투표 방식이 이번에 바껴서 그런가\"                                                                                                                                                                                                                                                                   \n[40] \"봉준호 감독님! 대한민국을 널리 알려주셔서 감사합니다. 수상소감 말씀하실때 눈물이 핑 돌더군요. 앞으로도 좋은 작품 세계적인 영화를 기대합니다. 축하드립니다.기생충! 봉준호 감독님! 화이팅입니다 ^^\"                                                                                                                                                                  \n[41] \"아마 반백년내에는 한국에서 이런 영화가 나오긴 힘들듯.. 칸 황금종려상과 아카데미 작품상을 동시에 수상한것도 전세계적으로 60년만에 일인거고.. 진짜 어메이징한 올해 한국의 최대 이변일듯..\"                                                                                                                                                                           \n[42] \"예전 살인의 추억을 극장에서 보고 작품,상업성을 다 갖춘 감독의 출현에 지금껏 응원해왔어요 정말 뭉클합니다\"                                                                                                                                                                                                                                                          \n[43] \"블랙리스트랑 작품의 완성도랑 무'슨상'관임? 예술적으로 완성도 높은 작품을 만들었으니 상을 받은거지. 문화계 블랙리스트가 뭐 능력떨어지는 예술인들에 대한 리스트도 아니고.\"                                                                                                                                                                                           \n[44] \"축하 합니다 쭉 좋은 작품 부탁 합니다.\"                                                                                                                                                                                                                                                                                                                             \n[45] \"감독님 정말 멋집니다!!! 헐리우드작품 감독님의 영화도 한번 보고싶습니다!! 화이팅~~~~~~\"                                                                                                                                                                                                                                                                             \n[46] \"작품상 받을 포스인데...\"                                                                                                                                                                                                                                                                                                                                           \n[47] \"축하합니다. 앞으로도 좋은 작품 기대하겠습니다.\"                                                                                                                                                                                                                                                                                                                    \n[48] \"축하드립니다~ 국제적 작품이 되엇어요 ♡~♡\"                                                                                                                                                                                                                                                                                                                          \n[49] \"황금종려상에 아카데미 작품상까지 정말 대단합니다\"                                                                                                                                                                                                                                                                                                                  \n[50] \"대단~^^ 작품상까지~ 올들어 젤 기쁜 기사네요\"                                                                                                                                                                                                                                                                                                                       \n\n\n\ncomment_topic %&gt;%\n  filter(topic == 1 & str_detect(reply, \"진심\")) %&gt;%\n  head(50) %&gt;%\n  pull(reply)\n\n [1] \"한국문화는 1등수준. 정치는 개돼지 3등수준. 대한민국 문화수준을 엎 그레이드 한 봉준호감독에게 존경을 표한다. 월드컵4강.봉준호감독 오스카수상. 한국국민들 절대로 잊혀지지않는 대사건이다. 정말 진심으로 축하드립니다.특히 CJ가 한국영화 산업에 큰 발전에 국민의 한 사람으로 감사드린다.\"\n [2] \"진심 축하드립니다. 대한민국 예술처럼 정치, 경제도 발전해서 살기좋은 나라가 되었으면 좋겠네요\"                                                                                                                                                                                         \n [3] \"이 작품을 기준으로 앞으로도 계속 쓰여질 것입니다. 진심으로 축하드립니다^^\"                                                                                                                                                                                                            \n [4] \"소름이 돋을 정도로 믿기지 않습니다! 진심으로 축하드립니다 살인의추억을 보고 우리나라에도 이렇게 훌륭한 영화를 만드는 감독이 있구나 느꼈고 그 이후로 계속 봉감독님 영화를 챙겨보게 되었는데 이제는 세계인도 인정해주는 감독이 되어 너무나도 벅찹니다 다시한번 진심으로 축하드립니다\"   \n [5] \"봉감독님 진심 축하드립니다! 아카데미에서 외국 영화 기생충이 4관왕을 차지했다는 것은 아카데미도 기류가 바뀌고 있다는 ... 험지에서 대단한 업적을 남긴 감독, 배우들, 스텝들에게 찬사를 보냅니다!\"                                                                                        \n [6] \"기생충!! 우와~~^^이게 실화인가요? 진심으로 축하드려요\"                                                                                                                                                                                                                                \n [7] \"정말 대단하다는 말밖에는~ 진심으로 축하드립니다. 앞으로도 좋은 작품 많이 만들어주세요~^^\"                                                                                                                                                                                             \n [8] \"진심으로 축하드립니다. 자랑스럽습니다.\"                                                                                                                                                                                                                                               \n [9] \"진심 축하드려요 봉준호감독님 진심 응원합니다\"                                                                                                                                                                                                                                         \n[10] \"한국영화계에 이런일이 생길 줄은 꿈에도 몰랐는데 4관왕 진심으로 축하드립니다.\"                                                                                                                                                                                                         \n[11] \"영화 보면서 봉준호는 천재다 라고 외쳤는데~ 진심 자랑스럽다 축하합니다\"                                                                                                                                                                                                                \n[12] \"와 진심 대박 ㅜㅜ 화이트 파티인 아카데미에서 ㅠㅜ\"                                                                                                                                                                                                                                    \n[13] \"와.. 대박 진심 호명순간 소리질렀어여\"                                                                                                                                                                                                                                                 \n[14] \"김기덕 감독님 영화는 이렇게 해야됩니다. 맨날 어둡고 보기 힘든 영화만 만들지 마시고 이런 작품도 만들어 주세요! 봉 감독님 진심 축하드립니다.\"                                                                                                                                           \n[15] \"세계가 인정하는 대한민국 예술! 진심으로 자랑스럽다ㅠㅠ\"                                                                                                                                                                                                                               \n[16] \"근데 나는 봉준호 송강호도 대단하지만 이선균이 진심 대단함. 이선균 필모는 넘사네 진짜. 작품도 다양하고.\"                                                                                                                                                                               \n[17] \"이걸 정치적으로 까는거보면 뇌가있나 싶다 진심으로 /\"                                                                                                                                                                                                                                  \n[18] \"축하드립니다. 대한민국 사람으로서 봉준호감독님 세계적인 거장에 한걸음 다가가신걸 진심으로 축하드려요~~^^\"                                                                                                                                                                             \n[19] \"우와 !!!!! 진심으로 축하드려요 !!!!\"                                                                                                                                                                                                                                                  \n[20] \"어~~ 진짜 되네? 진심으로 축하드려요\"                                                                                                                                                                                                                                                  \n[21] \"진심 축하드립니다~~~ 너무 멋져요!!\"                                                                                                                                                                                                                                                   \n[22] \"대박입니다~~ 진심으로 축하합니다~!!!\"                                                                                                                                                                                                                                                 \n[23] \"와 대박이네요 ~ 진심으로 축하드립니다.\"                                                                                                                                                                                                                                               \n[24] \"아카데미 4관왕 거머쥐신 봉준호 감독님 진심 추카드려요\"                                                                                                                                                                                                                                \n[25] \"미쳤다 진심 비명지르고\"                                                                                                                                                                                                                                                               \n[26] \"진심 대박이다.와\"                                                                                                                                                                                                                                                                     \n[27] \"캬 역사를만드셨네 진심 축하드립니다\"                                                                                                                                                                                                                                                  \n[28] \"와...진심 미쳤다이건.\"                                                                                                                                                                                                                                                                \n[29] \"진심 축하드려요 영화못봤는데 봐야겠네요\"                                                                                                                                                                                                                                              \n[30] \"진짜 자랑스럽고 하고싶은 말이 많은데 벅차서 글이 안써지네요 봉감독님~♡ 진심으로 축하드려요\"                                                                                                                                                                                           \n[31] \"너무 자랑스럽다. 진심. 레알.\"                                                                                                                                                                                                                                                         \n[32] \"진심 소름~~축하해요!\"                                                                                                                                                                                                                                                                 \n[33] \"봉준호 감독관님 진심으로 축하드립니다.\"                                                                                                                                                                                                                                               \n[34] \"와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요 진심으로!!!\"                                                                                                                                                                                                   \n[35] \"진심...너무 자랑스럽습니다. ㅠㅠㅠㅠㅠ 너무 감격스러워요 ㅠㅠㅠ\"                                                                                                                                                                                                                      \n[36] \"진심 역사적인 쾌거입니다.\"                                                                                                                                                                                                                                                            \n[37] \"진심 동시대에 살고 있는 것만으로도 영광스럽다ㅠㅠㅠ\"                                                                                                                                                                                                                                  \n[38] \"봉준호 감독! 기생충! 송강호외 배우님들 대한민국 진심으로 축하합니다~\"                                                                                                                                                                                                                 \n[39] \"진심으로 축하하며 여러분들이 진정한 애국자네요\"                                                                                                                                                                                                                                       \n\n\n\ncomment_topic %&gt;%\n  filter(topic == 1 & str_detect(reply, \"정치\")) %&gt;%\n  head(5) %&gt;%\n  pull(reply)\n\n[1] \"한국문화는 1등수준. 정치는 개돼지 3등수준. 대한민국 문화수준을 엎 그레이드 한 봉준호감독에게 존경을 표한다. 월드컵4강.봉준호감독 오스카수상. 한국국민들 절대로 잊혀지지않는 대사건이다. 정말 진심으로 축하드립니다.특히 CJ가 한국영화 산업에 큰 발전에 국민의 한 사람으로 감사드린다.\"\n[2] \"진심 축하드립니다. 대한민국 예술처럼 정치, 경제도 발전해서 살기좋은 나라가 되었으면 좋겠네요\"                                                                                                                                                                                         \n[3] \"좌좀세력, 태극기부대, 빨갱이, 일베애들과 이런것들을 이용애 먹는 정치인들만 없어지면 우리나라 엄청 발전할텐데...기업, 문화 모두 선진국인데 저런것들이 발목을 잡고 너무 깍아먹는다.\"                                                                                                    \n[4] \"정치충들 상은 봉준호가 받았는데 왜 정치얘기를하고있냐\"                                                                                                                                                                                                                                \n[5] \"문화는 일류 정치는 삼류에 개막장\"                                                                                                                                                                                                                                                     \n\n\n\n# 3. 토픽 이름 목록 만들기\nname_topic &lt;- tibble(topic = 1:8,\n                     name = c(\"1. 작품상 수상 축하, 정치적 댓글 비판\",\n                              \"2. 수상 축하, 시상식 감상\",\n                              \"3. 조국 가족, 정치적 해석\",\n                              \"4. 새 역사 쓴 세계적인 영화\",\n                              \"5. 자랑스럽고 감사한 마음\",\n                              \"6. 놀라운 4관왕 수상\",\n                              \"7. 문화계 블랙리스트, 보수 정당 비판\",\n                              \"8. 한국의 세계적 위상\"))\n\n\n# 토픽 이름과 주요 단어 시각화하기\n# 토픽 이름 결합하기\ntop_term_topic_name &lt;- top_term_topic %&gt;%\n  left_join(name_topic, name_topic, by = \"topic\")\n\ntop_term_topic_name\n\n# A tibble: 83 × 4\n# Groups:   topic [8]\n   topic term        beta name                                 \n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                                \n 1     1 작품     0.0299  1. 작품상 수상 축하, 정치적 댓글 비판\n 2     1 진심     0.0240  1. 작품상 수상 축하, 정치적 댓글 비판\n 3     1 정치     0.0192  1. 작품상 수상 축하, 정치적 댓글 비판\n 4     1 자랑     0.0181  1. 작품상 수상 축하, 정치적 댓글 비판\n 5     1 수상소감 0.0166  1. 작품상 수상 축하, 정치적 댓글 비판\n 6     1 댓글     0.0151  1. 작품상 수상 축하, 정치적 댓글 비판\n 7     1 외국     0.0122  1. 작품상 수상 축하, 정치적 댓글 비판\n 8     1 경사     0.0107  1. 작품상 수상 축하, 정치적 댓글 비판\n 9     1 훌륭     0.00998 1. 작품상 수상 축하, 정치적 댓글 비판\n10     1 좌파     0.00814 1. 작품상 수상 축하, 정치적 댓글 비판\n# ℹ 73 more rows\n\n\n\n# 막대 그래프 만들기\nggplot(top_term_topic_name,\n       aes(x = reorder_within(term, beta, name),\n           y = beta,\n           fill = factor(topic))) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ name, scales = \"free\", ncol = 2) +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"영화 기생충 아카데미상 수상 기사 댓글 토픽\",\n       subtitle = \"토픽별 주요 단어 Top 10\",\n       x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(text = element_text(family = \"nanumgothic\"),\n        title = element_text(size = 12),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n최적의 토픽 수 도출\n\n# 하이퍼파라미터 튜닝으로 토픽 수 정하기\n# 1. 토픽 수 바꿔가며 LDA 모델 여러 개 만들기\n# install.packages(\"ldatuning\") # 사양이 낮은 컴퓨터는 설치가 어려움, 에러 지속적으로 발생되는 것 확인\nlibrary(ldatuning)  # windows defender 알림 발생 시, '홈 네트워크, 회사 네트워크 등의 개인 네트워크'만 체크 후 '엑세스 허용' 선택\nmodels &lt;- FindTopicsNumber(dtm = dtm_comment,  # windows defender 알림 발생 시, '홈 네트워크, 회사 네트워크 등의 개인 네트워크'만 체크 후 '엑세스 허용' 선택\n                           topics = 2:20,\n                           return_models = T,\n                           control = list(seed = 1234))\n\nmodels %&gt;%\n  select(topics, Griffiths2004)\n\n   topics Griffiths2004\n1      20     -127213.1\n2      19     -127445.4\n3      18     -126984.0\n4      17     -127317.9\n5      16     -127139.2\n6      15     -126643.9\n7      14     -126742.4\n8      13     -126720.4\n9      12     -127429.4\n10     11     -126677.9\n11     10     -127039.5\n12      9     -127133.2\n13      8     -127234.1\n14      7     -128079.5\n15      6     -128948.9\n16      5     -129672.9\n17      4     -131006.8\n18      3     -133171.8\n19      2     -137154.4\n\n\n\n# 2. 최적 토픽 수 정하기\nFindTopicsNumber_plot(models)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at &lt;https://github.com/nikita-moor/ldatuning/issues&gt;.\n\n\n\n\n\n\n# 3. 모델 추출하기\n# 토픽 수가 8개인 모델 추출하기\noptimal_model &lt;- models %&gt;%\n  filter(topics == 8) %&gt;%\n  pull(LDA_model) %&gt;% # 모델 추출\n  .[[1]] # list 추출\n\n\n# optimal_model\ntidy(optimal_model, matrix = \"beta\")\n\n# A tibble: 47,960 × 3\n   topic term       beta\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1     1 한국  0.000405 \n 2     2 한국  0.0000364\n 3     3 한국  0.0000353\n 4     4 한국  0.00295  \n 5     5 한국  0.0000353\n 6     6 한국  0.0000356\n 7     7 한국  0.00661  \n 8     8 한국  0.0593   \n 9     1 자랑  0.0181   \n10     2 자랑  0.00440  \n# ℹ 47,950 more rows\n\n\n\n# lda_model\ntidy(lda_model, matrix = \"beta\")\n\n# A tibble: 47,960 × 3\n   topic term       beta\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1     1 한국  0.000405 \n 2     2 한국  0.0000364\n 3     3 한국  0.0000353\n 4     4 한국  0.00295  \n 5     5 한국  0.0000353\n 6     6 한국  0.0000356\n 7     7 한국  0.00661  \n 8     8 한국  0.0593   \n 9     1 자랑  0.0181   \n10     2 자랑  0.00440  \n# ℹ 47,950 more rows"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_2.html",
    "href": "teaching/media_ds/about/NLP_2.html",
    "title": "NLP",
    "section": "",
    "text": "Comparing the frequency of words\nData import\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nraw_moon &lt;- readLines('data/speech_moon.txt', encoding = 'UTF-8') # 문재인 대통령 연설문 불러오기\nmoon &lt;- raw_moon %&gt;% \n  as_tibble() %&gt;% \n  mutate(president = 'moon')\n\nraw_park &lt;- readLines('data/speech_park.txt', encoding = 'UTF-8')\npark &lt;- raw_park %&gt;% \n  as_tibble() %&gt;% \n  mutate(president = 'park')\n\nData merge\n\n# 데이터 합치기\nbind_speeches &lt;- bind_rows(moon, park) %&gt;% \n  select(president, value)\n\nhead(bind_speeches)\n\n# A tibble: 6 × 2\n  president value                                                               \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 moon      \"정권교체 하겠습니다!\"                                              \n2 moon      \"  정치교체 하겠습니다!\"                                            \n3 moon      \"  시대교체 하겠습니다!\"                                            \n4 moon      \"  \"                                                                \n5 moon      \"  ‘불비불명(不飛不鳴)’이라는 고사가 있습니다. 남쪽 언덕 나뭇가지에…\n6 moon      \"\"                                                                  \n\ntail(bind_speeches)\n\n# A tibble: 6 × 2\n  president value                                                              \n  &lt;chr&gt;     &lt;chr&gt;                                                              \n1 park      \"국민들이 꿈으로만 가졌던 행복한 삶을 실제로 이룰 수 있도록 도와드…\n2 park      \"\"                                                                 \n3 park      \"감사합니다.\"                                                      \n4 park      \"\"                                                                 \n5 park      \"2012년 7월 10일\"                                                  \n6 park      \"새누리당 예비후보 박근혜\"                                         \n\n\n집단별 단어 빈도 구하기 1. 기본적인 전처리 및 토큰화\n\n# 기본적인 전처리\nlibrary(stringr)\nspeeches &lt;- bind_speeches %&gt;% \n  mutate(value = str_replace_all(value, '[^가-힣]', ' '),\n         value = str_squish(value))\nspeeches\n\n# A tibble: 213 × 2\n   president value                                                              \n   &lt;chr&gt;     &lt;chr&gt;                                                              \n 1 moon      \"정권교체 하겠습니다\"                                              \n 2 moon      \"정치교체 하겠습니다\"                                              \n 3 moon      \"시대교체 하겠습니다\"                                              \n 4 moon      \"\"                                                                 \n 5 moon      \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안…\n 6 moon      \"\"                                                                 \n 7 moon      \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치… \n 8 moon      \"\"                                                                 \n 9 moon      \"\"                                                                 \n10 moon      \"우리나라 대통령 이 되겠습니다\"                                    \n# ℹ 203 more rows\n\n\nTokenization\n\n# 토큰화\nlibrary(tidytext)\nlibrary(KoNLP)\n\nChecking user defined dictionary!\n\nspeeches &lt;- speeches %&gt;% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\nspeeches\n\n# A tibble: 2,997 × 2\n   president word      \n   &lt;chr&gt;     &lt;chr&gt;     \n 1 moon      \"정권교체\"\n 2 moon      \"하겠습니\"\n 3 moon      \"정치\"    \n 4 moon      \"교체\"    \n 5 moon      \"하겠습니\"\n 6 moon      \"시대\"    \n 7 moon      \"교체\"    \n 8 moon      \"하겠습니\"\n 9 moon      \"\"        \n10 moon      \"불비불명\"\n# ℹ 2,987 more rows\n\n\n하위 집단별 단어 빈도 구하기 - count()\n\n# 샘플 텍스트로 작동 원리 알아보기\ndf &lt;- tibble(class = c('a','a','a','b','b','b'),\n             sex = c('female','male','female','male','male','female'))\ndf\n\n# A tibble: 6 × 2\n  class sex   \n  &lt;chr&gt; &lt;chr&gt; \n1 a     female\n2 a     male  \n3 a     female\n4 b     male  \n5 b     male  \n6 b     female\n\ndf %&gt;% count(class, sex)\n\n# A tibble: 4 × 3\n  class sex        n\n  &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n1 a     female     2\n2 a     male       1\n3 b     female     1\n4 b     male       2\n\n\n두 연설문의 단어 빈도 구하기\n\n# 두 연설문의 단어 빈도 구하기\nfrequency &lt;- speeches %&gt;% \n  count(president, word) %&gt;% \n  filter(str_count(word) &gt; 1)\nhead(frequency)\n\n# A tibble: 6 × 3\n  president word         n\n  &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;\n1 moon      가동         1\n2 moon      가사         1\n3 moon      가슴         2\n4 moon      가족         1\n5 moon      가족구조     1\n6 moon      가지         4\n\n\n자주 사용된 단어 추출하기, dplyr::slice_max()\n\n값이 큰 상위 n개의 행을 추출해 내림차순 정렬\n\n\n# 샘플 텍스트로 작동 원리 알아보기\ndf &lt;- tibble(x = c(1:100))\ndf\n\n# A tibble: 100 × 1\n       x\n   &lt;int&gt;\n 1     1\n 2     2\n 3     3\n 4     4\n 5     5\n 6     6\n 7     7\n 8     8\n 9     9\n10    10\n# ℹ 90 more rows\n\ndf %&gt;% slice_max(x, n = 3)\n\n# A tibble: 3 × 1\n      x\n  &lt;int&gt;\n1   100\n2    99\n3    98\n\n\n연설문에 가장 많이 사용된 단어 추출하기\n\n# 연설문에 가장 많이 사용된 단어 추출하기\ntop10 &lt;- frequency %&gt;% \n  group_by(president) %&gt;% \n  slice_max(n, n = 10)\ntop10\n\n# A tibble: 22 × 3\n# Groups:   president [2]\n   president word       n\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n 1 moon      국민      21\n 2 moon      일자리    21\n 3 moon      나라      19\n 4 moon      우리      17\n 5 moon      경제      15\n 6 moon      사회      14\n 7 moon      성장      13\n 8 moon      대통령    12\n 9 moon      정치      12\n10 moon      하게      12\n# ℹ 12 more rows\n\n\n\n# 단어 빈도 동점 처리 제외하고 추출하기, slice_max(with_ties = F) - 원본 데이터의 정렬 순서에 따라 행 추출\ntop10 %&gt;% \n  filter(president == 'park') # 박근혜 전 대통령의 연설문은 동점 처리로 인해, 단어 12개가 모두 추출되어버림\n\n# A tibble: 12 × 3\n# Groups:   president [1]\n   president word       n\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n 1 park      국민      72\n 2 park      행복      23\n 3 park      여러분    20\n 4 park      정부      17\n 5 park      경제      15\n 6 park      신뢰      11\n 7 park      국가      10\n 8 park      우리      10\n 9 park      교육       9\n10 park      사람       9\n11 park      사회       9\n12 park      일자리     9\n\n\n\n# 샘플 데이터로 작동 원리 알아보기\ndf &lt;- tibble(x = c('A','B','C','D'), y = c(4,3,2,2))\ndf %&gt;% \n  slice_max(y, n = 3)\n\n# A tibble: 4 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A         4\n2 B         3\n3 C         2\n4 D         2\n\ndf %&gt;% \n  slice_max(y, n = 3, with_ties = F)\n\n# A tibble: 3 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A         4\n2 B         3\n3 C         2\n\n\n\n# 연설문에 적용하기\ntop10 &lt;- frequency %&gt;% \n  group_by(president) %&gt;% \n  slice_max(n, n = 10, with_ties = F)\ntop10\n\n# A tibble: 20 × 3\n# Groups:   president [2]\n   president word       n\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n 1 moon      국민      21\n 2 moon      일자리    21\n 3 moon      나라      19\n 4 moon      우리      17\n 5 moon      경제      15\n 6 moon      사회      14\n 7 moon      성장      13\n 8 moon      대통령    12\n 9 moon      정치      12\n10 moon      하게      12\n11 park      국민      72\n12 park      행복      23\n13 park      여러분    20\n14 park      정부      17\n15 park      경제      15\n16 park      신뢰      11\n17 park      국가      10\n18 park      우리      10\n19 park      교육       9\n20 park      사람       9\n\n\nVisualization\n\n# 막대 그래프 만들기\n# 1. 변수의 항목별로 그래프 만들기 - facet_wrap()\nggplot(top10, aes(x = reorder(word, n),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president)\n\n\n\n\nfacet_wrap: scale free!\n\n# 2. 그래프별 y축 설정하기\nggplot(top10, aes(x = reorder(word, n),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, # president별 그래프 생성\n             scales = 'free_y') # y축 통일하지 않음\n\n\n\n\n\n# 3. 특정 단어 제외하고 막대 그래프 만들기\ntop10 &lt;- frequency %&gt;% \n  filter(word != '국민') %&gt;% \n  group_by(president) %&gt;% \n  slice_max(n, n = 10, with_ties = F)\n\nggplot(top10, aes(x = reorder(word, n),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = 'free_y')\n\n\n\n\nreorder_within!\n\n# 4. 축 정렬하기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = 'free_y')\n\n\n\n\n\n# 5. 변수 항목 제거하기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = \"free_y\") +\n  scale_x_reordered() +\n  labs(x = NULL) \n\n\n\n\n\n\nOdds Ratio\n상대적으로 중요한 단어 찾기\n\n# Long form을 Wide form으로 변환하기\n# Long form 데이터 살펴보기\ndf_long &lt;- frequency %&gt;% \n  group_by(president) %&gt;% \n  slice_max(n, n = 10) %&gt;% \n  filter(word %in% c('국민','우리','정치','행복'))\ndf_long\n\n# A tibble: 6 × 3\n# Groups:   president [2]\n  president word      n\n  &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n1 moon      국민     21\n2 moon      우리     17\n3 moon      정치     12\n4 park      국민     72\n5 park      행복     23\n6 park      우리     10\n\n\n\n# Long form을 Wide form으로 변형하기\ndf_wide &lt;- df_long %&gt;% \n  pivot_wider(names_from = president,\n              values_from = n)\ndf_wide\n\n# A tibble: 4 × 3\n  word   moon  park\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 국민     21    72\n2 우리     17    10\n3 정치     12    NA\n4 행복     NA    23\n\n\n\n# NA를 0으로 바꾸기\ndf_wide &lt;- df_long %&gt;% \n  pivot_wider(names_from = president,\n              values_from = n,\n              values_fill = list(n = 0))\ndf_wide\n\n# A tibble: 4 × 3\n  word   moon  park\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 국민     21    72\n2 우리     17    10\n3 정치     12     0\n4 행복      0    23\n\n\n\n# 연설문 단어 빈도를 Wide form으로 변환하기\nfrequency_wide &lt;- frequency %&gt;% \n  pivot_wider(names_from = president,\n              values_from = n,\n              values_fill = list(n = 0))\nfrequency_wide\n\n# A tibble: 955 × 3\n   word      moon  park\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;\n 1 가동         1     0\n 2 가사         1     0\n 3 가슴         2     0\n 4 가족         1     1\n 5 가족구조     1     0\n 6 가지         4     0\n 7 가치         3     1\n 8 각종         1     0\n 9 감당         1     0\n10 강력         3     0\n# ℹ 945 more rows\n\n\nOdds Ratio!\n\n# 오즈비 구하기\n# 1. 단어의 비중을 나타낸 변수 추가하기\nfrequency_wide &lt;- frequency_wide %&gt;%\n  mutate(ratio_moon = ((moon)/(sum(moon))), # moon 에서 단어의 비중\n         ratio_park = ((park)/(sum(park)))) # park 에서 단어의 비중\nfrequency_wide\n\n# A tibble: 955 × 5\n   word      moon  park ratio_moon ratio_park\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 가동         1     0   0.000749    0      \n 2 가사         1     0   0.000749    0      \n 3 가슴         2     0   0.00150     0      \n 4 가족         1     1   0.000749    0.00117\n 5 가족구조     1     0   0.000749    0      \n 6 가지         4     0   0.00299     0      \n 7 가치         3     1   0.00225     0.00117\n 8 각종         1     0   0.000749    0      \n 9 감당         1     0   0.000749    0      \n10 강력         3     0   0.00225     0      \n# ℹ 945 more rows\n\n\n\n# 어떤 단어가 한 연설문에 전혀 사용되지 않으면 빈도/오즈비 0, 단어 비중 비교 불가, \n#빈도가 0보다 큰 값이 되도록 모든 값에 +1\nfrequency_wide &lt;- frequency_wide %&gt;%\n  mutate(ratio_moon = ((moon + 1)/(sum(moon + 1))), # moon에서 단어의 비중\n         ratio_park = ((park + 1)/(sum(park + 1)))) # park에서 단어의 비중\nfrequency_wide\n\n# A tibble: 955 × 5\n   word      moon  park ratio_moon ratio_park\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 가동         1     0   0.000873   0.000552\n 2 가사         1     0   0.000873   0.000552\n 3 가슴         2     0   0.00131    0.000552\n 4 가족         1     1   0.000873   0.00110 \n 5 가족구조     1     0   0.000873   0.000552\n 6 가지         4     0   0.00218    0.000552\n 7 가치         3     1   0.00175    0.00110 \n 8 각종         1     0   0.000873   0.000552\n 9 감당         1     0   0.000873   0.000552\n10 강력         3     0   0.00175    0.000552\n# ℹ 945 more rows\n\n\nCreate a new odds ratio variable\n\n# 2. 오즈비 변수 추가하기\nfrequency_wide &lt;- frequency_wide %&gt;% \n  mutate(odds_ratio = ratio_moon / ratio_park)\nfrequency_wide\n\n# A tibble: 955 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 가동         1     0   0.000873   0.000552      1.58 \n 2 가사         1     0   0.000873   0.000552      1.58 \n 3 가슴         2     0   0.00131    0.000552      2.37 \n 4 가족         1     1   0.000873   0.00110       0.791\n 5 가족구조     1     0   0.000873   0.000552      1.58 \n 6 가지         4     0   0.00218    0.000552      3.96 \n 7 가치         3     1   0.00175    0.00110       1.58 \n 8 각종         1     0   0.000873   0.000552      1.58 \n 9 감당         1     0   0.000873   0.000552      1.58 \n10 강력         3     0   0.00175    0.000552      3.17 \n# ℹ 945 more rows\n\n\n\nfrequency_wide %&gt;% \n  arrange(-odds_ratio) # moon에서 상대적인 비중이 클수록 1보다 큰 값\n\n# A tibble: 955 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 복지국가     8     0    0.00393   0.000552       7.12\n 2 세상         6     0    0.00306   0.000552       5.54\n 3 여성         6     0    0.00306   0.000552       5.54\n 4 정의         6     0    0.00306   0.000552       5.54\n 5 강자         5     0    0.00262   0.000552       4.75\n 6 공평         5     0    0.00262   0.000552       4.75\n 7 대통령의     5     0    0.00262   0.000552       4.75\n 8 보통         5     0    0.00262   0.000552       4.75\n 9 상생         5     0    0.00262   0.000552       4.75\n10 지방         5     0    0.00262   0.000552       4.75\n# ℹ 945 more rows\n\nfrequency_wide %&gt;% \n  arrange(odds_ratio) # park에서 상대적인 비중이 클수록 1보다 작은 값\n\n# A tibble: 955 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 박근혜       0     8   0.000436    0.00496     0.0879\n 2 여러분       2    20   0.00131     0.0116      0.113 \n 3 행복         3    23   0.00175     0.0132      0.132 \n 4 실천         0     5   0.000436    0.00331     0.132 \n 5 정보         0     5   0.000436    0.00331     0.132 \n 6 투명         0     5   0.000436    0.00331     0.132 \n 7 과제         0     4   0.000436    0.00276     0.158 \n 8 국정운영     0     4   0.000436    0.00276     0.158 \n 9 시작         0     4   0.000436    0.00276     0.158 \n10 지식         0     4   0.000436    0.00276     0.158 \n# ℹ 945 more rows\n\n\n\nfrequency_wide %&gt;% \n  arrange(abs(1-odds_ratio)) # 두 연설문에서 단어 비중이 같으면 1\n\n# A tibble: 955 × 6\n   word    moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 때문       4     3    0.00218    0.00221      0.989\n 2 강화       3     2    0.00175    0.00165      1.06 \n 3 부담       3     2    0.00175    0.00165      1.06 \n 4 세계       3     2    0.00175    0.00165      1.06 \n 5 책임       3     2    0.00175    0.00165      1.06 \n 6 협력       3     2    0.00175    0.00165      1.06 \n 7 거대       2     1    0.00131    0.00110      1.19 \n 8 교체       2     1    0.00131    0.00110      1.19 \n 9 근본적     2     1    0.00131    0.00110      1.19 \n10 기반       2     1    0.00131    0.00110      1.19 \n# ℹ 945 more rows\n\n\n\n# 상대적으로 중요한 단어 추출하기\n# 오즈비가 가장 높거나 가장 낮은 단어 추출하기\ntop10 &lt;- frequency_wide %&gt;% \n  filter(rank(odds_ratio) &lt;= 10 | rank(-odds_ratio) &lt;= 10)\n\ntop10 %&gt;% \n  arrange(-odds_ratio)\n\n# A tibble: 20 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 복지국가     8     0   0.00393    0.000552     7.12  \n 2 세상         6     0   0.00306    0.000552     5.54  \n 3 여성         6     0   0.00306    0.000552     5.54  \n 4 정의         6     0   0.00306    0.000552     5.54  \n 5 강자         5     0   0.00262    0.000552     4.75  \n 6 공평         5     0   0.00262    0.000552     4.75  \n 7 대통령의     5     0   0.00262    0.000552     4.75  \n 8 보통         5     0   0.00262    0.000552     4.75  \n 9 상생         5     0   0.00262    0.000552     4.75  \n10 지방         5     0   0.00262    0.000552     4.75  \n11 과제         0     4   0.000436   0.00276      0.158 \n12 국정운영     0     4   0.000436   0.00276      0.158 \n13 시작         0     4   0.000436   0.00276      0.158 \n14 지식         0     4   0.000436   0.00276      0.158 \n15 행복         3    23   0.00175    0.0132       0.132 \n16 실천         0     5   0.000436   0.00331      0.132 \n17 정보         0     5   0.000436   0.00331      0.132 \n18 투명         0     5   0.000436   0.00331      0.132 \n19 여러분       2    20   0.00131    0.0116       0.113 \n20 박근혜       0     8   0.000436   0.00496      0.0879\n\n\nVisualization of relative importance\n\n# 막대 그래프 만들기\n# 1. 비중이 큰 연설문을 나타낸 변수 추가하기\ntop10 &lt;- top10 %&gt;%\n  mutate(president = ifelse(odds_ratio &gt; 1, \"moon\", \"park\"),\n         n = ifelse(odds_ratio &gt; 1, moon, park))\ntop10\n\n# A tibble: 20 × 8\n   word      moon  park ratio_moon ratio_park odds_ratio president     n\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt;\n 1 강자         5     0   0.00262    0.000552     4.75   moon          5\n 2 공평         5     0   0.00262    0.000552     4.75   moon          5\n 3 대통령의     5     0   0.00262    0.000552     4.75   moon          5\n 4 보통         5     0   0.00262    0.000552     4.75   moon          5\n 5 복지국가     8     0   0.00393    0.000552     7.12   moon          8\n 6 상생         5     0   0.00262    0.000552     4.75   moon          5\n 7 세상         6     0   0.00306    0.000552     5.54   moon          6\n 8 여러분       2    20   0.00131    0.0116       0.113  park         20\n 9 여성         6     0   0.00306    0.000552     5.54   moon          6\n10 정의         6     0   0.00306    0.000552     5.54   moon          6\n11 지방         5     0   0.00262    0.000552     4.75   moon          5\n12 행복         3    23   0.00175    0.0132       0.132  park         23\n13 과제         0     4   0.000436   0.00276      0.158  park          4\n14 국정운영     0     4   0.000436   0.00276      0.158  park          4\n15 박근혜       0     8   0.000436   0.00496      0.0879 park          8\n16 시작         0     4   0.000436   0.00276      0.158  park          4\n17 실천         0     5   0.000436   0.00331      0.132  park          5\n18 정보         0     5   0.000436   0.00331      0.132  park          5\n19 지식         0     4   0.000436   0.00276      0.158  park          4\n20 투명         0     5   0.000436   0.00331      0.132  park          5\n\n# 2. 막대 그래프 만들기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = 'free_y') +\n  scale_x_reordered()\n\n\n\n\n\n# 3. 그래프별로 축 설정하기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = \"free\") +\n  scale_x_reordered() +\n  labs(x = NULL) \n\n\n\n\n주요 단어가 사용된 문장 살펴보기\n\n# 1. 원문을 문장 기준으로 토큰화하기\nspeeches_sentence &lt;- bind_speeches %&gt;% \n  as_tibble() %&gt;% \n  unnest_tokens(input = value,\n                output = sentence,\n                token = 'sentences')\nspeeches_sentence\n\n# A tibble: 329 × 2\n   president sentence                                                          \n   &lt;chr&gt;     &lt;chr&gt;                                                             \n 1 moon      \"정권교체 하겠습니다!\"                                            \n 2 moon      \"정치교체 하겠습니다!\"                                            \n 3 moon      \"시대교체 하겠습니다!\"                                            \n 4 moon      \"\"                                                                \n 5 moon      \"‘불비불명(不飛不鳴)’이라는 고사가 있습니다.\"                     \n 6 moon      \"남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새.\"      \n 7 moon      \"그러나 그 새는 한번 날면 하늘 끝까지 날고, 한번 울면 천지를 뒤흔…\n 8 moon      \"그 동안 정치와 거리를 둬 왔습니다.\"                              \n 9 moon      \"그러나 암울한 시대가 저를 정치로 불러냈습니다.\"                  \n10 moon      \"더 이상 남쪽 나뭇가지에 머무를 수 없었습니다.\"                   \n# ℹ 319 more rows\n\nhead(speeches_sentence)\n\n# A tibble: 6 × 2\n  president sentence                                                    \n  &lt;chr&gt;     &lt;chr&gt;                                                       \n1 moon      \"정권교체 하겠습니다!\"                                      \n2 moon      \"정치교체 하겠습니다!\"                                      \n3 moon      \"시대교체 하겠습니다!\"                                      \n4 moon      \"\"                                                          \n5 moon      \"‘불비불명(不飛不鳴)’이라는 고사가 있습니다.\"               \n6 moon      \"남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새.\"\n\ntail(speeches_sentence)\n\n# A tibble: 6 × 2\n  president sentence                                                           \n  &lt;chr&gt;     &lt;chr&gt;                                                              \n1 park      국민 여러분의 행복이 곧 저의 행복입니다.                           \n2 park      사랑하는 조국 대한민국과 국민 여러분을 위해, 앞으로 머나 먼 길, 끝…\n3 park      그 길을 함께 해주시길 부탁드립니다.                                \n4 park      감사합니다.                                                        \n5 park      2012년 7월 10일                                                    \n6 park      새누리당 예비후보 박근혜                                           \n\n\n\n# 2. 주요 단어가 사용된 문장 추출하기 - str_detect()\nspeeches_sentence %&gt;% \n  filter(president == 'moon' & str_detect(sentence, '복지국가'))\n\n# A tibble: 8 × 2\n  president sentence                                                            \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 moon      ‘강한 복지국가’를 향해 담대하게 나아가겠습니다.                     \n2 moon      2백 년 전 이와 같은 소득재분배, 복지국가의 사상을 가진 위정자가 지… \n3 moon      이제 우리는 복지국가를 향해 담대하게 나아갈 때입니다.               \n4 moon      부자감세, 4대강 사업 같은 시대착오적 과오를 청산하고, 하루빨리 복지…\n5 moon      우리는 지금 복지국가로 가느냐, 양극화의 분열된 국가로 가느냐 하는 … \n6 moon      강한 복지국가일수록 국가 경쟁력도 더 높습니다.                      \n7 moon      결국 복지국가로 가는 길은 사람에 대한 투자, 일자리 창출, 자영업 고… \n8 moon      우리는 과감히 강한 보편적 복지국가로 가야 합니다.                   \n\n\n두 연설문 모두에서 중요했던 단어들은?\n\n# 중요도가 비슷한 단어 살펴보기\nfrequency_wide %&gt;% \n  arrange(abs(1 - odds_ratio)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 6\n   word    moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 때문       4     3    0.00218    0.00221      0.989\n 2 강화       3     2    0.00175    0.00165      1.06 \n 3 부담       3     2    0.00175    0.00165      1.06 \n 4 세계       3     2    0.00175    0.00165      1.06 \n 5 책임       3     2    0.00175    0.00165      1.06 \n 6 협력       3     2    0.00175    0.00165      1.06 \n 7 거대       2     1    0.00131    0.00110      1.19 \n 8 교체       2     1    0.00131    0.00110      1.19 \n 9 근본적     2     1    0.00131    0.00110      1.19 \n10 기반       2     1    0.00131    0.00110      1.19 \n\n# 중요도가 비슷하면서 빈도가 높은 단어\nfrequency_wide %&gt;% \n  filter(moon &gt;= 5 & park &gt;= 5) %&gt;%\n  arrange(abs(1 - odds_ratio)) %&gt;%\n  head(10)\n\n# A tibble: 10 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 사회        14     9    0.00655    0.00552      1.19 \n 2 사람         9     9    0.00436    0.00552      0.791\n 3 경제        15    15    0.00698    0.00883      0.791\n 4 지원         5     5    0.00262    0.00331      0.791\n 5 우리        17    10    0.00786    0.00607      1.29 \n 6 불안         7     8    0.00349    0.00496      0.703\n 7 산업         9     5    0.00436    0.00331      1.32 \n 8 대한민국    11     6    0.00524    0.00386      1.36 \n 9 국가         7    10    0.00349    0.00607      0.576\n10 교육         6     9    0.00306    0.00552      0.554\n\n\n\n\nLog Odds Ratio\n\n# 로그 오즈비 구하기\nfrequency_wide &lt;- frequency_wide %&gt;% \n  mutate(log_odds_ratio = log(odds_ratio))\nfrequency_wide\n\n# A tibble: 955 × 7\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 가동         1     0   0.000873   0.000552      1.58           0.459\n 2 가사         1     0   0.000873   0.000552      1.58           0.459\n 3 가슴         2     0   0.00131    0.000552      2.37           0.865\n 4 가족         1     1   0.000873   0.00110       0.791         -0.234\n 5 가족구조     1     0   0.000873   0.000552      1.58           0.459\n 6 가지         4     0   0.00218    0.000552      3.96           1.38 \n 7 가치         3     1   0.00175    0.00110       1.58           0.459\n 8 각종         1     0   0.000873   0.000552      1.58           0.459\n 9 감당         1     0   0.000873   0.000552      1.58           0.459\n10 강력         3     0   0.00175    0.000552      3.17           1.15 \n# ℹ 945 more rows\n\n\n\n# moon에서 비중이 큰 단어, 0보다 큰 양수\nfrequency_wide %&gt;%\n  arrange(-log_odds_ratio)\n\n# A tibble: 955 × 7\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 복지국가     8     0    0.00393   0.000552       7.12           1.96\n 2 세상         6     0    0.00306   0.000552       5.54           1.71\n 3 여성         6     0    0.00306   0.000552       5.54           1.71\n 4 정의         6     0    0.00306   0.000552       5.54           1.71\n 5 강자         5     0    0.00262   0.000552       4.75           1.56\n 6 공평         5     0    0.00262   0.000552       4.75           1.56\n 7 대통령의     5     0    0.00262   0.000552       4.75           1.56\n 8 보통         5     0    0.00262   0.000552       4.75           1.56\n 9 상생         5     0    0.00262   0.000552       4.75           1.56\n10 지방         5     0    0.00262   0.000552       4.75           1.56\n# ℹ 945 more rows\n\n\n\n# park에서 비중이 큰 단어, 0보다 작은 음수\nfrequency_wide %&gt;%\n  arrange(log_odds_ratio)\n\n# A tibble: 955 × 7\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   &lt;chr&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 박근혜       0     8   0.000436    0.00496     0.0879          -2.43\n 2 여러분       2    20   0.00131     0.0116      0.113           -2.18\n 3 행복         3    23   0.00175     0.0132      0.132           -2.03\n 4 실천         0     5   0.000436    0.00331     0.132           -2.03\n 5 정보         0     5   0.000436    0.00331     0.132           -2.03\n 6 투명         0     5   0.000436    0.00331     0.132           -2.03\n 7 과제         0     4   0.000436    0.00276     0.158           -1.84\n 8 국정운영     0     4   0.000436    0.00276     0.158           -1.84\n 9 시작         0     4   0.000436    0.00276     0.158           -1.84\n10 지식         0     4   0.000436    0.00276     0.158           -1.84\n# ℹ 945 more rows\n\n\n\n# 비중이 비슷한 단어, 0에 가까운\nfrequency_wide %&gt;%\n  arrange(abs(log_odds_ratio))\n\n# A tibble: 955 × 7\n   word    moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 때문       4     3    0.00218    0.00221      0.989        -0.0109\n 2 강화       3     2    0.00175    0.00165      1.06          0.0537\n 3 부담       3     2    0.00175    0.00165      1.06          0.0537\n 4 세계       3     2    0.00175    0.00165      1.06          0.0537\n 5 책임       3     2    0.00175    0.00165      1.06          0.0537\n 6 협력       3     2    0.00175    0.00165      1.06          0.0537\n 7 거대       2     1    0.00131    0.00110      1.19          0.171 \n 8 교체       2     1    0.00131    0.00110      1.19          0.171 \n 9 근본적     2     1    0.00131    0.00110      1.19          0.171 \n10 기반       2     1    0.00131    0.00110      1.19          0.171 \n# ℹ 945 more rows\n\n\n\n# 로그 오즈비를 이용해 중요한 단어 비교하기\ntop10 &lt;- frequency_wide %&gt;% \n  group_by(president = ifelse(log_odds_ratio &gt; 0, 'moon', 'park')) %&gt;% \n  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)\ntop10\n\n# A tibble: 20 × 8\n# Groups:   president [2]\n   word     moon  park ratio_moon ratio_park odds_ratio log_odds_ratio president\n   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n 1 복지국…     8     0   0.00393    0.000552     7.12             1.96 moon     \n 2 세상        6     0   0.00306    0.000552     5.54             1.71 moon     \n 3 여성        6     0   0.00306    0.000552     5.54             1.71 moon     \n 4 정의        6     0   0.00306    0.000552     5.54             1.71 moon     \n 5 강자        5     0   0.00262    0.000552     4.75             1.56 moon     \n 6 공평        5     0   0.00262    0.000552     4.75             1.56 moon     \n 7 대통령…     5     0   0.00262    0.000552     4.75             1.56 moon     \n 8 보통        5     0   0.00262    0.000552     4.75             1.56 moon     \n 9 상생        5     0   0.00262    0.000552     4.75             1.56 moon     \n10 지방        5     0   0.00262    0.000552     4.75             1.56 moon     \n11 박근혜      0     8   0.000436   0.00496      0.0879          -2.43 park     \n12 여러분      2    20   0.00131    0.0116       0.113           -2.18 park     \n13 행복        3    23   0.00175    0.0132       0.132           -2.03 park     \n14 실천        0     5   0.000436   0.00331      0.132           -2.03 park     \n15 정보        0     5   0.000436   0.00331      0.132           -2.03 park     \n16 투명        0     5   0.000436   0.00331      0.132           -2.03 park     \n17 과제        0     4   0.000436   0.00276      0.158           -1.84 park     \n18 국정운…     0     4   0.000436   0.00276      0.158           -1.84 park     \n19 시작        0     4   0.000436   0.00276      0.158           -1.84 park     \n20 지식        0     4   0.000436   0.00276      0.158           -1.84 park     \n\n\n\n# 주요 변수 추출\ntop10 %&gt;%\n  arrange(-log_odds_ratio) %&gt;%\n  select(word, log_odds_ratio, president)\n\n# A tibble: 20 × 3\n# Groups:   president [2]\n   word     log_odds_ratio president\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    \n 1 복지국가           1.96 moon     \n 2 세상               1.71 moon     \n 3 여성               1.71 moon     \n 4 정의               1.71 moon     \n 5 강자               1.56 moon     \n 6 공평               1.56 moon     \n 7 대통령의           1.56 moon     \n 8 보통               1.56 moon     \n 9 상생               1.56 moon     \n10 지방               1.56 moon     \n11 과제              -1.84 park     \n12 국정운영          -1.84 park     \n13 시작              -1.84 park     \n14 지식              -1.84 park     \n15 행복              -2.03 park     \n16 실천              -2.03 park     \n17 정보              -2.03 park     \n18 투명              -2.03 park     \n19 여러분            -2.18 park     \n20 박근혜            -2.43 park     \n\n# 막대 그래프 만들기\nggplot(top10, aes(x = reorder(word, log_odds_ratio),\n                  y = log_odds_ratio,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL)\n\n\n\n\n\n\nTF-IDF\nTerm Frequency-Inverse Document Frequency (TF-IDF)\n\nWeighting words \\(W_{(t,d)}\\) based on their importance within and across documents to see how the term t is original (or unique) is in a document d\n\n\\[\nW_{(t,d)}=tf_{(t,d)} \\times idf_{(t)}\n\\] \\[\nW_{(t,d)}=tf_{(t,d)} \\times log(\\frac{N}{df_{t}})\n\\]\n\n\\(t\\) = a term\n\\(d\\) = a document\n\\(tf_{t,d}\\) = frequency of term \\(t\\) (e.g. a word) in doc \\(d\\) (e.g. a sentence or an article)\n\\(df_{term}\\) = # of documents containing the term\n\n\nA high \\(tf_{t,d}\\) indicates that the term is highly significant within the document, while a high \\(df_{t}\\) suggests that the term is widely used across various documents (e.g., common verbs). Multiplying by \\(idf_{t}\\) helps to account for the term’s universality. Ultimately, tf-idf effectively captures a term’s uniqueness and importance, taking into consideration its prevalence across documents.\n\nAs an example,\n\ntexts &lt;- c(\"Text mining is important in academic research.\",\n           \"Feature extraction is a crucial step in text mining.\",\n           \"Cats and dogs are popular pets.\",\n           \"Elephants are large animals.\",\n           \"Whales are mammals that live in the ocean.\")\n\ntext_df &lt;- tibble(doc_id = 1:length(texts), text = texts)\ntext_df\n\n# A tibble: 5 × 2\n  doc_id text                                                \n   &lt;int&gt; &lt;chr&gt;                                               \n1      1 Text mining is important in academic research.      \n2      2 Feature extraction is a crucial step in text mining.\n3      3 Cats and dogs are popular pets.                     \n4      4 Elephants are large animals.                        \n5      5 Whales are mammals that live in the ocean.          \n\ntokens &lt;- text_df %&gt;%\n  unnest_tokens(word, text)\ntokens\n\n# A tibble: 34 × 2\n   doc_id word      \n    &lt;int&gt; &lt;chr&gt;     \n 1      1 text      \n 2      1 mining    \n 3      1 is        \n 4      1 important \n 5      1 in        \n 6      1 academic  \n 7      1 research  \n 8      2 feature   \n 9      2 extraction\n10      2 is        \n# ℹ 24 more rows\n\n# Calculate the TF-IDF scores\ntf_idf &lt;- tokens %&gt;%\n  count(doc_id, word) %&gt;%\n  bind_tf_idf(word, doc_id, n)\ntf_idf\n\n# A tibble: 34 × 6\n   doc_id word           n    tf   idf tf_idf\n    &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1      1 academic       1 0.143 1.61  0.230 \n 2      1 important      1 0.143 1.61  0.230 \n 3      1 in             1 0.143 0.511 0.0730\n 4      1 is             1 0.143 0.916 0.131 \n 5      1 mining         1 0.143 0.916 0.131 \n 6      1 research       1 0.143 1.61  0.230 \n 7      1 text           1 0.143 0.916 0.131 \n 8      2 a              1 0.111 1.61  0.179 \n 9      2 crucial        1 0.111 1.61  0.179 \n10      2 extraction     1 0.111 1.61  0.179 \n# ℹ 24 more rows\n\n# Spread into a wide format\ntf_idf_matrix &lt;- tf_idf %&gt;%\n  select(doc_id, word, tf_idf) %&gt;%\n  spread(key = word, value = tf_idf, fill = 0)\ntf_idf_matrix\n\n# A tibble: 5 × 28\n  doc_id     a academic   and animals    are  cats crucial  dogs elephants\n   &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1      1 0        0.230 0       0     0      0       0     0         0    \n2      2 0.179    0     0       0     0      0       0.179 0         0    \n3      3 0        0     0.268   0     0.0851 0.268   0     0.268     0    \n4      4 0        0     0       0.402 0.128  0       0     0         0.402\n5      5 0        0     0       0     0.0639 0       0     0         0    \n# ℹ 18 more variables: extraction &lt;dbl&gt;, feature &lt;dbl&gt;, important &lt;dbl&gt;,\n#   `in` &lt;dbl&gt;, is &lt;dbl&gt;, large &lt;dbl&gt;, live &lt;dbl&gt;, mammals &lt;dbl&gt;, mining &lt;dbl&gt;,\n#   ocean &lt;dbl&gt;, pets &lt;dbl&gt;, popular &lt;dbl&gt;, research &lt;dbl&gt;, step &lt;dbl&gt;,\n#   text &lt;dbl&gt;, that &lt;dbl&gt;, the &lt;dbl&gt;, whales &lt;dbl&gt;\n\n\nimport data & basic preprocessing\n\nraw_speeches &lt;- readr::read_csv(\"data/speeches_presidents.csv\")\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): president, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_speeches\n\n# A tibble: 4 × 2\n  president value                                                               \n  &lt;chr&gt;     &lt;chr&gt;                                                               \n1 문재인    \"정권교체 하겠습니다!   정치교체 하겠습니다!   시대교체 하겠습니다!…\n2 박근혜    \"존경하는 국민 여러분! 저는 오늘, 국민 한 분 한 분의 꿈이 이루어지… \n3 이명박    \"존경하는 국민 여러분, 사랑하는 한나라당 당원 동지 여러분! 저는 오… \n4 노무현    \"어느때인가 부터 제가 대통령이 되겠다고 말을 하기 시작했습니다. 많… \n\n# 기본적인 전처리\nspeeches &lt;- raw_speeches %&gt;%\n  mutate(value = str_replace_all(value, \"[^가-힣]\", \" \"),\n         value = str_squish(value))\n# 토큰화\nspeeches &lt;- speeches %&gt;%\n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\n\nWord frequency\n\n# 단어 빈도 구하기\nfrequecy &lt;- speeches %&gt;%\n  count(president, word) %&gt;%\n  filter(str_count(word) &gt; 1)\nfrequecy\n\n# A tibble: 1,513 × 3\n   president word      n\n   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n 1 노무현    가슴      2\n 2 노무현    가훈      2\n 3 노무현    갈등      1\n 4 노무현    감옥      1\n 5 노무현    강자      1\n 6 노무현    개편      4\n 7 노무현    개혁      4\n 8 노무현    건국      1\n 9 노무현    경선      1\n10 노무현    경쟁      1\n# ℹ 1,503 more rows\n\n\nTF-IDF calculation\n\n# TF-IDF 구하기\nlibrary(tidytext)\nfrequecy &lt;- frequecy %&gt;%\n  bind_tf_idf(term = word, # 단어\n              document = president, # 텍스트 구분 기준\n              n = n) %&gt;% # 단어 빈도\n  arrange(-tf_idf)\nfrequecy\n\n# A tibble: 1,513 × 6\n   president word         n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 노무현    공식         6 0.0163  1.39  0.0227\n 2 노무현    비젼         6 0.0163  1.39  0.0227\n 3 노무현    정계         6 0.0163  1.39  0.0227\n 4 이명박    리더십       6 0.0158  1.39  0.0219\n 5 노무현    권력         9 0.0245  0.693 0.0170\n 6 노무현    개편         4 0.0109  1.39  0.0151\n 7 이명박    당원         4 0.0105  1.39  0.0146\n 8 이명박    동지         4 0.0105  1.39  0.0146\n 9 이명박    일류국가     4 0.0105  1.39  0.0146\n10 박근혜    박근혜       8 0.00962 1.39  0.0133\n# ℹ 1,503 more rows\n\n\n\n# TF-IDF가 높은 단어 살펴보기\nfrequecy %&gt;% filter(president == \"문재인\")\n\n# A tibble: 688 × 6\n   president word         n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 문재인    복지국가     8 0.00608 1.39  0.00843\n 2 문재인    여성         6 0.00456 1.39  0.00633\n 3 문재인    공평         5 0.00380 1.39  0.00527\n 4 문재인    담쟁이       5 0.00380 1.39  0.00527\n 5 문재인    대통령의     5 0.00380 1.39  0.00527\n 6 문재인    보통         5 0.00380 1.39  0.00527\n 7 문재인    상생         5 0.00380 1.39  0.00527\n 8 문재인    우리나라    10 0.00760 0.693 0.00527\n 9 문재인    지방         5 0.00380 1.39  0.00527\n10 문재인    확대        10 0.00760 0.693 0.00527\n# ℹ 678 more rows\n\nfrequecy %&gt;% filter(president == \"박근혜\")\n\n# A tibble: 407 × 6\n   president word         n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 박근혜    박근혜       8 0.00962 1.39  0.0133 \n 2 박근혜    정보         5 0.00601 1.39  0.00833\n 3 박근혜    투명         5 0.00601 1.39  0.00833\n 4 박근혜    행복        23 0.0276  0.288 0.00795\n 5 박근혜    교육         9 0.0108  0.693 0.00750\n 6 박근혜    국정운영     4 0.00481 1.39  0.00666\n 7 박근혜    정부        17 0.0204  0.288 0.00588\n 8 박근혜    개개인       3 0.00361 1.39  0.00500\n 9 박근혜    개인         3 0.00361 1.39  0.00500\n10 박근혜    공개         3 0.00361 1.39  0.00500\n# ℹ 397 more rows\n\nfrequecy %&gt;% filter(president == \"이명박\")\n\n# A tibble: 202 × 6\n   president word         n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 이명박    리더십       6 0.0158  1.39  0.0219 \n 2 이명박    당원         4 0.0105  1.39  0.0146 \n 3 이명박    동지         4 0.0105  1.39  0.0146 \n 4 이명박    일류국가     4 0.0105  1.39  0.0146 \n 5 이명박    한나라       7 0.0184  0.693 0.0128 \n 6 이명박    나라        15 0.0395  0.288 0.0114 \n 7 이명박    도약         3 0.00789 1.39  0.0109 \n 8 이명박    일하         3 0.00789 1.39  0.0109 \n 9 이명박    사랑         5 0.0132  0.693 0.00912\n10 이명박    인생         5 0.0132  0.693 0.00912\n# ℹ 192 more rows\n\nfrequecy %&gt;% filter(president == \"노무현\")\n\n# A tibble: 216 × 6\n   president word         n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 노무현    공식         6 0.0163  1.39  0.0227 \n 2 노무현    비젼         6 0.0163  1.39  0.0227 \n 3 노무현    정계         6 0.0163  1.39  0.0227 \n 4 노무현    권력         9 0.0245  0.693 0.0170 \n 5 노무현    개편         4 0.0109  1.39  0.0151 \n 6 노무현    국회의원     3 0.00817 1.39  0.0113 \n 7 노무현    남북대화     3 0.00817 1.39  0.0113 \n 8 노무현    총리         3 0.00817 1.39  0.0113 \n 9 노무현    가훈         2 0.00545 1.39  0.00755\n10 노무현    개혁         4 0.0109  0.693 0.00755\n# ℹ 206 more rows\n\n\n\n# TF-IDF가 낮은 단어 살펴보기, 4개가 동일한 값으로 출력됨\nfrequecy %&gt;%\n  filter(president == \"문재인\") %&gt;%\n  arrange(tf_idf)\n\n# A tibble: 688 × 6\n   president word       n       tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 문재인    경쟁       6 0.00456      0      0\n 2 문재인    경제      15 0.0114       0      0\n 3 문재인    고통       4 0.00304      0      0\n 4 문재인    과거       1 0.000760     0      0\n 5 문재인    국민      21 0.0160       0      0\n 6 문재인    기회       5 0.00380      0      0\n 7 문재인    대통령    12 0.00913      0      0\n 8 문재인    동안       2 0.00152      0      0\n 9 문재인    들이       9 0.00684      0      0\n10 문재인    마음       2 0.00152      0      0\n# ℹ 678 more rows\n\nfrequecy %&gt;%\n  filter(president == \"박근혜\") %&gt;%\n  arrange(tf_idf)\n\n# A tibble: 407 × 6\n   president word       n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 박근혜    경쟁       1 0.00120     0      0\n 2 박근혜    경제      15 0.0180      0      0\n 3 박근혜    고통       4 0.00481     0      0\n 4 박근혜    과거       2 0.00240     0      0\n 5 박근혜    국민      72 0.0865      0      0\n 6 박근혜    기회       1 0.00120     0      0\n 7 박근혜    대통령     3 0.00361     0      0\n 8 박근혜    동안       3 0.00361     0      0\n 9 박근혜    들이       3 0.00361     0      0\n10 박근혜    마음       3 0.00361     0      0\n# ℹ 397 more rows\n\nfrequecy %&gt;%\n  filter(president == \"이명박\") %&gt;%\n  arrange(tf_idf)\n\n# A tibble: 202 × 6\n   president word       n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 이명박    경쟁       3 0.00789     0      0\n 2 이명박    경제       5 0.0132      0      0\n 3 이명박    고통       1 0.00263     0      0\n 4 이명박    과거       1 0.00263     0      0\n 5 이명박    국민      13 0.0342      0      0\n 6 이명박    기회       3 0.00789     0      0\n 7 이명박    대통령     4 0.0105      0      0\n 8 이명박    동안       1 0.00263     0      0\n 9 이명박    들이       1 0.00263     0      0\n10 이명박    마음       1 0.00263     0      0\n# ℹ 192 more rows\n\nfrequecy %&gt;%\n  filter(president == \"노무현\") %&gt;%\n  arrange(tf_idf)\n\n# A tibble: 216 × 6\n   president word       n      tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 노무현    경쟁       1 0.00272     0      0\n 2 노무현    경제       1 0.00272     0      0\n 3 노무현    고통       1 0.00272     0      0\n 4 노무현    과거       1 0.00272     0      0\n 5 노무현    국민       7 0.0191      0      0\n 6 노무현    기회       1 0.00272     0      0\n 7 노무현    대통령     6 0.0163      0      0\n 8 노무현    동안       2 0.00545     0      0\n 9 노무현    들이       4 0.0109      0      0\n10 노무현    마음       1 0.00272     0      0\n# ℹ 206 more rows\n\n\n\n# 막대 그래프 만들기\n# 주요 단어 추출\ntop10 &lt;- frequecy %&gt;%\n  group_by(president) %&gt;%\n  slice_max(tf_idf, n = 10, with_ties = F)\ntop10\n\n# A tibble: 40 × 6\n# Groups:   president [4]\n   president word         n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 노무현    공식         6 0.0163  1.39  0.0227 \n 2 노무현    비젼         6 0.0163  1.39  0.0227 \n 3 노무현    정계         6 0.0163  1.39  0.0227 \n 4 노무현    권력         9 0.0245  0.693 0.0170 \n 5 노무현    개편         4 0.0109  1.39  0.0151 \n 6 노무현    국회의원     3 0.00817 1.39  0.0113 \n 7 노무현    남북대화     3 0.00817 1.39  0.0113 \n 8 노무현    총리         3 0.00817 1.39  0.0113 \n 9 노무현    가훈         2 0.00545 1.39  0.00755\n10 노무현    개혁         4 0.0109  0.693 0.00755\n# ℹ 30 more rows\n\n\n\n# 그래프 순서 정하기\ntop10$president &lt;- factor(top10$president,\n                          levels = c(\"문재인\", \"박근혜\", \"이명박\", \"노무현\"))\n\n\n# 막대 그래프 만들기\nlibrary(ggplot2)\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nggplot(top10, aes(x = reorder_within(word, tf_idf, president),\n                  y = tf_idf,\n                  fill = president)) +\n  geom_col(show.legend = F) +\n  coord_flip() +\n  facet_wrap(~ president, scales = \"free\", ncol = 2) +\n  scale_x_reordered() +\n  labs(x = NULL)\n\n\n\n\n\n\nSentimental Analysis\n군산대 감성사전 csv\n\n# 감정 사전 불러오기\ndic &lt;- read_csv('data/knu_sentiment_lexicon.csv')\n\nRows: 14854 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): word\ndbl (1): polarity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 긍정 단어\ndic %&gt;%\n  filter(polarity == 2) %&gt;%\n  arrange(word)\n\n# A tibble: 2,602 × 2\n   word              polarity\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 가능성이 늘어나다        2\n 2 가능성이 있다고          2\n 3 가능하다                 2\n 4 가볍고 상쾌하다          2\n 5 가볍고 상쾌한            2\n 6 가볍고 시원하게          2\n 7 가볍고 편안하게          2\n 8 가볍고 환하게            2\n 9 가운데에서 뛰어남        2\n10 가장 거룩한              2\n# ℹ 2,592 more rows\n\n# 부정 단어\ndic %&gt;%\n  filter(polarity == -2) %&gt;%\n  arrange(word)\n\n# A tibble: 4,799 × 2\n   word            polarity\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 가난                  -2\n 2 가난뱅이              -2\n 3 가난살이              -2\n 4 가난살이하다          -2\n 5 가난설음              -2\n 6 가난에                -2\n 7 가난에 쪼들려서       -2\n 8 가난하게              -2\n 9 가난하고              -2\n10 가난하고 어렵다       -2\n# ℹ 4,789 more rows\n\n\n\n# 감정 단어의 종류 살펴보기\ndic %&gt;%\n  filter(word %in% c(\"좋은\", \"나쁜\"))\n\n# A tibble: 2 × 2\n  word  polarity\n  &lt;chr&gt;    &lt;dbl&gt;\n1 좋은         2\n2 나쁜        -2\n\ndic %&gt;%\n  filter(word %in% c(\"기쁜\", \"슬픈\"))\n\n# A tibble: 2 × 2\n  word  polarity\n  &lt;chr&gt;    &lt;dbl&gt;\n1 슬픈        -2\n2 기쁜         2\n\n\n\n# 이모티콘\nlibrary(stringr)\ndic %&gt;%\n  filter(!str_detect(word, \"[가-힣]\")) %&gt;%\n  arrange(word)\n\n# A tibble: 77 × 2\n   word  polarity\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 (-;          1\n 2 (-_-)       -1\n 3 (;_;)       -1\n 4 (T_T)       -1\n 5 (^-^)        1\n 6 (^^)         1\n 7 (^^*         1\n 8 (^_^)        1\n 9 (^_^;       -1\n10 (^o^)        1\n# ℹ 67 more rows\n\n\n\n# 총 14,854개 단어\ndic %&gt;%\n  mutate(sentiment = ifelse(polarity &gt;= 1, \"pos\",\n                            ifelse(polarity &lt;= -1, \"neg\", \"neu\"))) %&gt;%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 neg        9829\n2 neu         154\n3 pos        4871\n\n\n\n# 문장의 감정 점수 구하기\n# 1. 단어 기준으로 토큰화하기\ndf &lt;- tibble(sentence = c(\"디자인 예쁘고 마감도 좋아서 만족스럽다.\",\n                          \"디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다.\"))\n\nlibrary(tidytext)\ndf &lt;- df %&gt;%\n  unnest_tokens(input = sentence,\n                output = word,\n                token = \"words\",\n                drop = F)\ndf\n\n# A tibble: 12 × 2\n   sentence                                             word      \n   &lt;chr&gt;                                                &lt;chr&gt;     \n 1 디자인 예쁘고 마감도 좋아서 만족스럽다.              디자인    \n 2 디자인 예쁘고 마감도 좋아서 만족스럽다.              예쁘고    \n 3 디자인 예쁘고 마감도 좋아서 만족스럽다.              마감도    \n 4 디자인 예쁘고 마감도 좋아서 만족스럽다.              좋아서    \n 5 디자인 예쁘고 마감도 좋아서 만족스럽다.              만족스럽다\n 6 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 디자인은  \n 7 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 괜찮다    \n 8 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 그런데    \n 9 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 마감이    \n10 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 나쁘고    \n11 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 가격도    \n12 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 비싸다    \n\n\n\n# 2. 단어에 감정 점수 부여하기\ndf &lt;- df %&gt;%\n  left_join(dic, by = \"word\") %&gt;%\n  mutate(polarity = ifelse(is.na(polarity), 0, polarity))\ndf\n\n# A tibble: 12 × 3\n   sentence                                             word       polarity\n   &lt;chr&gt;                                                &lt;chr&gt;         &lt;dbl&gt;\n 1 디자인 예쁘고 마감도 좋아서 만족스럽다.              디자인            0\n 2 디자인 예쁘고 마감도 좋아서 만족스럽다.              예쁘고            2\n 3 디자인 예쁘고 마감도 좋아서 만족스럽다.              마감도            0\n 4 디자인 예쁘고 마감도 좋아서 만족스럽다.              좋아서            2\n 5 디자인 예쁘고 마감도 좋아서 만족스럽다.              만족스럽다        2\n 6 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 디자인은          0\n 7 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 괜찮다            1\n 8 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 그런데            0\n 9 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 마감이            0\n10 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 나쁘고           -2\n11 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 가격도            0\n12 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 비싸다           -2\n\n\n\n# 3. 문장별로 감정 점수 합산하기\nscore_df &lt;- df %&gt;%\n  group_by(sentence) %&gt;%\n  summarise(score = sum(polarity))\nscore_df\n\n# A tibble: 2 × 2\n  sentence                                             score\n  &lt;chr&gt;                                                &lt;dbl&gt;\n1 디자인 예쁘고 마감도 좋아서 만족스럽다.                  6\n2 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다.    -3\n\n\n\n\nSentimental Analysis for the comments\n댓글 감성 분석\n\n# 데이터 불러오기\nraw_news_comment &lt;- read_csv(\"data/news_comment_parasite.csv\")\n\nRows: 4150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): reply, press, title, url\ndttm (1): reg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_news_comment\n\n# A tibble: 4,150 × 5\n   reg_time            reply                                   press title url  \n   &lt;dttm&gt;              &lt;chr&gt;                                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2020-02-10 16:59:02 \"정말 우리 집에 좋은 일이 생겨 기쁘고 … MBC   '기…  http…\n 2 2020-02-10 13:32:24 \"와 너무 기쁘다! 이 시국에 정말 내 일…  SBS   [영…  http…\n 3 2020-02-10 12:30:09 \"우리나라의 영화감독분들 그리고 앞으로… 한겨… ‘기…  http…\n 4 2020-02-10 13:08:22 \"봉준호 감독과 우리나라 대한민국 모두 … 한겨… ‘기…  http…\n 5 2020-02-10 16:25:41 \"노벨상 탄느낌이네요\\r\\n축하축하 합니…  한겨… ‘기…  http…\n 6 2020-02-10 12:31:45 \"기생충 상 받을때 박수 쳤어요.감독상도… 한겨… ‘기…  http…\n 7 2020-02-10 12:31:33 \"대한민국 영화사를 새로 쓰고 계시네요 … 한겨… ‘기…  http…\n 8 2020-02-11 09:20:52 \"저런게 아카데미상 받으면  '태극기 휘…  한겨… ‘기…  http…\n 9 2020-02-10 20:53:27 \"다시한번 보여주세요 영화관에서 보고싶… 한겨… ‘기…  http…\n10 2020-02-10 20:22:41 \"대한민국 BTS와함께  봉준호감독님까지\\… 한겨… ‘기…  http…\n# ℹ 4,140 more rows\n\n\n\n# 기본적인 전처리\nlibrary(textclean)\nnews_comment &lt;- raw_news_comment %&gt;%\n  mutate(id = row_number(),\n         reply = str_squish(replace_html(reply)))\nnews_comment\n\n# A tibble: 4,150 × 6\n   reg_time            reply                             press title url      id\n   &lt;dttm&gt;              &lt;chr&gt;                             &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 2020-02-10 16:59:02 정말 우리 집에 좋은 일이 생겨 기… MBC   '기…  http…     1\n 2 2020-02-10 13:32:24 와 너무 기쁘다! 이 시국에 정말 …  SBS   [영…  http…     2\n 3 2020-02-10 12:30:09 우리나라의 영화감독분들 그리고 …  한겨… ‘기…  http…     3\n 4 2020-02-10 13:08:22 봉준호 감독과 우리나라 대한민국 … 한겨… ‘기…  http…     4\n 5 2020-02-10 16:25:41 노벨상 탄느낌이네요 축하축하 합…  한겨… ‘기…  http…     5\n 6 2020-02-10 12:31:45 기생충 상 받을때 박수 쳤어요.감…  한겨… ‘기…  http…     6\n 7 2020-02-10 12:31:33 대한민국 영화사를 새로 쓰고 계시… 한겨… ‘기…  http…     7\n 8 2020-02-11 09:20:52 저런게 아카데미상 받으면 '태극기… 한겨… ‘기…  http…     8\n 9 2020-02-10 20:53:27 다시한번 보여주세요 영화관에서 …  한겨… ‘기…  http…     9\n10 2020-02-10 20:22:41 대한민국 BTS와함께 봉준호감독님…  한겨… ‘기…  http…    10\n# ℹ 4,140 more rows\n\n\n\n# 데이터 구조 확인\nglimpse(news_comment)\n\nRows: 4,150\nColumns: 6\n$ reg_time &lt;dttm&gt; 2020-02-10 16:59:02, 2020-02-10 13:32:24, 2020-02-10 12:30:0…\n$ reply    &lt;chr&gt; \"정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일…\n$ press    &lt;chr&gt; \"MBC\", \"SBS\", \"한겨레\", \"한겨레\", \"한겨레\", \"한겨레\", \"한겨레…\n$ title    &lt;chr&gt; \"'기생충' 아카데미 작품상까지 4관왕…영화사 새로 썼다\", \"[영상…\n$ url      &lt;chr&gt; \"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=1…\n$ id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n\n\n\n# 단어 기준으로 토큰화하고 감정 점수 부여하기\n# 토큰화\nword_comment &lt;- news_comment %&gt;%\n  unnest_tokens(input = reply,\n                output = word,\n                token = \"words\",\n                drop = F)\n\nword_comment %&gt;%\n  select(word, reply)\n\n# A tibble: 37,718 × 2\n   word   reply                                                                \n   &lt;chr&gt;  &lt;chr&gt;                                                                \n 1 정말   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 2 우리   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 3 집에   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 4 좋은   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 5 일이   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 6 생겨   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 7 기쁘고 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 8 행복한 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 9 것처럼 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n10 나의   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n# ℹ 37,708 more rows\n\n\n\n# 감정 점수 부여\nword_comment &lt;- word_comment %&gt;%\n  left_join(dic, by = \"word\") %&gt;%\n  mutate(polarity = ifelse(is.na(polarity), 0, polarity))\n\nword_comment %&gt;%\n  select(word, polarity)\n\n# A tibble: 37,718 × 2\n   word   polarity\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 정말          0\n 2 우리          0\n 3 집에          0\n 4 좋은          2\n 5 일이          0\n 6 생겨          0\n 7 기쁘고        2\n 8 행복한        2\n 9 것처럼        0\n10 나의          0\n# ℹ 37,708 more rows\n\n\n\n# 자주 사용된 감정 단어 살펴보기\n# 1. 감정 분류하기\nword_comment &lt;- word_comment %&gt;%\n  mutate(sentiment = ifelse(polarity == 2, \"pos\",\n                            ifelse(polarity == -2, \"neg\", \"neu\")))\n\nword_comment %&gt;%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 neg         285\n2 neu       36671\n3 pos         762\n\n\n\n# 2. 막대 그래프 만들기\ntop10_sentiment &lt;- word_comment %&gt;%\n  filter(sentiment != \"neu\") %&gt;%\n  count(sentiment, word) %&gt;%\n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10)\n\ntop10_sentiment\n\n# A tibble: 22 × 3\n# Groups:   sentiment [2]\n   sentiment word       n\n   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n 1 neg       소름      56\n 2 neg       소름이    16\n 3 neg       아니다    15\n 4 neg       우울한     9\n 5 neg       해         8\n 6 neg       미친       7\n 7 neg       가난한     5\n 8 neg       어려운     5\n 9 neg       힘든       5\n10 neg       더러운     4\n# ℹ 12 more rows\n\n\n\nggplot(top10_sentiment, aes(x = reorder(word, n),\n                            y = n,\n                            fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) +\n  facet_wrap(~ sentiment, scales = \"free\") +\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +\n  labs(x = NULL) \n\n\n\n\n댓글별 감정 점수 구하고 댓글 살펴보기\n\n# 1. 댓글별 감정 점수 구하기\nscore_comment &lt;- word_comment %&gt;%\n  group_by(id, reply) %&gt;%\n  summarise(score = sum(polarity)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nscore_comment %&gt;%\n  select(score, reply)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   &lt;dbl&gt; &lt;chr&gt;                                                                  \n 1     6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복…\n 2     6 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요…\n 3     4 우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 영감… \n 4     3 봉준호 감독과 우리나라 대한민국 모두 자랑스럽다. 세계 어디를 가고 우리…\n 5     0 노벨상 탄느낌이네요 축하축하 합니다                                    \n 6     0 기생충 상 받을때 박수 쳤어요.감독상도 기대해요.봉준호 감독 화이팅^^    \n 7     0 대한민국 영화사를 새로 쓰고 계시네요 ㅊㅊㅊ                            \n 8     0 저런게 아카데미상 받으면 '태극기 휘날리며'' '광해' '명량''은 전부문 휩…\n 9     0 다시한번 보여주세요 영화관에서 보고싶은디                              \n10     2 대한민국 BTS와함께 봉준호감독님까지 대단하고 한국의 문화에 자긍심을 가…\n# ℹ 4,130 more rows\n\n\n\n# 2. 감정 점수 높은 댓글 살펴보기\n# 긍정 댓글\nscore_comment %&gt;%\n  select(score, reply) %&gt;%\n  arrange(-score)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   &lt;dbl&gt; &lt;chr&gt;                                                                  \n 1    11 아니 다른상을 받은것도 충분히 대단하고 굉장하지만 최고의 영예인 작품상…\n 2     9 봉준호의 위대한 업적은 진보 영화계의 위대한 업적이고 대한민국의 업적입…\n 3     7 이 수상소식을 듣고 억수로 기뻐하는 가족이 있을것 같다. SNS를 통해 자기…\n 4     7 감사 감사 감사 수상 소감도 3관왕 답네요                                \n 5     6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복…\n 6     6 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요…\n 7     6 축하 축하 축하 모두들 수고 하셨어요 기생충 화이팅                      \n 8     6 축하!!!! 축하!!!!! 오스카의 정복은 무엇보다 시나리오의 힘이다. 작가의 …\n 9     6 조여정 ㆍ예쁜얼굴때문에 연기력을 제대로 평가받지 못해 안타깝던 내가 좋…\n10     6 좋은 걸 좋다고 말하지 못하는 인간들이 참 불쌍해지네....댓글 보니 인생… \n# ℹ 4,130 more rows\n\n\n\n# 부정 댓글\nscore_comment %&gt;%\n  select(score, reply) %&gt;%\n  arrange(score)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   &lt;dbl&gt; &lt;chr&gt;                                                                  \n 1    -7 기생충 영화 한국인 으로써 싫다 대단히 싫다!! 가난한 서민들의 마지막 자…\n 2    -6 이 페미민국이 잘 되는 게 아주 싫다. 최악의 나쁜일들과 불운, 불행, 어둡…\n 3    -5 특정 인물의 성공을 국가의 부흥으로 연관짓는 것은 미개한 발상이다. 봉준…\n 4    -4 좌파들이 나라 망신 다 시킨다..ㅠ 설레발 오지게 치더니..꼴랑 각본상 하… \n 5    -4 부패한 386 민주화 세대 정권의 무분별한 포퓰리즘으로 탄생한 좀비들의 살…\n 6    -4 기생충 내용은 좋은데 제목이 그래요. 극 중 송강호가족이 부잣집에 대해서…\n 7    -4 이런 감독과 이런 배우보고 좌좀 이라고 지1랄하던 그분들 다 어디계시냐? …\n 8    -4 축하합니다. 근데 현실 세계인 한국에선 그보다 훨씬 나쁜 넘인 조로남불 … \n 9    -4 큰일이다....국제적 망신이다...전 세계사람들이 우리나라를 기생충으로 보…\n10    -4 더럽고 추잡한 그들만의 리그                                            \n# ℹ 4,130 more rows\n\n\n\n# 감정 경향 살펴보기\n# 1. 감정 점수 빈도 구하기\nscore_comment %&gt;%\n  count(score)\n\n# A tibble: 17 × 2\n   score     n\n   &lt;dbl&gt; &lt;int&gt;\n 1    -7     1\n 2    -6     1\n 3    -5     1\n 4    -4    17\n 5    -3    35\n 6    -2   175\n 7    -1   206\n 8     0  2897\n 9     1   222\n10     2   432\n11     3    57\n12     4    71\n13     5     7\n14     6    14\n15     7     2\n16     9     1\n17    11     1\n\n\n\n# 2. 감정 분류하고 막대 그래프 만들기\n# 감정 분류하기\nscore_comment &lt;- score_comment %&gt;%\n  mutate(sentiment = ifelse(score &gt;= 1, \"pos\",\n                            ifelse(score &lt;= -1, \"neg\", \"neu\")))\n\n\n# 감정 빈도와 비율 구하기\nfrequency_score &lt;- score_comment %&gt;%\n  count(sentiment) %&gt;%\n  mutate(ratio = n/sum(n)*100)\nfrequency_score\n\n# A tibble: 3 × 3\n  sentiment     n ratio\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 neg         436  10.5\n2 neu        2897  70.0\n3 pos         807  19.5\n\n\n\n# 막대 그래프 만들기\nggplot(frequency_score, aes(x = sentiment, y = n, fill = sentiment)) +\n  geom_col() +\n  geom_text(aes(label = n), vjust = -0.3) +\n  scale_x_discrete(limits = c(\"pos\", \"neu\", \"neg\"))\n\n\n\n\n\n# 3. 비율 누적 막대 그래프 만들기\ndf &lt;- tibble(contry = c(\"Korea\", \"Korea\", \"Japen\", \"Japen\"), # 축\n             sex = c(\"M\", \"F\", \"M\", \"F\"), # 누적 막대\n             ratio = c(60, 40, 30, 70)) # 값\ndf\n\n# A tibble: 4 × 3\n  contry sex   ratio\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 Korea  M        60\n2 Korea  F        40\n3 Japen  M        30\n4 Japen  F        70\n\nggplot(df, aes(x = contry, y = ratio, fill = sex)) + geom_col()\n\n\n\n\n\nggplot(df, aes(x = contry, y = ratio, fill = sex)) +\n  geom_col() +\n  geom_text(aes(label = paste0(ratio, \"%\")), # % 표시\n            position = position_stack(vjust = 0.5)) # 가운데 표시\n\n\n\n\n\n# 댓글의 감정 비율로 누적 막대 그래프 만들기\n# 더미 변수 생성\nfrequency_score$dummy &lt;- 0\nfrequency_score\n\n# A tibble: 3 × 4\n  sentiment     n ratio dummy\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 neg         436  10.5     0\n2 neu        2897  70.0     0\n3 pos         807  19.5     0\n\nggplot(frequency_score, aes(x = dummy, y = ratio, fill = sentiment)) +\n  geom_col() +\n  geom_text(aes(label = paste0(round(ratio, 1), \"%\")),\n            position = position_stack(vjust = 0.5)) +\n  theme(axis.title.x = element_blank(), # x축 이름 삭제\n        axis.text.x = element_blank(), # x축 값 삭제\n        axis.ticks.x = element_blank()) # x축 눈금 삭제\n\n\n\n\n\n\nWord Frequency by Sentiment Category\n감정 범주별 단어 빈도\n\n# 감정 범주별 단어 빈도 구하기\n# 1. 토큰화하고 두 글자 이상 한글 단어만 남기기\ncomment &lt;- score_comment %&gt;%\n  unnest_tokens(input = reply, # 단어 기준 토큰화\n                output = word,\n                token = \"words\",\n                drop = F) %&gt;%\n  filter(str_detect(word, \"[가-힣]\") & # 한글 추출\n           str_count(word) &gt;= 2) # 두 글자 이상 추출\n\n\n# 2. 감정 범주별 빈도 구하기\nfrequency_word &lt;- comment %&gt;%\n  filter(str_count(word) &gt;= 2) %&gt;%\n  count(sentiment, word, sort = T)\nfrequency_word\n\n# A tibble: 19,223 × 3\n   sentiment word               n\n   &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt;\n 1 neu       축하합니다       214\n 2 neu       봉준호           203\n 3 neu       기생충           164\n 4 neu       축하드립니다     155\n 5 neu       정말             146\n 6 neu       대박             134\n 7 neu       진짜             121\n 8 pos       봉준호           106\n 9 pos       정말              97\n10 neu       자랑스럽습니다    96\n# ℹ 19,213 more rows\n\n\n\n# 긍정 댓글 고빈도 단어\nfrequency_word %&gt;%\n  filter(sentiment == \"pos\")\n\n# A tibble: 5,234 × 3\n   sentiment word           n\n   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n 1 pos       봉준호       106\n 2 pos       정말          97\n 3 pos       대단하다      83\n 4 pos       진짜          79\n 5 pos       자랑스럽다    78\n 6 pos       축하          63\n 7 pos       대한민국      61\n 8 pos       영화          58\n 9 pos       멋지다        55\n10 pos       기생충        53\n# ℹ 5,224 more rows\n\n\n\n# 부정 댓글 고빈도 단어\nfrequency_word %&gt;%\n  filter(sentiment == \"neg\")\n\n# A tibble: 4,080 × 3\n   sentiment word             n\n   &lt;chr&gt;     &lt;chr&gt;        &lt;int&gt;\n 1 neg       소름            49\n 2 neg       봉준호          47\n 3 neg       기생충          33\n 4 neg       이런            33\n 5 neg       정말            32\n 6 neg       진짜            26\n 7 neg       좌빨            21\n 8 neg       너무            20\n 9 neg       블랙리스트에    19\n10 neg       영화            18\n# ℹ 4,070 more rows\n\n\n\n# 상대적으로 자주 사용된 단어 비교하기\n# 1. 로그 오즈비 구하기\n# wide form으로 변환\ncomment_wide &lt;- frequency_word %&gt;%\n  filter(sentiment != \"neu\") %&gt;%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = list(n = 0))\ncomment_wide\n\n# A tibble: 8,380 × 3\n   word         pos   neg\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;\n 1 봉준호       106    47\n 2 정말          97    32\n 3 대단하다      83     1\n 4 진짜          79    26\n 5 자랑스럽다    78     1\n 6 축하          63     0\n 7 대한민국      61     4\n 8 영화          58    18\n 9 멋지다        55     0\n10 기생충        53    33\n# ℹ 8,370 more rows\n\n# 로그 오즈비 구하기\ncomment_wide &lt;- comment_wide %&gt;%\n  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /\n                                ((neg + 1) / (sum(neg + 1)))))\ncomment_wide\n\n# A tibble: 8,380 × 4\n   word         pos   neg log_odds_ratio\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;\n 1 봉준호       106    47          0.589\n 2 정말          97    32          0.876\n 3 대단하다      83     1          3.52 \n 4 진짜          79    26          0.873\n 5 자랑스럽다    78     1          3.46 \n 6 축하          63     0          3.95 \n 7 대한민국      61     4          2.30 \n 8 영화          58    18          0.920\n 9 멋지다        55     0          3.81 \n10 기생충        53    33          0.250\n# ℹ 8,370 more rows\n\n# 2. 로그 오즈비가 가장 큰 단어 10개씩 추출하기\ntop10 &lt;- comment_wide %&gt;%\n  group_by(sentiment = ifelse(log_odds_ratio &gt; 0, \"pos\", \"neg\")) %&gt;%\n  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)\ntop10\n\n# A tibble: 20 × 5\n# Groups:   sentiment [2]\n   word         pos   neg log_odds_ratio sentiment\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;          &lt;dbl&gt; &lt;chr&gt;    \n 1 소름           2    49          -3.03 neg      \n 2 좌빨           1    21          -2.61 neg      \n 3 못한           0     7          -2.29 neg      \n 4 미친           0     7          -2.29 neg      \n 5 좌좀           0     6          -2.16 neg      \n 6 소름이         1    12          -2.08 neg      \n 7 가난한         0     5          -2.00 neg      \n 8 모르는         0     5          -2.00 neg      \n 9 아쉽다         0     5          -2.00 neg      \n10 닭그네         0     4          -1.82 neg      \n11 축하          63     0           3.95 pos      \n12 멋지다        55     0           3.81 pos      \n13 대단한        47     0           3.66 pos      \n14 좋은          42     0           3.55 pos      \n15 대단하다      83     1           3.52 pos      \n16 자랑스럽다    78     1           3.46 pos      \n17 최고          27     0           3.12 pos      \n18 세계적인      24     0           3.01 pos      \n19 최고의        23     0           2.97 pos      \n20 위대한        22     0           2.92 pos      \n\n# 3. 막대 그래프 만들기\nggplot(top10, aes(x = reorder(word, log_odds_ratio),\n                  y = log_odds_ratio,\n                  fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL)\n\n\n\n\n\n\nModify Emotion Words\n감정 단어 수정\n\n# 감정 단어가 사용된 원문 살펴보기\n# \"소름\"이 사용된 댓글\nscore_comment %&gt;%\n  filter(str_detect(reply, \"소름\")) %&gt;%\n  select(reply)\n\n# A tibble: 131 × 1\n   reply                                                                       \n   &lt;chr&gt;                                                                       \n 1 소름돋네요                                                                  \n 2 와..진짜소름 저 소리처음질렀어요 눈물나요.. ㅠㅠ                            \n 3 생중계 보며 봉준호 할 때 소름이~~~!! ㅠㅠ 수상소감들으며 함께 가슴이 벅차네…\n 4 와 보다가 소름 짝 수고들하셨어요                                            \n 5 한국어 소감 듣는데 소름돋네 축하드립니다                                    \n 6 대단하다!! 봉준호 이름 나오자마자 소름                                      \n 7 와우 브라보~ 키아누리브스의 봉준호, 순간 소름이.. 멋지십니다.               \n 8 소름 돋네요. 축하합니다                                                     \n 9 소름.... 기생충 각본집 산거 다시한번 잘했다는 생각이ㅠㅠㅠ 축하해요!!!!!!   \n10 소름끼쳤어요 너무 멋집니다 ^^!!!!                                           \n# ℹ 121 more rows\n\n# \"미친\"이 사용된 댓글\nscore_comment %&gt;%\n  filter(str_detect(reply, \"미친\")) %&gt;%\n  select(reply)\n\n# A tibble: 15 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 와 3관왕 미친                                                                \n 2 미친거야 이건~~                                                              \n 3 Korea 대단합니다 김연아 방탄 봉준호 스포츠 음악 영화 못하는게 없어요 좌빨 감…\n 4 청룡영화제에서 다른나라가 상을 휩쓴거죠? 와..미쳤다 미국영화제에서 한국이 빅…\n 5 설마했는데 감독상, 작품상, 각본상을 죄다 휩쓸어버릴 줄이야. 이건 미친 꿈이야…\n 6 완전 완전...미친촌재감~이런게 바로 애국이지~ 존경합니다~                     \n 7 이세상엔 참 미 친 인간들이 많다는걸 댓글에서 다시한번 느낀다..모두가 축하해… \n 8 올해 아카데미 최다 수상작이기도 하다 이건 진짜 미친사건이다                  \n 9 CJ회장이 저기서 왜 언급되는지... 미친 부회장.. 공과사 구분 못하는 정권의 홍… \n10 미친봉                                                                       \n11 미친 3관왕 ㄷㄷㄷㄷㄷ                                                        \n12 진짜 미친일...                                                               \n13 나도모르게 보다가 육성으로 미친...ㅋㅋㅋㅋ 대박ㅜ                            \n14 헐...감독상...미친...미쳤다..소름돋는다...                                   \n15 인정할건인정하자 봉감독 송배우 이배우 조배우등 인정하자 또 가로세로 ㅆㄹㄱ들…\n\ndic %&gt;% filter(word %in% c(\"소름\", \"소름이\", \"미친\"))\n\n# A tibble: 3 × 2\n  word   polarity\n  &lt;chr&gt;     &lt;dbl&gt;\n1 소름이       -2\n2 소름         -2\n3 미친         -2\n\n\n\n# 감정 사전 수정하기\nnew_dic &lt;- dic %&gt;%\n  mutate(polarity = ifelse(word %in% c(\"소름\", \"소름이\", \"미친\"), 2, polarity))\nnew_dic %&gt;% filter(word %in% c(\"소름\", \"소름이\", \"미친\"))\n\n# A tibble: 3 × 2\n  word   polarity\n  &lt;chr&gt;     &lt;dbl&gt;\n1 소름이        2\n2 소름          2\n3 미친          2\n\n# 수정한 사전으로 감정 점수 부여하기\nnew_word_comment &lt;- word_comment %&gt;%\n  select(-polarity) %&gt;%\n  left_join(new_dic, by = \"word\") %&gt;%\n  mutate(polarity = ifelse(is.na(polarity), 0, polarity))\n\n# 댓글별 감정 점수 구하기\nnew_score_comment &lt;- new_word_comment %&gt;%\n  group_by(id, reply) %&gt;%\n  summarise(score = sum(polarity)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nnew_score_comment %&gt;%\n  select(score, reply) %&gt;%\n  arrange(-score)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   &lt;dbl&gt; &lt;chr&gt;                                                                  \n 1    11 아니 다른상을 받은것도 충분히 대단하고 굉장하지만 최고의 영예인 작품상…\n 2     9 봉준호의 위대한 업적은 진보 영화계의 위대한 업적이고 대한민국의 업적입…\n 3     8 소름 소름 진짜 멋지다 대단하다                                         \n 4     7 이 수상소식을 듣고 억수로 기뻐하는 가족이 있을것 같다. SNS를 통해 자기…\n 5     7 감사 감사 감사 수상 소감도 3관왕 답네요                                \n 6     6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복…\n 7     6 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요…\n 8     6 축하 축하 축하 모두들 수고 하셨어요 기생충 화이팅                      \n 9     6 생중계 보며 봉준호 할 때 소름이~~~!! ㅠㅠ 수상소감들으며 함께 가슴이 … \n10     6 축하!!!! 축하!!!!! 오스카의 정복은 무엇보다 시나리오의 힘이다. 작가의 …\n# ℹ 4,130 more rows\n\n\n\n# 전반적인 감정 경향 살펴보기\n# 1. 감정 분류하기\n# 1점 기준으로 긍정 중립 부정 분류\nnew_score_comment &lt;- new_score_comment %&gt;%\n  mutate(sentiment = ifelse(score &gt;= 1, \"pos\",\n                            ifelse(score &lt;= -1, \"neg\", \"neu\")))\n\n# 2. 감정 범주별 빈도와 비율 구하기\n# 원본 감정 사전 활용\nscore_comment %&gt;%\n  count(sentiment) %&gt;%\n  mutate(ratio = n/sum(n)*100)\n\n# A tibble: 3 × 3\n  sentiment     n ratio\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 neg         436  10.5\n2 neu        2897  70.0\n3 pos         807  19.5\n\n# 수정한 감정 사전 활용\nnew_score_comment %&gt;%\n  count(sentiment) %&gt;%\n  mutate(ratio = n/sum(n)*100)\n\n# A tibble: 3 × 3\n  sentiment     n ratio\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 neg         368  8.89\n2 neu        2890 69.8 \n3 pos         882 21.3 \n\n\n\n# 3. 분석 결과 비교하기\nword &lt;- \"소름|소름이|미친\"\n\n# 원본 감정 사전 활용\nscore_comment %&gt;%\n  filter(str_detect(reply, word)) %&gt;%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 neg          73\n2 neu          63\n3 pos           9\n\n# 수정한 감정 사전 활용\nnew_score_comment %&gt;%\n  filter(str_detect(reply, word)) %&gt;%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 neg           5\n2 neu          56\n3 pos          84\n\n\n\n# 감정 범주별 주요 단어 살펴보기\n# 1. 두 글자 이상 한글 단어만 남기고 단어 빈도 구하기\n# 토큰화 및 전처리\nnew_comment &lt;- new_score_comment %&gt;%\n  unnest_tokens(input = reply,\n                output = word,\n                token = \"words\",\n                drop = F) %&gt;%\n  filter(str_detect(word, \"[가-힣]\") &\n           str_count(word) &gt;= 2)\n\n# 감정 및 단어별 빈도 구하기\nnew_frequency_word &lt;- new_comment %&gt;%\n  count(sentiment, word, sort = T)\n\n# 2. 로그 오즈비 구하기\n# Wide form으로 변환\nnew_comment_wide &lt;- new_frequency_word %&gt;%\n  filter(sentiment != \"neu\") %&gt;%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = list(n = 0))\n\n# 로그 오즈비 구하기\nnew_comment_wide &lt;- new_comment_wide %&gt;%\n  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /\n                                ((neg + 1) / (sum(neg + 1)))))\n\n# 3. 로그 오즈비가 큰 단어로 막대 그래프 만들기\nnew_top10 &lt;- new_comment_wide %&gt;%\n  group_by(sentiment = ifelse(log_odds_ratio &gt; 0, \"pos\", \"neg\")) %&gt;%\n  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)\n\nggplot(new_top10, aes(x = reorder(word, log_odds_ratio),\n                      y = log_odds_ratio,\n                      fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL) \n\n\n\n\n\n# 4. 주요 단어가 사용된 댓글 살펴보기\n# 긍정 댓글 원문\nnew_score_comment %&gt;%\n  filter(sentiment == \"pos\" & str_detect(reply, \"축하\")) %&gt;%\n  select(reply)\n\n# A tibble: 189 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복합니다…\n 2 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요 진심… \n 3 우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 영감을 주시…\n 4 아카데미나 다른 상이나 지들만의 잔치지~ 난 대한민국에서 받는 상이 제일 가치 …\n 5 정부에 빨대 꼽은 정치시민단체 기생충들이 득실거리는 떼한민국애서 훌륭한 영화…\n 6 대단해요 나는 안봤는데 그렇게 잘 만들어 한국인의 기백을 세계에 알리는 큰 일… \n 7 나한테 돌아오는게 하나도 없는데 왜이렇게 자랑스럽지?ㅎㅎㅎ 축하 합니다~작품… \n 8 한국영화 100년사에 한횟을 긋네요. 정말 축하 합니다                           \n 9 와 대단하다 진짜 축하드려요!!! 대박 진짜                                     \n10 각본상, 국제 영화상 수상 축하. 편집상은 꽝남.                                \n# ℹ 179 more rows\n\nnew_score_comment %&gt;%\n  filter(sentiment == \"pos\" & str_detect(reply, \"소름\")) %&gt;%\n  select(reply)\n\n# A tibble: 77 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 생중계 보며 봉준호 할 때 소름이~~~!! ㅠㅠ 수상소감들으며 함께 가슴이 벅차네… \n 2 와 보다가 소름 짝 수고들하셨어요                                             \n 3 대단하다!! 봉준호 이름 나오자마자 소름                                       \n 4 와우 브라보~ 키아누리브스의 봉준호, 순간 소름이.. 멋지십니다.                \n 5 소름 돋네요. 축하합니다                                                      \n 6 소름.... 기생충 각본집 산거 다시한번 잘했다는 생각이ㅠㅠㅠ 축하해요!!!!!!    \n 7 봉준호 아저씨 우리나라 자랑입니다 헐리웃 배우들과 화면에 같이 비춰지는게 아… \n 8 추카해요. 봉준호하는데 막 완전 소름 돋았어요.                                \n 9 소름돋아서 닭살돋고.. 그냥 막 감동이라 눈물이 나려했어요.. 대단하고 자랑스럽…\n10 한국 영화 최초 아카데미상 수상, 92년 역사의 국제 장편 영화상과 최우수작품상 …\n# ℹ 67 more rows\n\n# 부정 댓글 원문\nnew_score_comment %&gt;%\n  filter(sentiment == \"neg\" & str_detect(reply, \"좌빨\")) %&gt;%\n  select(reply)\n\n# A tibble: 34 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 자칭 보수들은 분노의 타이핑중 ㅋㅋㅋㅋㅋㅋ전세계를 좌빨로 몰수는 없고 자존심…\n 2 자칭보수 왈 : 미국에 로비했다 ㅋㅋ좌빨영화가 상받을리 없다 ㅋㅋㅋㅋㅋㅋㅋ 본…\n 3 좌빨 봉준호 영화는 쳐다도 안본다 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ                      \n 4 봉준호 그렇게 미국 싫어하는데 상은 쳐 받으러 가는 좌빨 수준ㅋㅋㅋ            \n 5 좌빨 기생충들은 댓글도 달지마라 미국 영화제 수상이 니들하고 뭔상관인데.      \n 6 얘들은 왜 인정을 안하냐? ㅋㅋ 니들 이미 변호인 찍을대 부터 송강호 욕해대고 … \n 7 넷상 보수들 만큼 이중적인 새1끼들 없음. 봉준호 송강호 보고 종북좌빨 홍어드립…\n 8 우선 축하합니다.그리고 다음에는 조씨가족을 모델로한 뻔뻔하고 거짓말을 밥 먹… \n 9 Korea 대단합니다 김연아 방탄 봉준호 스포츠 음악 영화 못하는게 없어요 좌빨 감…\n10 좌빨 감독이라고 블랙리스트에 올랐던 사람을 세계인이 인정해주네. 방구석에 앉… \n# ℹ 24 more rows\n\nnew_score_comment %&gt;%\n  filter(sentiment == \"neg\" & str_detect(reply, \"못한\")) %&gt;%\n  select(reply)\n\n# A tibble: 7 × 1\n  reply                                                                         \n  &lt;chr&gt;                                                                         \n1 한번도경험하지. 못한 조국가족사기단기생충. 개봉박두                           \n2 여기서 정치얘기하는 건 학창시절 공부 못한 거 인증하는 꼴... 주제좀 벗어나지 … \n3 이 기사를 반문으로 먹고 사는 자유왜국당과, mb아바타 간철수 댓글알바들이 매우 …\n4 한국미국일본 vs 주적북한,중국러시아 이 구도인 현 시대 상황 속에서, 미국 일본… \n5 친일수꼴 들과 자한당넘들이 나라에 경사만 있으면 엄청 싫어합니다, 맨날 사고만 …\n6 각본상,국제상,감독상 ...어디서 듣도보도 못한 아차상 같은 쩌리처리용 상 아닌가…\n7 난 밥을 먹고 기생충은 오스카를 먹다, 기생충은 대한민국의 국격을 높였는데 난 … \n\n\n\n# 5. 분석 결과 비교하기\n# 수정한 감정 사전 활용\nnew_top10 %&gt;%\n  select(-pos, -neg) %&gt;%\n  arrange(-log_odds_ratio)\n\n# A tibble: 20 × 3\n# Groups:   sentiment [2]\n   word       log_odds_ratio sentiment\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;    \n 1 축하                 3.88 pos      \n 2 멋지다               3.76 pos      \n 3 소름                 3.76 pos      \n 4 대단한               3.59 pos      \n 5 대단하다             3.49 pos      \n 6 좋은                 3.48 pos      \n 7 자랑스럽다           3.40 pos      \n 8 최고                 3.09 pos      \n 9 세계적인             2.94 pos      \n10 최고의               2.90 pos      \n11 닭그네              -1.89 neg      \n12 못하고              -1.89 neg      \n13 사회적              -1.89 neg      \n14 싫다                -1.89 neg      \n15 가난한              -2.07 neg      \n16 모르는              -2.07 neg      \n17 아쉽다              -2.07 neg      \n18 좌좀                -2.22 neg      \n19 못한                -2.36 neg      \n20 좌빨                -2.68 neg      \n\n# 원본 감정 사전 활용\ntop10 %&gt;%\n  select(-pos, -neg) %&gt;%\n  arrange(-log_odds_ratio)\n\n# A tibble: 20 × 3\n# Groups:   sentiment [2]\n   word       log_odds_ratio sentiment\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;    \n 1 축하                 3.95 pos      \n 2 멋지다               3.81 pos      \n 3 대단한               3.66 pos      \n 4 좋은                 3.55 pos      \n 5 대단하다             3.52 pos      \n 6 자랑스럽다           3.46 pos      \n 7 최고                 3.12 pos      \n 8 세계적인             3.01 pos      \n 9 최고의               2.97 pos      \n10 위대한               2.92 pos      \n11 닭그네              -1.82 neg      \n12 가난한              -2.00 neg      \n13 모르는              -2.00 neg      \n14 아쉽다              -2.00 neg      \n15 소름이              -2.08 neg      \n16 좌좀                -2.16 neg      \n17 못한                -2.29 neg      \n18 미친                -2.29 neg      \n19 좌빨                -2.61 neg      \n20 소름                -3.03 neg      \n\n# 수정 감정 사전 활용 시 \"미친\"이 목록에서 사라짐, 로그 오즈비가 10위 안에 들지 못할 정도로 낮아지기 때문\nnew_comment_wide %&gt;%\n  filter(word == \"미친\")\n\n# A tibble: 1 × 4\n  word    pos   neg log_odds_ratio\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;\n1 미친      7     0           1.80"
  },
  {
    "objectID": "teaching/media_ds/about/index.html#intro",
    "href": "teaching/media_ds/about/index.html#intro",
    "title": "Syllabus",
    "section": "Intro",
    "text": "Intro\n\nWeek 1: Course Intro\n\nDate: 20230302\nClass\n\nCourse Introduction PDF\nInstall R, RStudio, & Rtools"
  },
  {
    "objectID": "teaching/media_ds/about/index.html#data-science-basic",
    "href": "teaching/media_ds/about/index.html#data-science-basic",
    "title": "Syllabus",
    "section": "Data Science Basic",
    "text": "Data Science Basic\n\nWeek 2: R Basic Syntax (1)\n\nDate: 20230309\nPre-class: Basic syntax, Vector, Array\nClass: Hands-on practice PDF\n\nData in use: COV19_data\nProcess of creating the data above [Code]\n\n\n\n\nWeek 3: R Basic Syntax (2)\n\nDate: 20230316\nPre-class: Data.frame, List\nClass: Hands-on practice PDF\n\nData in use: List_KMP\n\n\n\n\nWeek 4: R Basic Skillset (1)\n\nDate: 20230323\nPre-class: Read, Write, Condition, Repetition\nClass: Hands-on practice [Code]\n\n\n\nWeek 5: R Basic Skillset (2)\n\nDate: 20230330\nPre-class: Function, Missing values, Outliers\nClass: Hands-on practice (Review Week 1 to 5)\n데이터사이언스를 활용한 미디어연구에 대한 고찰 [PDF]\n\nKCI 논문 검색 dbpia\nSSCI 논문 검색 google scholar\n미디어 분야에서 데이터사이언스 활용한 논문 목록 리스트 [XLSX]"
  },
  {
    "objectID": "teaching/media_ds/about/index.html#data-pre-processing-visualization",
    "href": "teaching/media_ds/about/index.html#data-pre-processing-visualization",
    "title": "Syllabus",
    "section": "Data Pre-processing & Visualization",
    "text": "Data Pre-processing & Visualization\n\nWeek 6: Data manipulation\n\nDate: 20230406\nPre-class: Data wrangling (Base R & Tidyverse)\nClass: Hands-on practice\n\nIntroducing tidyverse [click]\nGo to Posit cheatsheets [click]\nClass code [click]\n\nRecommended books for the further study\n\nStatistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\nis a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language.\n\nR for Data Science (written by Hadley Wickham and Garrett Grolemund)\n\nis an excellent resource for learning data science using R, covering data manipulation, visualization, and modeling with R. The book is available as a free online resource.\n\n\n\n\n\nWeek 7: Data visualization\n\nDate: 20230413\nPre-class: Data-visualization, ggplot2, and (ggplot practice, optional)\nClass: Hands-on practice\n\nggplot2 world [click]\nggplot2 extension gallery [click]\nClass code for understanding ggplot2 [click]\nClass code for titanic data visualization [click]\n\nRecommended books for the further study\n\nR Graphic Cookbook (written by Winston Chang)\n\nis a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems\n\nggplot2: elegant graphics for data analysis published by Springer"
  },
  {
    "objectID": "teaching/media_ds/about/index.html#text-data-analysis",
    "href": "teaching/media_ds/about/index.html#text-data-analysis",
    "title": "Syllabus",
    "section": "Text Data Analysis",
    "text": "Text Data Analysis\n\nWeek 8: Text mining (1)\n\nDate: 20230420\n전처리, 빈도 분석, 형태소 분석\nCode for the class [here]\nRecommended books for the text mining in R\n\nText Mining with R (written by Julia Silge & David Robinson)\nR로 하는 텍스트마이닝 (written by 안도현 교수님)\nDo it 쉽게 배우는 R 텍스트마이닝 (written by 김영우)\n\n\n\n\nWeek 9: 휴강\n\nDate: 20230427\n\n\n\nWeek 10: Text mining (2)\n\nDate: 20230504\nTF-IDF, 감정 분석\nCode for the class [here]\n\n\n\nWeek 11: Text mining (3)\n\nDate: 20230511\n동시출현 분석\nCode for the class [here]\n\n\n\nWeek 12: Text mining (4)\n\nDate: 20230518\n토픽 모델, 총정리 및 실습\nCode for the class [here]"
  },
  {
    "objectID": "teaching/media_ds/about/index.html#media-ds-application",
    "href": "teaching/media_ds/about/index.html#media-ds-application",
    "title": "Syllabus",
    "section": "Media & DS Application",
    "text": "Media & DS Application\n\nWeek 13: 데이터사이언스를 활용한 연구 기획을 위한 개인 미팅\n\n20230525(목) 외부 출장으로 인해 아래 시간 중 찾아올 것.\n개인 단위 연구도 가능하고 그룹 연구도 가능!\nOffice Hour:\n\n20230523 (화) 13:00 ~ 17:00\n20230524 (수) 13:00 ~ 16:00\n\n\n\n\nWeek 14: 데이터사이언스를 활용한 프로포절\n\nDate: 20230601\nClass:\n\n데이터사이언스를 활용해서 풀고 싶은 내용 (5분 발표)\n참고한 데이터사이언스를 활용한 논문들도 소개해주면 Even better!\n\n\n\n\nWeek 15: 최종 과제 제출\n\nDate: 20230608\n프로포절에서 받은 피드백을 바탕으로 ’자유 형식’으로 아래 구글폼을 통해 제출\n자유 형식이지만 약간의 Guide를 준다면,\n\n연구 배경: 연구의 중요성\n문제 제기: 연구의 독창성\n연구 질문: 풀고자 하는 문제\n연구 방법: 정의된 문제를 어떻게 풀지에 대한 내용\n예상 결과, 추진 계획 등\n\n\n\n\n&lt;p&gt;Loading…&lt;/p&gt;"
  },
  {
    "objectID": "teaching/media_ds/about/cov_anal.html",
    "href": "teaching/media_ds/about/cov_anal.html",
    "title": "Cov19 visualization practice",
    "section": "",
    "text": "Import libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(stringdist)\n\n\nAttaching package: 'stringdist'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n존스홉킨스 대학의 COVID19 데이터를 가져오는 코드\n\nclean_jhd_to_long &lt;- function(df) {\n  df_str &lt;- deparse(substitute(df))\n  var_str &lt;- substr(df_str, 1, str_length(df_str) - 4)\n  \n  df %&gt;% group_by(`Country/Region`) %&gt;%\n    filter(`Country/Region` != \"Cruise Ship\") %&gt;%\n    select(-`Province/State`, -Lat, -Long) %&gt;%\n    mutate_at(vars(-group_cols()), sum) %&gt;% \n    distinct() %&gt;%\n    ungroup() %&gt;%\n    rename(country = `Country/Region`) %&gt;%\n    pivot_longer(\n      -country, \n      names_to = \"date_str\", \n      values_to = var_str\n    ) %&gt;%\n    mutate(date = mdy(date_str)) %&gt;%\n    select(country, date, !! sym(var_str)) \n}\n\nconfirmed_raw &lt;- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndeaths_raw &lt;- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nhead(confirmed_raw)\n\n# A tibble: 6 × 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Afghanistan       33.9 67.7          0         0         0\n2 &lt;NA&gt;             Albania           41.2 20.2          0         0         0\n3 &lt;NA&gt;             Algeria           28.0  1.66         0         0         0\n4 &lt;NA&gt;             Andorra           42.5  1.52         0         0         0\n5 &lt;NA&gt;             Angola           -11.2 17.9          0         0         0\n6 &lt;NA&gt;             Antarctica       -71.9 23.3          0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\n\nconfirmed_raw[confirmed_raw$'Country/Region'==\"US\",]\n\n# A tibble: 1 × 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             US                  40  -100         1         1         2\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\n\nconfirmed_raw %&gt;% names\n\n   [1] \"Province/State\" \"Country/Region\" \"Lat\"            \"Long\"          \n   [5] \"1/22/20\"        \"1/23/20\"        \"1/24/20\"        \"1/25/20\"       \n   [9] \"1/26/20\"        \"1/27/20\"        \"1/28/20\"        \"1/29/20\"       \n  [13] \"1/30/20\"        \"1/31/20\"        \"2/1/20\"         \"2/2/20\"        \n  [17] \"2/3/20\"         \"2/4/20\"         \"2/5/20\"         \"2/6/20\"        \n  [21] \"2/7/20\"         \"2/8/20\"         \"2/9/20\"         \"2/10/20\"       \n  [25] \"2/11/20\"        \"2/12/20\"        \"2/13/20\"        \"2/14/20\"       \n  [29] \"2/15/20\"        \"2/16/20\"        \"2/17/20\"        \"2/18/20\"       \n  [33] \"2/19/20\"        \"2/20/20\"        \"2/21/20\"        \"2/22/20\"       \n  [37] \"2/23/20\"        \"2/24/20\"        \"2/25/20\"        \"2/26/20\"       \n  [41] \"2/27/20\"        \"2/28/20\"        \"2/29/20\"        \"3/1/20\"        \n  [45] \"3/2/20\"         \"3/3/20\"         \"3/4/20\"         \"3/5/20\"        \n  [49] \"3/6/20\"         \"3/7/20\"         \"3/8/20\"         \"3/9/20\"        \n  [53] \"3/10/20\"        \"3/11/20\"        \"3/12/20\"        \"3/13/20\"       \n  [57] \"3/14/20\"        \"3/15/20\"        \"3/16/20\"        \"3/17/20\"       \n  [61] \"3/18/20\"        \"3/19/20\"        \"3/20/20\"        \"3/21/20\"       \n  [65] \"3/22/20\"        \"3/23/20\"        \"3/24/20\"        \"3/25/20\"       \n  [69] \"3/26/20\"        \"3/27/20\"        \"3/28/20\"        \"3/29/20\"       \n  [73] \"3/30/20\"        \"3/31/20\"        \"4/1/20\"         \"4/2/20\"        \n  [77] \"4/3/20\"         \"4/4/20\"         \"4/5/20\"         \"4/6/20\"        \n  [81] \"4/7/20\"         \"4/8/20\"         \"4/9/20\"         \"4/10/20\"       \n  [85] \"4/11/20\"        \"4/12/20\"        \"4/13/20\"        \"4/14/20\"       \n  [89] \"4/15/20\"        \"4/16/20\"        \"4/17/20\"        \"4/18/20\"       \n  [93] \"4/19/20\"        \"4/20/20\"        \"4/21/20\"        \"4/22/20\"       \n  [97] \"4/23/20\"        \"4/24/20\"        \"4/25/20\"        \"4/26/20\"       \n [101] \"4/27/20\"        \"4/28/20\"        \"4/29/20\"        \"4/30/20\"       \n [105] \"5/1/20\"         \"5/2/20\"         \"5/3/20\"         \"5/4/20\"        \n [109] \"5/5/20\"         \"5/6/20\"         \"5/7/20\"         \"5/8/20\"        \n [113] \"5/9/20\"         \"5/10/20\"        \"5/11/20\"        \"5/12/20\"       \n [117] \"5/13/20\"        \"5/14/20\"        \"5/15/20\"        \"5/16/20\"       \n [121] \"5/17/20\"        \"5/18/20\"        \"5/19/20\"        \"5/20/20\"       \n [125] \"5/21/20\"        \"5/22/20\"        \"5/23/20\"        \"5/24/20\"       \n [129] \"5/25/20\"        \"5/26/20\"        \"5/27/20\"        \"5/28/20\"       \n [133] \"5/29/20\"        \"5/30/20\"        \"5/31/20\"        \"6/1/20\"        \n [137] \"6/2/20\"         \"6/3/20\"         \"6/4/20\"         \"6/5/20\"        \n [141] \"6/6/20\"         \"6/7/20\"         \"6/8/20\"         \"6/9/20\"        \n [145] \"6/10/20\"        \"6/11/20\"        \"6/12/20\"        \"6/13/20\"       \n [149] \"6/14/20\"        \"6/15/20\"        \"6/16/20\"        \"6/17/20\"       \n [153] \"6/18/20\"        \"6/19/20\"        \"6/20/20\"        \"6/21/20\"       \n [157] \"6/22/20\"        \"6/23/20\"        \"6/24/20\"        \"6/25/20\"       \n [161] \"6/26/20\"        \"6/27/20\"        \"6/28/20\"        \"6/29/20\"       \n [165] \"6/30/20\"        \"7/1/20\"         \"7/2/20\"         \"7/3/20\"        \n [169] \"7/4/20\"         \"7/5/20\"         \"7/6/20\"         \"7/7/20\"        \n [173] \"7/8/20\"         \"7/9/20\"         \"7/10/20\"        \"7/11/20\"       \n [177] \"7/12/20\"        \"7/13/20\"        \"7/14/20\"        \"7/15/20\"       \n [181] \"7/16/20\"        \"7/17/20\"        \"7/18/20\"        \"7/19/20\"       \n [185] \"7/20/20\"        \"7/21/20\"        \"7/22/20\"        \"7/23/20\"       \n [189] \"7/24/20\"        \"7/25/20\"        \"7/26/20\"        \"7/27/20\"       \n [193] \"7/28/20\"        \"7/29/20\"        \"7/30/20\"        \"7/31/20\"       \n [197] \"8/1/20\"         \"8/2/20\"         \"8/3/20\"         \"8/4/20\"        \n [201] \"8/5/20\"         \"8/6/20\"         \"8/7/20\"         \"8/8/20\"        \n [205] \"8/9/20\"         \"8/10/20\"        \"8/11/20\"        \"8/12/20\"       \n [209] \"8/13/20\"        \"8/14/20\"        \"8/15/20\"        \"8/16/20\"       \n [213] \"8/17/20\"        \"8/18/20\"        \"8/19/20\"        \"8/20/20\"       \n [217] \"8/21/20\"        \"8/22/20\"        \"8/23/20\"        \"8/24/20\"       \n [221] \"8/25/20\"        \"8/26/20\"        \"8/27/20\"        \"8/28/20\"       \n [225] \"8/29/20\"        \"8/30/20\"        \"8/31/20\"        \"9/1/20\"        \n [229] \"9/2/20\"         \"9/3/20\"         \"9/4/20\"         \"9/5/20\"        \n [233] \"9/6/20\"         \"9/7/20\"         \"9/8/20\"         \"9/9/20\"        \n [237] \"9/10/20\"        \"9/11/20\"        \"9/12/20\"        \"9/13/20\"       \n [241] \"9/14/20\"        \"9/15/20\"        \"9/16/20\"        \"9/17/20\"       \n [245] \"9/18/20\"        \"9/19/20\"        \"9/20/20\"        \"9/21/20\"       \n [249] \"9/22/20\"        \"9/23/20\"        \"9/24/20\"        \"9/25/20\"       \n [253] \"9/26/20\"        \"9/27/20\"        \"9/28/20\"        \"9/29/20\"       \n [257] \"9/30/20\"        \"10/1/20\"        \"10/2/20\"        \"10/3/20\"       \n [261] \"10/4/20\"        \"10/5/20\"        \"10/6/20\"        \"10/7/20\"       \n [265] \"10/8/20\"        \"10/9/20\"        \"10/10/20\"       \"10/11/20\"      \n [269] \"10/12/20\"       \"10/13/20\"       \"10/14/20\"       \"10/15/20\"      \n [273] \"10/16/20\"       \"10/17/20\"       \"10/18/20\"       \"10/19/20\"      \n [277] \"10/20/20\"       \"10/21/20\"       \"10/22/20\"       \"10/23/20\"      \n [281] \"10/24/20\"       \"10/25/20\"       \"10/26/20\"       \"10/27/20\"      \n [285] \"10/28/20\"       \"10/29/20\"       \"10/30/20\"       \"10/31/20\"      \n [289] \"11/1/20\"        \"11/2/20\"        \"11/3/20\"        \"11/4/20\"       \n [293] \"11/5/20\"        \"11/6/20\"        \"11/7/20\"        \"11/8/20\"       \n [297] \"11/9/20\"        \"11/10/20\"       \"11/11/20\"       \"11/12/20\"      \n [301] \"11/13/20\"       \"11/14/20\"       \"11/15/20\"       \"11/16/20\"      \n [305] \"11/17/20\"       \"11/18/20\"       \"11/19/20\"       \"11/20/20\"      \n [309] \"11/21/20\"       \"11/22/20\"       \"11/23/20\"       \"11/24/20\"      \n [313] \"11/25/20\"       \"11/26/20\"       \"11/27/20\"       \"11/28/20\"      \n [317] \"11/29/20\"       \"11/30/20\"       \"12/1/20\"        \"12/2/20\"       \n [321] \"12/3/20\"        \"12/4/20\"        \"12/5/20\"        \"12/6/20\"       \n [325] \"12/7/20\"        \"12/8/20\"        \"12/9/20\"        \"12/10/20\"      \n [329] \"12/11/20\"       \"12/12/20\"       \"12/13/20\"       \"12/14/20\"      \n [333] \"12/15/20\"       \"12/16/20\"       \"12/17/20\"       \"12/18/20\"      \n [337] \"12/19/20\"       \"12/20/20\"       \"12/21/20\"       \"12/22/20\"      \n [341] \"12/23/20\"       \"12/24/20\"       \"12/25/20\"       \"12/26/20\"      \n [345] \"12/27/20\"       \"12/28/20\"       \"12/29/20\"       \"12/30/20\"      \n [349] \"12/31/20\"       \"1/1/21\"         \"1/2/21\"         \"1/3/21\"        \n [353] \"1/4/21\"         \"1/5/21\"         \"1/6/21\"         \"1/7/21\"        \n [357] \"1/8/21\"         \"1/9/21\"         \"1/10/21\"        \"1/11/21\"       \n [361] \"1/12/21\"        \"1/13/21\"        \"1/14/21\"        \"1/15/21\"       \n [365] \"1/16/21\"        \"1/17/21\"        \"1/18/21\"        \"1/19/21\"       \n [369] \"1/20/21\"        \"1/21/21\"        \"1/22/21\"        \"1/23/21\"       \n [373] \"1/24/21\"        \"1/25/21\"        \"1/26/21\"        \"1/27/21\"       \n [377] \"1/28/21\"        \"1/29/21\"        \"1/30/21\"        \"1/31/21\"       \n [381] \"2/1/21\"         \"2/2/21\"         \"2/3/21\"         \"2/4/21\"        \n [385] \"2/5/21\"         \"2/6/21\"         \"2/7/21\"         \"2/8/21\"        \n [389] \"2/9/21\"         \"2/10/21\"        \"2/11/21\"        \"2/12/21\"       \n [393] \"2/13/21\"        \"2/14/21\"        \"2/15/21\"        \"2/16/21\"       \n [397] \"2/17/21\"        \"2/18/21\"        \"2/19/21\"        \"2/20/21\"       \n [401] \"2/21/21\"        \"2/22/21\"        \"2/23/21\"        \"2/24/21\"       \n [405] \"2/25/21\"        \"2/26/21\"        \"2/27/21\"        \"2/28/21\"       \n [409] \"3/1/21\"         \"3/2/21\"         \"3/3/21\"         \"3/4/21\"        \n [413] \"3/5/21\"         \"3/6/21\"         \"3/7/21\"         \"3/8/21\"        \n [417] \"3/9/21\"         \"3/10/21\"        \"3/11/21\"        \"3/12/21\"       \n [421] \"3/13/21\"        \"3/14/21\"        \"3/15/21\"        \"3/16/21\"       \n [425] \"3/17/21\"        \"3/18/21\"        \"3/19/21\"        \"3/20/21\"       \n [429] \"3/21/21\"        \"3/22/21\"        \"3/23/21\"        \"3/24/21\"       \n [433] \"3/25/21\"        \"3/26/21\"        \"3/27/21\"        \"3/28/21\"       \n [437] \"3/29/21\"        \"3/30/21\"        \"3/31/21\"        \"4/1/21\"        \n [441] \"4/2/21\"         \"4/3/21\"         \"4/4/21\"         \"4/5/21\"        \n [445] \"4/6/21\"         \"4/7/21\"         \"4/8/21\"         \"4/9/21\"        \n [449] \"4/10/21\"        \"4/11/21\"        \"4/12/21\"        \"4/13/21\"       \n [453] \"4/14/21\"        \"4/15/21\"        \"4/16/21\"        \"4/17/21\"       \n [457] \"4/18/21\"        \"4/19/21\"        \"4/20/21\"        \"4/21/21\"       \n [461] \"4/22/21\"        \"4/23/21\"        \"4/24/21\"        \"4/25/21\"       \n [465] \"4/26/21\"        \"4/27/21\"        \"4/28/21\"        \"4/29/21\"       \n [469] \"4/30/21\"        \"5/1/21\"         \"5/2/21\"         \"5/3/21\"        \n [473] \"5/4/21\"         \"5/5/21\"         \"5/6/21\"         \"5/7/21\"        \n [477] \"5/8/21\"         \"5/9/21\"         \"5/10/21\"        \"5/11/21\"       \n [481] \"5/12/21\"        \"5/13/21\"        \"5/14/21\"        \"5/15/21\"       \n [485] \"5/16/21\"        \"5/17/21\"        \"5/18/21\"        \"5/19/21\"       \n [489] \"5/20/21\"        \"5/21/21\"        \"5/22/21\"        \"5/23/21\"       \n [493] \"5/24/21\"        \"5/25/21\"        \"5/26/21\"        \"5/27/21\"       \n [497] \"5/28/21\"        \"5/29/21\"        \"5/30/21\"        \"5/31/21\"       \n [501] \"6/1/21\"         \"6/2/21\"         \"6/3/21\"         \"6/4/21\"        \n [505] \"6/5/21\"         \"6/6/21\"         \"6/7/21\"         \"6/8/21\"        \n [509] \"6/9/21\"         \"6/10/21\"        \"6/11/21\"        \"6/12/21\"       \n [513] \"6/13/21\"        \"6/14/21\"        \"6/15/21\"        \"6/16/21\"       \n [517] \"6/17/21\"        \"6/18/21\"        \"6/19/21\"        \"6/20/21\"       \n [521] \"6/21/21\"        \"6/22/21\"        \"6/23/21\"        \"6/24/21\"       \n [525] \"6/25/21\"        \"6/26/21\"        \"6/27/21\"        \"6/28/21\"       \n [529] \"6/29/21\"        \"6/30/21\"        \"7/1/21\"         \"7/2/21\"        \n [533] \"7/3/21\"         \"7/4/21\"         \"7/5/21\"         \"7/6/21\"        \n [537] \"7/7/21\"         \"7/8/21\"         \"7/9/21\"         \"7/10/21\"       \n [541] \"7/11/21\"        \"7/12/21\"        \"7/13/21\"        \"7/14/21\"       \n [545] \"7/15/21\"        \"7/16/21\"        \"7/17/21\"        \"7/18/21\"       \n [549] \"7/19/21\"        \"7/20/21\"        \"7/21/21\"        \"7/22/21\"       \n [553] \"7/23/21\"        \"7/24/21\"        \"7/25/21\"        \"7/26/21\"       \n [557] \"7/27/21\"        \"7/28/21\"        \"7/29/21\"        \"7/30/21\"       \n [561] \"7/31/21\"        \"8/1/21\"         \"8/2/21\"         \"8/3/21\"        \n [565] \"8/4/21\"         \"8/5/21\"         \"8/6/21\"         \"8/7/21\"        \n [569] \"8/8/21\"         \"8/9/21\"         \"8/10/21\"        \"8/11/21\"       \n [573] \"8/12/21\"        \"8/13/21\"        \"8/14/21\"        \"8/15/21\"       \n [577] \"8/16/21\"        \"8/17/21\"        \"8/18/21\"        \"8/19/21\"       \n [581] \"8/20/21\"        \"8/21/21\"        \"8/22/21\"        \"8/23/21\"       \n [585] \"8/24/21\"        \"8/25/21\"        \"8/26/21\"        \"8/27/21\"       \n [589] \"8/28/21\"        \"8/29/21\"        \"8/30/21\"        \"8/31/21\"       \n [593] \"9/1/21\"         \"9/2/21\"         \"9/3/21\"         \"9/4/21\"        \n [597] \"9/5/21\"         \"9/6/21\"         \"9/7/21\"         \"9/8/21\"        \n [601] \"9/9/21\"         \"9/10/21\"        \"9/11/21\"        \"9/12/21\"       \n [605] \"9/13/21\"        \"9/14/21\"        \"9/15/21\"        \"9/16/21\"       \n [609] \"9/17/21\"        \"9/18/21\"        \"9/19/21\"        \"9/20/21\"       \n [613] \"9/21/21\"        \"9/22/21\"        \"9/23/21\"        \"9/24/21\"       \n [617] \"9/25/21\"        \"9/26/21\"        \"9/27/21\"        \"9/28/21\"       \n [621] \"9/29/21\"        \"9/30/21\"        \"10/1/21\"        \"10/2/21\"       \n [625] \"10/3/21\"        \"10/4/21\"        \"10/5/21\"        \"10/6/21\"       \n [629] \"10/7/21\"        \"10/8/21\"        \"10/9/21\"        \"10/10/21\"      \n [633] \"10/11/21\"       \"10/12/21\"       \"10/13/21\"       \"10/14/21\"      \n [637] \"10/15/21\"       \"10/16/21\"       \"10/17/21\"       \"10/18/21\"      \n [641] \"10/19/21\"       \"10/20/21\"       \"10/21/21\"       \"10/22/21\"      \n [645] \"10/23/21\"       \"10/24/21\"       \"10/25/21\"       \"10/26/21\"      \n [649] \"10/27/21\"       \"10/28/21\"       \"10/29/21\"       \"10/30/21\"      \n [653] \"10/31/21\"       \"11/1/21\"        \"11/2/21\"        \"11/3/21\"       \n [657] \"11/4/21\"        \"11/5/21\"        \"11/6/21\"        \"11/7/21\"       \n [661] \"11/8/21\"        \"11/9/21\"        \"11/10/21\"       \"11/11/21\"      \n [665] \"11/12/21\"       \"11/13/21\"       \"11/14/21\"       \"11/15/21\"      \n [669] \"11/16/21\"       \"11/17/21\"       \"11/18/21\"       \"11/19/21\"      \n [673] \"11/20/21\"       \"11/21/21\"       \"11/22/21\"       \"11/23/21\"      \n [677] \"11/24/21\"       \"11/25/21\"       \"11/26/21\"       \"11/27/21\"      \n [681] \"11/28/21\"       \"11/29/21\"       \"11/30/21\"       \"12/1/21\"       \n [685] \"12/2/21\"        \"12/3/21\"        \"12/4/21\"        \"12/5/21\"       \n [689] \"12/6/21\"        \"12/7/21\"        \"12/8/21\"        \"12/9/21\"       \n [693] \"12/10/21\"       \"12/11/21\"       \"12/12/21\"       \"12/13/21\"      \n [697] \"12/14/21\"       \"12/15/21\"       \"12/16/21\"       \"12/17/21\"      \n [701] \"12/18/21\"       \"12/19/21\"       \"12/20/21\"       \"12/21/21\"      \n [705] \"12/22/21\"       \"12/23/21\"       \"12/24/21\"       \"12/25/21\"      \n [709] \"12/26/21\"       \"12/27/21\"       \"12/28/21\"       \"12/29/21\"      \n [713] \"12/30/21\"       \"12/31/21\"       \"1/1/22\"         \"1/2/22\"        \n [717] \"1/3/22\"         \"1/4/22\"         \"1/5/22\"         \"1/6/22\"        \n [721] \"1/7/22\"         \"1/8/22\"         \"1/9/22\"         \"1/10/22\"       \n [725] \"1/11/22\"        \"1/12/22\"        \"1/13/22\"        \"1/14/22\"       \n [729] \"1/15/22\"        \"1/16/22\"        \"1/17/22\"        \"1/18/22\"       \n [733] \"1/19/22\"        \"1/20/22\"        \"1/21/22\"        \"1/22/22\"       \n [737] \"1/23/22\"        \"1/24/22\"        \"1/25/22\"        \"1/26/22\"       \n [741] \"1/27/22\"        \"1/28/22\"        \"1/29/22\"        \"1/30/22\"       \n [745] \"1/31/22\"        \"2/1/22\"         \"2/2/22\"         \"2/3/22\"        \n [749] \"2/4/22\"         \"2/5/22\"         \"2/6/22\"         \"2/7/22\"        \n [753] \"2/8/22\"         \"2/9/22\"         \"2/10/22\"        \"2/11/22\"       \n [757] \"2/12/22\"        \"2/13/22\"        \"2/14/22\"        \"2/15/22\"       \n [761] \"2/16/22\"        \"2/17/22\"        \"2/18/22\"        \"2/19/22\"       \n [765] \"2/20/22\"        \"2/21/22\"        \"2/22/22\"        \"2/23/22\"       \n [769] \"2/24/22\"        \"2/25/22\"        \"2/26/22\"        \"2/27/22\"       \n [773] \"2/28/22\"        \"3/1/22\"         \"3/2/22\"         \"3/3/22\"        \n [777] \"3/4/22\"         \"3/5/22\"         \"3/6/22\"         \"3/7/22\"        \n [781] \"3/8/22\"         \"3/9/22\"         \"3/10/22\"        \"3/11/22\"       \n [785] \"3/12/22\"        \"3/13/22\"        \"3/14/22\"        \"3/15/22\"       \n [789] \"3/16/22\"        \"3/17/22\"        \"3/18/22\"        \"3/19/22\"       \n [793] \"3/20/22\"        \"3/21/22\"        \"3/22/22\"        \"3/23/22\"       \n [797] \"3/24/22\"        \"3/25/22\"        \"3/26/22\"        \"3/27/22\"       \n [801] \"3/28/22\"        \"3/29/22\"        \"3/30/22\"        \"3/31/22\"       \n [805] \"4/1/22\"         \"4/2/22\"         \"4/3/22\"         \"4/4/22\"        \n [809] \"4/5/22\"         \"4/6/22\"         \"4/7/22\"         \"4/8/22\"        \n [813] \"4/9/22\"         \"4/10/22\"        \"4/11/22\"        \"4/12/22\"       \n [817] \"4/13/22\"        \"4/14/22\"        \"4/15/22\"        \"4/16/22\"       \n [821] \"4/17/22\"        \"4/18/22\"        \"4/19/22\"        \"4/20/22\"       \n [825] \"4/21/22\"        \"4/22/22\"        \"4/23/22\"        \"4/24/22\"       \n [829] \"4/25/22\"        \"4/26/22\"        \"4/27/22\"        \"4/28/22\"       \n [833] \"4/29/22\"        \"4/30/22\"        \"5/1/22\"         \"5/2/22\"        \n [837] \"5/3/22\"         \"5/4/22\"         \"5/5/22\"         \"5/6/22\"        \n [841] \"5/7/22\"         \"5/8/22\"         \"5/9/22\"         \"5/10/22\"       \n [845] \"5/11/22\"        \"5/12/22\"        \"5/13/22\"        \"5/14/22\"       \n [849] \"5/15/22\"        \"5/16/22\"        \"5/17/22\"        \"5/18/22\"       \n [853] \"5/19/22\"        \"5/20/22\"        \"5/21/22\"        \"5/22/22\"       \n [857] \"5/23/22\"        \"5/24/22\"        \"5/25/22\"        \"5/26/22\"       \n [861] \"5/27/22\"        \"5/28/22\"        \"5/29/22\"        \"5/30/22\"       \n [865] \"5/31/22\"        \"6/1/22\"         \"6/2/22\"         \"6/3/22\"        \n [869] \"6/4/22\"         \"6/5/22\"         \"6/6/22\"         \"6/7/22\"        \n [873] \"6/8/22\"         \"6/9/22\"         \"6/10/22\"        \"6/11/22\"       \n [877] \"6/12/22\"        \"6/13/22\"        \"6/14/22\"        \"6/15/22\"       \n [881] \"6/16/22\"        \"6/17/22\"        \"6/18/22\"        \"6/19/22\"       \n [885] \"6/20/22\"        \"6/21/22\"        \"6/22/22\"        \"6/23/22\"       \n [889] \"6/24/22\"        \"6/25/22\"        \"6/26/22\"        \"6/27/22\"       \n [893] \"6/28/22\"        \"6/29/22\"        \"6/30/22\"        \"7/1/22\"        \n [897] \"7/2/22\"         \"7/3/22\"         \"7/4/22\"         \"7/5/22\"        \n [901] \"7/6/22\"         \"7/7/22\"         \"7/8/22\"         \"7/9/22\"        \n [905] \"7/10/22\"        \"7/11/22\"        \"7/12/22\"        \"7/13/22\"       \n [909] \"7/14/22\"        \"7/15/22\"        \"7/16/22\"        \"7/17/22\"       \n [913] \"7/18/22\"        \"7/19/22\"        \"7/20/22\"        \"7/21/22\"       \n [917] \"7/22/22\"        \"7/23/22\"        \"7/24/22\"        \"7/25/22\"       \n [921] \"7/26/22\"        \"7/27/22\"        \"7/28/22\"        \"7/29/22\"       \n [925] \"7/30/22\"        \"7/31/22\"        \"8/1/22\"         \"8/2/22\"        \n [929] \"8/3/22\"         \"8/4/22\"         \"8/5/22\"         \"8/6/22\"        \n [933] \"8/7/22\"         \"8/8/22\"         \"8/9/22\"         \"8/10/22\"       \n [937] \"8/11/22\"        \"8/12/22\"        \"8/13/22\"        \"8/14/22\"       \n [941] \"8/15/22\"        \"8/16/22\"        \"8/17/22\"        \"8/18/22\"       \n [945] \"8/19/22\"        \"8/20/22\"        \"8/21/22\"        \"8/22/22\"       \n [949] \"8/23/22\"        \"8/24/22\"        \"8/25/22\"        \"8/26/22\"       \n [953] \"8/27/22\"        \"8/28/22\"        \"8/29/22\"        \"8/30/22\"       \n [957] \"8/31/22\"        \"9/1/22\"         \"9/2/22\"         \"9/3/22\"        \n [961] \"9/4/22\"         \"9/5/22\"         \"9/6/22\"         \"9/7/22\"        \n [965] \"9/8/22\"         \"9/9/22\"         \"9/10/22\"        \"9/11/22\"       \n [969] \"9/12/22\"        \"9/13/22\"        \"9/14/22\"        \"9/15/22\"       \n [973] \"9/16/22\"        \"9/17/22\"        \"9/18/22\"        \"9/19/22\"       \n [977] \"9/20/22\"        \"9/21/22\"        \"9/22/22\"        \"9/23/22\"       \n [981] \"9/24/22\"        \"9/25/22\"        \"9/26/22\"        \"9/27/22\"       \n [985] \"9/28/22\"        \"9/29/22\"        \"9/30/22\"        \"10/1/22\"       \n [989] \"10/2/22\"        \"10/3/22\"        \"10/4/22\"        \"10/5/22\"       \n [993] \"10/6/22\"        \"10/7/22\"        \"10/8/22\"        \"10/9/22\"       \n [997] \"10/10/22\"       \"10/11/22\"       \"10/12/22\"       \"10/13/22\"      \n[1001] \"10/14/22\"       \"10/15/22\"       \"10/16/22\"       \"10/17/22\"      \n[1005] \"10/18/22\"       \"10/19/22\"       \"10/20/22\"       \"10/21/22\"      \n[1009] \"10/22/22\"       \"10/23/22\"       \"10/24/22\"       \"10/25/22\"      \n[1013] \"10/26/22\"       \"10/27/22\"       \"10/28/22\"       \"10/29/22\"      \n[1017] \"10/30/22\"       \"10/31/22\"       \"11/1/22\"        \"11/2/22\"       \n[1021] \"11/3/22\"        \"11/4/22\"        \"11/5/22\"        \"11/6/22\"       \n[1025] \"11/7/22\"        \"11/8/22\"        \"11/9/22\"        \"11/10/22\"      \n[1029] \"11/11/22\"       \"11/12/22\"       \"11/13/22\"       \"11/14/22\"      \n[1033] \"11/15/22\"       \"11/16/22\"       \"11/17/22\"       \"11/18/22\"      \n[1037] \"11/19/22\"       \"11/20/22\"       \"11/21/22\"       \"11/22/22\"      \n[1041] \"11/23/22\"       \"11/24/22\"       \"11/25/22\"       \"11/26/22\"      \n[1045] \"11/27/22\"       \"11/28/22\"       \"11/29/22\"       \"11/30/22\"      \n[1049] \"12/1/22\"        \"12/2/22\"        \"12/3/22\"        \"12/4/22\"       \n[1053] \"12/5/22\"        \"12/6/22\"        \"12/7/22\"        \"12/8/22\"       \n[1057] \"12/9/22\"        \"12/10/22\"       \"12/11/22\"       \"12/12/22\"      \n[1061] \"12/13/22\"       \"12/14/22\"       \"12/15/22\"       \"12/16/22\"      \n[1065] \"12/17/22\"       \"12/18/22\"       \"12/19/22\"       \"12/20/22\"      \n[1069] \"12/21/22\"       \"12/22/22\"       \"12/23/22\"       \"12/24/22\"      \n[1073] \"12/25/22\"       \"12/26/22\"       \"12/27/22\"       \"12/28/22\"      \n[1077] \"12/29/22\"       \"12/30/22\"       \"12/31/22\"       \"1/1/23\"        \n[1081] \"1/2/23\"         \"1/3/23\"         \"1/4/23\"         \"1/5/23\"        \n[1085] \"1/6/23\"         \"1/7/23\"         \"1/8/23\"         \"1/9/23\"        \n[1089] \"1/10/23\"        \"1/11/23\"        \"1/12/23\"        \"1/13/23\"       \n[1093] \"1/14/23\"        \"1/15/23\"        \"1/16/23\"        \"1/17/23\"       \n[1097] \"1/18/23\"        \"1/19/23\"        \"1/20/23\"        \"1/21/23\"       \n[1101] \"1/22/23\"        \"1/23/23\"        \"1/24/23\"        \"1/25/23\"       \n[1105] \"1/26/23\"        \"1/27/23\"        \"1/28/23\"        \"1/29/23\"       \n[1109] \"1/30/23\"        \"1/31/23\"        \"2/1/23\"         \"2/2/23\"        \n[1113] \"2/3/23\"         \"2/4/23\"         \"2/5/23\"         \"2/6/23\"        \n[1117] \"2/7/23\"         \"2/8/23\"         \"2/9/23\"         \"2/10/23\"       \n[1121] \"2/11/23\"        \"2/12/23\"        \"2/13/23\"        \"2/14/23\"       \n[1125] \"2/15/23\"        \"2/16/23\"        \"2/17/23\"        \"2/18/23\"       \n[1129] \"2/19/23\"        \"2/20/23\"        \"2/21/23\"        \"2/22/23\"       \n[1133] \"2/23/23\"        \"2/24/23\"        \"2/25/23\"        \"2/26/23\"       \n[1137] \"2/27/23\"        \"2/28/23\"        \"3/1/23\"         \"3/2/23\"        \n[1141] \"3/3/23\"         \"3/4/23\"         \"3/5/23\"         \"3/6/23\"        \n[1145] \"3/7/23\"         \"3/8/23\"         \"3/9/23\"        \n\n\n확진자 데이터 프레임을 만들어 보자.\n\nconfirmed_raw %&gt;% \n  filter(`Country/Region` %in% c(\"China\", \"Italy\", \"Japan\", \"United Kingdom\", \"US\", \"Korea, South\",\n                                 \"Spain\")) %&gt;%\n  select(-c(`Province/State`, Lat, Long)) %&gt;% \n  group_by(`Country/Region`) %&gt;% summarise_all(sum) -&gt; test\n\nnames(test)[1]&lt;-\"country\"\n\nmelt(data = test, id.vars = \"country\", measure.vars = names(test)[-1]) %&gt;% \n  separate(variable, into = c(\"mon\", \"day\", \"year\"), sep='/', extra = \"merge\") %&gt;% \n  filter(day %in% c(1)) %&gt;%\n  arrange(mon, day) %&gt;% \n  mutate(date=as.Date(with(.,paste(mon, day, year, sep=\"/\")), format = \"%m/%d/%y\")) %&gt;% \n  dcast(country ~ date) -&gt; df.conf.case\n\n\ndf.conf.case\n\n         country 2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\n1          China      11891      79932      84002      86850      87520\n2          Italy          2       1694     110574     207428     233197\n3          Japan         20        259       2535      14558      16778\n4   Korea, South         12       3736       9887      10780      11541\n5          Spain          1         84     104118     215216     239638\n6 United Kingdom          2         94      43755     183500     258979\n7             US          8         32     227903    1115972    1809384\n  2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01 2020-12-01 2021-01-01\n1      88344      91690      94363      95568      97250      99336     102649\n2     240760     247832     270189     317409     709335    1620901    2129376\n3      18732      37790      69018      84212     101936     150857     239005\n4      12904      14366      20449      23952      26732      35163      62593\n5     249659     288522     470973     778607    1185678    1656444    1928265\n6     285276     305558     339403     462780    1038056    1647165    2549671\n7    2698127    4605921    6088458    7292562    9254490   13866746   20397398\n  2021-02-01 2021-03-01 2021-04-01 2021-05-01 2021-06-01 2021-07-01 2021-08-01\n1     107902     109034     110169     111325     112329     113614     115473\n2    2560957    2938371    3607083    4035617    4220304    4260788    4355348\n3     392533     433334     477691     598754     749126     801337     936852\n4      78844      90372     104194     123240     141476     158549     201002\n5    2822805    3204531    3291394    3524077    3682778    3821305    4447044\n6    3846807    4194287    4364544    4434156    4506331    4844879    5907641\n7   26482919   28814420   30656330   32516226   33407540   33797251   35152818\n  2021-09-01 2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01 2022-03-01\n1     117991     119790     121477     123725     128132     134564     456582\n2    4546487    4675758    4774783    5043620    6266939   11116422   12829972\n3    1514400    1706516    1722427    1726913    1733835    2825414    5078276\n4     255401     316020     367974     457612     639083     884310    3492686\n5    4861883    4961128    5011148    5174720    6294745   10039126   11036085\n6    6856890    7878555    9140352   10333452   13174530   17543963   19120746\n7   39585475   43694428   46163201   48743340   55099948   75570589   79228450\n  2022-04-01 2022-05-01 2022-06-01 2022-07-01 2022-08-01 2022-09-01 2022-10-01\n1    1400358    2024284    2097282    2137169    2265424    2510703    2762150\n2   14719394   16504791   17440232   18610011   21059545   21888255   22500346\n3    6614278    7910179    8876113    9355427   12935010   19116684   21329519\n4   13639915   17295733   18129313   18379552   19932439   23417425   24819611\n5   11551574   11896152   12360256   12818184   13226579   13342530   13422984\n6   21379545   22214004   22492903   22941360   23515928   23738035   23893496\n7   80252748   81483804   84556267   87832253   91515236   94659072   96369625\n  2022-11-01 2022-12-01 2023-01-01 2023-02-01 2023-03-01\n1    2959481    3764783    4612203    4903498    4903524\n2   23531023   24260660   25143705   25453789   25576852\n3   22389872   24933509   29321601   32610584   33241180\n4   25670407   27208800   29139535   30213928   30533573\n5   13511768   13595504   13684258   13731478   13763336\n6   24122922   24251636   24365688   24507372   24603450\n7   97540736   98903928  100769628  102479379  103533872\n\n\n사망자 데이터 프레임을 만들어 보자.\n\ndeaths_raw %&gt;% \n  filter(`Country/Region` %in% c(\"China\", \"Italy\", \"Japan\", \"United Kingdom\", \"US\", \"Korea, South\",\n                                 \"Spain\")) %&gt;%\n  select(-c(`Province/State`, Lat, Long)) %&gt;% \n  group_by(`Country/Region`) %&gt;% summarise_all(sum) -&gt; test\n\nnames(test)[1]&lt;-\"country\"\n\nmelt(data = test, id.vars = \"country\", measure.vars = names(test)[-1]) %&gt;% \n  separate(variable, into = c(\"mon\", \"day\", \"year\"), sep='/', extra = \"merge\") %&gt;% \n  filter(day %in% c(1)) %&gt;%\n  arrange(mon, day) %&gt;% \n  mutate(date=as.Date(with(.,paste(mon, day, year, sep=\"/\")), format = \"%m/%d/%y\")) %&gt;% \n  dcast(country ~ date) -&gt; df.death.case\n\n\ndf.death.case\n\n         country 2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\n1          China        259       2872       3332       4698       4708\n2          Italy          0         34      13155      28236      33475\n3          Japan          0          6         72        510        900\n4   Korea, South          0         17        165        250        272\n5          Spain          0          0       9387      24543      27127\n6 United Kingdom          1          3       6070      39849      52768\n7             US          0          1       6996      68518     108624\n  2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01 2020-12-01 2021-01-01\n1       4713       4737       4797       4813       4814       4830       4884\n2      34788      35146      35491      35918      38826      56361      74621\n3        976       1013       1314       1583       1776       2193       3541\n4        282        301        326        416        468        526        942\n5      28364      28445      29152      31973      35878      45511      50837\n6      56338      57454      57995      58946      64667      78184      95917\n7     128134     155059     183855     206852     231054     273099     352844\n  2021-02-01 2021-03-01 2021-04-01 2021-05-01 2021-06-01 2021-07-01 2021-08-01\n1       4966       5011       5023       5031       5031       5033       5047\n2      88845      97945     109847     121033     126221     127587     128068\n3       5833       7948       9194      10326      13160      14808      15198\n4       1435       1606       1737       1833       1965       2024       2099\n5      59081      69609      75541      78216      79983      80883      81486\n6     132799     148935     153012     154085     154509     155010     156941\n7     448381     513045     549448     572904     590904     600972     609715\n  2021-09-01 2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01 2022-03-01\n1       5056       5069       5081       5089       5103       5119       5901\n2     129290     130973     132120     133931     137513     146925     155000\n3      16138      17685      18274      18361      18392      18885      23908\n4       2303       2504       2874       3705       5694       6787       8266\n5      84472      86463      87368      88080      89405      93633      99883\n6     160317     164780     169438     173903     178046     184840     188681\n7     639812     699021     746135     781422     825870     892252     952086\n  2022-04-01 2022-05-01 2022-06-01 2022-07-01 2022-08-01 2022-09-01 2022-10-01\n1      12869      14697      14899      14928      15052      15251      15719\n2     159537     163612     166756     168425     172207     175663     177130\n3      28202      29605      30659      31302      32707      40245      45023\n4      16929      22958      24212      24562      25084      26940      28489\n5     102541     104456     106493     108111     110719     112600     114179\n6     193232     198276     200347     201869     205574     207875     209346\n7     983972     996109    1007741    1017872    1030654    1046956    1059542\n  2022-11-01 2022-12-01 2023-01-01 2023-02-01 2023-03-01\n1      15965      16001      17167      97668     101051\n2     179101     181098     184642     186833     188094\n3      46817      49834      57521      68407      72494\n4      29239      30621      32272      33522      34003\n5     115078     115901     117095     118434     119380\n6     212435     214234     217175     220129     220721\n7    1070821    1081153    1092779    1109996    1120897\n\n\n확진자, 사망자 데이터 프레임을 행렬로 만들어보자.¶\n\n# country.name&lt;-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \ncountry.name&lt;-unlist(df.conf.case[c(1)])\n\n#str(df.conf.case)\nm.conf.case&lt;-as.matrix(df.conf.case[-1])\nrow.names(m.conf.case)&lt;-country.name\n\nm.death.case&lt;-as.matrix(df.death.case[-1])\nrow.names(m.death.case)=country.name\n\nm.death.rate&lt;-round(m.death.case/m.conf.case, 2)\n\n\n확진자 행렬: m.conf.case\n사망자 행렬: m.death.case\n\n\nm.conf.case\n\n               2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\nChina               11891      79932      84002      86850      87520\nItaly                   2       1694     110574     207428     233197\nJapan                  20        259       2535      14558      16778\nKorea, South           12       3736       9887      10780      11541\nSpain                   1         84     104118     215216     239638\nUnited Kingdom          2         94      43755     183500     258979\nUS                      8         32     227903    1115972    1809384\n               2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01\nChina               88344      91690      94363      95568      97250\nItaly              240760     247832     270189     317409     709335\nJapan               18732      37790      69018      84212     101936\nKorea, South        12904      14366      20449      23952      26732\nSpain              249659     288522     470973     778607    1185678\nUnited Kingdom     285276     305558     339403     462780    1038056\nUS                2698127    4605921    6088458    7292562    9254490\n               2020-12-01 2021-01-01 2021-02-01 2021-03-01 2021-04-01\nChina               99336     102649     107902     109034     110169\nItaly             1620901    2129376    2560957    2938371    3607083\nJapan              150857     239005     392533     433334     477691\nKorea, South        35163      62593      78844      90372     104194\nSpain             1656444    1928265    2822805    3204531    3291394\nUnited Kingdom    1647165    2549671    3846807    4194287    4364544\nUS               13866746   20397398   26482919   28814420   30656330\n               2021-05-01 2021-06-01 2021-07-01 2021-08-01 2021-09-01\nChina              111325     112329     113614     115473     117991\nItaly             4035617    4220304    4260788    4355348    4546487\nJapan              598754     749126     801337     936852    1514400\nKorea, South       123240     141476     158549     201002     255401\nSpain             3524077    3682778    3821305    4447044    4861883\nUnited Kingdom    4434156    4506331    4844879    5907641    6856890\nUS               32516226   33407540   33797251   35152818   39585475\n               2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01\nChina              119790     121477     123725     128132     134564\nItaly             4675758    4774783    5043620    6266939   11116422\nJapan             1706516    1722427    1726913    1733835    2825414\nKorea, South       316020     367974     457612     639083     884310\nSpain             4961128    5011148    5174720    6294745   10039126\nUnited Kingdom    7878555    9140352   10333452   13174530   17543963\nUS               43694428   46163201   48743340   55099948   75570589\n               2022-03-01 2022-04-01 2022-05-01 2022-06-01 2022-07-01\nChina              456582    1400358    2024284    2097282    2137169\nItaly            12829972   14719394   16504791   17440232   18610011\nJapan             5078276    6614278    7910179    8876113    9355427\nKorea, South      3492686   13639915   17295733   18129313   18379552\nSpain            11036085   11551574   11896152   12360256   12818184\nUnited Kingdom   19120746   21379545   22214004   22492903   22941360\nUS               79228450   80252748   81483804   84556267   87832253\n               2022-08-01 2022-09-01 2022-10-01 2022-11-01 2022-12-01\nChina             2265424    2510703    2762150    2959481    3764783\nItaly            21059545   21888255   22500346   23531023   24260660\nJapan            12935010   19116684   21329519   22389872   24933509\nKorea, South     19932439   23417425   24819611   25670407   27208800\nSpain            13226579   13342530   13422984   13511768   13595504\nUnited Kingdom   23515928   23738035   23893496   24122922   24251636\nUS               91515236   94659072   96369625   97540736   98903928\n               2023-01-01 2023-02-01 2023-03-01\nChina             4612203    4903498    4903524\nItaly            25143705   25453789   25576852\nJapan            29321601   32610584   33241180\nKorea, South     29139535   30213928   30533573\nSpain            13684258   13731478   13763336\nUnited Kingdom   24365688   24507372   24603450\nUS              100769628  102479379  103533872\n\n\n\nm.death.case\n\n               2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\nChina                 259       2872       3332       4698       4708\nItaly                   0         34      13155      28236      33475\nJapan                   0          6         72        510        900\nKorea, South            0         17        165        250        272\nSpain                   0          0       9387      24543      27127\nUnited Kingdom          1          3       6070      39849      52768\nUS                      0          1       6996      68518     108624\n               2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01\nChina                4713       4737       4797       4813       4814\nItaly               34788      35146      35491      35918      38826\nJapan                 976       1013       1314       1583       1776\nKorea, South          282        301        326        416        468\nSpain               28364      28445      29152      31973      35878\nUnited Kingdom      56338      57454      57995      58946      64667\nUS                 128134     155059     183855     206852     231054\n               2020-12-01 2021-01-01 2021-02-01 2021-03-01 2021-04-01\nChina                4830       4884       4966       5011       5023\nItaly               56361      74621      88845      97945     109847\nJapan                2193       3541       5833       7948       9194\nKorea, South          526        942       1435       1606       1737\nSpain               45511      50837      59081      69609      75541\nUnited Kingdom      78184      95917     132799     148935     153012\nUS                 273099     352844     448381     513045     549448\n               2021-05-01 2021-06-01 2021-07-01 2021-08-01 2021-09-01\nChina                5031       5031       5033       5047       5056\nItaly              121033     126221     127587     128068     129290\nJapan               10326      13160      14808      15198      16138\nKorea, South         1833       1965       2024       2099       2303\nSpain               78216      79983      80883      81486      84472\nUnited Kingdom     154085     154509     155010     156941     160317\nUS                 572904     590904     600972     609715     639812\n               2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01\nChina                5069       5081       5089       5103       5119\nItaly              130973     132120     133931     137513     146925\nJapan               17685      18274      18361      18392      18885\nKorea, South         2504       2874       3705       5694       6787\nSpain               86463      87368      88080      89405      93633\nUnited Kingdom     164780     169438     173903     178046     184840\nUS                 699021     746135     781422     825870     892252\n               2022-03-01 2022-04-01 2022-05-01 2022-06-01 2022-07-01\nChina                5901      12869      14697      14899      14928\nItaly              155000     159537     163612     166756     168425\nJapan               23908      28202      29605      30659      31302\nKorea, South         8266      16929      22958      24212      24562\nSpain               99883     102541     104456     106493     108111\nUnited Kingdom     188681     193232     198276     200347     201869\nUS                 952086     983972     996109    1007741    1017872\n               2022-08-01 2022-09-01 2022-10-01 2022-11-01 2022-12-01\nChina               15052      15251      15719      15965      16001\nItaly              172207     175663     177130     179101     181098\nJapan               32707      40245      45023      46817      49834\nKorea, South        25084      26940      28489      29239      30621\nSpain              110719     112600     114179     115078     115901\nUnited Kingdom     205574     207875     209346     212435     214234\nUS                1030654    1046956    1059542    1070821    1081153\n               2023-01-01 2023-02-01 2023-03-01\nChina               17167      97668     101051\nItaly              184642     186833     188094\nJapan               57521      68407      72494\nKorea, South        32272      33522      34003\nSpain              117095     118434     119380\nUnited Kingdom     217175     220129     220721\nUS                1092779    1109996    1120897\n\n\n선정된 국가에 대한 인구 벡터를 만들어 봅시다. 국가 이름 벡터. 인구 벡터에 이름을 붙여주기 위해 생성\n\ncountry.name\n\n        country1         country2         country3         country4 \n         \"China\"          \"Italy\"          \"Japan\"   \"Korea, South\" \n        country5         country6         country7 \n         \"Spain\" \"United Kingdom\"             \"US\" \n\n\n선정된 국가 순서대로 인구 수를 입력한 벡터\n\npop&lt;-c(1439323776, 60461826, 126476461, 51269185, 46754778, 67886011, 331002651)\npop\n\n[1] 1439323776   60461826  126476461   51269185   46754778   67886011  331002651\n\n\n아직은 벡터 값들에 이름이 붙어 있지 않은 것을 알 수 있다.\n\nnames(pop)\n\nNULL\n\n\npop 벡터에 각 인구수가 어느 국가이 인구수인지 names() 함수로 지정해줌\n\npop&lt;-c(1439323776, 60461826, 126476461, 51269185, 46754778, 67886011, 331002651)\nnames(pop)&lt;-country.name\npop\n\n         China          Italy          Japan   Korea, South          Spain \n    1439323776       60461826      126476461       51269185       46754778 \nUnited Kingdom             US \n      67886011      331002651 \n\n\nGDP 벡터를 만들어 봅시다 - 마찬가지로 names() 함수로 GDP가 어느 국가에 해당하는 GDP인지에 대한 정보를 준다.\n\n# round(m.conf.case/pop*1000, 2)\ncountry.name&lt;-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \nGDP&lt;-c(12237700479375,\n1943835376342,\n4872415104315,\n1530750923149,\n1314314164402,\n2637866340434,\n19485394000000)\nnames(GDP)&lt;-country.name\n\nGDP\n\n       China        Italy        Japan        Korea        Spain           UK \n1.223770e+13 1.943835e+12 4.872415e+12 1.530751e+12 1.314314e+12 2.637866e+12 \n          US \n1.948539e+13 \n\n\n선정된 국가에 대한 인구밀도 벡터를 만들어 봅시다.\n\ncountry.name&lt;-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \npop.density&lt;-c(148, 205, 347, 530, 94, 275, 36)\nnames(pop.density)&lt;-country.name\n\n각국의 GDP 시각화를 해보자.\n\nbarplot(GDP)\n\n\n\n\n\nbarplot(sort(GDP))\n\n\n\n\n\nbarplot(sort(GDP, decreasing = T))"
  },
  {
    "objectID": "teaching/ds101/index.html",
    "href": "teaching/ds101/index.html",
    "title": "Cultural Industry & Data Analytics",
    "section": "",
    "text": "a &lt;- \"Data\"\nb &lt;- \"Science\"\npaste0(\"Welcome to \", a,\" \",b,\" \",100+1)\n\n[1] \"Welcome to Data Science 101\"\n\n\n\n\n\n\n\n\n\n\nAbout Course\n\n\n\nWeekly Design\n\n\n\nPBL"
  },
  {
    "objectID": "teaching/cul_tech/weekly/index.html",
    "href": "teaching/cul_tech/weekly/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Intro\n\n\n\nWeek 1: Course Intro\n\nDate: 20230830\nClass: Course Introduction\n\nAn overview of the course, discussion on the relationship between culture and technology, and the significance of studying this interaction.\nPDF [download]\n\n\n\n\n\n\nPart I: Media as a Mediation of cultural formation and diffusion\n\n\n\n\nWeek 2: What is Media?\n\nDate: 20230906\nClass\n\nDefinition and types of media, exploring the purpose and influence of media on cultural development\nPDF [download]\n\n\n\n\nWeek 3: Broadcast & Tech\n\nDate: 20230913\nClass (Online)\n\nStudy of the broadcast industry’s evolution and its cultural implications, including radio and television.\n\nBroadcast Part I\n\nBroadcast Part II\n\nBroadcast Part III\n\nPDF [download]\n\n\n\n\nWeek 4: Press & Tech\n\nDate: 20230920\nClass\n\nExamination of print media such as newspapers, magazines, its history, evolution, and cultural impact, and Exploration of the Internet’s influence on culture, including the shift in information access and the emergence of digital culture.\nPDF [download]\n\n\n\n\nWeek 5: Social Media & Tech\n\nDate: 20230927\nClass\n\nAnalysis of the rise of social media platforms, their role in shaping modern culture, and discussion on phenomena like cancel culture, influencers, etc.\nPDF [download]\n\n\n\n\nWeek 6: Media Policy, Law, and Literacy\n\nDate: 20231004\nClass\n\nExamination of media regulations, media laws, and the importance of media literacy in today’s digital age.\nPDF [download]\n\n\n\n\nWeek 7: Internet, UI/UX, and Smart-media era\n\nDate: 20231011\nClass\n\nPDF [download]\n\n\n\n\n\n\nPart II: Voice from Culture Industry\n\n\n\n\nWeek 8: Special lecture (1): 엔터테인먼트 산업\n\nDate: 20231018\nPre-class video: How Netflix And YouTube Changed Entertainment Forever by CNBC Marathon\n\nClass: 10:00 ~ 11:30 특강\n\n이동찬 경영총괄 (TEO 유니버스)\n강연자료 PDF [download]\n\n\n\n\n\nWeek 9: Special lecture (2): 문화 콘텐츠 애플리케이션 개발\n\nDate: 20231025\nPre-class video: Opportunities in AI 2023 by Andrew Ng (Stanford Univ.)\n\nClass: 10:00 ~ 11:30 특강\n\n윤영훈 대표 (ASSI)\n시간사용 PDF [download]\n창업에 대한 핵심생각 PDF [download]\n\n\n\n\n\n\nPart III: Understanding the Cultural Industry\n\n\n\n\nWeek 10: Film\n\nDate: 20231101\nClass\n\nOverview of the film and television industries, analysis of their cultural implications, and their transformation with technology.\nPDF [download]\n\n\n\n\nWeek 11: Music & GenAI\n\nDate: 20231108\nClass\n\nStudy of the music industry and publishing sector, their evolution in the digital age, and the resultant cultural shifts.\nPDF [download]\n\n\n\n\nWeek 12: Immersive Tech, Game, & Entertainment Industry\n\nDate: 20231115\nPre-class video: How immersive technologies (AR/VR) will shape our future by Dinesh Punni (TED Talk)\n\nClass\n\nExploration of the gaming industry, its rise as a cultural phenomenon, and its influence on society.\nDiscussion on various forms of entertainment, their cultural impact, and the changes brought about by technological advancements.\nPDF [download]\n\n\n\n\nWeek 13: Advertising & PR\n\nDate: 20231122\nClass\n\nExamination of the advertising and PR industries, their relationship with culture, and the influence of technology on them.\nPDF [download]\n\n\n\n\n\n\nPart IV: Wrap-up & PBL Activity\n\n\n\n\nWeek 14: PBL mentoring (at ZOOM)\n\nDate: 20231129\nZoom link 는 당일 단톡방에서 확인\n미팅 시간표\n\n\n\nTime\nTeam\n\n\n\n\n09:00-09:20\n1\n\n\n09:20-09:40\n2\n\n\n09:40-10:00\n3\n\n\n10:00-10:20\n4\n\n\n10:20-10:40\n5\n\n\n10:40-11:00\n6\n\n\n11:00-11:20\n7\n\n\n11:20-11:40\n8\n\n\n\n\n\n\n\nWeek 15: Individual Essay about Culture & Technology\n\nDate: 20231206\n에세이\n\n내용: 수업에서 배운 내용을 중심으로 아래 주제 중에서 하나를 택해서 (또는 자유 주제) 2000자 (A4 2 페이지) 이하로 작성\n가능한 주제 (but not limited)\n\n미디어가 문화 형성에 미치는 영향\n방송 산업의 진화와 문화적 함의\n인쇄 미디어와 디지털 문화의 상호작용\n소셜 미디어의 부상과 새로운 문화 형성\n미디어 정책과 법률이 문화에 미치는 영향 (미디어 규제와 법률이 문화적 표현과 소통에 어떤 영향을 미치는지)\n인터넷, UI/UX, 그리고 스마트 미디어 시대 (인터넷과 사용자 인터페이스가 문화에 미치는 영향)\nAR/VR 기술이 엔터테인먼트 산업에 끼친 영향\n음악 산업의 디지털화와 문화적 변화\n광고 및 PR 산업의 기술적 변화와 문화적 영향\n스트리밍 서비스가 엔터테인먼트 산업에 끼친 영향\n\n주의사항\n\n2000자가 넘으면 안됨\n수업 시간에 배운 내용의 요약 + 본인의 의견이 잘 어우러지도록\n영어로 작성 가능 (Writing in English is available)\n생성형인공지능 활용한 글쓰기도 가능함 (유의해서 사용)\n파일명: CNT_essay_홍길동.hwp(doc)\n\n\n제출: 아래 구글 폼 링크에서 업로드\n\nhttps://forms.gle/tABHibRyQg48SK8s6\n\n\n\n\n\nWeek 16: 프로젝트 최종 보고서 발표 및 제출\n\nDate: 20231213\n보고서 = 발표 자료 (PPT)로 제출 가능\n발제 순서\n\n\n\nTime\nTeam\n\n\n\n\n09:10-09:30\nEnt1\n\n\n09:30-09:50\nEnt2\n\n\n09:50-10:10\nEnt3\n\n\n10:10-10:30\nEnt4\n\n\n10:30-10:50\nEnt5\n\n\n10:50-11:10\nApp1\n\n\n11:10-11:30\nApp2\n\n\n11:30-11:50\nApp3\n\n\n\n\n\n\n최종 팀 보고서 제출: 아래 구글 폼 링크에서 업로드\n\nhttps://forms.gle/U5wWe8b9kSRE8KB58\n파일 이름: CNT_PBL_final_1조.pptx(hwp, doc ..)\n파일이 너무 커서 업로드가 안되는 등의 이슈가 있을 시 메일로 보내주세요. (changjunlee@skku.edu)\n\n\n\n\n동료 평가 및 수업에 대한 설문\n\nhttps://forms.gle/BWgHoz9GoAtZzjDQ6"
  },
  {
    "objectID": "teaching/cul_tech/index.html",
    "href": "teaching/cul_tech/index.html",
    "title": "Culture & Technology (Vol.2)",
    "section": "",
    "text": "a &lt;- \"Culture\"\nb &lt;- \"Technology\"\npaste0(\"Welcome to \", a,\" & \",b,\" \",2)\n\n[1] \"Welcome to Culture & Technology 2\"\n\n\n\n\n\n\n\n\n\n\n\nAbout course\n(코스 소개)\n\n\n\nWeekly design\n(주차별 학습)\n\n\n\nPBL\n(PBL project)"
  },
  {
    "objectID": "teaching/cul_tech/about/index.html",
    "href": "teaching/cul_tech/about/index.html",
    "title": "Course description & Communication",
    "section": "",
    "text": "Course description\nCulture & Technology is a comprehensive course exploring the intersection of culture and various forms of media technology. Spanning from the early days of print media to the era of social media, the course traces the transformation of media and its influence on cultural formation and diffusion. Further, the course delves into various facets of the cultural industry, including film, music, publishing, gaming, advertising, public relations, and entertainment. Incorporating lectures, discussions, team activities, and project work, the course offers a multifaceted look into how technology shapes culture and how culture, in turn, shapes technology.\n\n\n\nGoal\nThe course aims to: \n\nUnderstand and analyze the role of different media in shaping and reflecting cultural values and norms.\nExplore the policies, laws, and literacy associated with different media.\nInvestigate various segments of the cultural industry and their impact on society.\nDevelop critical thinking and analytical skills through team activities and project work.\nFoster students’ ability to articulate and present their understanding of complex cultural and technological phenomena.\n\n\n\n\nWeekly Design\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nClass\nPBL\nNote\n\n\n\n\n1\n08/30/2023\nCourse Intro\n\n\n\n\n2\n09/06/2023\nWhat is Media?\n\n\n\n\n3\n09/13/2023\nBroadcast\n\n(녹화수업)\n\n\n4\n09/20/2023\nPress & Internet\nTeam assignment\n\n\n\n5\n09/27/2023\nSocial Media\nTeam builiding\n\n\n\n6\n10/04/2023\nMedia Policy, Law, and Literacy\nTeam builiding\n\n\n\n7\n10/11/2023\nCultural Innovation\nTeam builiding\n\n\n\n8\n10/18/2023\nSpecial lecture (1)\n- Entertainment\nProblem description\n엔터산업 PBL\n\n\n9\n10/25/2023\nSpecial lecture (2)\n- Culture Start-up\nProblem description\n스타트업 PBL\n\n\n10\n11/01/2023\nFilm & TV Show\nPBL activity (2)\n\n\n\n11\n11/08/2023\nMusic & Publishing\nPBL activity (3)\n\n\n\n12\n11/15/2023\nGame & Entertainment\nPBL activity (4)\n\n\n\n13\n11/22/2023\nAdvertising & PR\nPBL activity (5)\n\n\n\n14\n11/29/2023\nPBL mentoring\nMentoring\n\n\n\n15\n12/06/2023\nWrap-up QZ\n\n\n\n\n16\n12/13/2023\n최종 프로젝트 발표\n\n\n\n\n\n\n\n\nEvaluation\n\nAttendance (0 %): No attendance score but F for absence of ⅓ classes.\nParticipation (20 %): Points for active participation in some form, such as questioning and discussion during class.\nWrap-up QZ (30 %)\nPBL Score (50%)\n\nFinal score: (강의자 점수 30 + 현장 평가 점수 70) x (동료 평가 가중치)\n\n\n\n\n\nCommunication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/giqvpYCf\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "research/working.html",
    "href": "research/working.html",
    "title": "Working in Progress",
    "section": "",
    "text": "Dominant and Spark Technologies in Business Methods Innovation\n\nwith Keungoui Kim, Dieter F. Kogler, Junmin Lee\n\nGrandma’s digital play goes to FinTech\n\nwith Yoonwoo Choi\n\nMovie-Country Relatedness Density and Entry TOP20\n\nwith Sung Wook Ji"
  },
  {
    "objectID": "research/working.html#under-review",
    "href": "research/working.html#under-review",
    "title": "Working in Progress",
    "section": "",
    "text": "Dominant and Spark Technologies in Business Methods Innovation\n\nwith Keungoui Kim, Dieter F. Kogler, Junmin Lee\n\nGrandma’s digital play goes to FinTech\n\nwith Yoonwoo Choi\n\nMovie-Country Relatedness Density and Entry TOP20\n\nwith Sung Wook Ji"
  },
  {
    "objectID": "research/working.html#working-papers",
    "href": "research/working.html#working-papers",
    "title": "Working in Progress",
    "section": "Working papers",
    "text": "Working papers"
  },
  {
    "objectID": "research/published/kci_2022_ism.html",
    "href": "research/published/kci_2022_ism.html",
    "title": "산업별 디지털 전환 갈등 지표 개발 연구",
    "section": "",
    "text": "Abstract\n제4차산업혁명시대, 우버ㆍ카풀에서 시작하여 카카오, 타다 사건으로 절정을 맞았던 도시여객 운수업 갈등은 완전히 해소되지는 않았다. 그간 갈등을 다룬 공공행정 관련 기존 연구에서는 그 양상을 증폭되어 해소되어 가는 선형적인 틀 내에서 이해했으며 정성적 사례 분석이 주류를 이뤘다. 설문 기반 정량적 실증 연구들도 대개 개인이나 조직 내 갈등을 다뤘고, 나아가 디지털 전환 관련 조직 간 갈등을 분석한 연구는 없었다. 본 연구는 산업별 디지털 전환 갈등에 주목하고 갈등 상황의 변화를 측정하기 위한 방법으로 빈도, 강도, 강도 집중도 지표의 개발을 제안한다. 그리고 한국의 6대 산업을 대상으로 2003년 12월~2019년 6월까지 보도된 뉴스 본문을 수집하여 이 지표들을 측정하고 내ㆍ외적 타당성을 검증했다. 분석 결과 갈등 상황은 강도 수준의 변화를 볼 때 세부 단계가 반복, 순환하면서 비선형적으로 진화해갔다. 산업별 디지털 전환 갈등은 단순히 해소되는 대상이 아니라 관리해야 할 대상이다. 본 연구는 기존 연구의 연장선에서 산업별 디지털 전환 갈등을 빈도, 강도, 강도의 집중도만으로 측정했음에도 그 양상의 변화를 식별할 수 있는 가능성을 열었다는 점에서 기여가 있다."
  },
  {
    "objectID": "research/published/kci_2021_is_2.html",
    "href": "research/published/kci_2021_is_2.html",
    "title": "효율적인 기술 정책 제안을 위한 한국 인공지능 지식 구조와 진화 궤적의 탐색적 분석",
    "section": "",
    "text": "Abstract\n본 논문은 인공지능(AI) 관련 지식의 구성 요소와 AI 지식의 진화 궤적과 향후 연구개발 투자 방향에 대해 데이터 사이언스의 탐색적 분석 기법을 사용하여 알아보았다. 분석을 위해 한국특허정보원에서 제공하는 KIPRIS DB와 유럽 특허청의 PATSTAT DB를 활용하였으며, WIPO의 AI 분류 기준을 참고하여, 2001년부터 2020년까지 출원된 AI 특허를 식별하고 CPC 코드와 발명자의 주소지 정보를 활용하여 AI 관련 지식의 구조와 국내 AI 지식 진화 궤적을 분석하였다. 기술 통계 분석 결과, AI 지식의 복잡도가 커지고 있다는 것과 국내 AI 발명자 수의 절대 수와 비율이 증가 추세라는 것을 알 수 있었다. AI 지식구조 변화에 대한 발견을 요약하면, 먼저 AI 지식의 재료가 되는 기술들의 수와 균등 분포지수 모두 증가 추세이며, 주재료가 되는 핵심 기술로는 주로 물리학, 전기, 운송과 기계 가동 계열 기술들이 사용되고 있음을 확인하였다. 두 번째로 AI 융합 기술 네트워크의 크기가 거대해지는 데 반해 핵심 기술들의 노드 수는 크게 변하지 않았으며, 네트워크의 중심부로 더 밀집하고 있는 것을 발견하였는데, 이는 AI 융합 기술의 주재료가 되는 핵심 기술의 영향력이 더욱 증가하고 있다는 것을 의미한다. 또한, AI 지식의 현시비교우위 값과 관련성 밀도 지표를 측정하여 한국의 기술경쟁력이 상대적인 우위에 있음과 동시에 관련성 높은 전문화된 기술들이 활발하게 상호작용하고 있음을 알 수 있었고, 마지막으로 향후 한국 AI 지식 네트워크에 배태될 확률이 큰 기술들을 제안하였다. 본 연구는 AI 지식 구조와 진화에 대한 탐색적 분석에 대한 것으로 향후 AI 지식구조에 대한 이해를 넓히고, 국내 AI 지식의 성공적인 개발을 위한 단초를 제공한다는 것에 그 의의가 있다."
  },
  {
    "objectID": "research/published/kci_2021_cyb.html",
    "href": "research/published/kci_2021_cyb.html",
    "title": "재난상황에서의 가짜뉴스 식별: 영상 vs 문자",
    "section": "",
    "text": "Abstract\n가짜뉴스가 기존 문자매체를 넘어 최근 영상매체로 확대되면서 진위여부 판단이 더욱 어려워졌다는 우려의 목소리가 높아지고 있다. 특히, 재난재해 상황에서 뉴스는 정보전달 등 여러 중요한 역할을 갖기 때문에 재난상황에서의 가짜뉴스는 더 큰 피해를 야기할 수 있다. 이러한 상황에서 가짜뉴스를 식별하는 문제와 관련해 체계적인 분석 연구에 대한 필요성이 대두되고 있다. 하지만 가짜뉴스의 매체 형식이 다양해짐에도 불구하고 기존 연구들은 문자 형식의 가짜뉴스 식별에만 국한되어 있다. 따라서 본 연구에서는 뉴스의 내용은 동일하지만 매체 형식이 다를 때 해당 뉴스에 대한 수용자의 진위 여부 판단 능력에 어떤 차이가 있을지 다항로짓모형을 통해 살펴보고자 한다. 가짜뉴스 식별에 영향을 주는 요인으로 매체 형식, 개인의 인지 · 경험적 요인, 인구통계학적 요인을 고려하였고, 이를 바탕으로 수용자의 판단 능력 저하에 영향을 미치는 요인을 파악하였다. 분석 결과, 수용자는 영상 형식으로 제공된 가짜뉴스에 대해 자신이 판단한 진위여부에 더 확신을 가지는 것으로 나타났다. 또한, 뉴스에 대한 평소 신뢰도, 정신적 피해 경험, 뉴미디어 선호도도 가짜뉴스 진위여부 식별에 영향을 미치는 것으로 나타났다."
  },
  {
    "objectID": "research/published/kci_2016_ism.html",
    "href": "research/published/kci_2016_ism.html",
    "title": "효과적인 정보 확산을 위한 미디어 이용 패턴 분석",
    "section": "",
    "text": "Abstract\n본 연구는 미디어의 본질인 ’정보 전달’의 측면에서 효과적인 정보 확산 방법에 대한 연구이다. 특별히 미디어가 직접 이용자에게 정보를 전달하는 것보다 의견지도자라는 매개체를 통해 정보를 전달하는 것이 더 효과적이라는 2단계 유통이론을 기반으로, 미디어의 환경 변화를 반영한 온라인 의견지도력 측정 변수를 정립하여, 온라인 의견지도자를 식별하고 그들의 미디어 이용 패턴을 분석하였다. 요인분석을 이용하여 기존 연구를 바탕으로 새롭게 정의한 온라인 의견지도력 측정 변수를 정립하고, CART 분류 분석을 이용하여 온라인 의견지도력 중 유사한 미디어 이용 패턴을 보이는 자들을 집단화(clustering)하였으며, 미디어 레퍼토리를 유형화하여 미디어 이용 패턴을 분석하였다. 또한, 온라인 의견지도자의 내적, 외적 특성을 통해 기존 의견지도자와의 구분을 시도함으로써 온라인 의견지도자 그룹의 미디어레퍼토리 유형 형태의 차이점이 존재함을 밝혀냈다. 더 나아가, 온라인 의견지도자와 비의견지도자 그룹 간의 미디어 이용 형태를 미디어-시간, 콘텐츠-시간에 따라 분석하였으며, 마지막으로 미디어-콘텐츠-시간 3범주기준을 제시하고 온라인 의견지도자의 미디어 이용 형태를 분석하였다. 본 연구의 결과를 통해 정보 확산 매개체인 의견지도자들의 미디어 이용 패턴 분석을 통해 효과적인 정보 확산을 위한 함의를 제공하고 정보 공급자들에게 효과적인 정보 확산을 위한 미디어 레퍼토리 기반의 미디어 전략을 제안한다."
  },
  {
    "objectID": "research/published/2023_Na_Lee_Kim.html",
    "href": "research/published/2023_Na_Lee_Kim.html",
    "title": "The Optimal Open Innovation Strategy with Science-based Partners for Venture Firm’s Innovation Capabilities",
    "section": "",
    "text": "Abstract\nThis study explores the difference in the effects of innovation modes on innovation capabilities dividing various collaborative activities conducted by venture firms with science-based partners into innovation modes. Venture firms that relatively lack the resources and capabilities for innovation perform various collaborative activities such as marketing support and management support as well as research and development collaboration with science-based partners. However, this has not received proper attention in the literature on open innovation. We classified venture firms according to innovation modes and compared each group through a multi-treatment model. As a result, we found that there was a difference in the impact on innovation capability according to the innovation mode with science-based partners. This study contributes to enriching the empirical bodies of open innovation research. In addition, it suggests that there is a need for a portfolio approach considering the innovation modes by establishing policies at the level of collaborative activities in supporting the collaboration of venture firms."
  },
  {
    "objectID": "research/published/2023_Lee_Na_Kim.html",
    "href": "research/published/2023_Lee_Na_Kim.html",
    "title": "The effect of watching OTT late at night on the sleep pattern of users",
    "section": "",
    "text": "Abstract\nThis study aimed to answer the question of how media users will reallocate their sleep time when their main content channel changes from real-time broadcasting to selective viewing through the over-the-top (OTT) media streaming service. To draw a causal inference between OTT consumption and sleep patterns, the difference-in-difference (DID) estimation method was applied. With the DID approach, a clear distinction between treatment and control groups is essential because the main treatment effects can be screened by the compounding effects. While the conventional way of dividing two groups relies on the selection of limited variables, this study adopted random forest nearest-neighbor propensity score matching based on a machine learning algorithm to divide the two groups. This allows for meticulous matching of the two groups except for treatment. Results show that watching OTT late at night has a significant effect on reducing the total sleep duration on average by about 18–20 min (maximum about 30 min at 95% confidence level) and delaying bedtime by about 18 min (maximum about 26 min at 95% confidence level). This study showed that the selective viewing of content through OTT has the advantage of widening the range of content choices for media users and helping in arranging their time more autonomously, but watching content through OTT late at night leads to media users’ departure from the existing sleep routine."
  },
  {
    "objectID": "research/published/2022_Shon Lee Lee_IJTM.html",
    "href": "research/published/2022_Shon Lee Lee_IJTM.html",
    "title": "Inward or Outward? Direction of Knowledge Flow and Firm Efficiency",
    "section": "",
    "text": "Highlights\n\nWe categorize content providers (CPs) into three groups.\nWe compare the efficiency of CPs within and between the groups.\nThe average efficiency of CPs within each group is the highest in a closed platform.\nThe efficiency of CPs between groups is higher in an open platform than closed one.\n\n\nAbstract\nThis paper analyzes the platform environments in which content providers (CPs) may succeed by using a meta-frontier analysis that compares the efficiency of different groups in identical industries. The results illustrate that a group focusing on an iOS platform achieves a high average efficiency with low variance within the group because the iOS ecosystem manages the content novelty and uncertainty risk in the selection process. This quality control enables a CP to maximize value once the CP enters the ecosystem. From the meta-frontier viewpoint, however, Android-group firms have a higher efficiency level than iOS-group firms. Android transfers risk management to CPs who can conduct additional trial and error, causing CPs to endure the tough selection process. This explains the low initial technical efficiency, but in the long term, this group has the potential to achieve high efficiency. In addition, the group providing content to both platforms was the most efficient group because of the economies of scale."
  },
  {
    "objectID": "research/published/2022_Rocchetta et al_JEG.html",
    "href": "research/published/2022_Rocchetta et al_JEG.html",
    "title": "Technological Knowledge Space and the Resilience of European Regions",
    "section": "",
    "text": "Abstract\nRegional knowledge spaces are heterogeneous, and the structure of these knowledge spaces can play a significant role in shaping regional economic performances during economic downturns. This article explores the relationship between a region’s technological profile and its resilience to exogenous shocks. To identify the determinants of regional economic resilience, we perform panel analyses of EU 15 NUTS II level data covering the years before and after the 2008 financial crisis. The most significant results are that, beyond pure diversification effects, regions endowed with technologically coherent capabilities adapted better in times of economic downturn, and that resilience is influenced by a region’s capacity to generate new growth paths. These findings deepen our understanding of the evolution of regional economies and have relevant implications for the design of appropriate regional development policy instruments."
  },
  {
    "objectID": "research/published/2022_Jung Hwang Lee TFSC.html",
    "href": "research/published/2022_Jung Hwang Lee TFSC.html",
    "title": "Effective strategies to attract crowdfunding investment based on the novelty of business ideas",
    "section": "",
    "text": "Highlights\n\nStudy explores entrepreneurs’ action strategies during reward-based crowdfunding using natural language processing models.\nResults show that strategy effectiveness depends on the project’s novelty level.\nInformation updates show non-linear quadratic relationships with fundraising performance.\nTwo-sided communication helps stimulate investors.\n\n\nAbstract\nWhether the novelty of an idea is a factor that directly influences crowdfunding success remains an area of ambiguity. We hypothesize that target funder diversification is effective with incremental ideas. However, focused business proposals are better suited to assert radical ideas. We also hypothesize the impact of two different strategic actions that founders can take during fundraising campaigns, agile information update and communication, on crowdfunding success. A deep-learning-based novelty detection model combined with statistical analysis is used to empirically test 7406 crowdfunding projects crawled from online platform. Our results support our hypotheses and reveal that information updates from startup founders show non-linear quadratic relationships with fundraising performance, whereas two-sided communication helps stimulate investors. We also revealed that novelty level can influence strategic choice, indicating that a project with a higher novelty should have a focused target. Our finding suggests a solution to the conflicting conclusions in previous studies on the direct impact of novelty level and target diversification, by explaining the process of novelty-dependent behavioral strategies based on signaling theory."
  },
  {
    "objectID": "research/published/2022 Lee Shin Kim Kogler_TFSC.html",
    "href": "research/published/2022 Lee Shin Kim Kogler_TFSC.html",
    "title": "The effects of regional capacity in knowledge recombination on production efficiency",
    "section": "",
    "text": "Abstract\nKnowledge recombination, combining knowledge existing (exploitation) or new knowledge capacity (exploration) for creating knowledge collaboration, is one of the important ways of achieving innovation. However, little has known about how the knowledge recombination types affect differently to production efficiency at a regional level. This study explores the relationship between the knowledge recombination types of exploitation and exploration and regional technical efficiency by using the empirical data sets combining EPO PATSTAT, Eurostat, and Cambridge Econometrics regional database. For this purpose, three stages of analysis have been deployed. Firstly, CPC co-occurrence network analysis and relative comparative advantage (RCA) measures are used to construct knowledge space and measure regional capacity. Then, using the stochastic frontier analysis, the production efficiencies of the European NUTS 2 regions are measured. With all estimated measures, the effects of both exploration and exploitation of knowledge recombination on regional production efficiencies are estimated. The results show the positive effect of exploration on regional production efficiency, which highlights the importance of extending the range and variety of knowledge bases."
  },
  {
    "objectID": "research/published/2022 Jeon and Lee_TI.html",
    "href": "research/published/2022 Jeon and Lee_TI.html",
    "title": "Internet of Things Technology: Balancing Privacy Concerns with Convenience",
    "section": "",
    "text": "Highlights\n\nIoT service users have a significantly high perception of media privacy concerns.\nWearable IoT leads to an increase in anxiety about the risk of personal data leakage.\nNon-physical IoT such as cloud service does not have a significant effect.\n\n\nAbstract\nThe diversification of the use of Internet of Things (IoT) technology attracts a variety of consumers. Based on this social change, we examine the effect of IoT usage on users’ media privacy concerns, focusing on the use of wearable devices while considering cloud service usage. The data were derived from the Korea Media Panel Survey collected from 2018 to 2020 in South Korea. We extracted a final sample of 690 participants using the propensity score matching (PSM) method and applied logistic regression to analyze the data. We reveal the different perceptions of media privacy concerns between IoT service users and non-users. We found that greater use of wearable devices results in greater concern about media privacy. On the contrary, the concern is lower when cloud services are used. We propose that the use of physical IoT technology leads to an increase in anxiety about the risk of personal data leakage, whereas the use of non-physical IoT does not have a significant effect. Currently, the usage of IoT devices and services is on the rise, so this anxiety can also be expected to increase. We discuss the risks of privacy and data breaches and the need for countermeasures to balance the use of IoT technology."
  },
  {
    "objectID": "research/published/2022 Dieter et al JOTT.html",
    "href": "research/published/2022 Dieter et al JOTT.html",
    "title": "Regional knowledge spaces: the interplay of entry-relatedness and entry-potential for technological change and growth",
    "section": "",
    "text": "Abstract\nThis paper aims to uncover the mechanism of how the network properties of regional knowledge spaces contribute to technological change from the perspective of regional knowledge entry-relatedness and regional knowledge entry-potential. Entry-relatedness, which has been previously employed to investigate the technology evolution of regional economies, is advanced by introducing a knowledge gravity model. The entry-potential of a newly acquired regional specialisation has been largely ignored in the relevant literature; surprisingly given the high relevance that is attributed to the recombination potential of new capabilities. In other words, just adding new knowledge domains to a system is not sufficient alone, it really depends on how these fit into the existing system and thus can generate wider economic benefits. Based on an empirical analysis of EU-15 Metro and non-Metro regions from 1981 to 2015, we find that entry-relatedness has a significant negative association with novel inventive activities, while entry-potential has a significant positive association with the development of novel products and processes of economic value. This highlights that regions’ capacity to venture into high-potential areas of technological specialization in the knowledge space outperforms purely relatedness driven diversification that is frequently promoted in the relevant literature."
  },
  {
    "objectID": "research/published/2021_Lee Cho Lee_AJTI.html",
    "href": "research/published/2021_Lee Cho Lee_AJTI.html",
    "title": "The mechanism of innovation spill-over across sub-layers in the ICT industry",
    "section": "",
    "text": "Abstract\nWhile there has been a study regarding the mechanism of innovation spillover within an information and communication technology (ICT) ecosystem, to the best of our knowledge, there has been no research on the mechanism of innovation spillover between sublayers. To fill this gap, this study aimed to disentangle the empirical question of how innovation spills over between sublayers. We classified the ICT industry into three layers, Content, Goods, and Service, and collected financial data from KISVALUE over the last 18 years (2000–2017). Stochastic frontier and meta-frontier analysis were used to estimate the annual production function of each layer and the meta-production functions that encompass each sublayer’s frontiers. Based on the efficiency calculations, we introduced two types of instrumental variables: (1) each industry’s single radical innovation and (2) the effect of an industry’s incremental innovation, then regressed the firms’ efficiency on other industries’ radical and incremental innovation. The results showed that innovation from one sublayer affects the other sublayers’ productivity gains, thus triggering other types of innovation and causing reciprocal productivity gains in the other sublayers of the ICT ecosystem. This study sheds light on the mechanism of innovation spillover across sublayers in the ICT ecosystem, and the implications of the findings are discussed."
  },
  {
    "objectID": "research/published/2021_Jung et al_PLOS_ONE.html",
    "href": "research/published/2021_Jung et al_PLOS_ONE.html",
    "title": "The nature of ICT in technology convergence",
    "section": "",
    "text": "Abstract\nThis study aims to understand the nature of information and communication technology in technology convergence. We form a knowledge network by applying social network theories to Korean patent data collected from the European Patent Organization. A knowledge network consists of nodes representing technology sectors identified by their International Patent Classification codes and edges that link International Patent Classification codes when they appear concurrently in a patent. We test the proposed hypotheses using four indices (degree centrality, E-I index, entropy index, and clustering coefficient). The results show that information and communication technology is easily attached but tends to converge with similar technology and has the greatest influence on technology convergence over other technologies. This study is expected to help practitioners and policymakers understand the structure and interaction mechanisms of technology from a systematic perspective and improve national-level technology policies."
  },
  {
    "objectID": "research/published/2020_Lee Lee Shon_JETM.html",
    "href": "research/published/2020_Lee Lee Shon_JETM.html",
    "title": "Effect of efficient triple-helix collaboration on organizations",
    "section": "",
    "text": "Abstract\nTriple Helix (TH) theory, which emphasizes synergy between universities, industries, and government, has not been tested empirically from the perspective of differences in R&D efficiency. We examine how firms’ TH strategies affect R&D efficiency and how synergy varies according to stage of growth. First, we classify the firms into four groups by TH strategy. We estimate each firm’s R&D efficiency and compare each group’s technical gap ratio relative to the meta-frontier. The results show that synergy does exist in long-term R&D efficiency and potential. Discussion of the heterogeneous effects of TH strategies according to the firms’ stage of growth follows."
  },
  {
    "objectID": "research/published/2019_Kim Lee Do_PRPR.html",
    "href": "research/published/2019_Kim Lee Do_PRPR.html",
    "title": "The Effect of Adult Children’s Working Hours on Visits to Elderly Parents",
    "section": "",
    "text": "Abstract\nDespite its significant policy implications, little is known about the impact working hours have on how often workers visit their elderly parents. Evidence is particularly lacking on men’s overtime work and workers in Asia. We examine the causal impact of male workers’ working times on parental visits, using a natural experiment to eliminate potential endogeneity bias. In 2004, the Korean government began reducing its legal workweek from 44 to 40 h, gradually expanding it from larger to smaller establishments by 2011. Using annual longitudinal data from the 2005 to 2014 Korea Labor and Income Panel Study (N=7005 person-waves), we estimated an instrumental variable (IV) fixed-effects (FE) regression model. Our IV was an indicator variable of whether an individual full-time worker’s legal workweek was reduced to 40 h in a given year. The results showed that working one additional hour a week lowered the frequency of visits by 6.5% (95% confidence interval [−13.0%, 0.0%]), which was not apparent in a FE model without the IV. Working long hours has implications for workers’ interactions with their elderly parents, and the failure to consider endogeneity in actual working hours may understate the negative effect. Reducing work hours may serve as an effective policy intervention for improving the well-being of older adults in rapidly aging Asian countries in a work-oriented and family-centered culture. We also highlight the need for further attention to men’s work hours, which are often considered much less important than women’s work status in population research on intergenerational support."
  },
  {
    "objectID": "research/published/2018_Na Lee Hwang Lee_AE.html",
    "href": "research/published/2018_Na Lee Hwang Lee_AE.html",
    "title": "Research on the Mutual Relations between ISP and ASP Efficiency Changes",
    "section": "",
    "text": "Abstract\nInnovations in Internet networks and applications are equally important. However, there is controversy in the literature regarding whether it is network or application innovation that leads to the development and innovation of Internet industries. In this study, the mutual relations of Internet service providers’ (ISP) and application service providers’ efficiency changes are analysed empirically using a Granger causality test. Over the entire period of 1998–2011, efficiency changes of web service companies positively Granger-cause those of ISPs and vice versa. In the case of VoIP and streaming services, however, efficiency changes of VoIP and streaming services Granger-cause those of ISPs during the former half period, whereas those of ISPs negatively Granger-cause efficiency changes of streaming services during the latter half. As services that require heavy data traffic and QoS guarantees are launched, a policy that promotes virtuous circulation between ISPs and application service providers (ASPs) is necessary for the development of all Internet-related industries."
  },
  {
    "objectID": "research/published/2018_Lee Kim_IM.html",
    "href": "research/published/2018_Lee Kim_IM.html",
    "title": "The Evolutionary Trajectory of ICT Ecosystem",
    "section": "",
    "text": "Highlights\n\nWe integrate users into visualization and analysis of an ICT ecosystem with user data.\nWe propose a simple method of showing evolutionary trends in the ICT ecosystem.\nWe broaden the ecosystem layer model by using users’ media repertoire concept.\nThe evolutionary trends of firms and layers help practitioners make a strategic decision.\n\n\nAbstract\nWe integrate users into the visualization and analysis of an information and communication technology (ICT) ecosystem by using demand-side data. We also broaden the ecosystem layer model by using a media repertoire concept and propose a clear method of showing evolutionary trends. Consequently, we discover the nature of an evolutionary path in an ICT ecosystem. This path becomes more rigid and centralized as it matures, a finding that agrees with prior studies’ results that used supply-side data. Further, we analyze the trends of firms changing layer positions and suggest multiple approaches of visualizing and analyzing interfirm relationships for practitioners."
  },
  {
    "objectID": "research/published/2018_Kim Lee Do_JAH.html",
    "href": "research/published/2018_Kim Lee Do_JAH.html",
    "title": "The Effect of a Reduced Statutory Workweek on Familial Long-Term Care in Korea",
    "section": "",
    "text": "Abstract\n\nObjectives: We examine how statutory workweeks affect workers’ provision of long-term care for their non-coresident elderly parents.\nMethod: The Korean government reduced its statutory workweek from 44 to 40 hr, gradually from larger to smaller establishments, between 2004 and 2011. Using multiple regressions, we assess how the reduction affected visits, financial transfers, and in-kind transfers to parents. Annual longitudinal data come from the 2005 to 2013 waves of the Korea Labor and Income Panel Study.\nResults: The reduction caused an increase in the frequency of visits and in-kind transfers among male workers, with no significant impact on their financial transfers. Among female workers, we found no impact on any outcomes.\nDiscussion: We interpret the findings within the context of developed Asian countries with long work hours and Confucian traditions, and suggest regulating workweeks as a policy tool to encourage familial long-term care in the rapidly aging societies."
  },
  {
    "objectID": "research/published/2017_Lee Kim Lee_TI.html",
    "href": "research/published/2017_Lee Kim Lee_TI.html",
    "title": "Intra-industry innovation, spillovers, and industry evolution",
    "section": "",
    "text": "Highlights\n\nThis paper estimates technical efficiency of the Korean ICT industry.\nIt compares the efficiency of firms using meta-frontier analysis method.\nIt investigates the effects of innovation on other firms.\nIt also investigates how the industry evolves accordingly.\n\n\nAbstract\nCompanies innovate themselves to survive competition, and successful innovations are transferred to the industry as a whole through imitation. In this paper, the technical efficiency (TE) of the Korean information and communication technology (ICT) industry is estimated through stochastic frontier analysis, and the efficiencies from 2000 to 2013 are compared through meta-frontier analysis. In addition, by analyzing the effect of innovations that move the production function up on the TE and the meta-frontier efficiency through the two-stage least square model, this paper investigates the effects of innovation on other companies in the industry in the short- and long-term and how the industry evolves accordingly. As a result, sudden inflation of the meta-technology ratio caused by a few innovative firms automatically lowers the average TE. However, the innovation spills over intra-industry by the efforts of other firms trying to reach around the innovation (i.e., imitation). This recovers other firms’ TE and increases meta-frontier efficiency. In addition, this paper provides theoretical and practical implications of the results."
  },
  {
    "objectID": "research/published/2017_Lee Jung Kim_TI.html",
    "href": "research/published/2017_Lee Jung Kim_TI.html",
    "title": "Effect of a Policy Intervention on Handset Subsidies on the Intention to Change Handsets",
    "section": "",
    "text": "Highlights\n\nPolicy intervention on handset subsidies lowered users’ willingness to switch handsets.\nThe intervention increased spending on households’ expenses in handset installment.\nThe intervention has no significant effect on households’ expenses in online content.\n\n\nAbstract\nThis study examined effect of a policy intervention that provides an upper limit for handset subsidies on users’ intention to change handset and households’ expenses on mobile telecommunications. The Korean government has prohibited mobile network providers from providing excessive subsidies for mobile handsets to attract subscribers since Nov. 2014 according to the mobile act. Using the exogenous variation, we estimate the impact of the policy on the intention to change handsets and expenses on handset installment, total mobile communications, and online content. The longitudinal data are from the 2014 to 2015 waves of the Korea Media Panel Survey. The mobile act lowered the predicted probability of switching handsets by 0.4% points. Moreover, the mobile act increased the predicted probability of any expense on handset installment by 7.5% points and had a significant impact on the amount of expenses on handset installment, with an increase of 7.8%. The mobile act lowered users’ willingness to switch handsets and increased spending on handset installment. This increased burden in handset installment might shrink the online content market, which has a large need for government support, as well as decrease consumers’ welfare. We assert that the policy intervention on handset subsidies is questionable with regard to both consumer welfare and the healthiness of the ICT ecosystem."
  },
  {
    "objectID": "research/index_old.html",
    "href": "research/index_old.html",
    "title": "Publications",
    "section": "",
    "text": "International SSCI or SCIE\n\nLiu, H. S., Lee, C., Kim, K., Lee, J., Moon, A., Lee, D., & Park, M. (2023). An Analysis of Factors Influencing the Intention to Use” Untact” Services by Service Type. Sustainability, 15(4), 2870. https://www.mdpi.com/2071-1050/15/4/2870\nShim, D., Lee, C., & Oh, I. (2022). Analysis of OTT Users’ Watching Behavior for Identifying a Profitable Niche: Latent Class Regression Approach. Journal of Theoretical and Applied Electronic Commerce Research, 17(4), 1564-1580. https://www.mdpi.com/0718-1876/17/4/79\nKim, K., Kogler, D. F., Lee, C., & Kang, T.* (2022). Changes in regional knowledge bases and its effect on local labour markets in the midst of transition: Evidence from France over 1985–2015. Applied Spatial Analysis and Policy, 1-22. https://doi.org/10.1007/s12061-022-09444-4\nShon, M., Lee, D. & Lee, C.(2022). Inward or Outward? Direction of Knowledge Flow and Firm Efficiency. International Journal of Technology Management. 90(1-2), 102-121. https://doi.org/10.1504/IJTM.2022.124617\nPark, I., Lee, J., Lee, D., Lee, C., & Chung, W. Y.* (2022). Changes in consumption patterns during the COVID-19 pandemic: Analyzing the revenge spending motivations of different emotional groups. Journal of Retailing and Consumer Services, 65, 102874. https://doi.org/10.1016/j.jretconser.2021.102874\nLee, C., Shin, H., Kim, K., & Kogler, D. F.* (2022). The effects of regional capacity in knowledge recombination on production efficiency. Technological Forecasting and Social Change. 180, 121669. https://doi.org/10.1016/j.techfore.2022.121669\nJung, E., Lee, C.*, & Hwang, J. (2022). Effective strategies to attract crowdfunding investment based on the novelty of business ideas. Technological Forecasting and Social Change. 178, 121558. https://doi.org/10.1016/j.techfore.2022.121558\nJeon, H., & Lee, C.* (2022). Internet of Things Technology: Balancing Privacy Concerns with Convenience. Telematics and Informatics. 70, 101816. https://doi.org/10.1016/j.tele.2022.101816\nKogler, D. F., Davies, R. B., Lee, C., & Kim, K.* (2022). Regional knowledge spaces: the interplay of entry-relatedness and entry-potential for technological change and growth. The Journal of Technology Transfer. 1-24. https://doi.org/10.1007/s10961-022-09924-2\nTóth, G., Elekes, Z., Whittle, A., Lee, C.*, & Kogler, D. F. (2022). Technology network structure conditions the economic resilience of regions. Economic Geography. 98(4), 1-24. https://doi.org/10.1080/00130095.2022.2035715\nJo, H., Park, S., Shin, D., Shin, J.*, & Lee, C. (2022). Estimating cost of fighting against fake news during catastrophic situations. Telematics and Informatics. 66, 101734. https://doi.org/10.1016/j.tele.2021.101734\nPark, I., Shim, H., Kim, J., Lee, C., & Lee, D.* (2022). The Effects of Popularity Metrics in News Comments on the Formation of Public Opinion: Evidence from an Internet Portal Site. The Social Science Journal. doi:https://doi.org/10.1080/03623319.2020.1768485\nKim, K., Lee, J., & Lee, C.(2022). Which innovation type is better for production efficiency? A comparison between product/service, process, organizational, and marketing innovations using stochastic frontier and meta-frontier analysis. Technology Analysis & Strategic Management. doi: https://doi.org/10.1080/09537325.2021.1965979\nRocchetta, S., Mina, A., Lee, C., & Kogler, F. D. (2022). Technological Knowledge Space and the Resilience of European Regions. Journal of Economic Geography. 22(1), 27-51.* doi: https://doi.org/10.1093/jeg/lbab001\nLee, C., Cho, H., & Lee, D.* (2021). The mechanism of innovation spill-over across sub-layers in the ICT industry. Asian Journal of Technology Innovation. 29(2), 159-179. doi:https://doi.org/10.1080/19761597.2020.1796725\nJung, S., Kim, K. & Lee, C.(2021). The nature of ICT in technology convergence: A knowledge-based network analysis. PLOS ONE. 16(7): e0254424. https://doi.org/10.1371/journal.pone.0254424\nNa, C., Lee, D., Hwang, J., & Lee, C.* (2021). Strategic Groups Emerged by Selecting R&D Collaboration Partners and Firms’ Efficiency. Asian Journal of Technology Innovation. 29(1), 109-133. doi:https://doi.org/10.1080/19761597.2020.1788957\nLee, C., Lee, D., & Shon, M.* (2020). Effect of efficient triple-helix collaboration on organizations based on their stage of growth. Journal of Engineering and Technology Management. 58, 101604. https://doi.org/10.1016/j.jengtecman.2020.101604\nLee, C., Kogler, D.F., & Lee, D.* (2019). Capturing Information on Technology Convergence, International Collaboration, and Knowledge Flow from Patent Document: A Case of Information and Communication Technology. Information Processing & Management. 56(4), 1576-1591. doi:1016/j.ipm.2018.09.007\nKim, E.H.W.* & Lee, C. (2019). Does Working Long Hours Cause Marital Dissolution? Evidence from the Reduction in South Korea’s Workweek Standard. Asian Population Studies. 15(1), 87-104. doi:1080/17441730.2019.1565131\nKim E.H.W.*, Lee, C., & Do, Y.K. (2019). The Effect of Adult Children’s Working Times on Visiting with Elderly Parents: A Natural Experiment in Korea. Population Research and Policy Review. 38(1), 53-72. doi:1007/s11113-018-9486-0\n\nFeatured in: Straits Times, Lianhe Zaobao (In Chinese)\n\nLee, C.* & Hwang, J. (2018). The Influence of Giant Platform on Content Diversity. Technological Forecasting and Social Change. 136, 157-165. doi:1016/j.techfore.2016.11.029\nKim, E.H.W.*, Lee, C., & Do Y.K. (2018). The Effect of a Reduced Statutory Workweek on Familial Long-Term Care in Korea. Journal of Aging and Health. 30(10), 1620-1641. doi:1177/0898264318797469\n\nPoster Session Winner, 2018 Population Association of America Conference\n\n\n\n\nLee, C. & Kim, H.* (2018). The Evolutionary Trajectory of ICT Ecosystem: A Network Analysis based on Media User Data. Information & Management. 55(6), 795-805. doi:1016/j.im.2018.03.008\n\nBest Paper Award, 2015 Korea Media Panel Conference\n\nLee, C., Shin, J., & Hong, A.* (2018). Does Social Media Use Really Make People Politically Polarized? Direct and Indirect Effects of Social Media Use on Political Polarization. Telematics and Informatics. 35(1), 245-254. doi:1016/j.tele.2017.11.005\nNa, H.S., Lee, D., Hwang, J., & Lee, C.(2018). Research on the Mutual Relations between ISP and ASP Efficiency Changes for the Sustainable Growth of Internet Industry. Applied Economics. 50(11), 1238-1253. doi:1080/00036846.2017.1358443\nLee, C., Kim, J.H., & Lee, D.* (2017). Intra-industry Innovation, Spillovers, and Industry Evolution: Evidence from the Korean ICT industry. Telematics and Informatics. 34(8), 1503-1513. doi:1016/j.tele.2017.06.013\nLee, C., Jung, S., & Kim, K.O.* (2017). Effect of a Policy Intervention on Handset Subsidies on the Intention to Change Handsets and Households’ Expenses in Mobile Telecommunications. Telematics and Informatics. 34(8), 1524-1531. doi:1016/j.tele.2017.06.017\nLee, C., Kim, H., & Hong, A.* (2017). Ex-post Evaluation of Illegalizing Juvenile Online Game after Midnight: A Case of Shutdown Policy in South Korea. Telematics and Informatics. 34(8), 1597-1606. doi:1016/j.tele.2017.07.006\n\nCited in Nature Editorial http://www.nature.com/news/put-cult-online-games-to-the-test-1.22343\n\nLee, C., Lee, K., & Lee, D.* (2017). Mobile Healthcare Applications and Gamification for Sustained Health Maintenance. 9(5), 772. doi:10.3390/su9050772\nLee, C., Lee, D.*, & Hwang, J. (2015). Platform Openness and the Productivity of Content Providers: A meta-frontier analysis. Telecommunications Policy. 39(7). 553-562. doi:1016/j.telpol.2014.06.010\n\n\n\n\nKorean Citation Index (KCI) Journals\n\nChoi, M. & Lee, C.* (2022). The effect of Online Community Activities in Non-face-to-face Situations on Life Satisfaction : Focusing on the Comparison between Before(2017) and After(2021) COVID-19. Information Society & Media. 23(3), 83-124. https://doi.org/52558/ISM.2022.12.23.3.83\nYeom, J., Lee, S., & Lee, C.* (2022). Analysis of the Characteristics of Extreme Patriotism and Psychological Motives by Understanding the Cyber Conflict Between Chinese Fandom Patriotism and Hallyu Fandom. Journal of Cybercommunication Academic Society. 39(4). 5-49. https://doi.org/10.36494/JCAS.2022.12.39.4.5\nKang, S., Seo, Y., & Lee, C.* (2022). A Study of the Development of Sectoral Digital Transformation Conflict Indicator. Information Society & Media. 23(1), 41-68. https://doi.org/52558/ISM.2022.04.23.1.41\n\nBest Paper Award, 2022 Korea Media Management Association\n\nKim, K., Lee, J., & Lee, C.* (2021). Exploratory Analysis of Knowledge Structure and Evolutionary Trajectory in Korean Artificial Intelligence for Effective Technology Policy. Korean Innovation Study, 16(3). DOI:https://doi.org/10.46251/INNOS.2021.8.16.3.139\nJo, H., Oh, M., Shin, J.*, & Lee, C. (2021). Identifying Fake news in the Disastrous situations: Video versus Text. Journal of Cybercommunication Academic Society, 38(2).  DOI: https://doi.org/10.36494/JCAS.2021.06.38.2.83\nRoh, T., Jo, G., & Lee, C.* (2021). Coincidence Analysis with International Patent Classification (IPC) Network of US ICT Companies: Focusing on Technology Similarity and Application. Korean Innovation Study, 16(2).  doi:https://doi.org/10.46251/INNOS.2021.5.16.2.237\nLee, C., Woo, H.J., & Park, S.B. (2020). Factors of newly entering into the purchase on TV home-shopping: Random Forest Analysis based on users’ media repertoire data. Korean Innovation Study, 15(2), 113-149. doi:https://doi.org/10.46251/INNOS.2020.05.15.2.113\nKim, M.K., Lee, C., & Hong, A. (2016). The Analysis of Media Usage Pattern for Effective Diffusion of Information. Information Society & Media. 17(1). 77-113. doi:https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE06667546\n\n\n\n\nBook chapters\n\nLee, C. & Bae, Y. (2019). Development of Information Communication Technology Industry and Public Policy in South Korea, In Ahn, M.J., & Kim, Y. (Eds.), Public Administration and Public Policy in Korea. Springer. Doi: https://doi.org/10.1007/978-3-319-31816-5_3801-1"
  },
  {
    "objectID": "proj/ongoing/soscial_idx.html",
    "href": "proj/ongoing/soscial_idx.html",
    "title": "디지털전환에 따른 사회부문 국가발전지표 진단연구",
    "section": "",
    "text": "사이버커뮤니케이션학회\nhttp://www.cybercom.or.kr/\n\n\n\n\n\n\n빠르게 진화화는 디지털기술이 사회부문에 갖는 영향들은 계속해서 변화하고 있고, 그 변화들도 다양하게 확장되고 있기 때문에, 관련 통계지표들을 기술과 사회변화에 맞게 수정, 보완하고, 새롭게 등장하는 현상들을 이해분석하기 위한 새로운 통계지표들도 도출해야 할 필요성이 있음\n이를 위해서는 디지털전환으로 인한 사회변화를 분석할 수 있는 통계를 작성하는 프레임워크 구축이 필요함\n현재 한국에서 생산되는 디지털전환과 사회변화 관련 통계는 KOSIS 국가통계포털, ICT통계포털 등에서 제공하고 있는 ‘정보화통계조사’, ‘디지털정보격차실태조사’, ‘웹접근성실태조사’, ‘언론수용자 의식조사’, ‘국민여가활동조사’, ‘한국미디어패널조사’ 등이 있음.\n각 통계들은 그동안 한국이 정보사회로 접어들게 되면서 경험한 다양한 현상들을 포착하는 데 유용하게 활용되었으며, 대표적인 통계지표로는 인터넷 이용률, 스마트폰 보급률, 디지털 격차 지수, 전자상거래 거래액, 모바일 결제 이용률, 소셜미디어 이용률, 클라우드 서비스 이용률, 사이버 보안 사고 발생 건수 등이 있음.\n\n\n하지만 이 통계에 있어서 다음과 같은 한계들이 있음.\n\n\n먼저 디지털기술환경은 지속적으로 발전하며 새로운 기술과 플랫폼이 계속해서 등장하며, 이러한 변화는 디지털전환과 사회변화 관련된 통계를 최신 상태로 유지하고 관련성을 유지하는 것이 어렵게 만들고 있음.\n통계는 지속성이 중요한데, 디지털기전환과 사회에 대한 통계들은 시간이 조금만 지나면 의미가 줄어드는 통계들이 많아짐\n또한 새로운 디지털기술은 새로운 사회의 변화를 가져오는 데, 이때 새로운 통계지표들이 필요하기 때문에 기존의 통계들을 과감하게 그리고 지속적으로 수정보완해야 하는 특성이 있음.\n한편 디지털기술은 워낙 다양하며, 그것이 미치는 사회적 영향 역시 가시적이지 않은 것들도 있기 때문에 디지털기술이 미치는 사회적 영향 관련된 통계는 상관 관계를 보여줄 수는 있지만, 인과관계를 확립하는 것은 어려울 수 있음\n관찰된 결과에 기여하는 다른 기저 요인이 있을 수 있어 디지털기술의 정확한 영향을 결정하기 어려울 수 있음\n이에 인과관계까지 찾아낼 수 있는 지표들을 도출하여 그 프레임워크를 활용하는 것이 대단히 중요함.\n또한 디지털전환이 사회에 미치는 영향을 세부적으로 구분하여 자세하게 살펴보는 것이 필요함.\n디지털전환이 노동과 소득 등 경제활동에 미치는 영향, 신뢰, 사회자본 등 사회관계에 미치는 영향, 행정서비스활용 등 공공서비스활동에 미치는 영향, 행복도, 우울증, 건강, 복지 등 삶의 질에 미치는 영향 들을 모두 고려하여 이를 세부적으로 분석하면서도, 서로 연관될 수 있도록 하는 통계지표들과 그 프레임워크가 필요함."
  },
  {
    "objectID": "proj/ongoing/soscial_idx.html#연구수행주체",
    "href": "proj/ongoing/soscial_idx.html#연구수행주체",
    "title": "디지털전환에 따른 사회부문 국가발전지표 진단연구",
    "section": "",
    "text": "사이버커뮤니케이션학회\nhttp://www.cybercom.or.kr/"
  },
  {
    "objectID": "proj/ongoing/soscial_idx.html#개요",
    "href": "proj/ongoing/soscial_idx.html#개요",
    "title": "디지털전환에 따른 사회부문 국가발전지표 진단연구",
    "section": "",
    "text": "빠르게 진화화는 디지털기술이 사회부문에 갖는 영향들은 계속해서 변화하고 있고, 그 변화들도 다양하게 확장되고 있기 때문에, 관련 통계지표들을 기술과 사회변화에 맞게 수정, 보완하고, 새롭게 등장하는 현상들을 이해분석하기 위한 새로운 통계지표들도 도출해야 할 필요성이 있음\n이를 위해서는 디지털전환으로 인한 사회변화를 분석할 수 있는 통계를 작성하는 프레임워크 구축이 필요함\n현재 한국에서 생산되는 디지털전환과 사회변화 관련 통계는 KOSIS 국가통계포털, ICT통계포털 등에서 제공하고 있는 ‘정보화통계조사’, ‘디지털정보격차실태조사’, ‘웹접근성실태조사’, ‘언론수용자 의식조사’, ‘국민여가활동조사’, ‘한국미디어패널조사’ 등이 있음.\n각 통계들은 그동안 한국이 정보사회로 접어들게 되면서 경험한 다양한 현상들을 포착하는 데 유용하게 활용되었으며, 대표적인 통계지표로는 인터넷 이용률, 스마트폰 보급률, 디지털 격차 지수, 전자상거래 거래액, 모바일 결제 이용률, 소셜미디어 이용률, 클라우드 서비스 이용률, 사이버 보안 사고 발생 건수 등이 있음.\n\n\n하지만 이 통계에 있어서 다음과 같은 한계들이 있음.\n\n\n먼저 디지털기술환경은 지속적으로 발전하며 새로운 기술과 플랫폼이 계속해서 등장하며, 이러한 변화는 디지털전환과 사회변화 관련된 통계를 최신 상태로 유지하고 관련성을 유지하는 것이 어렵게 만들고 있음.\n통계는 지속성이 중요한데, 디지털기전환과 사회에 대한 통계들은 시간이 조금만 지나면 의미가 줄어드는 통계들이 많아짐\n또한 새로운 디지털기술은 새로운 사회의 변화를 가져오는 데, 이때 새로운 통계지표들이 필요하기 때문에 기존의 통계들을 과감하게 그리고 지속적으로 수정보완해야 하는 특성이 있음.\n한편 디지털기술은 워낙 다양하며, 그것이 미치는 사회적 영향 역시 가시적이지 않은 것들도 있기 때문에 디지털기술이 미치는 사회적 영향 관련된 통계는 상관 관계를 보여줄 수는 있지만, 인과관계를 확립하는 것은 어려울 수 있음\n관찰된 결과에 기여하는 다른 기저 요인이 있을 수 있어 디지털기술의 정확한 영향을 결정하기 어려울 수 있음\n이에 인과관계까지 찾아낼 수 있는 지표들을 도출하여 그 프레임워크를 활용하는 것이 대단히 중요함.\n또한 디지털전환이 사회에 미치는 영향을 세부적으로 구분하여 자세하게 살펴보는 것이 필요함.\n디지털전환이 노동과 소득 등 경제활동에 미치는 영향, 신뢰, 사회자본 등 사회관계에 미치는 영향, 행정서비스활용 등 공공서비스활동에 미치는 영향, 행복도, 우울증, 건강, 복지 등 삶의 질에 미치는 영향 들을 모두 고려하여 이를 세부적으로 분석하면서도, 서로 연관될 수 있도록 하는 통계지표들과 그 프레임워크가 필요함."
  },
  {
    "objectID": "proj/ongoing/kobaco_1.html",
    "href": "proj/ongoing/kobaco_1.html",
    "title": "국내 시청기록 분석을 통한 미디어다양성 진단연구",
    "section": "",
    "text": "(최종목표) 방송시청기록 데이터 등을 활용하여 국내 방송미디어의 다양성을 진단하고, 미디어다양성과 관련한 제도 개선방안을 도출함\n(과업1) 방송시청기록을 포함한 각종 미디어데이터를 분석하여 국내 미디어 다양성의 현황 진단\n\n방송시청기록 및 기타 미디어다양성관련 지표 분석을 통한 미디어다양성 변화의 시계열적 진단\n방송산업의 기술적‧정책적‧산업적‧사회적 환경변화를 분석하여 방송시청행위 및 미디어다양성에 영향을 미치는 영향요인을 연역적으로 추론\n시청점유율조사, N-스크린 이용행태 조사, 미디어다양성조사 등 미디어다양성 증진 관련 조사사업의 누적데이터의 활용성 제고 방안 마련\n\n(과업2) 과업1의 진단 결과를 바탕으로 미디어다양성 증진 관련 현행 제도의 개선방안을 도출\n\n미디어 다양성 정책 변화 양상 진단 및 정책 방향성 제언\n미디어 다양성 규제정책 및 거버넌스의 개선 필요성과 당위성 검토\n미디어 다양성 관련 교육, 미디어 이용기록 조사 및 데이터 활용 등 미디어 다양성 증진을 위한 세부 정책사업안 발굴"
  },
  {
    "objectID": "proj/ongoing/kobaco_1.html#과업-목적-및-범위",
    "href": "proj/ongoing/kobaco_1.html#과업-목적-및-범위",
    "title": "국내 시청기록 분석을 통한 미디어다양성 진단연구",
    "section": "",
    "text": "(최종목표) 방송시청기록 데이터 등을 활용하여 국내 방송미디어의 다양성을 진단하고, 미디어다양성과 관련한 제도 개선방안을 도출함\n(과업1) 방송시청기록을 포함한 각종 미디어데이터를 분석하여 국내 미디어 다양성의 현황 진단\n\n방송시청기록 및 기타 미디어다양성관련 지표 분석을 통한 미디어다양성 변화의 시계열적 진단\n방송산업의 기술적‧정책적‧산업적‧사회적 환경변화를 분석하여 방송시청행위 및 미디어다양성에 영향을 미치는 영향요인을 연역적으로 추론\n시청점유율조사, N-스크린 이용행태 조사, 미디어다양성조사 등 미디어다양성 증진 관련 조사사업의 누적데이터의 활용성 제고 방안 마련\n\n(과업2) 과업1의 진단 결과를 바탕으로 미디어다양성 증진 관련 현행 제도의 개선방안을 도출\n\n미디어 다양성 정책 변화 양상 진단 및 정책 방향성 제언\n미디어 다양성 규제정책 및 거버넌스의 개선 필요성과 당위성 검토\n미디어 다양성 관련 교육, 미디어 이용기록 조사 및 데이터 활용 등 미디어 다양성 증진을 위한 세부 정책사업안 발굴"
  },
  {
    "objectID": "proj/ongoing/gunbo.html",
    "href": "proj/ongoing/gunbo.html",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "연구 수행 기관: 창의성과인터랙션 연구소 \n\n\n\n\n\n\n\n\n\n연구책임자\n선임연구원\n연구원\n객원연구원\n\n\n\n\n\n\n\n\n\n\n이창준 교수\n이승경 박사과정\n박영주 석사과정\n조한슬 박사과정\n\n\n한양대학교 ERICA\n정보사회미디어\n한양대학교\n미디어인포매틱스\n한양대학교\n미디어인포매틱스\n경희대학교\n빅데이터응용학과\n\n\n\n\n\n\n\n건강보험공단이 긍정적인 공공 이미지를 유지하고 부정적인 이슈에 능동적으로 대응하기 위해서는 효과적인 홍보 전략과 미디어 및 온라인 트렌드에 대한 실시간 모니터링이 필요하다. 이를 위해 소셜빅데이터 분석팀은 매스 미디어와 소셜 미디어에서 건강보험공단과 관련된 텍스트를 수집하고 수면에 드러나지 않는 이슈들을 탐색적으로 도출해보고자 한다. 이를 통해 건강보험공단이 해당 이슈에 대해 체계적으로 대응하고 PR 전략을 설정할 수 있는 지표가 되고자 한다.\n\n\n\n\n\n\n기존의 분류와 같이 매스 미디어와 소셜 미디어로 구분하여 텍스트를 수집하고자 함. 아래 열거한 매체와 소스를 위주로 데이터를 수집해나갈 계획이지만 공급 업체의 웹사이트 운영 방침이나 기술적인 문제 등으로 텍스트 수집이 불가능한 경우가 있을 수 있음. 매스 미디어 중 방송사 관련 뉴스들은 거의 모든 방송사들이 공식 유튜브 계정을 가지고 있기 때문에 공식 유튜브 계정을 중심으로 수집하고자 함. 또한 해당 뉴스에 대한 댓글들은 실명 기반의 계정들의 댓글이기 때문에 포털 등의 뉴스 댓글에 비해 혐오성, 공격성 댓글이 적어 담론 수집에 용이함.\n\n\n전국일간지(11개): 경향신문, 국민일보, 내일신문, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보\n경제일간지(8개): 매일경제, 머니투데이, 서울경제, 아시아경제, 아주경제,\n파이낸셜뉴스, 한국경제, 헤럴드경제\n지역일간지(28개): 강원도민일보, 강원일보, 경기일보, 경남도민일보, 경남신문, 경상일보, 경인일보, 광주매일신문, 광주일보, 국제신문, 대구일보, 대전일보, 매일신문, 무등일보, 부산일보, 영남일보, 울산매일, 전남일보, 전북도민일보, 전북일보, 제민일보, 중도일보, 중부매일, 중부일보, 충북일보, 충청일보,\n충청투데이, 한라일보\n보건의료전문지(25개): 뉴스더보이스헬스케어, 데일리팜, 데일리메디, 데일리메디팜, 메디게이트뉴스, 메디칼업저버, 메디칼타임즈, 메디파나뉴스, 메디팜스투데이, 메디포뉴스, 병원신문, 보건신문, 약사공론, 약사신문, 약업신문, 의료정보, 의사신문, 의약뉴스, 일간보사, 의협신문, 청년의사, 헬스포커스, 현대건강신문, 후생신보, 히트뉴스\n전문지(2개): 디지털타임스, 전자신문\n방송 언론: 공식 유튜브 채널을 통해 수집(30개): 연합뉴스Yonhapnews, 연합뉴스TV, 뉴스1TV, 뉴시스, KBSNews, MBCNEWS, SBS뉴스, YTN, MBNNews, 채널A뉴스, JTBCNews, 뉴스TVCHOSUN, 이런경향, 동아일보, 매일경제TV, MTN 머니투데이방송, 서울경제TV, 서울신문, 세계일보, 아주경제, 조선일보, 중앙일보, tvFN, 한겨레TV, 한국경제TV, 한국일보, 내일신문, 이데일리TV, 아시아투데이,시사포커스TV\n\n\n\n\n\n기존 소셜 미디어의 수집처였던 페이스북과 트위터는 국민건강보험 관련 담론에 대한 담론이 활발하게 생성되지 않고 있고 특히 트위터의 경우 정치적 성향이 한쪽으로 치우친 경향이 있어 적절한 수집처로 보기 어려움. 또한 인스타그램의 경우는 이미지 기반의 소셜네트워크서비스로 텍스트 기반 담론이 형성되고 있지 않음. 따라서 소셜빅데이터 팀은 유튜브 댓글 위주의 소셜 데이터를 수집하고 이후에 추가적으로 온라인 커뮤니티로 담론 수집을 확장하고자 함. 다만 온라인 커뮤니티별로 정치적 성향이 확연히 다르기 때문에 균형적인 담론 수집을 위한 전략이 필요함.\n\n\n블로그/카페(공개게시글)(2개): 네이버, 다음\n커뮤니티(7개): 각 커뮤니티의 성향을 파악하고 균형적인 담론 수집 전략을 세울 예정: 디시인사이드, 네이트판, 뽐뿌, MLB파크, 클리앙, 더쿠, 딴지일보\n\n건강보험 등의 키워드로 관련 담론 활발히 생성되는 추가적인 커뮤니티 탐색이 필요\n폐쇄적 구조와 집단극화 발생 가능성, 정확한 데이터 수집/분석 우려\n\n유튜브 댓글(30개): 매스 미디어 방송사 공식 유튜브 채널과 동일한 소스를 활용하여 기사 내용과 담론을 연결할 예정\n\n\n\n\n\n\n\n\n빅데이터 수집 및 구축\n\n매스 미디어: 빅카인즈에서 수집가능한 언론사는 빅카인즈 API를 활용하여 수집, 그 외에는 해당 언론사 웹페이지에서 크롤링\n소셜 미디어: 각 소셜 미디어 서비스의 API를 활용하거나 온라인 커뮤니티 웹에서 크롤링을 통해 수집\n\n\n\n\n\n\n빈도 분석 → 감성 분석 → 토픽 추출 → 추가 분석\n\n\n건강보험공단 관련 단어(명사)의 빈도 분석\n\n제시된 키워드와의 동시 출현단어(명사) TOP 20 추출\n분석기간 동안 시계열 추이 확인\n\n감성분석\n\n자연어 처리(NLP) 기술을 적용하여 수집된 데이터의 감정을 분석할 수 있음. 기사(또는 포스팅) 내용에 대한 감정 점수를 결정하여 대중의 인식을 측정하고자 함.\n제시된 키워드와의 동시 출현단어(형용사) 추출\n형용사 기반의 감성분석 진행\n\n토픽 모델링\n\n건강보험공단과 관련된 대중의 관심사와 관심사의 우선순위를 이해하기 위해 추출된 단어의 분포와 머신 러닝 알고리즘을 사용하여 주제를 도출하고자 함.\n제시된 키워드로 추출된 기사(또는 포스팅)의 제목으로 토픽(주제)를 도출\n건강보험공단과 관련된 주제들의 1개월 단위 추이 탐색\n\n추가 분석 논의\n\n위 분석들의 결과와 관련하여 추가적으로 탐색이 필요하다고 판단될 경우 추가 분석을 진행할 예정\n(예) 소셜 네트워크 분석: 단어들 간의 연결성을 파악하여 해당 주제에 대한 담론을 구체적으로 살펴볼 수 있음. 또한 기사에서 등장하는 다양한 행위자(개인, 조직, 언론 매체) 간의 관계를 분석할 수 있음."
  },
  {
    "objectID": "proj/ongoing/gunbo.html#개요",
    "href": "proj/ongoing/gunbo.html#개요",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "건강보험공단이 긍정적인 공공 이미지를 유지하고 부정적인 이슈에 능동적으로 대응하기 위해서는 효과적인 홍보 전략과 미디어 및 온라인 트렌드에 대한 실시간 모니터링이 필요하다. 이를 위해 소셜빅데이터 분석팀은 매스 미디어와 소셜 미디어에서 건강보험공단과 관련된 텍스트를 수집하고 수면에 드러나지 않는 이슈들을 탐색적으로 도출해보고자 한다. 이를 통해 건강보험공단이 해당 이슈에 대해 체계적으로 대응하고 PR 전략을 설정할 수 있는 지표가 되고자 한다."
  },
  {
    "objectID": "proj/ongoing/gunbo.html#분석-매체",
    "href": "proj/ongoing/gunbo.html#분석-매체",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "기존의 분류와 같이 매스 미디어와 소셜 미디어로 구분하여 텍스트를 수집하고자 함. 아래 열거한 매체와 소스를 위주로 데이터를 수집해나갈 계획이지만 공급 업체의 웹사이트 운영 방침이나 기술적인 문제 등으로 텍스트 수집이 불가능한 경우가 있을 수 있음. 매스 미디어 중 방송사 관련 뉴스들은 거의 모든 방송사들이 공식 유튜브 계정을 가지고 있기 때문에 공식 유튜브 계정을 중심으로 수집하고자 함. 또한 해당 뉴스에 대한 댓글들은 실명 기반의 계정들의 댓글이기 때문에 포털 등의 뉴스 댓글에 비해 혐오성, 공격성 댓글이 적어 담론 수집에 용이함.\n\n\n전국일간지(11개): 경향신문, 국민일보, 내일신문, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보\n경제일간지(8개): 매일경제, 머니투데이, 서울경제, 아시아경제, 아주경제,\n파이낸셜뉴스, 한국경제, 헤럴드경제\n지역일간지(28개): 강원도민일보, 강원일보, 경기일보, 경남도민일보, 경남신문, 경상일보, 경인일보, 광주매일신문, 광주일보, 국제신문, 대구일보, 대전일보, 매일신문, 무등일보, 부산일보, 영남일보, 울산매일, 전남일보, 전북도민일보, 전북일보, 제민일보, 중도일보, 중부매일, 중부일보, 충북일보, 충청일보,\n충청투데이, 한라일보\n보건의료전문지(25개): 뉴스더보이스헬스케어, 데일리팜, 데일리메디, 데일리메디팜, 메디게이트뉴스, 메디칼업저버, 메디칼타임즈, 메디파나뉴스, 메디팜스투데이, 메디포뉴스, 병원신문, 보건신문, 약사공론, 약사신문, 약업신문, 의료정보, 의사신문, 의약뉴스, 일간보사, 의협신문, 청년의사, 헬스포커스, 현대건강신문, 후생신보, 히트뉴스\n전문지(2개): 디지털타임스, 전자신문\n방송 언론: 공식 유튜브 채널을 통해 수집(30개): 연합뉴스Yonhapnews, 연합뉴스TV, 뉴스1TV, 뉴시스, KBSNews, MBCNEWS, SBS뉴스, YTN, MBNNews, 채널A뉴스, JTBCNews, 뉴스TVCHOSUN, 이런경향, 동아일보, 매일경제TV, MTN 머니투데이방송, 서울경제TV, 서울신문, 세계일보, 아주경제, 조선일보, 중앙일보, tvFN, 한겨레TV, 한국경제TV, 한국일보, 내일신문, 이데일리TV, 아시아투데이,시사포커스TV\n\n\n\n\n\n기존 소셜 미디어의 수집처였던 페이스북과 트위터는 국민건강보험 관련 담론에 대한 담론이 활발하게 생성되지 않고 있고 특히 트위터의 경우 정치적 성향이 한쪽으로 치우친 경향이 있어 적절한 수집처로 보기 어려움. 또한 인스타그램의 경우는 이미지 기반의 소셜네트워크서비스로 텍스트 기반 담론이 형성되고 있지 않음. 따라서 소셜빅데이터 팀은 유튜브 댓글 위주의 소셜 데이터를 수집하고 이후에 추가적으로 온라인 커뮤니티로 담론 수집을 확장하고자 함. 다만 온라인 커뮤니티별로 정치적 성향이 확연히 다르기 때문에 균형적인 담론 수집을 위한 전략이 필요함.\n\n\n블로그/카페(공개게시글)(2개): 네이버, 다음\n커뮤니티(7개): 각 커뮤니티의 성향을 파악하고 균형적인 담론 수집 전략을 세울 예정: 디시인사이드, 네이트판, 뽐뿌, MLB파크, 클리앙, 더쿠, 딴지일보\n\n건강보험 등의 키워드로 관련 담론 활발히 생성되는 추가적인 커뮤니티 탐색이 필요\n폐쇄적 구조와 집단극화 발생 가능성, 정확한 데이터 수집/분석 우려\n\n유튜브 댓글(30개): 매스 미디어 방송사 공식 유튜브 채널과 동일한 소스를 활용하여 기사 내용과 담론을 연결할 예정"
  },
  {
    "objectID": "proj/ongoing/gunbo.html#분석-방법",
    "href": "proj/ongoing/gunbo.html#분석-방법",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "빅데이터 수집 및 구축\n\n매스 미디어: 빅카인즈에서 수집가능한 언론사는 빅카인즈 API를 활용하여 수집, 그 외에는 해당 언론사 웹페이지에서 크롤링\n소셜 미디어: 각 소셜 미디어 서비스의 API를 활용하거나 온라인 커뮤니티 웹에서 크롤링을 통해 수집\n\n\n\n\n\n\n빈도 분석 → 감성 분석 → 토픽 추출 → 추가 분석\n\n\n건강보험공단 관련 단어(명사)의 빈도 분석\n\n제시된 키워드와의 동시 출현단어(명사) TOP 20 추출\n분석기간 동안 시계열 추이 확인\n\n감성분석\n\n자연어 처리(NLP) 기술을 적용하여 수집된 데이터의 감정을 분석할 수 있음. 기사(또는 포스팅) 내용에 대한 감정 점수를 결정하여 대중의 인식을 측정하고자 함.\n제시된 키워드와의 동시 출현단어(형용사) 추출\n형용사 기반의 감성분석 진행\n\n토픽 모델링\n\n건강보험공단과 관련된 대중의 관심사와 관심사의 우선순위를 이해하기 위해 추출된 단어의 분포와 머신 러닝 알고리즘을 사용하여 주제를 도출하고자 함.\n제시된 키워드로 추출된 기사(또는 포스팅)의 제목으로 토픽(주제)를 도출\n건강보험공단과 관련된 주제들의 1개월 단위 추이 탐색\n\n추가 분석 논의\n\n위 분석들의 결과와 관련하여 추가적으로 탐색이 필요하다고 판단될 경우 추가 분석을 진행할 예정\n(예) 소셜 네트워크 분석: 단어들 간의 연결성을 파악하여 해당 주제에 대한 담론을 구체적으로 살펴볼 수 있음. 또한 기사에서 등장하는 다양한 행위자(개인, 조직, 언론 매체) 간의 관계를 분석할 수 있음."
  },
  {
    "objectID": "proj/index.html#ongoing-projects",
    "href": "proj/index.html#ongoing-projects",
    "title": "Projects",
    "section": "Ongoing Projects",
    "text": "Ongoing Projects\n\n\n\n\n\n\n\n\n\n\n도심에서 개인 동선의 녹지 노출이 신체적, 정신적 건강에 끼치는 영향\n\n\n\n\nFunding\n\n\nSKKU College of Computing and Informatics\n\n\n\n\nStart\n\n\n2024-05-01\n\n\n\n\nDue\n\n\n2024-10-30\n\n\n\n\nState\n\n\nOngoing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n스크린 멀티모달 인터랙션 연구\n\n\n\n\nFunding\n\n\nSamsung Electronics\n\n\n\n\nStart\n\n\n2024-05-01\n\n\n\n\nDue\n\n\n2024-11-30\n\n\n\n\nState\n\n\nOngoing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이커머스 플랫폼과 사용자 보호\n\n\n\n\nFunding\n\n\nNAVER\n\n\n\n\nStart\n\n\n2024-05-01\n\n\n\n\nDue\n\n\n2024-08-30\n\n\n\n\nState\n\n\nOngoing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석 vol(2)\n\n\n\n\nFunding\n\n\n건강보험공단\n\n\n\n\nStart\n\n\n2024-04-01\n\n\n\n\nDue\n\n\n2024-12-31\n\n\n\n\nState\n\n\nOngoing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n커뮤니케이션과 인간공학이 만나는 심리 상담 챗봇\n\n\n\n\nFunding\n\n\n한국연구재단(2023S1A5A2A21086671)\n\n\n\n\nStart\n\n\n2023-05-01\n\n\n\n\nDue\n\n\n2024-04-30\n\n\n\n\nState\n\n\nOngoing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n가상세계 멀티 페르소나 성향과 사용자의 인지 강화\n\n\n\n\nFunding\n\n\n한국연구재단(2022S1A5A805107011)\n\n\n\n\nStart\n\n\n2022-05-01\n\n\n\n\nDue\n\n\n2025-04-30\n\n\n\n\nState\n\n\nOngoing\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "proj/index.html#completed-projects",
    "href": "proj/index.html#completed-projects",
    "title": "Projects",
    "section": "Completed Projects",
    "text": "Completed Projects\n\n\n\n\n\n\n\n\n\n\n언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석\n\n\n\n\nFunding\n\n\n건강보험공단\n\n\n\n\nStart\n\n\n2023-04-01\n\n\n\n\nDue\n\n\n2023-12-31\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n국내 시청기록 분석을 통한 미디어다양성 진단연구\n\n\n\n\nFunding\n\n\nKOBACO\n\n\n\n\nStart\n\n\n2023-09-01\n\n\n\n\nDue\n\n\n2023-12-30\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n디지털 전환 생태계 분석 및 토픽 모델링을 통한 산업 분류 방안 모색\n\n\n\n\nFunding\n\n\nKISDI\n\n\n\n\nStart\n\n\n2023-09-01\n\n\n\n\nDue\n\n\n2023-11-30\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n디지털전환에 따른 사회부문 국가발전지표 진단연구\n\n\n\n\nFunding\n\n\n통계청\n\n\n\n\nStart\n\n\n2023-05-01\n\n\n\n\nDue\n\n\n2023-11-30\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플랫폼 산업의 경제 효용 추정: 자국 검색 플랫폼이 온라인 산업에 미치는 영향\n\n\n\n\nFunding\n\n\n한국연구재단(2020S1A5A2A0304148012)\n\n\n\n\nStart\n\n\n2020-07-01\n\n\n\n\nDue\n\n\n2023-06-30\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnology Evolution in Regional Economies\n\n\n\n\nFunding\n\n\nERC 715631, TechEvo\n\n\n\n\nStart\n\n\n2017-03-01\n\n\n\n\nDue\n\n\n2023-02-28\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n미래 기술 정책을 위한 한국형 스마트 특성화 전략 모델 구축\n\n\n\n\nFunding\n\n\n한국연구재단(2020R1G1A101245313)\n\n\n\n\nStart\n\n\n2020-03-01\n\n\n\n\nDue\n\n\n2023-02-28\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n탄소시장 대응 녹색기술 이전 및 기후변화 대응 기술사업모델을 통한 탄소시장 진출의 전략 대응도 및 전략 마련 : 특허분석을 기반으로\n\n\n\n\nFunding\n\n\n녹색기술센터\n\n\n\n\nStart\n\n\n2022-06-23\n\n\n\n\nDue\n\n\n2022-10-30\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n육군 인력획득 홍보 대상의 성향 분석 빅데이터 모형 연구\n\n\n\n\nFunding\n\n\n국방부 육군\n\n\n\n\nStart\n\n\n2022-02-01\n\n\n\n\nDue\n\n\n2022-07-31\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n건강 형평성 파악을 위한 안산시 건강지도 제작 연구\n\n\n\n\nFunding\n\n\n안산시지속가능발전협의회\n\n\n\n\nStart\n\n\n2021-06-01\n\n\n\n\nDue\n\n\n2021-12-22\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Hours and Familial Supports\n\n\n\n\nFunding\n\n\nNUS\n\n\n\n\nStart\n\n\n2015-09-01\n\n\n\n\nDue\n\n\n2017-08-31\n\n\n\n\nState\n\n\nCompleted\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "proj/completed/7_proj.html",
    "href": "proj/completed/7_proj.html",
    "title": "Technology Evolution in Regional Economies",
    "section": "",
    "text": "Technology Evolution in Regional Economies"
  },
  {
    "objectID": "blogs/posts/9_ott_strategy_talk.html",
    "href": "blogs/posts/9_ott_strategy_talk.html",
    "title": "OTT Movie Launching Strategy",
    "section": "",
    "text": "Slide"
  },
  {
    "objectID": "blogs/posts/7_avatar_custo.html",
    "href": "blogs/posts/7_avatar_custo.html",
    "title": "메타버스 속 멀티 페르소나 경향이 태도, 몰입, 지속사용의도에 끼치는 영향",
    "section": "",
    "text": "Slide"
  },
  {
    "objectID": "blogs/posts/5_adaboost.html",
    "href": "blogs/posts/5_adaboost.html",
    "title": "Understanding the AdaBoost",
    "section": "",
    "text": "The AdaBoost algorithm is a type of ensemble learning algorithm that combines multiple “weak” classifiers to create a “strong” classifier. A weak classifier is one that performs only slightly better than random guessing (i.e., its accuracy is slightly better than 50%). In contrast, a strong classifier is one that performs well on the classification task.\nThe basic idea behind AdaBoost is to iteratively train a sequence of weak classifiers, and then combine their predictions using a weighted majority vote to obtain a strong classifier. In each iteration, the algorithm assigns higher weights to the misclassified samples, so that the subsequent classifiers focus more on the difficult samples."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#overview",
    "href": "blogs/posts/5_adaboost.html#overview",
    "title": "Understanding the AdaBoost",
    "section": "",
    "text": "The AdaBoost algorithm is a type of ensemble learning algorithm that combines multiple “weak” classifiers to create a “strong” classifier. A weak classifier is one that performs only slightly better than random guessing (i.e., its accuracy is slightly better than 50%). In contrast, a strong classifier is one that performs well on the classification task.\nThe basic idea behind AdaBoost is to iteratively train a sequence of weak classifiers, and then combine their predictions using a weighted majority vote to obtain a strong classifier. In each iteration, the algorithm assigns higher weights to the misclassified samples, so that the subsequent classifiers focus more on the difficult samples."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#algorithm-steps",
    "href": "blogs/posts/5_adaboost.html#algorithm-steps",
    "title": "Understanding the AdaBoost",
    "section": "Algorithm Steps",
    "text": "Algorithm Steps\nHere are the steps of the AdaBoost algorithm:\n\nInitialize the sample weights \\(w_i\\) to 1/n, where n is the number of samples.\nFor t in 1:T, where T is the number of iterations:\n\n\nTrain a weak classifier \\(h_t(x)\\) on the training data using the current weights.\nCalculate the error rate \\(ε_t\\) of \\(h_t(x)\\) on the training data. The error rate is defined as the weighted sum of the misclassified samples:\n\n\\[\nε_t = Σ_i w_i \\times I(y_i \\neq h_t(x_i))\n\\]\n\nwhere \\(y_i\\) is the true class label of sample i, \\(h_t(x_i)\\) is the predicted class label of \\(h_t(x)\\) for sample i, and \\(I()\\) is the indicator function that returns 1 if the argument is true and 0 otherwise.\n\n\nCalculate the weight \\(α_t\\) of \\(h_t(x)\\) as \\(α_t = \\frac{log((1 - ε_t)}{ε_t}\\). The weight \\(α_t\\) measures the “importance” of the weak classifier \\(h_t(x)\\) in the ensemble. The weight is larger for classifiers that perform well (i.e., have a low error rate) and smaller for classifiers that perform poorly (i.e., have a high error rate).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that \\(ε_t\\) must be strictly less than 0.5 to ensure that \\(α_t\\) is positive.\n\n\n\nUpdate the sample weights as \\(w_i = w_i \\times exp(α_t)\\). The weight update gives higher weight to the misclassified samples and lower weight to the correctly classified samples. The weight update is equivalent to:\n\n\nif y_i = h_t(x_i), then w_i = w_i * exp(-α_t)\nif y_i ≠ h_t(x_i), then w_i = w_i * exp(α_t)\n\n\nNormalize the weights so that they sum to 1. The normalization ensures that the weights are valid probability distributions.\n\n\nCombine the weak classifiers using the weighted majority vote rule to obtain the final prediction. The final prediction is given by:\n\n\\[\nH(x) = sign(Σ_t (α_t \\times h_t(x))),\n\\]\nwhere sign() is the sign function that returns -1 for negative values and 1 for positive values."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#implementing-adaboost-in-r",
    "href": "blogs/posts/5_adaboost.html#implementing-adaboost-in-r",
    "title": "Understanding the AdaBoost",
    "section": "Implementing AdaBoost in R",
    "text": "Implementing AdaBoost in R\nTo implement AdaBoost in R, we can use the adabag package, which provides an implementation of the algorithm. Here’s an example of how to use adabag to train an AdaBoost classifier on the iris dataset:\n\nlibrary(adabag)\n\nLoading required package: rpart\n\n\nLoading required package: caret\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\nLoading required package: foreach\n\n\nLoading required package: doParallel\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Load the iris dataset\ndata(iris)\n\n# Convert the species to a binary variable\niris$Species &lt;- as.factor(ifelse(iris$Species == \"setosa\", 1, 0))\n\n# Split the dataset into training and testing sets\ntrain_idx &lt;- sample(1:nrow(iris), size = 100, replace = FALSE)\ntrain_data &lt;- iris[train_idx, ]\ntest_data &lt;- iris[-train_idx, ]\n\n# Train an AdaBoost classifier with 50 iterations\nada_model &lt;- boosting(Species ~ ., \n                      data = train_data, \n                      boos = TRUE, \n                      mfinal = 50)\n\n\n# Make predictions on the testing data\npred &lt;- predict.boosting(ada_model, newdata = test_data)\n\n# Calculate the accuracy of the classifier\nacc &lt;- sum(pred$class == test_data$Species) / nrow(test_data)\nprint(paste0(\"Accuracy: \", acc))\n\n[1] \"Accuracy: 1\"\n\n\nIn this example, we first load the adabag package and the iris dataset. We then convert the species variable to a binary variable (-1 for “setosa” and 1 for “versicolor” and “virginica”). We split the dataset into a training set (100 samples) and a testing set (50 samples).\nNext, we train an AdaBoost classifier with 50 iterations using the boosting() function from adabag. We specify the formula (Species ~ .) and the training data (train_data), and set the boos parameter to TRUE to enable AdaBoost.\nWe then make predictions on the testing data using the predict.boosting() function, and calculate the accuracy of the classifier by comparing the predicted class labels to the true class labels.\nYou can modify this example by changing the number of iterations (mfinal) or the dataset to fit your specific needs. Additionally, you can try using other weak classifiers, such as decision trees or logistic regression, and compare their performance to AdaBoost."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#similarities-difference-with-random-forest",
    "href": "blogs/posts/5_adaboost.html#similarities-difference-with-random-forest",
    "title": "Understanding the AdaBoost",
    "section": "Similarities & Difference with Random Forest",
    "text": "Similarities & Difference with Random Forest\nSimilarities\n\nBoth Random Forest and AdaBoost are ensemble learning algorithms that combine multiple “weak” models to create a “strong” model.\nBoth algorithms use a form of bootstrap sampling to generate multiple training sets, which helps to reduce overfitting and improve the generalization performance of the models.\nBoth algorithms are widely used in machine learning and can be applied to a wide range of classification and regression tasks.\n\nDifferences\n\nRandom Forest combines multiple decision trees, each trained on a different subset of the features and samples, and uses a majority vote to make predictions. In contrast, AdaBoost combines multiple weak models, with each model trained on the same dataset but with different weights assigned to the samples.\nRandom Forest places equal weight on all the samples, while AdaBoost assigns higher weights to the misclassified samples in each iteration, so that the subsequent models focus more on the difficult samples.\nRandom Forest uses a simple majority vote to make predictions, while AdaBoost combines the predictions of the weak models using weighted majority vote, with each model weighted by its importance in the ensemble.\nRandom Forest can handle a wide range of datasets and is less sensitive to outliers and noise, while AdaBoost is more sensitive to noisy and unbalanced datasets and may require more data preprocessing.\nRandom Forest typically performs well with large feature sets and high-dimensional data, while AdaBoost may require careful feature selection or dimensionality reduction to prevent overfitting and improve performance."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#summary",
    "href": "blogs/posts/5_adaboost.html#summary",
    "title": "Understanding the AdaBoost",
    "section": "Summary",
    "text": "Summary\nThe AdaBoost algorithm is a powerful ensemble learning algorithm that can improve the performance of weak classifiers by combining their predictions. It works by iteratively training a sequence of weak classifiers, and then combining their predictions using a weighted majority vote to obtain a strong classifier. The algorithm assigns higher weights to the misclassified samples in each iteration, so that the subsequent classifiers focus more on the difficult samples. The weight updates and normalization ensure that the subsequent classifiers focus more on the difficult samples, and the final prediction is based on the weighted majority vote rule, which gives more weight to the predictions of the strong classifiers.\nOverall, AdaBoost is a powerful and widely used algorithm in machine learning, particularly for classification problems. It is relatively simple to implement and can be applied to a wide range of classification tasks. Additionally, it has been shown to perform well even with noisy and unbalanced datasets."
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "",
    "text": "In this blog post, we will explore two essential concepts in evaluating classification models: the confusion matrix and the Receiver Operating Characteristic (ROC) curve. We will go through the basics, discuss how to interpret these metrics, and provide R code snippets to create and visualize them using the popular caret and pROC packages. We’ll demonstrate these concepts using the Titanic dataset."
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#introduction",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#introduction",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "",
    "text": "In this blog post, we will explore two essential concepts in evaluating classification models: the confusion matrix and the Receiver Operating Characteristic (ROC) curve. We will go through the basics, discuss how to interpret these metrics, and provide R code snippets to create and visualize them using the popular caret and pROC packages. We’ll demonstrate these concepts using the Titanic dataset."
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#loading-the-titanic-dataset",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#loading-the-titanic-dataset",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "Loading the Titanic Dataset",
    "text": "Loading the Titanic Dataset\nThe Titanic dataset is not available in R by default, but it can be loaded using the titanic package. First, install and load the package:\n\n# Install the titanic package if you haven't already\n# if (!requireNamespace(\"titanic\", quietly = TRUE)) {\n#   install.packages(\"titanic\")\n# }\n\n# Load the titanic package\nlibrary(titanic)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the Titanic dataset\ndata(\"titanic_train\")\ndata(\"titanic_test\")\n\n# Combine the training and testing datasets\ntitanic_data &lt;- bind_rows(titanic_train, titanic_test)\n\nBefore we proceed, let’s preprocess the dataset by selecting relevant features and handling missing values:\n\n# Select relevant features and remove rows with missing values\ntitanic_data &lt;- titanic_data[, c(\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\")]\ntitanic_data &lt;- na.omit(titanic_data)\n\n# Convert the 'Sex' variable to a factor\ntitanic_data$Sex &lt;- as.factor(titanic_data$Sex)"
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#confusion-matrix",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#confusion-matrix",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nA confusion matrix is a tabular representation of the predictions made by a classification model, showing the number of correct and incorrect predictions for each class. It is a useful tool to evaluate a model’s performance and identify its strengths and weaknesses.\n\nCreating a Confusion Matrix in R\nTo demonstrate the confusion matrix, we will use the preprocessed Titanic dataset and create a logistic regression model. First, let’s load the required packages and split the dataset into training (80%) and testing (20%) sets:\n\n# Load required packages\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Split the dataset into training (80%) and testing (20%) sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(titanic_data$Survived, p = 0.8, list = FALSE)\ntitanic_train &lt;- titanic_data[train_index, ]\ntitanic_test &lt;- titanic_data[-train_index, ]\n\nNow, let’s create a logistic regression model and make predictions on the test set:\n\n# Create the logistic regression model\nmodel &lt;- glm(Survived ~ ., data = titanic_train, family = \"binomial\")\n\n# Make predictions on the test dataset\npredicted_probs &lt;- predict(model, titanic_test, type = \"response\")\npredicted_classes &lt;- ifelse(predicted_probs &gt; 0.5, 1, 0)\n\nNext, we will create the confusion matrix using the caret package:\n\n# Create the confusion matrix\ncm &lt;- confusionMatrix(table(predicted_classes, titanic_test$Survived))\nprint(cm)\n\nConfusion Matrix and Statistics\n\n                 \npredicted_classes  0  1\n                0 78 17\n                1 11 36\n                                          \n               Accuracy : 0.8028          \n                 95% CI : (0.7278, 0.8648)\n    No Information Rate : 0.6268          \n    P-Value [Acc &gt; NIR] : 4.43e-06        \n                                          \n                  Kappa : 0.5687          \n                                          \n Mcnemar's Test P-Value : 0.3447          \n                                          \n            Sensitivity : 0.8764          \n            Specificity : 0.6792          \n         Pos Pred Value : 0.8211          \n         Neg Pred Value : 0.7660          \n             Prevalence : 0.6268          \n         Detection Rate : 0.5493          \n   Detection Prevalence : 0.6690          \n      Balanced Accuracy : 0.7778          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nInterpreting the Confusion Matrix\nThe confusion matrix displays the following information:\n\nTrue Positives (TP): 78, the number of correctly predicted non-survivors (class 0)\nTrue Negatives (TN): 36, the number of correctly predicted survivors (class 1)\nFalse Negatives (FN): 11, the number of non-survivors incorrectly predicted as survivors (class 1)\nFalse Positives (FP): 17, the number of survivors incorrectly predicted as non-survivors (class 0)\n\nStatistics:\n\nAccuracy: 0.8028 (80.28%), the proportion of correct predictions (both true positives and true negatives) among the total number of cases. The 95% CI (confidence interval) for the accuracy is (0.7278, 0.8648), meaning we can be 95% confident that the true accuracy lies within this range.\nNo Information Rate (NIR): 0.6268, the accuracy that could be obtained by always predicting the majority class (class 0 in this case).\nP-Value [Acc &gt; NIR]: 4.43e-06, the p-value for a statistical test comparing the accuracy of the model to the NIR. A small p-value (typically less than 0.05) indicates that the model’s accuracy is significantly better than the NIR.\nKappa: 0.5687, a metric that considers both the true positive rate and the false positive rate, providing a more balanced assessment of the model’s performance. Kappa ranges from -1 to 1, with 0 indicating no better than random chance, and 1 indicating perfect agreement between predictions and true values.\nMcnemar's Test P-Value: 0.3447, the p-value for a statistical test comparing the number of false positives and false negatives. A large p-value (typically greater than 0.05) indicates that there is no significant difference between the number of false positives and false negatives.\n\nSensitivity, Specificity, and Other Metrics:\n\nSensitivity (Recall or True Positive Rate): 0.8764, the proportion of actual positive cases (survivors) that were correctly identified by the model.\nSpecificity: 0.6792, the proportion of actual negative cases (non-survivors) that were correctly identified by the model.\nPositive Predictive Value (PPV): 0.8211, the proportion of positive predictions (predicted survivors) that were actually positive (true survivors).\nNegative Predictive Value (NPV): 0.7660, the proportion of negative predictions (predicted non-survivors) that were actually negative (true non-survivors).\nPrevalence: 0.6268, the proportion of the true positive cases (survivors) in the dataset.\nDetection Rate: 0.5493, the proportion of true positive cases that were correctly detected by the model.\nDetection Prevalence: 0.6690, the proportion of cases predicted as positive (survivors) by the model.\nBalanced Accuracy: 0.7778, the average of sensitivity and specificity, providing a balanced assessment of the model’s performance across both classes.\n\nThe ‘Positive’ class is set to 0 (non-survivors) in this analysis."
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#roc-curve",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#roc-curve",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "ROC Curve",
    "text": "ROC Curve\nThe Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier’s performance across all possible decision thresholds. It plots the True Positive Rate (TPR, also known as sensitivity or recall) against the False Positive Rate (FPR, or 1 - specificity) at various threshold settings.\n\nCreating an ROC Curve in R\nTo create an ROC curve, we will use the pROC package. First, let’s install and load the required package:\n\n# Install the pROC package if you haven't already\n# if (!requireNamespace(\"pROC\", quietly = TRUE)) {\n#   install.packages(\"pROC\")\n# }\n\n# Load the pROC package\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nNow, let’s create the ROC curve using the predicted probabilities from our logistic regression model:\n\n# Create the ROC curve\nroc_obj &lt;- roc(titanic_test$Survived, predicted_probs)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n# Plot the ROC curve\nplot(roc_obj, main = \"ROC Curve for the Logistic Regression Model\")\nabline(0, 1, lty = 2, col = \"gray\")  # Add a reference line for a random classifier\n\n\n\n\n\n\nInterpreting the ROC Curve\nThe ROC curve helps us visualize the trade-off between sensitivity (True Positive Rate) and specificity (1 - False Positive Rate) for various threshold values. A perfect classifier would have an ROC curve that passes through the top-left corner of the plot (100% sensitivity and 100% specificity). In contrast, a random classifier would have an ROC curve that follows the diagonal reference line (gray dashed line in our plot).\nThe area under the ROC curve (AUC) is a scalar value that summarizes the performance of the classifier. An AUC of 1.0 indicates a perfect classifier, while an AUC of 0.5 suggests that the classifier is no better than random chance. We can calculate the AUC using the auc function from the pROC package:\n\n# Calculate the AUC\nauc_value &lt;- auc(roc_obj)\ncat(\"AUC:\", auc_value, \"\\n\")\n\nAUC: 0.8181047 \n\n\nLet’s go through the process of drawing dots and lines in the ROC curve step by step.\n\nUnderstand the components of the ROC curve: The ROC curve consists of several points (dots) that represent the true positive rate (sensitivity) and false positive rate (1 - specificity) at various decision thresholds. To draw the curve, you need to connect these points with lines.\nDetermine decision thresholds: You must first identify the decision thresholds you want to use. These thresholds represent the probability cut-off points for classifying an observation as positive or negative. In most cases, you can use the unique predicted probabilities in your dataset as the thresholds.\n\n\n# Extract unique predicted probabilities\nthresholds &lt;- unique(predicted_probs)\n\n# Sort the thresholds in descending order\nthresholds &lt;- sort(thresholds, decreasing = TRUE)\n\n\nCalculate TPR and FPR for each threshold: For each threshold, calculate the true positive rate (sensitivity) and false positive rate (1 - specificity).\n\n\n# Initialize empty vectors for TPR and FPR\ntpr &lt;- numeric(length(thresholds))\nfpr &lt;- numeric(length(thresholds))\n\n# Calculate TPR and FPR for each threshold\nfor (i in seq_along(thresholds)) {\n\n  threshold &lt;- thresholds[i]\n\n  # Classify observations based on the current threshold\n  predicted_classes &lt;- as.integer(predicted_probs &gt;= threshold)\n\n  # Create a confusion matrix\n  cm &lt;- table(Predicted = predicted_classes, \n              Actual = titanic_test$Survived)\n\n  # Calculate TPR and FPR\n  if(sum(dim(cm))==4){\n    tpr[i] &lt;- cm[\"1\", \"1\"] / (cm[\"1\", \"1\"] + cm[\"0\", \"1\"])\n    fpr[i] &lt;- cm[\"1\", \"0\"] / (cm[\"1\", \"0\"] + cm[\"0\", \"0\"])\n  }\n  \n}\n\n\nPlot the ROC curve: Now that you have calculated the TPR and FPR for each threshold, you can plot the ROC curve by connecting the dots (points) with lines.\n\n\n# Create a data frame for plotting\nroc_df &lt;- data.frame(Threshold = thresholds, TPR = tpr, FPR = fpr)\n\n# Create the ROC plot\nroc_plot &lt;- ggplot(roc_df, aes(x = FPR, y = TPR)) +\n  geom_point(size = 2, color = \"red\") +  # Add points (dots)\n  geom_line(color = \"blue\", size = 1) +  # Connect the points with lines\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    title = \"ROC Curve for the Logistic Regression Model\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Display the plot\nprint(roc_plot)\n\n\n\n\nThis will create an ROC curve with dots (points) representing each decision threshold and lines connecting these dots. The resulting plot allows you to visualize the trade-offs between sensitivity and specificity at various threshold settings.\nLet’s see the process of drawing the ROC curve step by step.\n\nlibrary(gganimate)\n\nanimated_roc_plot &lt;- ggplot(roc_df, aes(x = FPR, y = TPR)) +\n  geom_point(aes(group = seq_along(Threshold), \n                 color = as.factor(seq_along(Threshold))),\n             size = 2, show.legend = FALSE) +\n  geom_line(aes(group = seq_along(Threshold), \n                color = as.factor(seq_along(Threshold))),\n            size = 1, show.legend = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    title = \"ROC Curve for the Logistic Regression Model\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\"\n  ) +\n  theme_minimal() +\n  transition_states(states = seq_along(roc_df$Threshold), transition_length = 2, state_length = 1) +\n  enter_fade() + exit_fade() +\n  shadow_mark(alpha = 0.5, size = 1) # Add the trajectory\n\n# Display the animated plot\nanimate(animated_roc_plot, nframes = 200, end_pause = 50)"
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#conclusion",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#conclusion",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have explored the confusion matrix and ROC curve as valuable tools for evaluating classification models. We demonstrated how to create and interpret these metrics using R code snippets and the Titanic dataset. With a solid understanding of these concepts, you can better assess the performance of your classification models and make informed decisions about model selection and tuning."
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html",
    "href": "blogs/posts/6_interactive_graph.html",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis, and static graphs are often the go-to method for many analysts. However, interactive graphs can significantly enhance user engagement and understanding of the data. In this blog post, we’ll walk you through the process of transforming a static ggplot graph into an interactive visualization using the Plotly package in R."
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html#introduction",
    "href": "blogs/posts/6_interactive_graph.html#introduction",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "",
    "text": "Data visualization is a crucial aspect of data analysis, and static graphs are often the go-to method for many analysts. However, interactive graphs can significantly enhance user engagement and understanding of the data. In this blog post, we’ll walk you through the process of transforming a static ggplot graph into an interactive visualization using the Plotly package in R."
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html#step-1-install-and-load-the-necessary-packages",
    "href": "blogs/posts/6_interactive_graph.html#step-1-install-and-load-the-necessary-packages",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "Step 1: Install and Load the Necessary Packages",
    "text": "Step 1: Install and Load the Necessary Packages\nBefore you begin, make sure to install and load the required packages: ggplot2 and plotly. If you don’t have them installed, you can install them using the following code:\n\n# Install packages if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\n# Load packages\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout"
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html#step-2-create-a-sample-dataset-and-ggplot-graph",
    "href": "blogs/posts/6_interactive_graph.html#step-2-create-a-sample-dataset-and-ggplot-graph",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "Step 2: Create a Sample Dataset and ggplot Graph",
    "text": "Step 2: Create a Sample Dataset and ggplot Graph\nFor this tutorial, we’ll create a sample dataset and a ggplot graph. You can replace this dataset and graph with your own data and visualization.\n\n# Sample dataset\ndata &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100),\n  group = factor(rep(1:2, each = 50))\n)\n\n# Create a ggplot graph\np &lt;- ggplot(data, aes(x = x, y = y, color = group)) +\n  geom_point() +\n  theme_minimal()\n\np"
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html#step-3-convert-the-ggplot-graph-into-an-interactive-plotly-graph",
    "href": "blogs/posts/6_interactive_graph.html#step-3-convert-the-ggplot-graph-into-an-interactive-plotly-graph",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "Step 3: Convert the ggplot Graph into an Interactive Plotly Graph",
    "text": "Step 3: Convert the ggplot Graph into an Interactive Plotly Graph\nNow that we have our ggplot graph, it’s time to make it interactive! We’ll use the ggplotly() function from the plotly package to convert our static graph into an interactive one.\n\n# Convert ggplot graph to an interactive plotly graph\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive graph\ninteractive_plot\n\n\n\n\n\nYou now have an interactive scatter plot, which allows users to hover over the points, zoom in and out, and pan the graph for a better view of the data. You can also customize the tooltip and other interactive features according to your needs."
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html#step-4-display-custom-labels-instead-of-x-and-y-coordinates",
    "href": "blogs/posts/6_interactive_graph.html#step-4-display-custom-labels-instead-of-x-and-y-coordinates",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "Step 4: Display Custom Labels Instead of x and y Coordinates",
    "text": "Step 4: Display Custom Labels Instead of x and y Coordinates\nTo show custom labels instead of the default x and y coordinates when hovering over the points, you can modify the text aesthetic in ggplot and set the tooltip attribute in the ggplotly function.\n\n# Add a custom label column to the dataset\ndata$label &lt;- paste(\"Group:\", data$group, \"&lt;br&gt;Point ID:\", seq_along(data$x))\n\n# Create a ggplot graph with custom hover labels\np &lt;- ggplot(data, aes(x = x, y = y, color = group, text = label)) +\n  geom_point() +\n  theme_minimal()\n\n# Convert ggplot graph to an interactive plotly graph with custom tooltips\ninteractive_plot &lt;- ggplotly(p, tooltip = \"text\")\n\n# Display the interactive graph with custom labels\ninteractive_plot\n\n\n\n\n\nIn this example, we’ve added a new column to the dataset called label containing custom text to be displayed when hovering over the points. We’ve updated the aes() function in ggplot to include the text aesthetic, and then specified the tooltip attribute in the ggplotly function to use the “text” aesthetic for tooltips.\nNow, when you hover over the points, you’ll see custom labels (e.g., “Group: 1, Point ID: 1”) instead of the default x and y coordinates."
  },
  {
    "objectID": "blogs/posts/6_interactive_graph.html#conclusion",
    "href": "blogs/posts/6_interactive_graph.html#conclusion",
    "title": "Make Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly",
    "section": "Conclusion",
    "text": "Conclusion\nTransforming your static ggplot graphs into interactive visualizations with Plotly is a simple and powerful way to enhance user engagement and understanding. By following these steps, you can create captivating, interactive graphs for your data analysis projects and impress your audience with dynamic, data-driven insights. So, go ahead and make your graphs interactive!"
  },
  {
    "objectID": "blogs/posts/8_text_mining_talk.html",
    "href": "blogs/posts/8_text_mining_talk.html",
    "title": "Text Mining in Academic Research",
    "section": "",
    "text": "Slide"
  },
  {
    "objectID": "proj/completed/2_proj.html",
    "href": "proj/completed/2_proj.html",
    "title": "미래 기술 정책을 위한 한국형 스마트 특성화 전략 모델 구축",
    "section": "",
    "text": "개요\n\n본 과제는 전 세계 특허 데이터와 양자간 무역 데이터를 활용하여 한국의 미래 기술 혁신과 4차 산업 인재 양성 전략에 기여할 수 있는 정책 개발에 그 목적을 두고 있다.\n기존의 국가 기술 정책의 경우 특정 지역의 기술적 상황이나 지식 축적의 정도를 제대로 반영하지 못하는 한계점을 가지고 있어 지역 혁신 정책의 기반이 되는 단서를 제공하는 융복합 방법론 개발이 필요하다.\n따라서 이를 융복합 학문인 경제지리학과 데이터과학을 통해 지역 기술 혁신 정책을 근거 중심으로 체계화 하는 창의적 학술연구를 수행한다.\n나아가 현재 전 세계에서 일어나고 있는 무역 전쟁의 핵심인 기술 경쟁의 본질을 양자간 무역 데이터를 통해 동아시아의 기술의 흐름을 분석해 봄으로써 향후 정책 방향을 모색하는 도전적 학술연구를 수행한다.\n마지막으로 연구 과정에서 나오는 성과와 혜안을 공유하기 위한 웹 기반 플랫폼을 구축하고 홍보하여 정부 정책 입안 방향에 적용될 수 있도록 돕는다.\n\n\n\n연구 배경\n\n한국은 1960년대부터 전 세계에서 유례없는 성장률인 연 평균 7%를 기록하며 급속도로 성장해왔다. 그 결과 1960년대 158불(USD)이었던 일인당 국내 총생산이 2017년에는 약 3만불(29,742 USD)까지 증가했다. 현재 한국은 아시아에서는 4번째, 전 세계에서는 12번째로 큰 경제국이다. 호의적이었던 국제 시장 상황, 높은 비율의 숙련공, 노동력의 높은 교육 수준 등의 요소가 물론 주요했지만, 학계에서는 수출 주도의 산업 정책이 성공 요인으로 빈번하게 거론되어 왔다. 하지만 글로벌 경제 위기 이후 한국은 예전의 성장력을 회복하지 못하고 저성장의 길로 들어섰다. 한국은행과 국제통화기금(IMF)는 모두 한국경제의 잠재성장률을 2% 수준으로 보고 있으며 2020년대와 30년대는 1%대로 떨어질 것으로 예측하고 있다. 이는 물론 국제적인 불황과 노동 인구의 감소가 큰 축을 담당하고 있지만 그런 요인들은 현재 모든 선진국의 당면 과제들이다.\n\n유럽 연합(EU)은 이러한 문제점을 조기에 인식하고 국가를 중심으로 저성장, 실업문제를 해결할 지역정책 모색과 산업정책인 스마트 특성화 전략(Smart Specialization Strategy, S3)을 수행 중이며 하향식(Top-down)과 상향식(Bottom-up) 방법을 적절히 혼합하여 지역 혁신의 성과를 이루어 내고 있다. 유럽 내의 모든 국가와 지역에서 주력 산업 및 미래 성장동력산업 육성을 위해 획일적으로 적용되어 왔던 기술 정책들을 각 지역의 역량, 특성 및 잠재력에 기초하여 새로운 지역별 혁신 모델을 정착시키고 있다. 하지만, 한국은 달라진 기술 환경에 적응하지 못하고 기존의 혁신 모델에서 크게 벗어나고 있지 못하고 있다. 기존에 이루어진 많은 기술 정책들은 지역의 지식 역량을 고려하지 않은 채 바이오, 나노, 정보통신(IT) 등의 유사한 지식 클러스터를 특정 지역에 특화 시키려고 노력했으나 새로운 융합 기술이나 산업의 구조적 변화를 이끌어 내는 데는 한계가 있었다. 또한 이로 인해 지식기반이 획일화되고 국가 및 지역별 지식 구조의 독창성과 차별성을 상실했다는 비판도 면할 수 없었다. 이에, 지역 지식 역량을 고려한 전략과 투자를 차별화하여 불필요한 중복 투자를 막고 지역의 지식, 기술, 인적 자원을 최대한 활용한 지역 혁신정책의 필요성이 제기되고 있다.\n\n따라서 본 과제에서는 지역별 지식 구조를 분석하는 방법을 체계화하고 이를 근거로 지역의 기술 혁신 정책, 즉 한국형 스마트 특성화 전략(Korean Smart Specialization Strategy, KS3)을 개발하고자 한다. 구체적으로는 한국의 지식 구조의 진화 과정을 특허 데이터에 포함되어 있는 메타 정보를 활용하여 분석해보고 이를 지역별 지식 구조의 진화 과정과 비교해본다. 이를 통해 각 지역의 지식 역량을 기반으로 한 스마트 특성화 전략을 도출할 수 있다. 또한 특허 데이터를 통해 얻어진 지역의 지식 구조와 양국간 무역 데이터와의 연결을 통해 일본과 중국의 특성화된 기술에 비해 한국의 경쟁력이 낮은 기술들을 개발할 때 어떤 지역에서 해당 기술을 특화시켜야 하는지에 대한 전략도 수립해 보고자 한다.\n\n\n\n기대효과\n\n4차 산업 혁명을 대비해 많은 국가들이 인공지능, 로보틱스 등의 신산업을 육성하기 위해 기술 로드맵을 개발하고 있지만 정작 지역별 지식 공간을 분석하여 근거를 가지고 특정 지역을 특정 기술로 특화 시키거나 개발하는 노력은 부족해왔다.\n본 과제의 성공적 수행을 통해 첫째로 한국형 스마트 특성화 전략에 대한 학문적 근거를 마련할 수 있을 것으로 보인다. 둘째로 현재 이슈가 되고 있는 한국과 미국, 일본, 중국 등의 무역 갈등 기저에 있는 기술 갈등 역시 학문적 접근이 가능하다.\n이는 정책적으로 상당한 파급효과가 있을 것으로 예상된다. 학문적 성과 외에도 연구 결과가 정책 입안자들이 실용적으로 시의적절한 참고가 될 수 있도록 웹 기반 플랫폼을 구축할 예정이다. 이를 통해 연구 결과가 실제로 정책으로 이어지는 파급 효과가 예상된다.\n\n\n\n\n성과\n\n컨퍼런스 발표: 국내 4회, 국제 1회\n\n20200612: 사이버커뮤니케이션학회 춘계정기학술대회. 사이버커뮤니케이션학회\n\n미디어연관도 분석을 통한 미디어 수용자의 선택적 노출 측정과 정치적 양극화\n\n20200925: 제8회 한국미디어패널학술대회. 정보통신정책연구원.\n\n늦은 밤 OTT 시청이 수면에 끼치는 영향\n\n\n\n\n20201120: 정보통신정책학회 정기학술대회. 정보통신정책연구원.\n\n한국 AI 지식의 진화, 그리고 미래\n\n20210903: 제9회 한국미디어패널학술대회. 정보통신정책연구원.\n\n개인적 특성, 환경적 요인, 시간대와 요일효과를 고려한 OTT 선택 요인 분석: TV vs. OTT\n\n20221017: ERC TechEvo Workshop. European Research Council. @Vienna Complex Hub Lab, Austria\n\nNetwork Methods to Analyze Collective Knowledge\n\n\n\n\n논문 발간: S(S)CI 6편, KCI 1편\n\nShon, M., Lee, D. & Lee, C.(2022). Inward or Outward? Direction of Knowledge Flow and Firm Efficiency. International Journal of Technology Management. 90(1-2), 102-121. https://doi.org/10.1504/IJTM.2022.124617\nTóth, G., Elekes, Z., Whittle, A., Lee, C.*, & Kogler, D. F. (2022). Technology network structure conditions the economic resilience of regions. Economic Geography. 98(4), 1-24. https://doi.org/10.1080/00130095.2022.2035715\nKim, K., Lee, J., & Lee, C.(2022). Which innovation type is better for production efficiency? A comparison between product/service, process, organizational, and marketing innovations using stochastic frontier and meta-frontier analysis. Technology Analysis & Strategic Management. doi: https://doi.org/10.1080/09537325.2021.1965979\nRocchetta, S., Mina, A., Lee, C., & Kogler, F. D. (2022). Technological Knowledge Space and the Resilience of European Regions. Journal of Economic Geography. 22(1), 27-51.* doi: https://doi.org/10.1093/jeg/lbab001\nLee, C., Cho, H., & Lee, D.* (2021). The mechanism of innovation spill-over across sub-layers in the ICT industry. Asian Journal of Technology Innovation. 29(2), 159-179. doi:https://doi.org/10.1080/19761597.2020.1796725\nKim, K., Lee, J., & Lee, C.* (2021). Exploratory Analysis of Knowledge Structure and Evolutionary Trajectory in Korean Artificial Intelligence for Effective Technology Policy. Korean Innovation Study, 16(3). DOI:https://doi.org/10.46251/INNOS.2021.8.16.3.139\nLee, C., Lee, D., & Shon, M.* (2020). Effect of efficient triple-helix collaboration on organizations based on their stage of growth. Journal of Engineering and Technology Management. 58, 101604. https://doi.org/10.1016/j.jengtecman.2020.101604"
  },
  {
    "objectID": "proj/ongoing/1_proj.html",
    "href": "proj/ongoing/1_proj.html",
    "title": "가상세계 멀티 페르소나 성향과 사용자의 인지 강화",
    "section": "",
    "text": "연구 목표\n메타버스가 새로운 사회적 공간으로 급부상하면서, 그러한 가상세계에서 사용자를 대표하는 아바타 역시 제 2의 사회적 자아로 자리 잡고 있는 상황이다. 그러나 관련된 연구 현황은 변화한 사회 흐름을 반영하지 못한 채 기존의 온라인 플랫폼 속 아바타에 대한 분석에만 머무르고 있는 실정이다. 이에 본 연구는 새로운 가상세계인 메타버스 속 아바타가 어떠한 의도와 양상으로 표현되는지를 탐구하고자 한다. 특히 메타버스 사용자의 아바타 창조에 초점을 맞춰, 멀티 페르소나(다중 정체성) 지표를 개발하고 그에 따른 다양한 실험을 수행할 것이다. 또한 이를 통해 멀티 페르소나 성향이 메타버스 사용자의 감각적 몰입과 창의성에 어떤 영향을 끼치는지를 분석하고자 한다. 이는 국내외 연구에서 단 한 번도 시도된 적 없는 새로운 미디어-사용자 상호작용 프레임 워크의 개발로서, 기존 가상세계의 아바타와는 확연히 구분되는 결과를 제시할 것으로 예측한다.\n\n\n기대 효과\n본 연구의 결과물은 아바타 창조에 대한 단편적 분석 수준을 넘어 1) 사회적 측면에서 메타버스의 주요 사용 계층인 1020세대의 미디어 사용 현상과 경험에 관한 심층적 이해를 제시할 수 있을 것이다. 또한 이는 급변하는 미디어 환경에 적절히 대응하도록 하는 미디어 리터러시 교육에 대한 단초까지도 제공할 수 있을 것이라 기대한다. 2) 학문적 측면에서 역시 메타버스라는 새로운 플랫폼 사용자의 아바타 활용 성향에 창의적이고 도전적으로 접근함으로써, 뉴미디어 연구 분야에 다양하고 폭넓은 논의점을 제시할 수 있을 것이라 기대한다. 나아가, 설문과 실험 기법의 창의적인 융합은 복잡다난해지는 미디어 연구 영역에서 보다 과학적이고 체계적인 연구 방법을 제안하는 일이 될 것이다. 3) 연구인력 양성의 측면에서, 본 연구는 아직 충분히 연구되지 못한 메타버스 영역의 관련 서비스와 사용자에 대한 전문적인 연구 인력을 배출할 수 있을 것이다.\n\n\n연구 요약\n새롭게 등장한 가상세계 플랫폼인 메타버스는 사회 전반에 걸친 다양한 영역과 연계될 수 있다는 높은 가능성을 지니고 있음에도, 관련된 이해와 탐구는 여전히 미미한 수준이다. 특히 메타버스에서 사용자를 대신할 수 있는 아바타는 제2, 제3의 자아로 존재하며 사회적 상호작용과 가상세계로의 몰입에 중요한 매개체가 되어가고 있다. 이에 본 연구는 관련한 심층적 연구의 필요성을 인식하여 1) 메타버스 속 아바타 창조의도에 대한 탐구를 통해 사용자의 멀티 페르소나 지표를 개발하고 2) 메타버스 가상세계에서의 멀티 페르소나 특성이 사용자의 감각적 몰입에 끼치는 영향에 대해 분석하며 3) 메타버스 가상세계에서의 멀티 페르소나 특성이 사용자의 창의성에 끼치는 영향에 대해 분석하고자 한다. 그에 따라 구체적으로, 1차 년도에는 메타버스 가상공간 속 멀티 페르소나에 대한 이론적 프레임워크를 개발하여 이를 기반으로 멀티 페르소나 지수 측정 아이템을 고안한다. 이는 추후에 이어질 연구에서 사용자의 멀티 페르소나 성향을 제시하는 중요한 단초가 된다. 2차 년도에는 메타버스 속 감각적 몰입에 대한 정의와 측정 아이템을 개발한 후, 멀티 페르소나 성향과 감각적 몰입 간의 관계를 분석한다. 이를 위해, 100% 가상환경(제페토)과 가상-현실 혼합 환경(게더타운)을 중심으로 아바타에 발현되는 멀티 페르소나 성향과 감각적 몰입에 관한 창의적인 실험 환경을 구축할 예정이다. 마지막으로 3차 년도에는 메타버스 속 인지강화 경험 중 창의성에 대한 측정 아이템을 개발하고, 전 해에 구성된 창의적 실험 디자인을 활용하여 멀티 페르소나 성향과 창의성 간의 관계를 분석한다. 이와 같은 과정의 전반은 과거에 시도된 적 없는 매우 새롭고 창의적인 일이며, 미래에 이어질 후속연구에 유의미한 가치를 제공할 수 있을 것이다.\n\n\n\n1차 년도 실적(2022.05.01.~2023.04.30.)\n\n멀티페르소나 지수 측정 도구 개발 및 설문 실시\n본 연구는 1년차 연구 목표를 위해 메타버스 사용자의 멀티 페르소나 지수 측정 척도 개발을 수행하였으며, 특히 ’아바타 커스터마이징’을 중심으로 설문 도구를 제작하였다. 아바타 커스터마이징에 관한 기존의 연구들은 게임·디자인 영역에 한정되어, 디자인 요소 혹은 디지털 기술 향상에 관한 논의만이 주를 이뤄왔다. 본 연구는 그러한 한계점을 극복하고자 아바타 커스터마이징 항목을 미디어와 사회적 차원으로 가져와 학제 간 융합을 시도하였고, 메타버스 내 멀티 페르소나 지수 측정을 위한 척도 개발에 응용하였다. 구체적으로, 가상의 커스터마이징이 가능한 설문 문항을 통해 현실과 가상세계 아바타의 모습이 얼마나 다른지를 측정하였다. 설문에는 한양대 ERICA 언론정보대학 재학생 112명이 참여하였으며, 그에 따른 주요 연구 성과로서 사회자본이 메타버스 내 멀티 페르소나 지수와 연관이 있음을 도출하였다.\n500명 규모의 2차 설문 실시\n메타버스 사용자의 멀티 페르소나 특성을 측정한 첫 설문 이후, 분석 결과를 토대로 문항을 수정하여 2차 설문에 착수하였다. 본 연구에서는 약 500명 규모의 참여자를 대상으로, 1회의 파일럿(200명) 테스트와 1회의 본 조사(300명)를 계획하였다. 이를 위해, ㈜마크로밀엠브레인을 통해 2023년 2월 2일부터 파일럿 테스트를 실시하였으며, 그 결과 212명의 데이터를 확보하였다. 추가적으로, 파일럿 테스트에서 확보된 데이터의 심층적 검토를 거쳐 설문 도구를 최종 수정하여 300명 이상 규모의 본 조사를 실시할 예정이다.\n논문 투고\n본 연구에서는 1년차에 개발된 멀티 페르소나 척도를 메타버스에 관한 다양한 시각에 접목해, 사용자의 내적‧외적 측면을 깊이 있게 탐색할 수 있는 구체적 연구를 설계하였다. 특히 앞서 언급한 아바타 커스터마이징 행위를 통해 사용자의 자아 표현 양상을 다뤘을뿐만 아니라, 그것을 사회적 상호작용의 영역과 연계하였다. 이러한 연구 결과는 KCI급 저널에 투고될 예정이다.\nC&I studies 주관 특별 세미나 ‘메타버스 새로운 미래, 새로운 인간’ 개최\n본 연구팀은 2022년 10월 26일 한양대 ERICA 언론정보관 4층 MCN Open Studio에서 메타버스에 관한 특별 세미나를 실시하였다. 이는 메타버스와 아바타, 멀티페르소나에 대한 총 3개의 섹션(‘메타버스 트렌드와 전망, 수용자 특성의 주요 쟁점’, ‘가상세계 멀티 페르소나 성향과 사용자의 인지 강화’, ‘관계형성의 장으로서 메타버스에 관한 실증적 접근’)으로 나뉘어 진행되었다. 세미나를 통해, 관련 내용을 타 학자들과 논의함으로써 당해 연구 목표인 멀티 페르소나 지수 측정의 주요 요인들을 도출하였다. 또한 2차년도에 수행될 실험 연구를 위해, 보다 정교한 연구 방법(VR기기 활용 및 자체적 실험 환경 구축) 도입을 계획 및 논의 할 수 있는 교류의 장을 마련하였다. \n \n한국미디어경영학회, 프리드리히나우만재단 주관 특별 세미나 ‘메타버스 산업과 이용자, 그리고 디지털 리터러시’ 및 발표\n본 연구팀은 2022년 11월 8일 홍대 RYSE 호텔 SPACE에서 개최된 세미나에 참석하여, ’메타버스 사용자의 사회적 자본, 아바타 커스터마이징과 프레즌스’를 주제로 한 발표를 수행하였다(발표자: 이창준, 한양대학교 교수). 이를 통해 1년차 연구 목표인 멀티 페르소나 측정을 위한 아바타 커스터마이징 방법을 학계에 제시하였으며, 이러한 연구가 단순한 학술적 차원 이상의 실증적·실무적 차원까지 발전할 수 있음을 논의하였다. 또한 세미나의 주제인 진화하는 메타버스 환경에서 필요한 이용자 활동과 경험, 그에 따른 사회현상에 관해 타 학자들과의 활발한 교류를 진행하였다."
  },
  {
    "objectID": "proj/ongoing/kisdi_1.html",
    "href": "proj/ongoing/kisdi_1.html",
    "title": "디지털 전환 생태계 분석 및 토픽 모델링을 통한 산업 분류 방안 모색",
    "section": "",
    "text": "연구 목적 및 내용\n\n(목적) 디지털 기술이 서비스 산업에 접목되며 서비스 방식의 변화, 서비스 산업 지형의 변화를 초래하고 있어 이러한 변화를 조망할 수 있는 분석 프레임워크 개발을 위한 연구를 수행\n(내용) 디지털 전환을 이끌고 있는 혁신 스타트업을 대상으로한 토픽모델링을 통해 서비스의 융합, 디지털 전환 트렌드 분석\n\n토픽 모델링을 통해 서비스 산업 분류방법 고찰 및 디지털 전환 트렌드 분석\n네트워크 방법론을 활용한 기술-BM 융합 트렌드 분석\n디지털 전환 생태계에서 고용이 일어나는 요인 규명\n\n\n수행방법\n\n토픽 모델링을 통해 서비스 산업 분류방법 고찰 및 디지털 전환 트렌드 분석\n\n토픽 모델링을 통해 디지털 트랜스포메이션이 가져온 서비스 산업의 변화 및 융합 산업 분류 방법에 대한 고찰\n기존 산업 분류 내에서 새로운 토픽의 분포 분석을 통한 새로운 산업 분류 분석 프레임 워크 개발\n\n네트워크 방법론을 활용한 기술-BM 융합 트렌드 분석\n\n스타트업 '혁신의숲' 데이터를 통해 한국의 스타트업 기업들의 기술과 사업 모델에 대한 태그에 대한 공동 출현 정보로 네트워크로 전환\n네트워크 방법론을 활용하여 산업별로 디지털 전환 생태계 내의 기술과 사업 모델 융합이 어떤 식으로 이루어지는지 앞서 얻어진 토픽 별로 분석\n\n디지털 전환을 이끄는 혁신기업의 성장 요인 규명\n\n디지털 전환을 이끄는 혁신기업의 성장 (성장은 고용(신규 인력 채용 등) 등을 활용하여 측정)의 주요 요인(ex. 기술-BM융합, 산업(토픽), 투자 단계, 투자 시점 등의 요인) 분석"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Published works",
    "section": "",
    "text": "Time-of-Day and Day-of-Week Effects on TV and OTT Media Choices\n\n\nEvidence from South Korea\n\n\n\n\n\n\nYun-Woo Choi, Changjun Lee*\n\n\n2024-01\n\n\n2024-01\n\n\nJournal of Theoretical and Applied Electronic Commerce Research\n\n\n\n\n\n\n  \n\n\n\n\n시니어의 심리적 요인과 가구구성형태가 적극적인 SNS 이용에 미치는 영향\n\n\n자아 존중감, 인지 욕구, 가구구성형태를 중심으로\n\n\n\n\n\n\n박지은 , 이승경 , 이창준*\n\n\n2023-12\n\n\n2023-12\n\n\n정보사회와미디어\n\n\n\n\n\n\n  \n\n\n\n\nThe Effects of Popularity Metrics in News Comments on the Formation of Public Opinion\n\n\nEvidence from an Internet Portal Site\n\n\n\n\n\n\nInyoung Park, Hyungbo Shim, Jang Hyun Kim, Changjun Lee, Daeho Lee*\n\n\n2023-10\n\n\n2020-06\n\n\nThe Social Science Journal\n\n\n\n\n\n\n  \n\n\n\n\nChanges in regional knowledge bases and its effect on local labour markets in the midst of transition\n\n\nEvidence from France over 1985–2015\n\n\n\n\n\n\nKeungoui Kim, Dieter F. Kogler, Changjun Lee, Taewon Kang*\n\n\n2023-09\n\n\n2022-05\n\n\nApplied Spatial Analysis and Policy\n\n\n\n\n\n\n  \n\n\n\n\nThe Return of the King:\n\n\nThe Importance of Killer Content in a Competitive OTT Market\n\n\n\n\n\n\nJongwha Kim, Changjun Lee*\n\n\n2023-05\n\n\n2023-05\n\n\nJournal of Theoretical and Applied Electronic Commerce Research\n\n\n\n\n\n\n  \n\n\n\n\nThe effect of watching OTT late at night on the sleep pattern of users\n\n\n\n\n\n\n\n\n\nChangjun Lee, Cheongho Na, Keoungoui Kim*\n\n\n2023-05\n\n\n2023-05\n\n\nSleep and Biological Rhythms\n\n\n\n\n\n\n  \n\n\n\n\nThe Optimal Open Innovation Strategy with Science-based Partners for Venture Firm’s Innovation Capabilities\n\n\nFocusing on Innovation Modes\n\n\n\n\n\n\nCheongho Na, Changjun Lee, Eungdo Kim*\n\n\n2023-04\n\n\n2023-04\n\n\nScience, Technology and Society\n\n\n\n\n\n\n  \n\n\n\n\nRegional knowledge spaces: the interplay of entry-relatedness and entry-potential for technological change and growth\n\n\n\n\n\n\n\n\n\nDieter F. Kogler, Ronald B. Davies, Changjun Lee, Keungoui Kim*\n\n\n2023-04\n\n\n2022-03\n\n\nThe Journal of Technology Transfer\n\n\n\n\n\n\n  \n\n\n\n\nWhich innovation type is better for production efficiency?\n\n\nA comparison between product/service, process, organizational, and marketing innovations using stochastic frontier and meta-frontier analysis\n\n\n\n\n\n\nKeungoui Kim, Junmin Lee, Changjun Lee*\n\n\n2023-03\n\n\n2021-08\n\n\nTechnology Analysis & Strategic Management\n\n\n\n\n\n\n  \n\n\n\n\nAn Analysis of Factors Influencing the Intention to Use Untact Services by Service Type\n\n\n\n\n\n\n\n\n\nHyunsuk Liu, Changjun Lee, Keungoui Kim, Junmin Lee, Ahram Moon, Daeho Lee, Myeongjun Park*\n\n\n2023-02\n\n\n2023-02\n\n\nSustainability\n\n\n\n\n\n\n  \n\n\n\n\n중국의 애국주의와 한류 팬덤 간의 사이버 갈등\n\n\nBTS 리더 RM의 ‘밴 플리트 상’ 수상소감 관련 온라인 댓글 분석을 중심으로\n\n\n\n\n\n\n염자몽 , 이승경 , 이창준*\n\n\n2022-12\n\n\n2022-12\n\n\n사이버커뮤니케이션학보\n\n\n\n\n\n\n  \n\n\n\n\n비대면 상황에서의 온라인 커뮤니티 활동이 삶 만족도에 미치는 영향\n\n\n코로나19 이전(2017년)과 이후(2021년) 비교를 중심으로\n\n\n\n\n\n\n최미연 , 이창준*\n\n\n2022-12\n\n\n2022-12\n\n\n정보사회와미디어\n\n\n\n\n\n\n  \n\n\n\n\nTechnology network structure conditions the economic resilience of regions\n\n\n\n\n\n\n\n\n\nGergő Tóth, Zoltán Elekes, Adam Whittle, Changjun Lee*, Dieter F. Kogler\n\n\n2022-12\n\n\n2022-03\n\n\nEconomic Geography\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of OTT Users’ Watching Behavior for Identifying a Profitable Niche\n\n\nLatent Class Regression Approach\n\n\n\n\n\n\nDongnyok Shim, Changjun Lee, Inha Oh*\n\n\n2022-11\n\n\n2022-11\n\n\nJournal of Theoretical and Applied Electronic Commerce Research\n\n\n\n\n\n\n  \n\n\n\n\nThe effects of regional capacity in knowledge recombination on production efficiency\n\n\n\n\n\n\n\n\n\nChangjun Lee, Hyunha Shin, Keungoui Kim, Dieter F. Kogler\n\n\n2022-07\n\n\n2022-04\n\n\nTechnological Forecasting and Social Change\n\n\n\n\n\n\n  \n\n\n\n\nInward or Outward? Direction of Knowledge Flow and Firm Efficiency\n\n\nA meta-frontier analysis\n\n\n\n\n\n\nMinjung Shon, Daeho Lee, Changjun Lee*\n\n\n2022-06\n\n\n2022-06\n\n\nInternational Journal of Technology Management\n\n\n\n\n\n\n  \n\n\n\n\nInternet of Things Technology: Balancing Privacy Concerns with Convenience\n\n\n\n\n\n\n\n\n\nHyesoo Jeon, Changjun Lee*\n\n\n2022-05\n\n\n2022-04\n\n\nTelematics and Informatics\n\n\n\n\n\n\n  \n\n\n\n\nEffective strategies to attract crowdfunding investment based on the novelty of business ideas\n\n\n\n\n\n\n\n\n\nEunjun Jung, Changjun Lee*, Junseok Hwang\n\n\n2022-05\n\n\n2022-02\n\n\nTechnological Forecasting and Social Change\n\n\n\n\n\n\n  \n\n\n\n\n산업별 디지털 전환 갈등 지표 개발 연구\n\n\n\n\n\n\n\n\n\n강송희 , 서영희 , 이창준*\n\n\n2022-04\n\n\n2022-04\n\n\n정보사회와미디어\n\n\n\n\n\n\n  \n\n\n\n\nChanges in consumption patterns during the COVID-19 pandemic\n\n\nAnalyzing the revenge spending motivations of different emotional groups\n\n\n\n\n\n\nInyoung Park, Jieun Lee, Daeho Lee, Changjun Lee, Won Young Chung*\n\n\n2022-03\n\n\n2021-12\n\n\nJournal of Retailing and Consumer Services\n\n\n\n\n\n\n  \n\n\n\n\nEstimating cost of fighting against fake news during catastrophic situations\n\n\n\n\n\n\n\n\n\nHanseul Jo, Soyeong Park, Dongcheol Shin, Jungwoo Shin*, Changjun Lee\n\n\n2022-01\n\n\n2021-11\n\n\nTelematics and Informatics\n\n\n\n\n\n\n  \n\n\n\n\nTechnological Knowledge Space and the Resilience of European Regions\n\n\n\n\n\n\n\n\n\nSilvia Rocchetta*, Andrea Mina, Changjun Lee, Dieter F. Kogler\n\n\n2022-01\n\n\n2021-05\n\n\nJournal of Economic Geography\n\n\n\n\n\n\n  \n\n\n\n\n효율적인 기술 정책 제안을 위한 한국 인공지능 지식 구조와 진화 궤적의 탐색적 분석\n\n\n\n\n\n\n\n\n\n김경외 , 이준민 , 이창준*\n\n\n2021-09\n\n\n2021-08\n\n\n한국혁신학회지\n\n\n\n\n\n\n  \n\n\n\n\nThe mechanism of innovation spill-over across sub-layers in the ICT industry\n\n\n\n\n\n\n\n\n\nChangjun Lee, Hosoo Cho, Daeho Lee*\n\n\n2021-09\n\n\n2020-07\n\n\nAsian Journal of Technology Innovation\n\n\n\n\n\n\n  \n\n\n\n\nThe nature of ICT in technology convergence\n\n\nA knowledge-based network analysis\n\n\n\n\n\n\nSungdo Jung, Keungoui Kim, Changjun Lee*\n\n\n2021-07\n\n\n2021-07\n\n\nPLOS ONE\n\n\n\n\n\n\n  \n\n\n\n\n재난상황에서의 가짜뉴스 식별: 영상 vs 문자\n\n\n\n\n\n\n\n\n\n조한슬 , 오명진 , 신정우 , 이창준\n\n\n2021-06\n\n\n2021-06\n\n\n사이버커뮤니케이션학보\n\n\n\n\n\n\n  \n\n\n\n\n미국 ICT기업의 국제특허분류 공동출현 네트워크 특성에 관한 연구\n\n\n기술유사성과 기술응용성을 중심으로\n\n\n\n\n\n\n노태우 , 조길수 , 이창준*\n\n\n2021-05\n\n\n2021-05\n\n\n한국혁신학회지\n\n\n\n\n\n\n  \n\n\n\n\nStrategic Groups Emerged by Selecting R&D Collaboration Partners and Firms’ Efficiency\n\n\n\n\n\n\n\n\n\nChungho Na, Daeho Lee, Junseok Hwang, Changjun Lee*\n\n\n2021-03\n\n\n2020-07\n\n\nAsian Journal of Technology Innovation\n\n\n\n\n\n\n  \n\n\n\n\nEffect of efficient triple-helix collaboration on organizations\n\n\nbased on their stage of growth\n\n\n\n\n\n\nChangjun Lee, Daeho Lee, Minjung Shon*\n\n\n2020-11\n\n\n2020-11\n\n\nJournal of Engineering and Technology Management\n\n\n\n\n\n\n  \n\n\n\n\nTV홈쇼핑 이용자의 상품구매 입문 요인 분석\n\n\n랜덤포레스트 분석을 활용한 개인의 미디어 레퍼토리 데이터를 중심으로\n\n\n\n\n\n\n이창준* , 우형진 , 박성복\n\n\n2020-05\n\n\n2020-05\n\n\n한국혁신학회지\n\n\n\n\n\n\n  \n\n\n\n\nDevelopment of Information Communication Technology Industry and Public Policy in South Korea\n\n\nPublic Administration and Public Policy in Korea\n\n\n\n\n\n\nChangjun Lee*, Yooil Bae\n\n\n2019-10\n\n\n2019-10\n\n\nSpringer\n\n\n\n\n\n\n  \n\n\n\n\nCapturing Information on Technology Convergence, International Collaboration, and Knowledge Flow from Patent Document\n\n\nA Case of Information and Communication Technology\n\n\n\n\n\n\nChangjun Lee, Dieter F. Kogler, Daeho Lee*\n\n\n2019-05\n\n\n2018-09\n\n\nInformation Processing & Management\n\n\n\n\n\n\n  \n\n\n\n\nDoes Working Long Hours Cause Marital Dissolution?\n\n\nEvidence from the Reduction in South Korea’s Workweek Standard\n\n\n\n\n\n\nErin Hye-Won Kim*, Changjun Lee\n\n\n2019-03\n\n\n2019-01\n\n\nAsian Population Studies\n\n\n\n\n\n\n  \n\n\n\n\nThe Effect of Adult Children’s Working Hours on Visits to Elderly Parents\n\n\nA Natural Experiment in Korea\n\n\n\n\n\n\nErin Hye-Won Kim*, Changjun Lee, Young Kyung Do\n\n\n2019-02\n\n\n2018-08\n\n\nPopulation Research and Policy Review\n\n\n\n\n\n\n  \n\n\n\n\nThe Effect of a Reduced Statutory Workweek on Familial Long-Term Care in Korea\n\n\n\n\n\n\n\n\n\nErin Hye-Won Kim*, Changjun Lee, Young Kyung Do\n\n\n2018-12\n\n\n2018-09\n\n\nJournal of Aging and Health\n\n\n\n\n\n\n  \n\n\n\n\nThe influence of giant platform on content diversity\n\n\n\n\n\n\n\n\n\nChangjun Lee*, Junseok Hwang\n\n\n2018-10\n\n\n2016-12\n\n\nTechnological Forecasting and Social Change\n\n\n\n\n\n\n  \n\n\n\n\nResearch on the Mutual Relations between ISP and ASP Efficiency Changes\n\n\nfor the Sustainable Growth of Internet Industry\n\n\n\n\n\n\nHo Seoung Na, Daeho Lee, Junseok Hwang, Changjun Lee*\n\n\n2018-08\n\n\n2017-07\n\n\nApplied Economics\n\n\n\n\n\n\n  \n\n\n\n\nThe Evolutionary Trajectory of ICT Ecosystem\n\n\nA Network Analysis based on Media User Data\n\n\n\n\n\n\nChangjun Lee, Hongbum Kim*\n\n\n2018-07\n\n\n2018-03\n\n\nInformation & Management\n\n\n\n\n\n\n  \n\n\n\n\nDoes Social Media Use Really Make People Politically Polarized?\n\n\nDirect and Indirect Effects of Social Media Use on Political Polarization\n\n\n\n\n\n\nChangjun Lee, Jieun Shin, Ahreum Hong*\n\n\n2018-01\n\n\n2017-09\n\n\nTelematics and Informatics\n\n\n\n\n\n\n  \n\n\n\n\nEx-post evaluation of illegalizing juvenile online game after midnight\n\n\nA case of shutdown policy in South Korea\n\n\n\n\n\n\nChangjun Lee, Hongbum Kim, Ahreum Hong*\n\n\n2017-11\n\n\n2017-07\n\n\nTelematics and Informatics\n\n\n\n\n\n\n  \n\n\n\n\nEffect of a Policy Intervention on Handset Subsidies on the Intention to Change Handsets\n\n\nand Households’ Expenses in Mobile Telecommunications\n\n\n\n\n\n\nChangjun Lee, Sungdo Jung, Keungoui Kim*\n\n\n2017-11\n\n\n2017-06\n\n\nTelematics and Informatics\n\n\n\n\n\n\n  \n\n\n\n\nIntra-industry innovation, spillovers, and industry evolution\n\n\nEvidence from the Korean ICT industry\n\n\n\n\n\n\nChangjun Lee, Jang Hyun Kim, Daeho Lee*\n\n\n2017-11\n\n\n2017-06\n\n\nTelematics and Informatics\n\n\n\n\n\n\n  \n\n\n\n\nMobile Healthcare Applications and Gamification for Sustained Health Maintenance\n\n\n\n\n\n\n\n\n\nChangjun Lee, Kyoungsun Lee, Daeho Lee*\n\n\n2017-05\n\n\n2017-05\n\n\nSustainability\n\n\n\n\n\n\n  \n\n\n\n\n효과적인 정보 확산을 위한 미디어 이용 패턴 분석\n\n\n온라인 의견지도자의 미디어 레퍼토리를 중심으로\n\n\n\n\n\n\n김명규 , 이창준 , 홍아름*\n\n\n2016-04\n\n\n2016-04\n\n\n정보사회와미디어\n\n\n\n\n\n\n  \n\n\n\n\nPlatform Openness and the Productivity of Content Providers\n\n\nA meta-frontier analysis\n\n\n\n\n\n\nChangjun Lee, Daeho Lee*, Junseok Hwang\n\n\n2015-09\n\n\n2014-08\n\n\nTelecommunications Policy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/published/2015_Lee Lee Hwang_TP.html",
    "href": "research/published/2015_Lee Lee Hwang_TP.html",
    "title": "Platform Openness and the Productivity of Content Providers",
    "section": "",
    "text": "Highlights\n\nWe categorize content providers (CPs) into three groups.\nWe compare the efficiency of CPs within and between the groups.\nThe average efficiency of CPs within each group is the highest in a closed platform.\nThe efficiency of CPs between groups is higher in an open platform than closed one.\n\n\nAbstract\nThis paper analyzes the platform environments in which content providers (CPs) may succeed by using a meta-frontier analysis that compares the efficiency of different groups in identical industries. The results illustrate that a group focusing on an iOS platform achieves a high average efficiency with low variance within the group because the iOS ecosystem manages the content novelty and uncertainty risk in the selection process. This quality control enables a CP to maximize value once the CP enters the ecosystem. From the meta-frontier viewpoint, however, Android-group firms have a higher efficiency level than iOS-group firms. Android transfers risk management to CPs who can conduct additional trial and error, causing CPs to endure the tough selection process. This explains the low initial technical efficiency, but in the long term, this group has the potential to achieve high efficiency. In addition, the group providing content to both platforms was the most efficient group because of the economies of scale."
  },
  {
    "objectID": "research/published/2017_Lee Kim Hong_TI.html",
    "href": "research/published/2017_Lee Kim Hong_TI.html",
    "title": "Ex-post evaluation of illegalizing juvenile online game after midnight",
    "section": "",
    "text": "Media attention\n\nCited in Nature Editorial\nhttp://www.nature.com/news/put-cult-online-games-to-the-test-1.22343\n\n\nHighlights\n\nWe examines an effect of legalizing a ban of online gaming late at night for youths.\nThe policy caused a decrease in predicted probability of Internet addiction by 0.7 percent points.\nThe policy caused an increase in sleep duration of 1.5 min.\nAll results showed a gender difference in the effect of the policy.\n\n\nAbstract\nIn November 2011, the Korean government legalized blocking access to online games for youths younger than age 16 late at night; this is called the shutdown policy. Using multiple regressions we examined how the compulsory block affected youths’ Internet hours and sleep duration. Data were drawn from the 2011, 2012 Korea Youth Behavior Risk Factor Survey, a cross-sectional online survey of middle and high school students aged 13–18 years. Legalizing a ban of online gaming late at night for youths caused an increase in the predicted probability of being in a high-ranked Internet user group by 1.6 percent points, a decrease in the predicted probability of Internet addiction by 0.7 percent points, and an increase in sleep duration of 1.5 min. All results showed a gender difference in the effect of the policy. Although the net effect of the shutdown policy was statistically significant, the small effect size, the partial effect on female youths, and the side effects related to human basic rights and inappropriate regulation of the game industry made the effectiveness of the policy arguable."
  },
  {
    "objectID": "research/published/2017_Lee Lee Lee_Sustainability.html",
    "href": "research/published/2017_Lee Lee Lee_Sustainability.html",
    "title": "Mobile Healthcare Applications and Gamification for Sustained Health Maintenance",
    "section": "",
    "text": "Abstract\nThis paper examines how gamification affects user intention to use mobile healthcare applications (mHealth) and how the effect of gamification works differently according to health status, age, and gender. We use data from a mobile survey conducted by a Korean representative survey agency. We estimate the effect of gamification on user intention to use mobile healthcare applications based on a structural equation model and examine the moderating effects of self-reported health status, age, and gender. We find that gamification is effective in increasing user intention to use mHealth, especially in the healthy and younger groups. These findings suggest that mHealth, with the gamification factor, would encourage healthy (but lack exercise) people as well as unhealthy people to maintain their health status, and thus the mHealth developers need to consider the gamification factor when they develop mHealth services for healthy people."
  },
  {
    "objectID": "research/published/2018_Lee Hwang_TFSC.html",
    "href": "research/published/2018_Lee Hwang_TFSC.html",
    "title": "The influence of giant platform on content diversity",
    "section": "",
    "text": "Highlights\n\nExpansion of platform do reduce the internal content diversity\nHigh content diversity increases in innovation capacity of the platform ecosystem.\nRetaining their uniqueness of content is positively correlated with content diversity.\nUsers’ will to find other sources of content recovers decreased content diversity.\n\n\nAbstract\nScholars have studied how giant platforms influence on the innovation ecosystem as platform providers (PPs) has grown in large size because of their unique and necessary position in the ICT ecosystem. To find the influence of giant PPs’ market power on diversity of CPs and the role of content diversity in the innovation ecosystem, we modeled the platform ecosystem and examined the effect of a PP’s expansion on various outcomes by using a simulation model based on genetic algorithm. Results show that, firstly, a PP’s expansion do reduce the internal content diversity, secondly, high content diversity influences on increasing in innovation capacity of the platform ecosystem, thirdly, the ratio of CPs that retain their uniqueness of content is positively correlated with content diversity, and lastly, the users’ will to find alternative sources of content aids in the recovery of decreased content diversity. This might contribute for policy makers and stakeholders in the platform ecosystem to understand the overall picture of the industry dynamics and the innovation strategy in ecological perspective."
  },
  {
    "objectID": "research/published/2018_Lee Shin Hong_TI.html",
    "href": "research/published/2018_Lee Shin Hong_TI.html",
    "title": "Does Social Media Use Really Make People Politically Polarized?",
    "section": "",
    "text": "Abstract\nTo help inform the debate over whether social media is related to political polarization, we investigated the effects of social media use on changes in political view using panel data collected in South Korea (N=6411) between 2012 and 2016. We found that, although there were no direct effects of social media use, social media indirectly contributed to polarization through increased political engagement. Those who actively used social network sites were more likely to engage in political processes, which led them to develop more extreme political attitudes over time than those who did not use social network sites. In particular, we observed a clear trend toward a more liberal direction among both politically neutral users and moderately liberal users. In this study, we highlight the role of social media in activating political participation, which eventually pushes the users toward the ideological poles. The implications of these findings are discussed."
  },
  {
    "objectID": "research/published/2019_Kim and Lee_APS.html",
    "href": "research/published/2019_Kim and Lee_APS.html",
    "title": "Does Working Long Hours Cause Marital Dissolution?",
    "section": "",
    "text": "Abstract\nDespite its important implications, little is known about the possible impact on marital dissolution of workweek standards, which set the maximum working hours for full-time workers and may, therefore, reduce their likelihood of working long hours. Moreover, evidence on the effect of working hours on marital dissolution comes predominantly from non-causal studies on Western women’s work status. The Korean government reduced its workweek standard from 44 to 40 hours between 2004 and 2011. A discrete-time event history analysis of longitudinal data from the 2000 to 2015 Korea Labor and Income Panel Study shows that this reduction lowered male workers’ risk of divorce. The estimated effect is large in absolute size, and we speculate about possible explanations. We cautiously call for further attention to be paid to the plausible causal link between men’s overwork and marital dissolution in the work-oriented and gender-divided East Asian societies."
  },
  {
    "objectID": "research/published/2019_Lee Kogler Lee_IPM.html",
    "href": "research/published/2019_Lee Kogler Lee_IPM.html",
    "title": "Capturing Information on Technology Convergence, International Collaboration, and Knowledge Flow from Patent Document",
    "section": "",
    "text": "Abstract\nIn addressing persistent gaps in existing theories, recent advances in data-driven research approaches offer novel perspectives and exciting insights across a spectrum of scientific fields concerned with technological change and the socio-economic impact thereof. The present investigation suggests a novel approach to identify and analyze the evolution of technology sectors, in this case, information and communications technology (ICT), considering international collaboration patterns and knowledge flows and spillovers via information inputs derived from patent documents.\nThe objective is to utilize and explore information regarding inventors’ geo-location, technology sector classifications, and patent citation records to construct various types of networks. This, in turn, will open up avenues to discover the nature of evolutionary pathways in ICT trajectories and will also provide evidence of how the overall ICT knowledge space, as well as directional knowledge flows within the ICT space, have evolved differently. It is expected that this data-driven inquiry will deliver intuitive results for decision makers seeking evidence for future resource allocation and who are interested in identifying well-suited collaborators for the development of potential next-generation technologies. Further, it will equip researchers in technology management, economic geography, or similar fields with a systematic approach to analyze evolutionary pathways of technological advancements and further enable exploitation and development of new theories regarding technological change and its socio-economic consequences."
  },
  {
    "objectID": "research/published/2020_Park et al_SS.html",
    "href": "research/published/2020_Park et al_SS.html",
    "title": "The Effects of Popularity Metrics in News Comments on the Formation of Public Opinion",
    "section": "",
    "text": "Abstract\nThe influence of online comment sections on the news has increased based on the development of collective online behaviors in the digitalized news media era. In this study, we focus on the effect of comment order (e.g., sorting comments by the number of likes or by the time of posting) on the formation of public opinion. We explore whether reading comments sorted by number of likes (a) induces more comments from users, (b) increases the expression of user opinions in response to others’ comments through the action of liking or disliking comments and (c) consolidates user opinion. For the empirical verification of the effects of popularity metrics, we chose a common topic (increasing minimum wage), collected actual data (reviewing 3,251 articles and the numbers of associated comments, likes, and dislikes), and compared news categories based on the existence of popularity metrics. Semantic network analysis was conducted with UCINET and python for K-means clustering, and cosine similarity. Our results show how the comment order in the internet news environment affects the commenting behavior of news consumers."
  },
  {
    "objectID": "research/published/2021_Kim Lee Lee_TSAM.html",
    "href": "research/published/2021_Kim Lee Lee_TSAM.html",
    "title": "Which innovation type is better for production efficiency?",
    "section": "",
    "text": "Abstract\nInnovation has been recognised as essential in a firm’s success, but not all firms can actively engage in innovation activities due to their limited resources and the potential risks involved in such activities. In such a situation, a better understanding of the difference in the effects of the different innovation types is needed. However, little is known about the differences in technical efficiency in terms of innovation types. This study compared technical efficiency across different firms that applied different production function technologies differentiated by innovation type. Data from the seventh version of the Workplace Panel Survey of the Korean Labor Institute was used. Manufacturing firms that adopted a single innovation type were included in the final sample, based on the results of an exploratory data analysis. Using stochastic frontier analysis, the efficiencies of the four groups were estimated, and the efficiencies of different production function technologies were compared using meta-frontier analysis. The estimation results indicate that the organisation innovation group showed better efficiency than other groups, while the product/service innovation group exhibited the highest potential in its production technology."
  },
  {
    "objectID": "research/published/2021_Na et al_AJTI.html",
    "href": "research/published/2021_Na et al_AJTI.html",
    "title": "Strategic Groups Emerged by Selecting R&D Collaboration Partners and Firms’ Efficiency",
    "section": "",
    "text": "Abstract\nThis paper highlights an effect of firms’ strategic mix in research and development (R&D) collaboration shown by their accumulated collaboration behaviours on efficiency, which has not received proper attention in the literature, which has focused on network effects – size, diversity – assuming homogeneity of partners. With a strategic perspective, we try to catch differences among partner portfolios at the level of collaboration types. We cluster firms into strategic groups by using the K-means clustering technique based on the firms’ R&D partner selection portfolios and estimate each group’s frontier function and meta-frontier function to measure each firm’s technical efficiency and technology gap ratio. We conduct multiple regressions to identify the factors affecting firms’ efficiency and found there can be a significant difference in firms’ efficiency given the same amount of input when firms have evolved toward different strategic groups determined by their accumulated R&D partner portfolios. The effects can also vary by firms’ R&D scales, sizes, and the industries to which they belong. The current study contributes to enriching the empirical bodies in the strategic group and innovation studies and by giving managerial implications to corporate leaders and policymakers for the smart strategic mix in selecting R&D collaboration partners."
  },
  {
    "objectID": "research/published/2022 Gergo et al EG.html",
    "href": "research/published/2022 Gergo et al EG.html",
    "title": "Technology network structure conditions the economic resilience of regions",
    "section": "",
    "text": "Abstract\nThis article assesses the network robustness of the technological capability base of 269 European metropolitan areas against the potential elimination of some of their capabilities. By doing so, it provides systematic evidence on how network robustness conditioned the economic resilience of these regions in the context of the 2008 economic crisis. The analysis concerns calls in the relevant literature for more in-depth analysis on the link between regional economic network structures and the resilience of regions to economic shocks. By adopting a network science approach that is novel to economic geographic inquiry, the objective is to stress test the technological resilience of regions by utilizing information on the coclassification of CPC (Cooperative Patent Classification) classes listed on European Patent Office patent documents. We find that European metropolitan areas show heterogeneous levels of technology network robustness. Further findings from regression analysis indicate that metropolitan regions with a more robust technological knowledge network structure exhibit higher levels of resilience with respect to changes in employment rates. This finding is robust to various random and targeted elimination strategies concerning the most frequently combined technological capabilities. Regions with high levels of employment in industry but with a vulnerable technological capacity base are particularly challenged by this aspect of regional economic resilience."
  },
  {
    "objectID": "research/published/2022 Kim Kogler Lee Kang_ASAP.html",
    "href": "research/published/2022 Kim Kogler Lee Kang_ASAP.html",
    "title": "Changes in regional knowledge bases and its effect on local labour markets in the midst of transition",
    "section": "",
    "text": "Abstract\nIn the 2000s, the European labour market experienced a number of significant changes including the transition to a more knowledge-intensive economy as well as the introduction of various economic policies (e.g. Eurozone, subsidized jobs, and social tax cuts). In times like these, the role of knowledge, which is essentially the driving force of innovation and thus promoting technological change and economic growth, is shifting due to new labour market conditions. The present study aims to explore how processes of local knowledge bases have been altered in this transformative environment and how these have impacted on local employment growth. The investigation considers three different knowledge bases in conjunction, incl. knowledge size, knowledge creation, and knowledge application. The study is based on an econometric analysis of a panel of 94 France NUTS-3 regions covering the period 1985–2015, utilizing patent data from European Patent Office (EPO) Statistical Patent Database (PATSTAT), and regional data from European Regional Database (ERD). The result shows that the role of knowledge for employment growth has indeed changed towards more specialized inputs in applications while the importance of greater knowledge size remains still important."
  },
  {
    "objectID": "research/published/2022_Jo et al_TI.html",
    "href": "research/published/2022_Jo et al_TI.html",
    "title": "Estimating cost of fighting against fake news during catastrophic situations",
    "section": "",
    "text": "Highlights\n\nThe fake news problem in the national disaster situations is a critical issue.\nThe quantitative effect of fake news from the audience’s perspective is still unknown.\nWe analyzed the willingness-to-payment for the public fact-checking systems.\nA household’s willingness-to-payment is 10,652 KRW (9 USD), on average.\nIndividuals with psychological damage and high reliance on news answer higher payment.\n\n\nAbstract\nAs the battle with COVID-19 continues, an Infodemic problem has been raised. Even though the distribution of false news in national disaster situations has been reported for a long time, little attention has been given to the quantitative research of the fake news problem from the audience’s perspective. This study, therefore, aims to estimate how much tax taxpayers would gladly pay for a virtual public-run fact-checking system. Using a one-and-one-half bounded dichotomous choice contingent valuation method, a survey was conducted on 525 respondents in Korea, and the spike model was applied to distinguish zero willingness-to-pay (WTP). The results show that a household’s WTP for the public fact-checking system is 10,652 KRW (9 USD), on average, in the form of income tax for five years. Given the amount is a regular payment in perpetuity, the total WTP is estimated at 23 billion KRW ($196 M) every year. The result also shows that an individual’s WTP increases as his or her psychological damage caused by fake news is high, as well as his or her high reliance on news in a disaster situation."
  },
  {
    "objectID": "research/published/2022_Park et al_JRCS.html",
    "href": "research/published/2022_Park et al_JRCS.html",
    "title": "Changes in consumption patterns during the COVID-19 pandemic",
    "section": "",
    "text": "Abstract\nPeople tend to alleviate their negative emotions by shopping. Considering the change of shopping behavior during COVID-19 outbreak, negative emotions are the key contributors to this change. In this light, this study aims to investigate how negative emotions caused by COVID-19 affect shopping behaviors. This study classified consumer groups based on their perceived negative emotions (i.e., anxiety, fear, depression, anger, and boredom). By clustering analysis, four groups (i.e., group of anxiety, depression, anger, and indifference) were derived. Then, this study examined how each of the emotional groups differently affect the shopping-related motivations (i.e., mood alleviation, shopping enjoyment, socialization seeking, and self-control seeking) and shopping behaviors (i.e., shopping for high-priced goods and buying of bulk goods). Results revealed all emotional groups affect socialization seeking and influence high-priced shopping intentions. However, depression and indifference are positively associated with socialization seeking and influence bulk shopping intentions. In addition, other emotions except for anxiety affect mood alleviation and influence high-priced shopping intentions. Finally, anger is associated with self-control seeking and affects bulk shopping intentions. This study enables practitioners and researchers to better understand how people control negative emotions by shopping in pandemic situations such as the current COVID-19 crisis."
  },
  {
    "objectID": "research/published/2022_Shim_Lee_Oh_JTAECR.html",
    "href": "research/published/2022_Shim_Lee_Oh_JTAECR.html",
    "title": "Analysis of OTT Users’ Watching Behavior for Identifying a Profitable Niche",
    "section": "",
    "text": "Abstract\nOver-the-top (OTT) firms must overcome the hurdle of the competitive Korean media market to achieve sustainable growth. To do so, understating how users enjoy OTT and analyzing usage patterns is essential. This research aims to empirically identify a profitable niche in the Korean OTT market by applying market segmentation theory. In addition, it investigates an effective content strategy to convert free users into paying customers belonging to profitable niche segments. The latent class regression model was applied to Korean Media Panel Survey data to divide Korean OTT customers into submarkets. According to an empirical analysis, Korean OTT users can be divided into three submarkets based on their OTT usage patterns, with the third segment serving as a profitable niche market. An additional analysis of the profitable niche market revealed that bundling content, such as foreign content, original content, and movies, is a crucial content strategy for increasing paying subscribers in a profitable niche segment."
  },
  {
    "objectID": "research/published/2023_Kim_Lee_JTAECR.html",
    "href": "research/published/2023_Kim_Lee_JTAECR.html",
    "title": "The Return of the King:",
    "section": "",
    "text": "Abstract\nAs the over-the-top (OTT) service market continues to evolve, with new large global players entering the already crowded market, competition between various OTT services for subscribers has intensified. In this study, we aim to investigate the impact of user preference content on the selection of specific OTT services by consumers. Specifically, we employ the conjoint experiment (CE) method to examine consumer utility, relative importance, and marginal willingness to pay (MWTP) for over-the-top (OTT) subscription service attributes. Especially, the presence of users’ killer content and its impact on MWTP is the focus of our study. As a result of calculating the MWTP for each attribute, we found that users are willing to pay about 7633 KRW (5.8 USD) for the first-ranked killer content in their first preferred genre. To gain a deeper understanding of users’ willingness to pay for OTT services, we further analyzed the data by age group and the number of OTT services in use. Based on the results, we suggest strategic plans for local OTT operators to compete effectively in the fiercely competitive OTT market."
  },
  {
    "objectID": "research/published/2023_Liu_et_al_sus.html",
    "href": "research/published/2023_Liu_et_al_sus.html",
    "title": "An Analysis of Factors Influencing the Intention to Use Untact Services by Service Type",
    "section": "",
    "text": "Abstract\nSince COVID-19, social distancing has become common, and the demand for untact services has increased rapidly, resulting in an economic phenomenon centered on untact worldwide. Due to social distancing, the untact service area is expanding not only to shopping but also to online learning, home training, and telemedicine, and untact services are expected to expand to more diverse areas in the future. This study investigates four types of untact services: online lectures, online meetings related to work and study, online seminars, and online performances, and the effects of concerns about untact services on the intention of use have been examined using a path analysis model. As a result of the analysis, the perceived usefulness had a positive effect on the user’s continuous intention to use untact services. However, depending on the type of untact service, it can be confirmed that the factors that affect the intention to continue using the service differ from each other. Practitioners can use the results of this study when designing untact services in the future."
  },
  {
    "objectID": "research/published/kci_2020_is.html",
    "href": "research/published/kci_2020_is.html",
    "title": "TV홈쇼핑 이용자의 상품구매 입문 요인 분석",
    "section": "",
    "text": "Abstract\n본 논문은 TV홈쇼핑 구매 경험이 없는 소비자들 중 1년 후 TV홈쇼핑 구매에 입문한 소비자의 미디어 매체, 연결, 행위로 구성되는 미디어 레퍼토리 패턴을 분석하고, 해당 소비자가 어떤 요인으로 인해 TV홈쇼핑에서 상품을 처음 구매하게 되는지에 대한 요인을 랜덤포레스트와 의사결정나무 알고리즘을 활용하여 분석하였다. 사용 가능한 171개의 잠재 변수를 분석에 활용하여 랜덤포레스트 알고리즘이 데이터를 학습하여 기존 문헌에서 발견되지 않았던 새로운 요인들을 발견 할 수 있었다. 예를 들어, 개인정보에 대한 우려 정도, 영화관에서의 지출 정도, 적극적 정보 탐색자 여부와 같은 요인들도 TV홈쇼핑 구매로 이어지는 중요한 요인이라는 것을 보였다. 분석에서는 연령별, 성별로 총 12개의 하위 샘플을 나누고, 각각의 샘플에 대하여 미디어 레퍼토리 관점에서 TV홈쇼핑 입문 요인을 살펴보았다. 이후, 샘플별로 선정된 상위 30개 요인들만을 가지고 최종 의사결정나무를 구성하여 TV홈쇼핑 구매 입문 과정을 최종 분류해 보았다. 본 연구는 기존의 이론주도 요인 분석에 데이터주도 방식을 혼합하여 향후 미디어 상품과 서비스에 대한 연구가 나아갈 방향을 제시한다."
  },
  {
    "objectID": "research/published/kci_2021_is.html",
    "href": "research/published/kci_2021_is.html",
    "title": "미국 ICT기업의 국제특허분류 공동출현 네트워크 특성에 관한 연구",
    "section": "",
    "text": "Abstract\n4차산업혁명 시대가 도래하면서 사물인터넷, 인공지능, 빅데이터 등 관련 기술에 관한 관심이 증가하고 있다. 기술은 혁신을 통해 생성되기 때문에 기업은 기술혁신을 통한 기업성장에 주목해야 한다. 기업의 기술혁신을 통해 생성된 지식은 생산성을 향상하는 등 기업성장에 중심적인 역할을 한다. 미국의 4대 ICT 기업이자 미래를 선도하는 기업인 MAGA(Microsoft, Apple, Google, Amazon)는 특허를 통해 지식재산을 축적하였다. 본 연구는 과연 미국 ICT 산업을 선도하는 MAGA가 가지는 기술유사성은 어느 정도인지를 확인하고 각 기업이 가지는 기술응용성을 확인하고자 한다. MAGA가 보유한 기술을 국제특허분류(IPC, international patent classification)를 통해 어떤 기술들이 다른 기술들과 결합하는지 살펴보고, 기업별로 기술응용에 나타나는 차이를 살펴보았다. MAGA의 기술유사성을 확인하기 위해 본 연구는 IPC 공동출현 네트워크를 이용하였다. MAGA로 한정한 IPC 표본의 수는 등록 및 출원특허를 합한 68,732개이며, 이 정보는 빈도수 분석과 네트워크 분석을 통해 검증하였다. 기업별 IPC는 빈도순으로 상위 10개를 기업끼리 짝(pair)을 이루어서 공통으로 사용하고 있는 기술을 살펴보았다. 기업별 중심성(centrality)을 정도(degree) 기준으로 상위 10개의 IPC를 추출하여 유사성(similarity)을 확인하고 엣지(edge) 값으로 기술들의 연관성을 보았다. 분석결과 MAGA가 공통으로 가장 많이 등록, 출원한 기술은 G06F으로 전자 정보처리(electrical digital data processing)를 가장 많이 사용하는 것으로 나타나며 나머지 기술들도 대부분 유사한 것으로 검증되었다."
  },
  {
    "objectID": "research/published/kci_2022_cyb.html",
    "href": "research/published/kci_2022_cyb.html",
    "title": "중국의 애국주의와 한류 팬덤 간의 사이버 갈등",
    "section": "",
    "text": "Abstract\n중국의 애국주의가 인터넷상에서 점차 극단적으로 변질됨에 따라 한류 팬덤과 온라인 애국주의 간의 이념적 갈등이 빈번하게 발생하고 있다. 이러한 상황에서 본 연구는 BTS 리더 RM의 ‘밴 플리트 상’ 수상소감과 관련된 중국 네티즌의 반응을 관찰하여 중국의 팬덤 애국주의와 한류 팬덤 간의 갈등 발생 심리를 분석하고, 이를 통해 팬덤 애국주의의 모순을 드러내고자 한다. 기존의 중국 극단 애국주의 연구가 주로 표현 방식의 특징과 문제점에만 초점을 맞추었다면, 본 연구에서는 중국의 글로벌 팬덤과 애국주의자 간의 갈등을 포함하였고 애국주의의 원인 또한 국가적 측면과 사회적 측면을 총체적으로 고려하였다. 분석 단계에서는 2020년 10월 11일부터 2021년 2월 21일까지 RM의 발언과 관련된 ‘신나 웨이보’ 게시물의 댓글 214개를 수집하였으며, 단어 빈도에 대한 통계 분석과 함께 질적 텍스트 분석을 수행해 애국주의 표현의 심리적 특성을 살펴보고 애국주의 팬덤의 심층적 동기, 집단 정체성에 대한 인식, 다른 집단에 대한 태도 등을 분석하였다. 그 결과 애국주의 팬덤이 중국 내부의 분열과 상호비방을 확대한다는 것과 다른 민족⋅국가 간의 원한을 살 수 있다는 해석을 도출하였다. 이는 두 가지 상반된 사상이 내면에서 충돌했을 때, 사안의 복잡성과 불확실성에 맞서 서로 대립한 관점을 이해하기보다 상황을 이분법적으로 바라보고 한 가지를 채택함으로써 스스로를 불확실성이나 모순 속에 두지 않게 하려는 심리가 그 기저에 있기 때문이라고 해석할 수 있다. 이러한 사고 방식은 소수의 의견이 형성되는 것을 초기에 말살하여 중국 애국주의가 본래 담고자 했던 평화와 공존에 대한 가치를 훼손하고 온라인상에서 분열을 조장하는 등의 부작용을 낳는다."
  },
  {
    "objectID": "research/published/kci_2022_ism_2.html",
    "href": "research/published/kci_2022_ism_2.html",
    "title": "비대면 상황에서의 온라인 커뮤니티 활동이 삶 만족도에 미치는 영향",
    "section": "",
    "text": "Abstract\n본 연구는 코로나19 펜데믹 전인 2017년과 이후인 2021년의 한국미디어패널 데이터를 활용하여 사회적 자본 관점에서 온라인 커뮤니티 활동과 삶의 만족도에 미치는 영향에 대해 연령과 시기를 비교분석하였다. 전국 조사대상자 중 만13세 이상을 대상으로 수집된 2017년(8,907명)과 2021년(9,820명)의 응답 데이터를 분석하여 온라인 커뮤니티 활동과 사회적 자본 간의 관계를 밝히고, 각 시기, 연령 간 삶의 만족도에 어떠한 차이가 나타나는지 분석하였다. 분석결과, 결속적 온라인 활동을 많이 할수록 삶의 만족도에 정(+)의 영향을 미치고, 연령의 조절효과는 결속적 온라인 활동에서만 통계적으로 유의하게 삶의 만족도에 영향을 미치는 것으로 나타났다. 이전의 선행연구에서 온라인 활동과 개인의 사회적 자본에 대해 논의되어 왔지만, 본 연구를 통해 비대면 상황과 대면 상황에서 연도별로 비교함으로써 삶의 만족도와 연령 간 차이, 시기별 차이를 다루며 사회적 자본의 확장 측면에서 실증적으로 다루었다는 데에 의의가 있다. 이러한 실증적 연구를 통해 미디어 이용 변화와 커뮤니케이션 유형의 변화 등을 고찰하여 비대면 상황에서의 사회적 상호작용이 온라인을 매개로 확산된다는 점에서 인적 네트워크 형성과 사회자본 축적의 가능성을 제시한다는 점에서 의의가 있을 것이다."
  },
  {
    "objectID": "teaching/comvis/index.html",
    "href": "teaching/comvis/index.html",
    "title": "Computer Vision and Unstructured Data Analysis for Social Science Research",
    "section": "",
    "text": "Introduction\nIn today’s digital age, vast amounts of data are being generated in the form of text, sound, image, and video. Unstructured data, in particular, holds tremendous potential for social science research, as it can reveal insights that would be difficult or impossible to obtain through traditional methods. However, working with unstructured data requires specialized skills and techniques, which many social scientists may not possess.\nThis course, Unstructured Data Analysis for Social Science Research, is designed to equip students with the knowledge and skills needed to effectively analyze unstructured data. Throughout the 15-week course, students will learn how to collect, preprocess, and analyze text, sound, image, and video data, and how to integrate different forms of unstructured data to gain deeper insights. They will also explore ethical considerations related to using unstructured data and best practices for presenting findings.\nBy the end of this course, students will have a deep understanding of the potential of unstructured data in social science research and the skills needed to analyze it. They will be able to apply their knowledge to real-world problems, and they will have a portfolio of projects demonstrating their ability to work with unstructured data. The course is suitable for graduate students and researchers in social sciences who want to expand their research methods and explore new avenues for analysis, as well as professionals who work with data in a social science context.\n\n\n\nSyllabus\nWeek 1: Introduction to unstructured data analysis in social science research\n\nOverview of unstructured data and its relevance in social science research\nUnderstanding the different forms of unstructured data\nEthical considerations in using unstructured data\n\nWeek 2: Introduction to text data analysis\n\nCollecting and preprocessing text data\nText mining techniques for exploratory analysis\nSentiment analysis and topic modeling\n\nWeek 3: Advanced text data analysis\n\nNamed entity recognition and extraction\nText classification and clustering\nDeep learning for text analysis\n\nWeek 4: Introduction to sound data analysis\n\nCollecting and preprocessing sound data\nSound visualization and analysis\nFeature extraction techniques\n\nWeek 5: Advanced sound data analysis\n\nMusic information retrieval\nSpeech recognition and sentiment analysis\nDeep learning for sound analysis\n\nWeek 6: Introduction to image data analysis\n\nCollecting and preprocessing image data\nImage visualization and analysis\nFeature extraction techniques\n\nWeek 7: Advanced image data analysis\n\nObject detection and recognition\nImage classification and clustering\nDeep learning for image analysis\n\nWeek 8: Introduction to video data analysis\n\nCollecting and preprocessing video data\nVideo visualization and analysis\nFeature extraction techniques\n\nWeek 9: Advanced video data analysis\n\nAction recognition and detection\nVideo classification and clustering\nDeep learning for video analysis\n\nWeek 10: Data integration and fusion\n\nCombining different forms of unstructured data\nFusion techniques for unstructured data analysis\nCase studies on integrated data analysis\n\nWeek 11: Social media data analysis\n\nCollecting and preprocessing social media data\nSentiment analysis and opinion mining\nNetwork analysis and visualization\n\nWeek 12: Web data analysis\n\nWeb scraping and preprocessing\nWeb content analysis and classification\nWeb usage and access analysis\n\nWeek 13: Geospatial data analysis\n\nGeospatial data collection and preprocessing\nGeospatial data visualization and analysis\nGeospatial data fusion with other unstructured data\n\nWeek 14: Visualization and presentation of unstructured data analysis\n\nVisualization techniques for unstructured data\nStorytelling with unstructured data\nBest practices for presenting unstructured data analysis\n\nWeek 15: Project presentation and feedback\n\nStudents present their project work and receive feedback from the instructor and peers\nWrap-up and future directions in unstructured data analysis for social science research"
  },
  {
    "objectID": "teaching/cul_tech/icpbl/index.html",
    "href": "teaching/cul_tech/icpbl/index.html",
    "title": "PBL Project",
    "section": "",
    "text": "목표\n협력 기업에서 주어진 문제 시나리오를 수업 시간에 배운 문화 사업과 미디어에 대한 이해를 바탕으로 팀 별로 해결을 위한 아이디어를 고안해본다. 인터뷰, 설문, 데이터 분석 등 모든 방법을 동원하여 실제 기업의 문제를 풀어 봄으로써 문화 산업과 기술에 대한 이해를 높이고 현장의 멘토링과 평가를 통해 현장감을 높이고 실전 경험을 갖춘다.\n\n\n\n협력 기업\n\nTEO 유니버스\n\n담당: 이동찬 경영총괄 (dclee@teouniverse.kr)\n유튜브 계정: https://www.youtube.com/@TEO_universe\n인스타 계정: https://www.instagram.com/teo.universe/\n\n(주)ASSI\n\n담당: 윤영훈 대표이사 (younghoon.yun@my-assi.com)\n독서 어플 리더스: https://withreaders.com/\nGoogle Play Store [download]\nAPPLE App Store [download]\n\n\n\n\n\nTeam Assignment\n\nRandom assignment\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n김근아\n김명진\n김세린\n김재희\n등시우\n왕일범\n윤동주\n이소언\n\n\n임서영\n최지현\n최진영\n강태현\n고재희\n김가희\n김민기\n김민우\n\n\n신소망\n양제니\n염다인\n왕욱경\n윤지민\n이자의\n이채은\n장현군\n\n\n채자선\n최은서\n최혜린\n김근아\n김승연\n김승주\n김지훈\n박지원\n\n\n박혜원\n왕자문\n유사여\n이수연\n임연우\n정서윤\n \n \n\n\n\n\n\n\n\n시나리오\n시나리오 1: 엔터테인먼트 기업\n\nTEO는 많은 채널과 OTT 중 어느 곳과 전략적 관계를 강화하는 것이 유의미할지? 그 이유는?\n넷플릭스의 경우 고정 수익을 게런티해주지만 제작사로서 작품 IP는 확보하지 못하는 한계가 있는데 넷플릭스와 중장기적으로 함께할 경우 사업적 장단점은 무엇일지?\nTEO가 글로벌 시장에서 인정받을 수 있는 콘텐츠를 제작하기 위해서 준비해야할 요소는 무엇일지? 글로벌 흥행과 국내 흥행 중 무엇이 먼저일지?\n\n\n시나리오 2: 문화 기술 스타트업\n\n책의 소비자 입장\n\n좋은 책이란 무엇일까?\n책을 선택하게 되는 방법은 무엇이 있을까?\n좋은 책을 고르는 방법은 어떤 것들이 있을까?\n그 과정을 기술(IT, AI 등)로 풀어낼 수 있을까? (이 과정에서 더 필요한 것들이 있다면 어떤 것이 필요할까?)\n그 과정에 BM을 붙일 수 있을까?\n\n책의 공급자(저자/출판사/유통사) 입장\n\n책을 제작하고 유통하는 이해관계자들의 경제적 해자는 어디로 부터 기인하는가?\n1) 기술의 발전 2) 트렌드 변화가 영향을 끼치는 것들이 있을까?\n어떤 변화와 기회를 잡는 사람이 주도권을 잡게 될 것인가? (책의 공급자에게 중요해지는/필요한 역량은 무엇일까?)\n\n벤치마킹/브레인스토밍: 다른 산업에서 힌트 얻기\n\n리더스 서비스에서, 독서 트레이너(PT)를 출시하기 위해, 트레이너가 있는 운동 산업에서 힌트를 얻어보고자 한다.\n헬스를 할때 PT를 받는 사람과 그렇지 않은 있다. PT를 받는 사람과 그렇지 않은 사람의 의사결정은 어떤 것에 의해 이루어 지는가?\n선생님(트레이너)과 함께하는 것이 일반적인 운동과 그렇지 않은 운동들이 있다. 무엇이 차이를 만드는가?\n위에 조사를 통해 얻은 시사점을 “독서”라는 행위에 접목해서 서비스 요소(feature)를 설계해보기\n\n\n\n\n\nPBL 주차별 진행:\n\nW4. Team assignment (4~5 students in a team)\nW5 ~ W7: Team building\nW8 ~ W9: Special lecture & Problem description (문제 제공 기업)\nW10 ~ W13: Team Activity in class (수업 중 진행)\nW14: Team activity with mentoring (수업 또는 현장)\nW16: Final project presentation (현장 평가 참여)"
  },
  {
    "objectID": "teaching/cul_tech/index_old.html",
    "href": "teaching/cul_tech/index_old.html",
    "title": "Culture & Technology",
    "section": "",
    "text": "Introduction\nCulture & Technology is an interdisciplinary course designed to explore the complex relationship between culture and technology, and the ways in which technological innovations have shaped and continue to shape our societies. The course will cover a wide range of topics, from the historical perspective of technology’s impact on culture to the contemporary issues raised by the rapid pace of technological change.\nThroughout the course, students will delve into the influences of technology on various aspects of culture, such as art, education, communication, identity, work, and the environment. The course will also address critical concerns like the digital divide, ethical considerations, and the role of regulation and policy in shaping technology.\nBy integrating a liberal arts perspective with technical insights, this course aims to foster a deeper understanding of the interplay between culture and technology, encouraging students to think critically about the role technology plays in our lives and to consider the ethical and societal implications of technological advancements.\nThroughout the course, students should engage in discussions, group projects, and individual assignments that explore the relationship between culture and technology. Encourage students to think critically about the role technology plays in shaping our cultural, social, and individual experiences, and to consider the ethical and societal implications\n\n\n\nSyllabus\nWeek 1: Introduction to Culture & Technology\n\nDefining culture and technology\nHistorical perspective on technology and its influence on culture\nThe role of technology in shaping human societies\n\nWeek 2: Technological Innovations and Cultural Shifts\n\nKey technological innovations throughout history\nThe impact of inventions on cultural, social, and economic changes\nThe role of technology in globalization\n\nWeek 3: The Internet and Digital Culture\n\nThe history of the internet and its impact on culture\nThe emergence of digital culture\nOnline communities, social media, and virtual worlds\n\nWeek 4: Communication Technologies and Media\n\nEvolution of communication technologies\nThe impact of media on culture (radio, television, and the internet)\nThe rise of new media and its effects on traditional media\n\nWeek 5: Art and Technology\n\nThe influence of technology on artistic expression\nDigital art forms and multimedia\nTechnology in the performing arts\n\nWeek 6: Technology and Education\n\nThe role of technology in education and learning\nThe impact of technology on traditional educational institutions\nOnline learning, MOOCs, and the future of education\n\nWeek 7: Technology and Language\n\nThe influence of technology on language and communication\nThe impact of technology on linguistic diversity\nLanguage technologies and natural language processing\n\nWeek 8: Midterm Exam Week\nWeek 9: Technology and Identity\n\nThe role of technology in shaping individual and collective identities\nOnline identity, anonymity, and privacy\nThe impact of technology on personal relationships and social interactions\n\nWeek 10: Artificial Intelligence and Society\n\nIntroduction to artificial intelligence (AI)\nThe societal impact of AI and automation\nEthical considerations in AI development and deployment\n\nWeek 11: Technology and Work\n\nThe influence of technology on work and employment\nTechnological innovations and the changing nature of work\nThe gig economy, remote work, and the future of work\n\nWeek 12: Technology and the Environment\n\nThe impact of technology on the environment and natural resources\nSustainable technologies and green innovations\nBalancing technological development with environmental responsibility\n\nWeek 13: Digital Divide and Inequality\n\nThe digital divide and its implications for social equality\nThe role of technology in perpetuating or alleviating social inequalities\nStrategies for fostering digital inclusion and equal access to technology\n\nWeek 14: Ethics, Regulation, and Policy in Technology\n\nEthical considerations in the development and use of technology\nThe role of regulation and policy in shaping technology\nBalancing innovation with the need for responsible technological development\n\nWeek 15: Final Exam Week"
  },
  {
    "objectID": "teaching/data_journalism/index.html",
    "href": "teaching/data_journalism/index.html",
    "title": "Data Journalism",
    "section": "",
    "text": "Introduction\nWelcome to our Data Journalism course, an exciting and comprehensive journey designed to equip you with the skills necessary to become a proficient data journalist. Over 15 weeks, you will learn about the fundamentals of journalism and data-driven storytelling, master data manipulation and visualization techniques using R and Tableau, and delve into ethical and legal considerations.\nThroughout the course, you’ll develop a strong foundation in identifying newsworthy stories, conducting interviews, and fact-checking information. You will also gain hands-on experience in data cleaning, preprocessing, exploratory analysis, and various data visualization techniques, including advanced chart types and interactivity.\nBy the end of this course, you will have completed a data journalism project incorporating data analysis, visualization, and journalistic storytelling. With a solid understanding of the concepts and tools covered, you’ll be well-prepared to apply your skills in the ever-evolving field of data journalism.\n\n\n\nSyllabus\nWeek 1: Introduction to Journalism and Data Journalism\n\nWhat is Journalism?\nFundamentals of news writing and reporting\nThe importance of data-driven storytelling\nWhat is Data Journalism?\nOverview of tools: R, Tableau, and others\n\nWeek 2: Finding and Evaluating News Stories\n\nIdentifying newsworthy stories\nGenerating story ideas\nEvaluating story angles and potential impact\nSourcing data for stories\nEvaluating data quality and credibility\n\nWeek 3: Interviewing and Fact-Checking\n\nPrinciples of journalistic interviewing\nPreparing for and conducting interviews\nFact-checking and verifying information\nEthical considerations in interviewing and reporting\n\nWeek 4: Data Cleaning and Preprocessing\n\nIntroduction to data manipulation in R (tidyverse)\nData cleaning, filtering, and aggregation\nData transformation and handling missing data\nData normalization and scaling\n\nWeek 5: Descriptive Statistics and Exploration\n\nDescriptive statistics in R\nExploratory data analysis (EDA) with R\nIdentifying trends, patterns, and outliers\nAsking the right questions\n\nWeek 6: Introduction to Data Visualization with R\n\nIntroduction to ggplot2\nGrammar of graphics with ggplot2\nCustomizing plots: themes, scales, labels, and titles\nDifferent types of plots and when to use them\n\nWeek 7: Advanced Data Visualization with R\n\nAdvanced ggplot2 techniques\nFaceting and multi-panel plots\nTime series and geospatial data visualization\nInteractive visualizations with plotly or ggplotly\n\nWeek 8: Introduction to Tableau\n\nTableau interface and basics\nConnecting Tableau to data sources\nCreating and customizing visualizations in Tableau\n\nWeek 9: Advanced Data Visualization with Tableau\n\nAdvanced chart types and techniques in Tableau\nCreating dashboards and stories in Tableau\nInteractive and dynamic visualizations in Tableau\nGeospatial data visualization in Tableau\n\nWeek 10: Combining R and Tableau\n\nExporting R data and visualizations to Tableau\nIntegrating R scripts within Tableau\nLeveraging the strengths of both tools for data journalism\n\nWeek 11: Crafting Compelling News Stories\n\nStructuring news articles and data-driven stories\nBalancing text and visualizations\nWriting clear and concise news copy\nIncorporating quotes and interview material\nEnsuring accuracy and transparency\n\nWeek 12: Digital Writing and Interactive Media with Shiny and Quarto\n\nIntroduction to Shiny and Quarto for interactive media\nCreating Shiny apps and Quarto websites for digital storytelling\nEmbedding data visualizations and interactive elements\nBest practices for designing engaging and accessible digital content\n\nWeek 13: Legal and Ethical Considerations\n\nData privacy and security\nCopyright and licensing issues\nResponsible data reporting and fact-checking\nAvoiding bias and promoting inclusivity in journalism\n\nWeek 14: Team project consultation\nWeek 15: Project presentation\n\nStudents present their data journalism projects\nProjects should incorporate data analysis, visualization, and journalistic storytelling\nFeedback and discussion on projects\nReflecting on the course and potential future applications in the field of data journalism"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Courses",
    "section": "",
    "text": "Mon 15:00 (3h) 실감미디어문화테크놀로지 (Immersive Media & CT)\nTue 09:00 (3h) 문화데이터와머신러닝 (ML101)\nWed 09:00 (3h) HCI기초통계와데이터사이언스 (STAT101)"
  },
  {
    "objectID": "teaching/index.html#undergraduate",
    "href": "teaching/index.html#undergraduate",
    "title": "Courses",
    "section": "Undergraduate",
    "text": "Undergraduate\n\n\n\n\n\n\n\n\n\n\nCultural Industry & Data Analytics\n\n\nData Science 101\n\n\n\n\n\n\n\n\n\n\n\n\n\nCultural Data & Machine Learning\n\n\nMachine Learning 101\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#graduate",
    "href": "teaching/index.html#graduate",
    "title": "Courses",
    "section": "Graduate",
    "text": "Graduate\n\n\n\n\n\n\n\n\n\n\nSTAT101\n\n\nHCI기초통계와데이터사이언스\n\n\n\n\n\n\n\n\n\n\n\n\n\nImmersive Media & CT\n\n\n실감미디어문화테크놀로지\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html",
    "href": "teaching/media_ds/about/ggplot.html",
    "title": "Data Visualization with ggplot2 in R",
    "section": "",
    "text": "ggplot2 is a powerful data visualization package in R that allows you to create complex and aesthetically pleasing visualizations using a simple and consistent syntax. This course aims to provide a detailed guide to ggplot2, from basic concepts to advanced techniques, along with hands-on practice to help you master this versatile package."
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#import-libraries",
    "href": "teaching/media_ds/about/ggplot.html#import-libraries",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Import libraries",
    "text": "Import libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#key-components",
    "href": "teaching/media_ds/about/ggplot.html#key-components",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Key components",
    "text": "Key components\nEvery ggplot2 plot has three key components:\n\ndata,\nA set of aesthetic mappings between variables in the data and visual properties, and\nAt least one layer which describes how to render each observation. Layers are usually created with a geom function.\n\nHere's a simple example:\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n\n\n\n\nColor, size, shape and other aesthetic attributes\n\naes(displ, hwy, colour = class)\naes(displ, hwy, shape = drv)\naes(displ, hwy, size = cyl)\n\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\n\n\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point(aes(colour = \"blue\"))\n\n\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point(colour = \"blue\")\n\n\n\n\n\n\nFaceting\n\nggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  facet_wrap(~class)"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#section",
    "href": "teaching/media_ds/about/ggplot.html#section",
    "title": "Data Visualization with ggplot2 in R",
    "section": "",
    "text": "One variable (Discrete)\n\nb &lt;- ggplot(mpg, aes(fl))\nb + geom_bar()"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#one-variable-cont.",
    "href": "teaching/media_ds/about/ggplot.html#one-variable-cont.",
    "title": "Data Visualization with ggplot2 in R",
    "section": "One variable (Cont.)",
    "text": "One variable (Cont.)\n\na &lt;- ggplot(mpg, aes(hwy))\n\na + geom_area(stat = \"bin\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\na + geom_density(kernel = \"gaussian\")\n\n\n\na + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\na + geom_freqpoly()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\na + geom_histogram(binwidth = 5)"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#two-variables-cont.-cont.",
    "href": "teaching/media_ds/about/ggplot.html#two-variables-cont.-cont.",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Two variables (Cont. & Cont.)",
    "text": "Two variables (Cont. & Cont.)\n\nf &lt;- ggplot(mpg, aes(cty, hwy))\nf + geom_blank()\n\n\n\nf + geom_jitter()\n\n\n\nf + geom_point()\n\n\n\n# install.packages(\"quantreg\")\nlibrary(quantreg)\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nf + geom_quantile() + \n  geom_jitter()\n\nSmoothing formula not specified. Using: y ~ x\n\n\n\n\nf + geom_rug(sides = \"bl\") + \n  geom_jitter()\n\n\n\nf + geom_rug(sides = \"bl\") + \n  geom_point()\n\n\n\nf + geom_smooth(model = lm) +  \n  geom_point()\n\nWarning in geom_smooth(model = lm): Ignoring unknown parameters: `model`\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nf + geom_text(aes(label = cty)) + \n  geom_jitter()\n\n\n\nf + geom_text(aes(label = fl))\n\n\n\n# install.packages(\"ggimage\")\nlibrary(ggimage)\n\nimg &lt;- list.files(system.file(\"extdata\", \n                              package=\"ggimage\"),\n                  pattern=\"png\", full.names=TRUE)\n\nf + geom_image(aes(image=img[2]))"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#two-variables-discrete-cont.",
    "href": "teaching/media_ds/about/ggplot.html#two-variables-discrete-cont.",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Two variables (Discrete & Cont.)",
    "text": "Two variables (Discrete & Cont.)\n\ng &lt;- ggplot(mpg, aes(class, hwy))\n\nlevels(as.factor(mpg$class))\n\n[1] \"2seater\"    \"compact\"    \"midsize\"    \"minivan\"    \"pickup\"    \n[6] \"subcompact\" \"suv\"       \n\nstr(mpg$class)\n\n chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nlevels(as.factor(mpg$class))\n\n[1] \"2seater\"    \"compact\"    \"midsize\"    \"minivan\"    \"pickup\"    \n[6] \"subcompact\" \"suv\"       \n\nunique(mpg$class)\n\n[1] \"compact\"    \"midsize\"    \"suv\"        \"2seater\"    \"minivan\"   \n[6] \"pickup\"     \"subcompact\"\n\ng + geom_bar(stat = \"identity\")\n\n\n\ng + geom_boxplot() \n\n\n\n# Let's specify some cars\nmpg %&gt;% \n  select(manufacturer, class, hwy) %&gt;% \n  group_by(class) %&gt;% \n  arrange(desc(hwy)) %&gt;% \n  head(10) -&gt; text_in_graph\ntext_in_graph\n\n# A tibble: 10 × 3\n# Groups:   class [2]\n   manufacturer class        hwy\n   &lt;chr&gt;        &lt;chr&gt;      &lt;int&gt;\n 1 volkswagen   compact       44\n 2 volkswagen   subcompact    44\n 3 volkswagen   subcompact    41\n 4 toyota       compact       37\n 5 honda        subcompact    36\n 6 honda        subcompact    36\n 7 toyota       compact       35\n 8 toyota       compact       35\n 9 honda        subcompact    34\n10 honda        subcompact    33\n\ng + geom_boxplot() +\n  geom_text(data=text_in_graph, \n            aes(label = manufacturer))\n\n\n\ng + geom_dotplot(binaxis = \"y\",\n                 stackdir = \"center\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\ng + geom_violin(scale = \"area\")"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#two-variables-discrete-discrete",
    "href": "teaching/media_ds/about/ggplot.html#two-variables-discrete-discrete",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Two variables (Discrete & Discrete)",
    "text": "Two variables (Discrete & Discrete)\n\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nh &lt;- ggplot(diamonds, aes(cut, color))\nh + geom_jitter()"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#continuous-bivariate-distribution",
    "href": "teaching/media_ds/about/ggplot.html#continuous-bivariate-distribution",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Continuous Bivariate Distribution",
    "text": "Continuous Bivariate Distribution\n\n# install.packages(\"ggplot2movies\")\nlibrary(ggplot2movies)\ni &lt;- ggplot(movies, aes(year, rating))\ni + geom_bin2d(binwidth = c(5, 0.5))\n\n\n\ni + geom_density2d()\n\n\n\n# install.packages(\"hexbin\")\nlibrary(hexbin)\ni + geom_hex()"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#continuous-functions-time-series",
    "href": "teaching/media_ds/about/ggplot.html#continuous-functions-time-series",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Continuous functions (time-series)",
    "text": "Continuous functions (time-series)\n\nj &lt;- ggplot(economics, aes(date, unemploy))\nj + geom_area()\n\n\n\nj + geom_line()\n\n\n\nj + geom_step(direction = \"hv\")"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#visualizing-bars-with-errors",
    "href": "teaching/media_ds/about/ggplot.html#visualizing-bars-with-errors",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Visualizing bars with errors",
    "text": "Visualizing bars with errors\n\n# Visualizing error\ndf &lt;- data.frame(grp = c(\"A\", \"B\"), fit = 4:5, se = 1:2)\nk &lt;- ggplot(df, aes(grp, fit, ymin = fit-se, ymax = fit+se))\n\nk + geom_crossbar(fatten = 2)\n\n\n\nk + geom_errorbar()\n\n\n\nk + geom_linerange()\n\n\n\nk + geom_pointrange()"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#three-variables",
    "href": "teaching/media_ds/about/ggplot.html#three-variables",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Three variables",
    "text": "Three variables\n\nseals$z &lt;- with(seals, sqrt(delta_long^2 + delta_lat^2))\nm &lt;- ggplot(seals, aes(long, lat))\n\nm + geom_tile(aes(fill = z))\n\n\n\nm + geom_contour(aes(z = z))\n\n\n\nm + geom_raster(aes(fill = z), hjust=0.5,\n                vjust=0.5, interpolate=FALSE)"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#scales",
    "href": "teaching/media_ds/about/ggplot.html#scales",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Scales",
    "text": "Scales\n\nn &lt;- b + geom_bar(aes(fill = fl))\nn\n\n\n\nn + scale_fill_manual(\n  values = c(\"skyblue\", \"royalblue\", \"blue\", \"navy\"),\n  limits = c(\"d\", \"e\", \"p\", \"r\"), breaks =c(\"d\", \"e\", \"p\", \"r\"),\n  name = \"fuel\", labels = c(\"D\", \"E\", \"P\", \"R\"))\n\n\n\n# Color and fill scales\nn &lt;- b + geom_bar(aes(fill = fl))\no &lt;- a + geom_dotplot(aes(fill = ..x..))\n\n# install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nn + scale_fill_brewer(palette = \"Blues\")\n\n\n\ndisplay.brewer.all()\n\n\n\nn + scale_fill_grey(\n  start = 0.2, end = 0.8,\n  na.value = \"red\")\n\n\n\no + scale_fill_gradient(\n  low = \"red\",\n  high = \"yellow\")\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\no + scale_fill_gradientn(\n  colours = terrain.colors(6))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n# Also: rainbow(), heat.colors(),\n# topo.colors(), cm.colors(),\n# RColorBrewer::brewer.pal()\n\n# Shape scales\nf\n\n\n\np &lt;- f + geom_point(aes(shape = fl))\np\n\n\n\np + scale_shape(solid = FALSE)\n\n\n\np + scale_shape_manual(values = c(3:7))\n\n\n\n# Size scales\nq &lt;- f + geom_point(aes(size = cyl))"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#coordinate-systems",
    "href": "teaching/media_ds/about/ggplot.html#coordinate-systems",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Coordinate systems",
    "text": "Coordinate systems\n\nr &lt;- b+geom_bar()\nr + coord_cartesian(xlim = c(0, 5))\n\n\n\nr + coord_fixed(ratio = 1/2)\n\n\n\nr + coord_fixed(ratio = 1/10)\n\n\n\nr + coord_fixed(ratio = 1/100)\n\n\n\nr + coord_flip()\n\n\n\nr + coord_polar(theta = \"x\", direction=1 )"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#position-adjustments",
    "href": "teaching/media_ds/about/ggplot.html#position-adjustments",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Position adjustments",
    "text": "Position adjustments\n\ns &lt;- ggplot(mpg, aes(fl, fill = drv))\n\ns + geom_bar(position = \"dodge\")\n\n\n\n# Arrange elements side by side\ns + geom_bar(position = \"fill\")\n\n\n\n# Stack elements on top of one another, normalize height\ns + geom_bar(position = \"stack\")\n\n\n\n# Stack elements on top of one another\nf + geom_point(position = \"jitter\")\n\n\n\n# Add random noise to X and Y position of each element to avoid overplotting"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#themes",
    "href": "teaching/media_ds/about/ggplot.html#themes",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Themes",
    "text": "Themes\n\n# Theme\nr + theme_bw()\n\n\n\nr + theme_classic()\n\n\n\nr + theme_grey()\n\n\n\nr + theme_minimal()"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#faceting-1",
    "href": "teaching/media_ds/about/ggplot.html#faceting-1",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Faceting",
    "text": "Faceting\n\n# Faceting\n\nt &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point()\nt + facet_grid(. ~ fl)\n\n\n\nt + facet_grid(fl ~ .)\n\n\n\n# facet into columns based on fl\nt + facet_grid(year ~ .)\n\n\n\n# facet into rows based on year\nt + facet_grid(year ~ fl)\n\n\n\n# facet into both rows and columns\nt + facet_wrap(~ fl)\n\n\n\n# wrap facets into a rectangular layout"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#labels",
    "href": "teaching/media_ds/about/ggplot.html#labels",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Labels",
    "text": "Labels\n\n# Labels\nt + ggtitle(\"New Plot Title \")\n\n\n\n# Add a main title above the plot\nt + xlab(\"New X label\")\n\n\n\n# Change the label on the X axis\nt + ylab(\"New Y label\")\n\n\n\n# Change the label on the Y axis\nt + labs(title =\" New title\", x = \"New x\", y = \"New y\")\n\n\n\n# All of the above"
  },
  {
    "objectID": "teaching/media_ds/about/ggplot.html#qz",
    "href": "teaching/media_ds/about/ggplot.html#qz",
    "title": "Data Visualization with ggplot2 in R",
    "section": "Qz",
    "text": "Qz\nQuestion: Using the mpg dataset in R, create a horizontal bar chart that displays the average highway miles per gallon (MPG) for each car manufacturer. Arrange the data in descending order based on the average highway MPG. Use a gradient color scale ranging from red (low values) to green (high values) for the bars. Remove the x-axis label.\nHint: Use dplyr and ggplot2 functions such as group_by(), summarise(), arrange(), aes(), geom_bar(), coord_flip(), and scale_fill_gradient()."
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html",
    "href": "teaching/media_ds/about/NLP_1.html",
    "title": "NLP",
    "section": "",
    "text": "speech_moon.txt\nspeech_park.txt"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#download-example-data",
    "href": "teaching/media_ds/about/NLP_1.html#download-example-data",
    "title": "NLP",
    "section": "",
    "text": "speech_moon.txt\nspeech_park.txt"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#install-required-packages",
    "href": "teaching/media_ds/about/NLP_1.html#install-required-packages",
    "title": "NLP",
    "section": "Install required packages",
    "text": "Install required packages\n\n# install.packages('stringr') \n# install.packages('magrittr')\n# install.packages('glue')\n# install.packages('stringi')\n# install.packages('tidytext')\n\n# stringr 설치가 안되면 아래 코드로 설치\n# install.packages('https://cran.r-project.org/src/contrib/Archive/stringr/stringr_1.4.1.tar.gz', repos = NULL, type = 'source')"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#import-required-libraries",
    "href": "teaching/media_ds/about/NLP_1.html#import-required-libraries",
    "title": "NLP",
    "section": "Import required libraries",
    "text": "Import required libraries\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(tidytext)"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#text-pre-processing",
    "href": "teaching/media_ds/about/NLP_1.html#text-pre-processing",
    "title": "NLP",
    "section": "Text Pre-processing",
    "text": "Text Pre-processing\nImport speeches\n\n# Set to the path within the file where the current R script exists\nraw_moon &lt;- readLines('data/speech_moon.txt', encoding = 'UTF-8')\nhead(raw_moon)\n\n[1] \"정권교체 하겠습니다!\"                                                                                                                                                            \n[2] \"  정치교체 하겠습니다!\"                                                                                                                                                          \n[3] \"  시대교체 하겠습니다!\"                                                                                                                                                          \n[4] \"  \"                                                                                                                                                                              \n[5] \"  ‘불비불명(不飛不鳴)’이라는 고사가 있습니다. 남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새. 그러나 그 새는 한번 날면 하늘 끝까지 날고, 한번 울면 천지를 뒤흔듭니다.\"\n[6] \"\"                                                                                                                                                                                \n\n\nRemove unnecessary characters - str_replace_all()\n\n# Learn how it works with sample text\ntxt &lt;- \"치킨은!! 맛있다. xyz 정말 맛있다!@#\"\ntxt\n\n[1] \"치킨은!! 맛있다. xyz 정말 맛있다!@#\"\n\n# string = 처리할 텍스트, \n# pattern = 규칙, \n# replacement = 바꿀 문자\nstr_replace_all(string = txt, pattern = '[^가-힣]', replacement = ' ')\n\n[1] \"치킨은   맛있다      정말 맛있다   \"\n\n\n\n# raw_moon의 불필요한 문자 제거하기\nmoon &lt;- raw_moon %&gt;%\n  str_replace_all('[^가-힣]', ' ')\nhead(moon)\n\n[1] \"정권교체 하겠습니다 \"                                                                                                                                                        \n[2] \"  정치교체 하겠습니다 \"                                                                                                                                                      \n[3] \"  시대교체 하겠습니다 \"                                                                                                                                                      \n[4] \"  \"                                                                                                                                                                          \n[5] \"   불비불명       이라는 고사가 있습니다  남쪽 언덕 나뭇가지에 앉아   년 동안 날지도 울지도 않는 새  그러나 그 새는 한번 날면 하늘 끝까지 날고  한번 울면 천지를 뒤흔듭니다 \"\n[6] \"\"                                                                                                                                                                            \n\n\nRemove Consecutive Spaces\n\ntxt &lt;- \"치킨은 맛있다 정말 맛있다 \"\ntxt\n\n[1] \"치킨은 맛있다 정말 맛있다 \"\n\nstr_squish(txt)\n\n[1] \"치킨은 맛있다 정말 맛있다\"\n\n\n\n# moon에 있는 연속된 공백 제거하기\nmoon &lt;- moon %&gt;% \n  str_squish()\nhead(moon)\n\n[1] \"정권교체 하겠습니다\"                                                                                                                                          \n[2] \"정치교체 하겠습니다\"                                                                                                                                          \n[3] \"시대교체 하겠습니다\"                                                                                                                                          \n[4] \"\"                                                                                                                                                             \n[5] \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울지도 않는 새 그러나 그 새는 한번 날면 하늘 끝까지 날고 한번 울면 천지를 뒤흔듭니다\"\n[6] \"\"                                                                                                                                                             \n\n\nConvert data to tibble structure - as_tibble()\n\nmoon &lt;- dplyr::as_tibble(moon)\nmoon\n\n# A tibble: 117 × 1\n   value                                                                        \n   &lt;chr&gt;                                                                        \n 1 \"정권교체 하겠습니다\"                                                        \n 2 \"정치교체 하겠습니다\"                                                        \n 3 \"시대교체 하겠습니다\"                                                        \n 4 \"\"                                                                           \n 5 \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울…\n 6 \"\"                                                                           \n 7 \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치로 불러냈습…\n 8 \"\"                                                                           \n 9 \"\"                                                                           \n10 \"우리나라 대통령 이 되겠습니다\"                                              \n# ℹ 107 more rows\n\n\nPre-processing at once (feat. %&gt;%)\n\nmoon &lt;- raw_moon %&gt;% \n  str_replace_all('[^가-힣]', ' ') %&gt;% # 한글만 남기기\n  str_squish() %&gt;% # 연속된 공백 제거\n  as_tibble() # tibble로 변환\nmoon\n\n# A tibble: 117 × 1\n   value                                                                        \n   &lt;chr&gt;                                                                        \n 1 \"정권교체 하겠습니다\"                                                        \n 2 \"정치교체 하겠습니다\"                                                        \n 3 \"시대교체 하겠습니다\"                                                        \n 4 \"\"                                                                           \n 5 \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울…\n 6 \"\"                                                                           \n 7 \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치로 불러냈습…\n 8 \"\"                                                                           \n 9 \"\"                                                                           \n10 \"우리나라 대통령 이 되겠습니다\"                                              \n# ℹ 107 more rows"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#tokenization---unnest_tokens",
    "href": "teaching/media_ds/about/NLP_1.html#tokenization---unnest_tokens",
    "title": "NLP",
    "section": "Tokenization - unnest_tokens()",
    "text": "Tokenization - unnest_tokens()\nPractice with sample data\n\ntext &lt;- tibble(value = \"대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\")\ntext\n\n# A tibble: 1 × 1\n  value                                                                        \n  &lt;chr&gt;                                                                        \n1 대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민…\n\n\nSentence-based tokenization\n\ntext %&gt;% # 문장 기준 토큰화\n  unnest_tokens(input = value, # 토큰화할 텍스트\n                output = word, # 토큰을 담을 변수명\n                token = 'sentences') # 문장 기준\n\n# A tibble: 2 × 1\n  word                                                             \n  &lt;chr&gt;                                                            \n1 대한민국은 민주공화국이다.                                       \n2 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n\n\nWords-based Tokenization (Tokenization by spacing)\n\ntext %&gt;% # 띄어쓰기 기준 토큰화\n  unnest_tokens(input = value,\n                output = word,\n                token = 'words')\n\n# A tibble: 10 × 1\n   word          \n   &lt;chr&gt;         \n 1 대한민국은    \n 2 민주공화국이다\n 3 대한민국의    \n 4 주권은        \n 5 국민에게      \n 6 있고          \n 7 모든          \n 8 권력은        \n 9 국민으로부터  \n10 나온다        \n\n\nCharacter-Based Tokenization\n\ntext %&gt;% # 문자 기준 토큰화\n  unnest_tokens(input = value,\n                output = word,\n                token = 'characters')\n\n# A tibble: 40 × 1\n   word \n   &lt;chr&gt;\n 1 대   \n 2 한   \n 3 민   \n 4 국   \n 5 은   \n 6 민   \n 7 주   \n 8 공   \n 9 화   \n10 국   \n# ℹ 30 more rows\n\n\nTokenizing speeches (Words-based)\n\n# 연설문 토큰화하기 \nword_space &lt;- moon %&gt;% \n  unnest_tokens(input = value,\n                output = word,\n                token = 'words')\nword_space\n\n# A tibble: 2,025 × 1\n   word      \n   &lt;chr&gt;     \n 1 정권교체  \n 2 하겠습니다\n 3 정치교체  \n 4 하겠습니다\n 5 시대교체  \n 6 하겠습니다\n 7 불비불명  \n 8 이라는    \n 9 고사가    \n10 있습니다  \n# ℹ 2,015 more rows"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#word-frequency-visualization",
    "href": "teaching/media_ds/about/NLP_1.html#word-frequency-visualization",
    "title": "NLP",
    "section": "Word frequency visualization",
    "text": "Word frequency visualization\nFind word frequency - count()\n\n# 단어 빈도 구하기 - count()\ntemp_word_space &lt;- word_space %&gt;% \n  count(word, sort = T)\ntemp_word_space\n\n# A tibble: 1,440 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 합니다          27\n 2 수              16\n 3 있습니다        13\n 4 저는            13\n 5 등              12\n 6 있는            12\n 7 함께            12\n 8 만들겠습니다    11\n 9 일자리          10\n10 국민의           9\n# ℹ 1,430 more rows\n\n\nRemove single-letter words - filter(str_count()) str_count() = Count the number of characters in a string\n\n# 한 글자로 된 단어 제거하기 - filter(str_count())\n# str_count = 문자열의 글자 수 구하기\nstr_count('배')\n\n[1] 1\n\nstr_count('사과')\n\n[1] 2\n\n\nleave no more than two characters\n\n# 두 글자 이상만 남기기\ntemp_word_space &lt;- temp_word_space %&gt;% \n  filter(str_count(word) &gt; 1)\ntemp_word_space\n\n# A tibble: 1,384 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n# ℹ 1,374 more rows\n\n\nLet’s work at once\n\n# 한 번에 작업하기\nword_space &lt;- word_space %&gt;% \n  count(word, sort = T) %&gt;% \n  filter(str_count(word) &gt; 1)\nword_space\n\n# A tibble: 1,384 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n# ℹ 1,374 more rows\n\n\nExtract frequently used words (Top 20)\n\n# 자주 사용된 단어 추출하기\ntop20 &lt;- word_space %&gt;% \n  head(20)\ntop20\n\n# A tibble: 20 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n11 새로운           8\n12 위해             8\n13 그리고           7\n14 나라             7\n15 나라가           7\n16 지금             7\n17 낡은             6\n18 대통령이         6\n19 되겠습니다       6\n20 없는             6\n\n\nCreate a bar graph - geom_col()\n\n# 막대 그래프 만들기 - geom_col()\n# mac 사용자, 그래프에 한글 지원폰트로 변경\n# theme_set(theme_gray(base_family = \"AppleGothic\"))\nggplot2::ggplot(top20, aes(x = reorder(word, n), y = n)) + # 단어 빈도순 정렬\n  geom_col() +\n  coord_flip() # 회전\n\n\n\n\nGraph Refinement\n\nggplot(top20, aes(x = reorder(word, n), y = n)) +\n  geom_col() + \n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) +\n  labs(title = '문재인 대통령 출마 연설문 단어 빈도',\n       x = NULL, y = NULL) +\n  theme(title = element_text(size = 12))\n\n\n\n\nCreating a word cloud - geom_text_wordcloud()\n\n# 워드 클라우드 만들기 - geom_text_wordcloud() \n# install.packages('ggwordcloud')\nlibrary(ggwordcloud)\n\nggplot(word_space, aes(label = word, size = n)) +\n  geom_text_wordcloud(seed = 1234) +\n  scale_radius(limits = c(3, NA), # 최소, 최대 단어 빈도\n               range = c(3, 30)) # 최소, 최대 글자 크기\n\n\n\n\nGraph Refinement\n\nggplot(word_space, \n       aes(label = word, \n           size = n,\n           col = n)) + # 빈도에 따라 색깔 표현\n  geom_text_wordcloud(seed = 1234) +\n  scale_radius(limits = c(3, NA),\n               range = c(3, 30)) +\n  scale_color_gradient(low = '#66aaf2', # 최소 빈도 색깔\n                       high = '#004EA1') + # 최대 빈도 색깔\n  theme_minimal() # 배경 없는 테마 적용\n\n\n\n\nChange the graph font 1. Loading Google Fonts - font_add_google()\n\n# 그래프 폰트 바꾸기\n# 1. 구글 폰트 불러오기 - font_add_google()\n# install.packages('showtext')\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n# install.packages('jsonlite')\n# install.packages('curl')\n\nfont_add_google(name = 'Nanum Gothic', family = 'nanumgothic')\nshowtext_auto()\n\n\nAssign fonts to graphs\n\n\nTo prevent errors (or warnings), extrafont is installed and fonts in the operating system are imported into R with font_import. -&gt; Takes some time..\n\n(시간이 매우 오래 걸리니 수업 후에 실행해줍니다, 한 시간 정도 걸림)\n\n# install.packages(\"extrafont\")\nlibrary(extrafont)\n\nRegistering fonts with R\n\n\n\nAttaching package: 'extrafont'\n\n\nThe following object is masked from 'package:showtextdb':\n\n    font_install\n\n# font_import(paths=NULL, recursive = TRUE, prompt=TRUE, pattern=NULL)\n\n\n# 2. 그래프에 폰트 지정하기\nggplot(word_space,\n       aes(label = word,\n           size = n,\n           col = n)) +\n  geom_text_wordcloud(seed = 1234,\n                      family = 'nanumgothic') + # 폰트 적용\n  scale_radius(limits = c(3,NA),\n               range = c(3,30)) +\n  scale_color_gradient(low = '#66aaf2',\n                       high = '#004EA1') +\n  theme_minimal()\n\n\n\n\nFont change (Black Gothick)\n\n# '검은고딕' 폰트 적용\nfont_add_google(name = 'Black Han Sans', family = 'blackhansans')\nshowtext_auto()\n\nggplot(word_space,\n       aes(label = word,\n           size = n,\n           col = n)) +\n  geom_text_wordcloud(seed = 1234,\n                      family = 'blackhansans') + # 폰트 적용\n  scale_radius(limits = c(3,NA),\n               range = c(3,30)) +\n  scale_color_gradient(low = '#66aaf2',\n                       high = '#004EA1') +\n  theme_minimal()\n\n\n\n\nFont change (gamjaflower family)\n\n# 3. ggplot2 패키지로 만든 그래프의 폰트 바꾸기\nfont_add_google(name = 'Gamja Flower', family = 'gamjaflower')\nshowtext_auto()\n\nggplot(top20, aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) + \n  labs(title = '문재인 대통령 출마 연설문 단어 빈도',\n       x = NULL, y = NULL) +\n  theme(title = element_text(size = 12), text = element_text(family = 'gamjaflower')) # 폰트 적용\n\n\n\n\nOptional: If you don’t’ want to specify the font using theme() every time, set the default theme font of the ggplot2 package like below.\n\n# ggplot2 기본 테마 폰트 변경하기 --------------------------------------------------------\n# 매번 theme()를 이용해 폰트를 지정하는게 번거롭다면 ggplot2 패키지 기본 테마 폰트 설정\ntheme_set(theme_gray(base_family = 'nanumgothic'))"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#morphological-analysis-형태소-단위-분석",
    "href": "teaching/media_ds/about/NLP_1.html#morphological-analysis-형태소-단위-분석",
    "title": "NLP",
    "section": "Morphological analysis (형태소 단위 분석)",
    "text": "Morphological analysis (형태소 단위 분석)\n[KoNLP] Installing the Korean Morphological Analysis Package. The order of installation is important, so be sure to do it in that order.\n\nInstall Java and rJava packages\n\n\n# install.packages('multilinguer')\nlibrary(multilinguer)\n\n\nAfter installing Amazon Corretto, close RStudio + restart\n\n\n# install_jdk()\n\n\nInstalling KoNLP dependencies\n\n\n# install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = 'binary')\n\n\nInstalling KoNLP dependencies\n\n\n# install.packages('remotes')\n# remotes::install_github('haven-jeon/KoNLP',\n#                         upgrade = 'never',\n#                         INSTALL_opts = c('--no-multiarch'))\n\n\n# 'scala-library-2.11.8.jar' 에러 발생 시, download.file 코드 실행\n\n# download.file(url = \"https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar\",\n#               destfile = paste0(.libPaths()[1], \"/KoNLP/Java/scala-library-2.11.8.jar\"))\n\n\nlibrary(KoNLP) # Fail to locate \n\nChecking user defined dictionary!\n\n# Checking user defined dictionary! &lt;- This is not an error\n\n# useNIADic() \n# 다운로드 항목 출력 시, 'All' 선택하여 다운로드\n# When printing download items, select 'All' to download\n\n\nTokenize using a morpheme analyzer(형태소 분석기) - see how it works with noun extraction sample text\n\ntext &lt;- tibble(\n  value = c(\"대한민국은 민주공화국이다.\",\n            \"대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\"))\ntext\n\n# A tibble: 2 × 1\n  value                                                            \n  &lt;chr&gt;                                                            \n1 대한민국은 민주공화국이다.                                       \n2 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n\n\nextraNoun(): Output nouns extracted from sentences in a list structure\n\n# extraNoun(): 문장에서 추출한 명사를 list 구조로 출력\nextractNoun(text$value)\n\n[[1]]\n[1] \"대한민국\"   \"민주공화국\"\n\n[[2]]\n[1] \"대한민국\" \"주권\"     \"국민\"     \"권력\"     \"국민\"    \n\n\nExtracting nouns using unnest_tokens(), outputting nouns in a tractable tibble structure\n\n# unnest_tokens()를 이용해 명사 추출하기, 다루기 쉬운 tibble 구조로 명사 출력\nlibrary(tidytext)\n\ntext %&gt;% \n  unnest_tokens(input = value, # 분석 대상\n                output = word, # 출력 변수명\n                token = extractNoun) # 토큰화 함수\n\n# A tibble: 7 × 1\n  word      \n  &lt;chr&gt;     \n1 대한민국  \n2 민주공화국\n3 대한민국  \n4 주권      \n5 국민      \n6 권력      \n7 국민      \n\n\nLet’s compare with spacing-based extraction\n\n# 띄어쓰기 기준 추출과 비교해보자\ntext %&gt;% \n  unnest_tokens(input = value,\n                output = word,\n                token = 'words')\n\n# A tibble: 10 × 1\n   word          \n   &lt;chr&gt;         \n 1 대한민국은    \n 2 민주공화국이다\n 3 대한민국의    \n 4 주권은        \n 5 국민에게      \n 6 있고          \n 7 모든          \n 8 권력은        \n 9 국민으로부터  \n10 나온다        \n\n\nExtracting Nouns from Speeches\n\n# 연설문에서 명사 추출하기\n# 문재인 대통령 연설문 불러오기\n\nraw_moon &lt;- readLines('data/speech_moon.txt', encoding = 'UTF-8')\n\nlibrary(stringr)\nlibrary(textclean)\n\nmoon &lt;- raw_moon %&gt;% \n  str_replace_all('[^가-힣]', ' ') %&gt;% \n  str_squish() %&gt;% \n  as_tibble()\nmoon\n\n# A tibble: 117 × 1\n   value                                                                        \n   &lt;chr&gt;                                                                        \n 1 \"정권교체 하겠습니다\"                                                        \n 2 \"정치교체 하겠습니다\"                                                        \n 3 \"시대교체 하겠습니다\"                                                        \n 4 \"\"                                                                           \n 5 \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울…\n 6 \"\"                                                                           \n 7 \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치로 불러냈습…\n 8 \"\"                                                                           \n 9 \"\"                                                                           \n10 \"우리나라 대통령 이 되겠습니다\"                                              \n# ℹ 107 more rows\n\n# 명사 기준 토큰화\nword_noun &lt;- moon %&gt;% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\nword_noun\n\n# A tibble: 1,757 × 1\n   word      \n   &lt;chr&gt;     \n 1 \"정권교체\"\n 2 \"하겠습니\"\n 3 \"정치\"    \n 4 \"교체\"    \n 5 \"하겠습니\"\n 6 \"시대\"    \n 7 \"교체\"    \n 8 \"하겠습니\"\n 9 \"\"        \n10 \"불비불명\"\n# ℹ 1,747 more rows\n\n\nFind word frequency\n\n# 단어 빈도 구하기\nword_noun &lt;- word_noun %&gt;% \n  count(word, sort = T) %&gt;% # 단어 빈도 구해 내림차순 정렬\n  filter(str_count(word) &gt; 1) # 두 글자 이상만 남기기\nword_noun\n\n# A tibble: 704 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 국민      21\n 2 일자리    21\n 3 나라      19\n 4 우리      17\n 5 경제      15\n 6 사회      14\n 7 성장      13\n 8 대통령    12\n 9 정치      12\n10 하게      12\n# ℹ 694 more rows\n\n\nComparison with the previous spacing-based extraction\n\n# 띄어쓰기 기준 추출과 비교\nmoon %&gt;% \n  unnest_tokens(input = value,\n                output = word,\n                token = 'words') %&gt;% \n  count(word, sort = T) %&gt;% \n  filter(str_count(word) &gt; 1)\n\n# A tibble: 1,384 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n# ℹ 1,374 more rows\n\n\nDo at once (Noun extraction from the speech)\n\n# 명사 추출\nmoon %&gt;% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun) %&gt;% \n  count(word, sort = T) %&gt;% \n  filter(str_count(word) &gt; 1)\n\n# A tibble: 704 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 국민      21\n 2 일자리    21\n 3 나라      19\n 4 우리      17\n 5 경제      15\n 6 사회      14\n 7 성장      13\n 8 대통령    12\n 9 정치      12\n10 하게      12\n# ℹ 694 more rows\n\n\nExtract Top 20 Nouns and create a bar graph\n\n# 상위 20개 단어 추출\ntop20 &lt;- word_noun %&gt;% \n  head(20)\ntop20\n\n# A tibble: 20 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 국민        21\n 2 일자리      21\n 3 나라        19\n 4 우리        17\n 5 경제        15\n 6 사회        14\n 7 성장        13\n 8 대통령      12\n 9 정치        12\n10 하게        12\n11 대한민국    11\n12 평화        11\n13 복지        10\n14 우리나라    10\n15 확대        10\n16 들이         9\n17 사람         9\n18 산업         9\n19 정부         9\n20 복지국가     8\n\n# 막대 그래프 만들기\nlibrary(showtext)\nfont_add_google(name = 'Nanum Gothic', family = 'nanumgothic')\nshowtext_auto()\n\nggplot(top20, aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) +\n  labs(x = NULL) +\n  theme(text = element_text(family = 'nanumgothic'))\n\n\n\n\nWord Cloud of Nouns extracted from the speec\n\n# 워드 클라우드 만들기\nfont_add_google(name = 'Black Han Sans', family = 'blackhansans')\nshowtext_auto()\n\nlibrary(ggwordcloud)\nggplot(word_noun, aes(label = word, size = n, col = n)) +\n  geom_text_wordcloud(seed = 1234, family = 'blackhansans') +\n  scale_radius(limits = c(3,NA),\n               range = c(3,15)) +\n  scale_color_gradient(low = '#66aaf2', high = '#004EA1') +\n  theme_minimal()"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html#extract-sentences-with-specific-words",
    "href": "teaching/media_ds/about/NLP_1.html#extract-sentences-with-specific-words",
    "title": "NLP",
    "section": "Extract sentences with specific words",
    "text": "Extract sentences with specific words\n\nTokenize by sentence\n\n\n# 문장 기준으로 토큰화하기\nsentences_moon &lt;- raw_moon %&gt;% \n  str_squish() %&gt;% \n  as_tibble() %&gt;% \n  unnest_tokens(input = value,\n                output = sentence,\n                token = 'sentences')\nsentences_moon\n\n# A tibble: 207 × 1\n   sentence                                                               \n   &lt;chr&gt;                                                                  \n 1 정권교체 하겠습니다!                                                   \n 2 정치교체 하겠습니다!                                                   \n 3 시대교체 하겠습니다!                                                   \n 4 ‘불비불명(不飛不鳴)’이라는 고사가 있습니다.                            \n 5 남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새.             \n 6 그러나 그 새는 한번 날면 하늘 끝까지 날고, 한번 울면 천지를 뒤흔듭니다.\n 7 그 동안 정치와 거리를 둬 왔습니다.                                     \n 8 그러나 암울한 시대가 저를 정치로 불러냈습니다.                         \n 9 더 이상 남쪽 나뭇가지에 머무를 수 없었습니다.                          \n10 이제 저는 국민과 함께 높이 날고 크게 울겠습니다.                       \n# ℹ 197 more rows\n\n\nExtract sentences with specific words - str_detect()\n\n# 특정 단어가 사용된 문장 추출하기 - str_detect()\n# 예시\nstr_detect('치킨은 맛있다', '치킨')\n\n[1] TRUE\n\nstr_detect('치킨은 맛있다', '피자')\n\n[1] FALSE\n\n\nExample (1): Extract sentences with “국민”\n\n# 특정 단어가 사용된 문장 추출하기, '국민'\nsentences_moon %&gt;% \n  filter(str_detect(sentence, '국민'))\n\n# A tibble: 19 × 1\n   sentence                                                                     \n   &lt;chr&gt;                                                                        \n 1 이제 저는 국민과 함께 높이 날고 크게 울겠습니다.                             \n 2 오늘 저는 제18대 대통령선거 출마를 국민 앞에 엄숙히 선언합니다.              \n 3 존경하는 국민 여러분!                                                        \n 4 국민이 모두 아픕니다.                                                        \n 5 국민 한 사람 한 사람이 모두 아픕니다.                                        \n 6 국민들에게 희망을 주는 정치가 절실하게 필요합니다.                           \n 7 국민의 뜻이 대통령의 길입니다.                                               \n 8 저는 대선출마를 결심하고 국민 여러분께 출마선언문을 함께 쓰자고 제안 드렸습… \n 9 시민의 한숨과 눈물을 닦아주지 못하는 정치가 있었고, 오히려 국민의 걱정거리가…\n10 상식이 통하는 사회, 권한과 책임이 비례하는 사회, 다름을 인정하는 세상, 개천… \n11 그러나 거창하게만 들리는 이 국가비전 역시 국민의 마음속에 있었습니다.        \n12 더욱 낮아지고 겸손해져서 국민의 마음속으로 들어가라.                         \n13 국민들이 제게 준 가르침입니다.                                               \n14 국민의 뜻에서 대통령의 길을 찾겠습니다.                                      \n15 문화혁신을 통해 모든 국민의 창조성을 높이고 이를 통해 기술혁신과 신산업 형성…\n16 이렇게 하면 국민의 살림이 서서히 나아질 것이며 5년 뒤에는 큰 성과가 나타날 … \n17 이명박 정부의 방해에도 불구하고 끝내 국민이 지켜준 세종시, 혁신도시를 지방 … \n18 존경하는 국민 여러분!                                                        \n19 국민의 마음에서 길을 찾는 우리나라 대통령이 되겠습니다.                      \n\n\nExample (2): Extract sentences with “일자리”\n\n# 특정 단어가 사용된 문장 추출하기, '일자리'\nsentences_moon %&gt;% \n  filter(str_detect(sentence, '일자리'))\n\n# A tibble: 18 × 1\n   sentence                                                                     \n   &lt;chr&gt;                                                                        \n 1 빚 갚기 힘들어서, 아이 키우기 힘들어서, 일자리가 보이지 않아서 아픕니다.     \n 2 상생과 평화의 대한민국은 공평과 정의에 바탕을 두고, 성장의 과실을 함께 누리… \n 3 복지의 확대를 통해 보육, 교육, 의료, 요양 등 사회서비스 부문에 수많은 일자리…\n 4 결국 복지국가로 가는 길은 사람에 대한 투자, 일자리 창출, 자영업 고통 경감, … \n 5 ‘일자리 정부’로 ‘일자리 혁명’을 이루겠습니다.                                \n 6 복지의 확대와 함께 저는 강력한 ‘일자리 혁명’을 이루고자 합니다.              \n 7 지금 너무나 많은 젊은이들과 실업자, 비정규직 종사자, 근로능력이 있는 고령자… \n 8 좋은 일자리 창출을 위해 비정규직의 정규직 전환 촉진, 비정규직에 대한 차별철… \n 9 또한 정보통신 산업, 바이오산업, 나노 산업, 신재생에너지 산업, 문화산업과 콘… \n10 그리고 앞에서 말한 보육, 교육, 의료, 복지 등 사회서비스 부문은 무궁무진한 잠…\n11 일자리 없는 곳에서 희망을 찾을 수 없습니다.                                  \n12 지방 일자리에 대해 특별한 노력을 기울이겠습니다.                             \n13 지역균형발전은 곧 산업 균형, 일자리 균형이 목표입니다.                       \n14 이명박 정부의 방해에도 불구하고 끝내 국민이 지켜준 세종시, 혁신도시를 지방 … \n15 이 모든 정책의 실효성을 담보하기 위해 대통령이 되면 저는 가장 먼저 대통령 직…\n16 저는 먼 훗날 ‘일자리 혁명을 일으킨 대통령’으로 평가받기를 희망합니다.        \n17 또한 좋은 일자리와 산업혁신을 위해서는 평생학습체제가 뒷받침되어야 합니다.   \n18 노인 일자리를 늘리고, 특히 그 연륜과 경험을 지역사회에 활용할 수 있는 방안도…"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_3.html",
    "href": "teaching/media_ds/about/NLP_3.html",
    "title": "NLP",
    "section": "",
    "text": "동시 출현 단어 분석\n\n# 기본적인 전처리\n# 기생충 기사 댓글 불러오기\nraw_news_comment &lt;- readr::read_csv(\"data/news_comment_parasite.csv\")\n\nRows: 4150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): reply, press, title, url\ndttm (1): reg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 전처리\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\nlibrary(textclean)\nnews_comment &lt;- raw_news_comment %&gt;%\n  select(reply) %&gt;%\n  mutate(reply = str_replace_all(reply, \"[^가-힣]\", \" \"),\n         reply = str_squish(reply),\n         id = row_number())\n\n\nnews_comment %&gt;% head(10)\n\n# A tibble: 10 × 2\n   reply                                                                      id\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합…     1\n 2 와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려요 진…      2\n 3 우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 영감…      3\n 4 봉준호 감독과 우리나라 대한민국 모두 자랑스럽다 세계 어디를 가고 우리…      4\n 5 노벨상 탄느낌이네요 축하축하 합니다                                         5\n 6 기생충 상 받을때 박수 쳤어요 감독상도 기대해요 봉준호 감독 화이팅           6\n 7 대한민국 영화사를 새로 쓰고 계시네요                                        7\n 8 저런게 아카데미상 받으면 태극기 휘날리며 광해 명량 은 전부문 휩쓸어야…      8\n 9 다시한번 보여주세요 영화관에서 보고싶은디                                   9\n10 대한민국 와함께 봉준호감독님까지 대단하고 한국의 문화에 자긍심을 가지…     10\n\n\n\n# 토큰화하기\n# 1. 형태소 분석기를 이용해 품사 기준으로 토큰화하기\nlibrary(tidytext)\nlibrary(KoNLP)\n\nChecking user defined dictionary!\n\ncomment_pos &lt;- news_comment %&gt;%\n  unnest_tokens(input = reply,\n                output = word,\n                token = SimplePos22,\n                drop = F)\n\ncomment_pos %&gt;%\n  select(reply, word)\n\n# A tibble: 39,956 × 2\n   reply                                                                   word \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 정말…\n 2 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 우리…\n 3 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 집/n…\n 4 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 좋/p…\n 5 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 일/n…\n 6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 생기…\n 7 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 기쁘…\n 8 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 행복…\n 9 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 것/n…\n10 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 나/n…\n# ℹ 39,946 more rows\n\n\n\n# 2. 품사 분리하여 행 구성하기\nlibrary(tidyr)\ncomment_pos &lt;- comment_pos %&gt;%\n  separate_rows(word, sep = \"[+]\")\n\ncomment_pos %&gt;%\n  select(word, reply)\n\n# A tibble: 70,553 × 2\n   word    reply                                                                \n   &lt;chr&gt;   &lt;chr&gt;                                                                \n 1 정말/ma 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 2 우리/np 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 3 집/nc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 4 에/jc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 5 좋/pa   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 6 은/et   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 7 일/nc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 8 이/jc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 9 생기/pv 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n10 어/ec   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n# ℹ 70,543 more rows\n\n\n\n# 3. 품사 추출하기\n# 명사 추출하기\nnoun &lt;- comment_pos %&gt;%\n  filter(str_detect(word, \"/n\")) %&gt;%\n  mutate(word = str_remove(word, \"/.*$\"))\n\nnoun %&gt;%\n  select(word, reply)\n\n# A tibble: 27,457 × 2\n   word   reply                                                                \n   &lt;chr&gt;  &lt;chr&gt;                                                                \n 1 우리   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 2 집     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 3 일     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 4 행복한 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 5 것     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 6 나     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 7 일     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 8 양     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 9 행복   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n10 행복   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n# ℹ 27,447 more rows\n\n\n\n# 명사 빈도 구하기\nnoun %&gt;%\n  count(word, sort = T)\n\n# A tibble: 8,069 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 영화       463\n 2 기생충     445\n 3 봉준호     372\n 4 것         353\n 5 아카데미   252\n 6 축하       232\n 7 나         230\n 8 대한민국   226\n 9 자랑       222\n10 작품상     218\n# ℹ 8,059 more rows\n\n\n동사와 형용사! - 명사와 어떻게 연결되는지 알기 위해\n\n# 동사, 형용사 추출하기\npvpa &lt;- comment_pos %&gt;%\n  filter(str_detect(word, \"/pv|/pa\")) %&gt;% # \"/pv\", \"/pa\" 추출\n  mutate(word = str_replace(word, \"/.*$\", \"다\")) # \"/\"로 시작 문자를 \"다\"로 바꾸기\n\npvpa %&gt;%\n  select(word, reply)\n\n# A tibble: 5,317 × 2\n   word       reply                                                             \n   &lt;chr&gt;      &lt;chr&gt;                                                             \n 1 좋다       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 2 생기다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 3 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 4 축하드리다 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 5 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 6 기쁘다     와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려… \n 7 기쁘다     와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려… \n 8 축하드리다 와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려… \n 9 불다       우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 …\n10 크다       우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 …\n# ℹ 5,307 more rows\n\n\n\n# 품사 결합\ncomment &lt;- bind_rows(noun, pvpa) %&gt;%\n  filter(str_count(word) &gt;= 2) %&gt;%\n  arrange(id)\n\ncomment %&gt;%\n  select(word, reply)\n\n# A tibble: 26,860 × 2\n   word       reply                                                            \n   &lt;chr&gt;      &lt;chr&gt;                                                            \n 1 우리       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 2 행복한     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 3 행복       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 4 행복       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 5 좋다       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 6 생기다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 7 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 8 축하드리다 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 9 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n10 시국       와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려…\n# ℹ 26,850 more rows\n\n\n\n# ***명사, 동사, 형용사를 한 번에 추출하기\ncomment_new &lt;- comment_pos %&gt;%\n  separate_rows(word, sep = \"[+]\") %&gt;%\n  filter(str_detect(word, \"/n|/pv|/pa\")) %&gt;%\n  mutate(word = ifelse(str_detect(word, \"/pv|/pa\"),\n                       str_replace(word, \"/.*$\", \"다\"),\n                       str_remove(word, \"/.*$\"))) %&gt;%\n  filter(str_count(word) &gt;= 2) %&gt;%\n  arrange(id)\n\ncomment_new %&gt;% head(10)\n\n# A tibble: 10 × 3\n   reply                                                                id word \n   &lt;chr&gt;                                                             &lt;int&gt; &lt;chr&gt;\n 1 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 우리 \n 2 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 좋다 \n 3 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 생기…\n 4 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 기쁘…\n 5 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 행복…\n 6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 행복 \n 7 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 축하…\n 8 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 행복 \n 9 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …     1 기쁘…\n10 와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려…     2 기쁘…\n\n\n동시 출현 빈도 by using pairwise_count()\n\n# 단어 동시 출현 빈도 구하기\n# install.packages(\"widyr\")\nlibrary(widyr)\npair &lt;- comment %&gt;%\n  pairwise_count(item = word,\n                 feature = id,\n                 sort = T)\npair\n\n# A tibble: 245,920 × 3\n   item1      item2      n\n   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n 1 영화       기생충   111\n 2 기생충     영화     111\n 3 감독       봉준호    86\n 4 봉준호     감독      86\n 5 감독님     봉준호    66\n 6 봉준호     감독님    66\n 7 만들다     영화      57\n 8 영화       만들다    57\n 9 기생충     봉준호    54\n10 블랙리스트 감독      54\n# ℹ 245,910 more rows\n\n\n\n# 특정 단어와 자주 함께 사용된 단어 살펴보기\npair %&gt;% filter(item1 == \"영화\")\n\n# A tibble: 2,313 × 3\n   item1 item2        n\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 영화  기생충     111\n 2 영화  만들다      57\n 3 영화  봉준호      52\n 4 영화  받다        48\n 5 영화  한국        46\n 6 영화  아카데미    42\n 7 영화  같다        41\n 8 영화  감독        39\n 9 영화  아니다      38\n10 영화  좋다        35\n# ℹ 2,303 more rows\n\npair %&gt;% filter(item1 == \"봉준호\")\n\n# A tibble: 1,579 × 3\n   item1  item2          n\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n 1 봉준호 감독          86\n 2 봉준호 감독님        66\n 3 봉준호 기생충        54\n 4 봉준호 영화          52\n 5 봉준호 블랙리스트    48\n 6 봉준호 대한민국      38\n 7 봉준호 자랑          33\n 8 봉준호 축하드리다    30\n 9 봉준호 송강호        30\n10 봉준호 축하          25\n# ℹ 1,569 more rows\n\n\n\n\n동시 출현 네트워크\n네트워크 그래프 데이터 만들기\n\n# 네트워크 그래프 데이터 만들기\n# install.packages(\"tidygraph\")\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ngraph_comment &lt;- pair %&gt;%\n  filter(n &gt;= 25) %&gt;%\n  as_tbl_graph()\n\ngraph_comment\n\n# A tbl_graph: 30 nodes and 108 edges\n#\n# A directed simple graph with 2 components\n#\n# A tibble: 30 × 1\n  name  \n  &lt;chr&gt; \n1 영화  \n2 기생충\n3 감독  \n4 봉준호\n5 감독님\n6 만들다\n# ℹ 24 more rows\n#\n# A tibble: 108 × 3\n   from    to     n\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     2   111\n2     2     1   111\n3     3     4    86\n# ℹ 105 more rows\n\nstr(graph_comment)\n\nClasses 'tbl_graph', 'igraph'  hidden list of 10\n $ : num 30\n $ : logi TRUE\n $ : num [1:108] 0 1 2 3 4 3 5 0 1 6 ...\n $ : num [1:108] 1 0 3 2 3 4 0 5 3 2 ...\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ :List of 4\n  ..$ : num [1:3] 1 0 1\n  ..$ : Named list()\n  ..$ :List of 1\n  .. ..$ name: chr [1:30] \"영화\" \"기생충\" \"감독\" \"봉준호\" ...\n  ..$ :List of 1\n  .. ..$ n: num [1:108] 111 111 86 86 66 66 57 57 54 54 ...\n $ :&lt;environment: 0x000001e7de9929f8&gt; \n - attr(*, \"active\")= chr \"nodes\"\n\n\nTable to Graph object\n\n# 네트워크 그래프 만들기\n# install.packages(\"ggraph\")\nlibrary(ggraph)\n\nggraph(graph_comment) +\n  geom_edge_link() + # 엣지\n  geom_node_point() + # 노드\n  geom_node_text(aes(label = name)) # 텍스트\n\nUsing \"stress\" as default layout\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nRefine the graph\n\n# 그래프 다듬기\n# 한글 폰트 설정\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(name = \"Nanum Gothic\", family = \"nanumgothic\")\nshowtext_auto()\n\n# 엣지와 노드의 색깔, 크기, 텍스트 위치 수정\nset.seed(1234) # 난수 고정\n\nggraph(graph_comment, layout = \"fr\") + # 레이아웃\n  geom_edge_link(color = \"gray50\", # 엣지 색깔\n                 alpha = 0.5) + # 엣지 명암\n  geom_node_point(color = \"lightcoral\", # 노드 색깔\n                  size = 5) + # 노드 크기\n  geom_node_text(aes(label = name), # 텍스트 표시\n                 repel = T, # 노드밖 표시\n                 size = 5, # 텍스트 크기\n                 family = \"nanumgothic\") + # 폰트\n  theme_graph() # 배경 삭제\n\n\n\n\n네트워크 그래프 함수 만들기\n\n# 네트워크 그래프 함수 만들기\nword_network &lt;- function(x) {\n  ggraph(x, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\",\n                   alpha = 0.5) +\n    geom_node_point(color = \"lightcoral\",\n                    size = 5) +\n    geom_node_text(aes(label = name),\n                   repel = T,\n                   size = 5,\n                   family = \"nanumgothic\") +\n    theme_graph()\n}\n\n함수를 활용해서 그래프 그리기\n\nset.seed(1234)\nword_network(graph_comment)\n\n\n\n\n유의어 처리하기\n\n# 유의어 처리하기\ncomment &lt;- comment %&gt;%\n  mutate(word = ifelse(str_detect(word, \"감독\") &\n                         !str_detect(word, \"감독상\"), \"봉준호\", word),\n         word = ifelse(word == \"오르다\", \"올리다\", word),\n         word = ifelse(str_detect(word, \"축하\"), \"축하\", word))\n\n\n# 단어 동시 출현 빈도 구하기\npair &lt;- comment %&gt;%\n  pairwise_count(item = word,\n                 feature = id,\n                 sort = T)\n\n\n# 네트워크 그래프 데이터 만들기\ngraph_comment &lt;- pair %&gt;%\n  filter(n &gt;= 25) %&gt;%\n  as_tbl_graph()\n\n# 네트워크 그래프 만들기\nset.seed(1234)\nword_network(graph_comment)\n\n\n\n\n\n# 연결중심성과 커뮤니티 표현하기\n# 1. 네트워크 그래프 데이터에 연결 중심성, 커뮤니티 변수 추가하기\nset.seed(1234)\ngraph_comment &lt;- pair %&gt;%\n  filter(n &gt;= 25) %&gt;%\n  as_tbl_graph(directed = F) %&gt;%\n  mutate(centrality = centrality_degree(), # 연결 중심성\n         group = as.factor(group_infomap())) # 커뮤니티\ngraph_comment\n\n# A tbl_graph: 36 nodes and 152 edges\n#\n# An undirected multigraph with 1 component\n#\n# A tibble: 36 × 3\n  name       centrality group\n  &lt;chr&gt;           &lt;dbl&gt; &lt;fct&gt;\n1 봉준호             62 4    \n2 축하               34 2    \n3 영화               26 3    \n4 블랙리스트          6 6    \n5 기생충             26 1    \n6 대한민국           10 3    \n# ℹ 30 more rows\n#\n# A tibble: 152 × 3\n   from    to     n\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     2   198\n2     1     2   198\n3     1     3   119\n# ℹ 149 more rows\n\n\n\n# 2. 네트워크 그래프에 연결 중심성, 커뮤니티 표현하기\nset.seed(1234)\nggraph(graph_comment, layout = \"fr\") + # 레이아웃\n  geom_edge_link(color = \"gray50\", # 엣지 색깔\n                 alpha = 0.5) + # 엣지 명암\n  geom_node_point(aes(size = centrality, # 노드 크기\n                      color = group), # 노드 색깔\n                  show.legend = F) + # 범례 삭제\n  scale_size(range = c(5, 15)) + # 노드 크기 범위\n  geom_node_text(aes(label = name), # 텍스트 표시\n                 repel = T, # 노드밖 표시\n                 size = 5) + # 폰트\n  theme_graph() # 배경 삭제\n\n\n\n\n\n# 3. 네트워크의 주요 단어 살펴보기\ngraph_comment %&gt;%\n  filter(name == \"봉준호\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# An unrooted tree\n#\n# A tibble: 1 × 3\n  name   centrality group\n  &lt;chr&gt;       &lt;dbl&gt; &lt;fct&gt;\n1 봉준호         62 4    \n#\n# A tibble: 0 × 3\n# ℹ 3 variables: from &lt;int&gt;, to &lt;int&gt;, n &lt;dbl&gt;\n\ngraph_comment\n\n# A tbl_graph: 36 nodes and 152 edges\n#\n# An undirected multigraph with 1 component\n#\n# A tibble: 36 × 3\n  name       centrality group\n  &lt;chr&gt;           &lt;dbl&gt; &lt;fct&gt;\n1 봉준호             62 4    \n2 축하               34 2    \n3 영화               26 3    \n4 블랙리스트          6 6    \n5 기생충             26 1    \n6 대한민국           10 3    \n# ℹ 30 more rows\n#\n# A tibble: 152 × 3\n   from    to     n\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     2   198\n2     1     2   198\n3     1     3   119\n# ℹ 149 more rows\n\n\n\n# 같은 커뮤니티로 분류된 단어 살펴보기\ngraph_comment %&gt;%\n  filter(group == 4) %&gt;%\n  arrange(-centrality) %&gt;%\n  data.frame()\n\n    name centrality group\n1 봉준호         62     4\n2   받다         10     4\n3   자랑          6     4\n4 만들다          4     4\n\n\n\n# 연결 중심성이 높은 주요 단어 살펴보기\ngraph_comment %&gt;%\n  arrange(-centrality)\n\n# A tbl_graph: 36 nodes and 152 edges\n#\n# An undirected multigraph with 1 component\n#\n# A tibble: 36 × 3\n  name     centrality group\n  &lt;chr&gt;         &lt;dbl&gt; &lt;fct&gt;\n1 봉준호           62 4    \n2 축하             34 2    \n3 영화             26 3    \n4 기생충           26 1    \n5 작품상           14 5    \n6 대한민국         10 3    \n# ℹ 30 more rows\n#\n# A tibble: 152 × 3\n   from    to     n\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     2   198\n2     1     2   198\n3     1     3   119\n# ℹ 149 more rows\n\n\n\n# \"2번\" 커뮤니티로 분류된 단어\ngraph_comment %&gt;%\n  filter(group == 2) %&gt;%\n  arrange(-centrality) %&gt;%\n  data.frame()\n\n    name centrality group\n1   축하         34     2\n2   좋다          8     2\n3   진심          4     2\n4   수상          4     2\n5   없다          4     2\n6   대단          2     2\n7 기쁘다          2     2\n\n\n\n# 4. 주요 단어가 사용된 원문 살펴보기\nnews_comment %&gt;%\n  filter(str_detect(reply, \"봉준호\") & str_detect(reply, \"대박\")) %&gt;%\n  select(reply)\n\n# A tibble: 19 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 대박 대박 진짜 대박 봉준호 감독님과 우리 배우들 너무 다랑스러워요            \n 2 내가 죽기전에 아카데미에서 한국어를 들을줄이야 봉준호대박 기생충대박         \n 3 대박 관왕이라니 축하합니다 봉준호를 배출한 충무로 그리고 문화강국 대한 민국  \n 4 우와 대박 진자 대단하다 봉준호                                               \n 5 봉준호 경사났네 대박중에 대에박 축하합니다                                   \n 6 봉준호 작품상 탔다 대박                                                      \n 7 봉준호 군대 면제시켜도될듯 대박 여윽시 위대한 한국에는 위대한 봉준호 형님이 …\n 8 아니 다른상을 받은것도 충분히 대단하고 굉장하지만 최고의 영예인 작품상을 받… \n 9 봉준호 군대 면제시켜도될듯 대박 여윽시 위대한 한국에는 위대한 봉준호 형님이 …\n10 봉준호감독님대박 축하합니다                                                  \n11 와 봉준호 대박 축하드려요                                                    \n12 대박이다 감격의 한해입니다 봉준호 감독님 정말 축하드립니다                   \n13 좌파영화인 봉준호가 좌파영화로 아카데미 작품상 대박 배 아프겠다 안보겄다던 … \n14 각본상 외국여영화상 수상 대박입니다 축하하고 잠시후에 봉준호 감독상과 영어 … \n15 한국 역사상 최초인 오스카상 관왕 진짜 대박 대한민국 위상과 국격을 세계인들에…\n16 미쳣다 감독상은 진짜 예상못햇는데 마틴 스콜쎄지 퀜틴타란티노 스티븐스필버그… \n17 봉준호감독 짱 가 대박났네                                                    \n18 와 대박 소름돋아 으악 봉준호 감독님 너무너무 축하드려요                      \n19 와 진짜 대박이다 봉준호 언젠가 정말 세계적으로 인정 받는 날이 올줄은 알았지… \n\n\n\nnews_comment %&gt;%\n  filter(str_detect(reply, \"박근혜\") & str_detect(reply, \"블랙리스트\")) %&gt;%\n  select(reply)\n\n# A tibble: 63 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 일베와 자한당이 싫어하는 봉준호 감독이 아카데미에서 상받으니 쪽바리들처럼 엄…\n 2 박근혜 블랙리스트 로 낙인찍은 봉준호 감독님이 아시아 최초로 오스카에서 상을 …\n 3 우리나라에서만 좌파다 빨갱이다 라고 비하함 박근혜 때 이런 세계적 감독을 블랙…\n 4 박근혜 최순실 블랙리스트에 오른 훌륭하신 감독님 축하합니다                   \n 5 박근혜정부가 얼마나 썩고 무능했냐면 각종 영화제에서 최고상 수상을 받는 유능… \n 6 넷상 보수들 만큼 이중적인 새 끼들 없음 봉준호 송강호 보고 종북좌빨 홍어드립 …\n 7 박근혜 자한당 독재시절 봉준호 송강호를 블랙리스트 올려놓고 활동 방해 감시하… \n 8 대단합니다 김연아 방탄 봉준호 스포츠 음악 영화 못하는게 없어요 좌빨 감독이라…\n 9 송강호 봉준호 박근혜 이명박 시절 블랙리스트 이제 어떻게 깔려구               \n10 이명박근혜정권당시 좌파감독이라고 블랙리스트까지 올랏던 봉준호 역사적위업을 …\n# ℹ 53 more rows\n\n\n\nnews_comment %&gt;%\n  filter(str_detect(reply, \"기생충\") & str_detect(reply, \"조국\")) %&gt;%\n  select(reply)\n\n# A tibble: 64 × 1\n   reply                                                                        \n   &lt;chr&gt;                                                                        \n 1 조국이가 받아야 한다 기생충 스토리 제공                                      \n 2 한번도경험하지 못한 조국가족사기단기생충 개봉박두                            \n 3 와 조국 가족 사기단 부제 기생충 최고                                         \n 4 문재인과 조국 기생충 리얼                                                    \n 5 기생충은 좌좀 조국 가족을 패러디한 영화라서 우파들도 열광하고 있는 것이다 같…\n 6 조국 가족이 기생충 영화를 꼭 봐야되는데                                      \n 7 좌파 인생영화인데 좌파 기생충들에게 이 상을 받쳐라 조국 서울대 문서위조학과 …\n 8 기생충 조국 봉준호 만세                                                      \n 9 봉준호감독님 글로벌 영화계 큰상수상을 진심으로 축하합니다 다만 기생충 작품은…\n10 기생충보면서 조국생각난사람 나쁜일라나 봉준호 감독님이 현 시대를 참 잘 반영… \n# ℹ 54 more rows\n\n\n\n\n단어 상관 분석\n\n# 파이 계수 구하기\nword_cors &lt;- comment %&gt;%\n  add_count(word) %&gt;%\n  filter(n &gt;= 20) %&gt;%\n  pairwise_cor(item = word,\n               feature = id,\n               sort = T)\nword_cors\n\n# A tibble: 26,732 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 올리다     블랙리스트       0.478\n 2 블랙리스트 올리다           0.478\n 3 역사       쓰다             0.370\n 4 쓰다       역사             0.370\n 5 박근혜     블랙리스트       0.322\n 6 블랙리스트 박근혜           0.322\n 7 가족       조국             0.306\n 8 조국       가족             0.306\n 9 작품상     감독상           0.276\n10 감독상     작품상           0.276\n# ℹ 26,722 more rows\n\n\n\n# 특정 단어와 관련성이 큰 단어 살펴보기\nword_cors %&gt;%\n  filter(item1 == \"대한민국\")\n\n# A tibble: 163 × 3\n   item1    item2  correlation\n   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n 1 대한민국 국민        0.182 \n 2 대한민국 자랑        0.158 \n 3 대한민국 위상        0.149 \n 4 대한민국 국격        0.129 \n 5 대한민국 위대한      0.100 \n 6 대한민국 세계        0.0910\n 7 대한민국 문화        0.0757\n 8 대한민국 감사합      0.0724\n 9 대한민국 나라        0.0715\n10 대한민국 오늘        0.0715\n# ℹ 153 more rows\n\nword_cors %&gt;%\n  filter(item1 == \"역사\")\n\n# A tibble: 163 × 3\n   item1 item2    correlation\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1 역사  쓰다          0.370 \n 2 역사  최초          0.117 \n 3 역사  한국          0.0982\n 4 역사  순간          0.0910\n 5 역사  한국영화      0.0821\n 6 역사  아니다        0.0774\n 7 역사  감사          0.0654\n 8 역사  영광          0.0640\n 9 역사  영화제        0.0596\n10 역사  오스카        0.0593\n# ℹ 153 more rows\n\n\n\n# 파이 계수로 막대 그래프 만들기\n# 1. 관심 단어별로 파이 계수가 큰 단어 추출하기\n# 관심 단어 목록 생성\ntarget &lt;- c(\"대한민국\", \"역사\", \"수상소감\", \"조국\", \"박근혜\", \"블랙리스트\")\ntop_cors &lt;- word_cors %&gt;%\n  filter(item1 %in% target) %&gt;%\n  group_by(item1) %&gt;%\n  slice_max(correlation, n = 8)\n\n\n# 2. 막대 그래프 만들기\n# 그래프 순서 정하기\ntop_cors$item1 &lt;- factor(top_cors$item1, levels = target)\nlibrary(ggplot2)\nggplot(top_cors, aes(x = reorder_within(item2, correlation, item1),\n                     y = correlation,\n                     fill = item1)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(x = NULL) +\n  theme(text = element_text(family = \"nanumgothic\"))\n\n\n\n\n\n# 파이 계수로 네트워크 그래프 만들기\n# 1. 네트워크 그래프 데이터 만들기, 연결 중심성과 커뮤니티 추가하기\nset.seed(1234)\ngraph_cors &lt;- word_cors %&gt;%\n  filter(correlation &gt;= 0.15) %&gt;%\n  as_tbl_graph(directed = F) %&gt;%\n  mutate(centrality = centrality_degree(),\n         group = as.factor(group_infomap()))\n\n\n# 2. 네트워크 그래프 만들기\nset.seed(1234)\nggraph(graph_cors, layout = \"fr\") +\n  geom_edge_link(color = \"gray50\",\n                 aes(edge_alpha = correlation, # 엣지 명암\n                     edge_width = correlation), # 엣지 두께\n                 show.legend = F) + # 범례 삭제\n  scale_edge_width(range = c(1, 4)) + # 엣지 두께 범위\n  geom_node_point(aes(size = centrality,\n                      color = group),\n                  show.legend = F) +\n  scale_size(range = c(5, 10)) +\n  geom_node_text(aes(label = name),\n                 repel = T,\n                 size = 5,\n                 family = \"nanumgothic\") +\n  theme_graph()\n\n\n\n\n\n\n연이어 사용된 단어쌍\n\n# 엔그램으로 토큰화하기\ntext &lt;- tibble(value = \"대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\")\ntext\n\n# A tibble: 1 × 1\n  value                                                                        \n  &lt;chr&gt;                                                                        \n1 대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민…\n\n\n\n# 바이그램 토큰화\ntext %&gt;%\n  unnest_tokens(input = value,\n                output = word,\n                token = \"ngrams\",\n                n = 2)\n\n# A tibble: 9 × 1\n  word                     \n  &lt;chr&gt;                    \n1 대한민국은 민주공화국이다\n2 민주공화국이다 대한민국의\n3 대한민국의 주권은        \n4 주권은 국민에게          \n5 국민에게 있고            \n6 있고 모든                \n7 모든 권력은              \n8 권력은 국민으로부터      \n9 국민으로부터 나온다      \n\n\n\n# 트라이그램 토큰화\ntext %&gt;%\n  unnest_tokens(input = value,\n                output = word,\n                token = \"ngrams\",\n                n = 3)\n\n# A tibble: 8 × 1\n  word                                \n  &lt;chr&gt;                               \n1 대한민국은 민주공화국이다 대한민국의\n2 민주공화국이다 대한민국의 주권은    \n3 대한민국의 주권은 국민에게          \n4 주권은 국민에게 있고                \n5 국민에게 있고 모든                  \n6 있고 모든 권력은                    \n7 모든 권력은 국민으로부터            \n8 권력은 국민으로부터 나온다          \n\n\n\n# 유니그램 토큰화 = 단어 기준 토큰화\ntext %&gt;%\n  unnest_tokens(input = value,\n                output = word,\n                token = \"ngrams\",\n                n = 1)\n\n# A tibble: 10 × 1\n   word          \n   &lt;chr&gt;         \n 1 대한민국은    \n 2 민주공화국이다\n 3 대한민국의    \n 4 주권은        \n 5 국민에게      \n 6 있고          \n 7 모든          \n 8 권력은        \n 9 국민으로부터  \n10 나온다        \n\n\n\n# 기사 댓글로 바이그램 만들기\n# 1. 명사, 동사, 형용사 추출하기\ncomment_new &lt;- comment_pos %&gt;%\n  separate_rows(word, sep = \"[+]\") %&gt;%\n  filter(str_detect(word, \"/n|/pv|/pa\")) %&gt;%\n  mutate(word = ifelse(str_detect(word, \"/pv|/pa\"),\n                       str_replace(word, \"/.*$\", \"다\"),\n                       str_remove(word, \"/.*$\"))) %&gt;%\n  filter(str_count(word) &gt;= 2) %&gt;%\n  arrange(id)\n\n\n# 2. 유의어 처리하기\ncomment_new &lt;- comment_new %&gt;%\n  mutate(word = ifelse(str_detect(word, \"감독\") &\n                         !str_detect(word, \"감독상\"), \"봉준호\", word),\n         word = ifelse(word == \"오르다\", \"올리다\", word),\n         word = ifelse(str_detect(word, \"축하\"), \"축하\", word))\n\n\n# 3. 한 댓글이 하나의 행이 되도록 결합하기\ncomment_new %&gt;%\n  select(word)\n\n# A tibble: 26,860 × 1\n   word  \n   &lt;chr&gt; \n 1 우리  \n 2 좋다  \n 3 생기다\n 4 기쁘다\n 5 행복한\n 6 행복  \n 7 축하  \n 8 행복  \n 9 기쁘다\n10 기쁘다\n# ℹ 26,850 more rows\n\n\n\nline_comment &lt;- comment_new %&gt;%\n  group_by(id) %&gt;%\n  summarise(sentence = paste(word, collapse = \" \"))\n\nline_comment\n\n# A tibble: 4,007 × 2\n      id sentence                                                               \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 우리 좋다 생기다 기쁘다 행복한 행복 축하 행복 기쁘다                   \n 2     2 기쁘다 시국 기쁘다 감사하다 축하 진심                                  \n 3     3 우리나라 봉준호 불다 크다 영감 봉준호 공동각본쓴 한진 작가님 축하 축하…\n 4     4 봉준호 봉준호 우리나라 대한민국 자랑 세계 어디 우리 한국인 힘내다 삽시 \n 5     5 노벨상 탄느낌이네요 축하                                               \n 6     6 기생충 받다 박수 치다 감독상 기대다 봉준호 봉준호                      \n 7     7 대한민국 영화사 쓰다 계시다                                            \n 8     8 아카데미상 받다 태극기 휘날리다 광해 명량 전부문 휩쓸어야겠            \n 9     9 다시한번 보이다 영화관                                                 \n10    10 대한민국 봉준호 대단 한국의 문화 자긍심 가지                           \n# ℹ 3,997 more rows\n\n\n\n# 4. 바이그램으로 토큰화하기\nbigram_comment &lt;- line_comment %&gt;%\n  unnest_tokens(input = sentence,\n                output = bigram,\n                token = \"ngrams\",\n                n = 2)\nbigram_comment\n\n# A tibble: 23,348 × 2\n      id bigram       \n   &lt;int&gt; &lt;chr&gt;        \n 1     1 우리 좋다    \n 2     1 좋다 생기다  \n 3     1 생기다 기쁘다\n 4     1 기쁘다 행복한\n 5     1 행복한 행복  \n 6     1 행복 축하    \n 7     1 축하 행복    \n 8     1 행복 기쁘다  \n 9     2 기쁘다 시국  \n10     2 시국 기쁘다  \n# ℹ 23,338 more rows\n\n\n\n# 연이어 사용된 단어쌍 빈도 구하기\n# 1. 바이그램 분리하기\nbigram_seprated &lt;- bigram_comment %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\nbigram_seprated\n\n# A tibble: 23,348 × 3\n      id word1  word2 \n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; \n 1     1 우리   좋다  \n 2     1 좋다   생기다\n 3     1 생기다 기쁘다\n 4     1 기쁘다 행복한\n 5     1 행복한 행복  \n 6     1 행복   축하  \n 7     1 축하   행복  \n 8     1 행복   기쁘다\n 9     2 기쁘다 시국  \n10     2 시국   기쁘다\n# ℹ 23,338 more rows\n\n\n\n# 2. 단어쌍 빈도 구하기\npair_bigram &lt;- bigram_seprated %&gt;%\n  count(word1, word2, sort = T) %&gt;%\n  na.omit()\npair_bigram\n\n# A tibble: 19,030 × 3\n   word1      word2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;int&gt;\n 1 봉준호     봉준호       155\n 2 블랙리스트 올리다        64\n 3 진심       축하          64\n 4 봉준호     축하          57\n 5 봉준호     송강호        34\n 6 영화       만들다        31\n 7 축하       봉준호        31\n 8 대단       축하          27\n 9 봉준호     블랙리스트    27\n10 대박       축하          26\n# ℹ 19,020 more rows\n\n\n\n# 3. 단어쌍 살펴보기\n# 동시 출현 단어쌍\npair %&gt;%\n  filter(item1 == \"대한민국\")\n\n# A tibble: 1,010 × 3\n   item1    item2        n\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n 1 대한민국 봉준호      70\n 2 대한민국 축하        54\n 3 대한민국 자랑        44\n 4 대한민국 영화        30\n 5 대한민국 기생충      27\n 6 대한민국 국민        22\n 7 대한민국 세계        16\n 8 대한민국 아카데미    16\n 9 대한민국 위상        15\n10 대한민국 좋다        14\n# ℹ 1,000 more rows\n\n\n\n# 바이그램 단어쌍\npair_bigram %&gt;%\n  filter(word1 == \"대한민국\")\n\n# A tibble: 109 × 3\n   word1    word2      n\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;\n 1 대한민국 국민      21\n 2 대한민국 자랑      15\n 3 대한민국 영화      11\n 4 대한민국 국격       8\n 5 대한민국 위상       7\n 6 대한민국 만세       6\n 7 대한민국 봉준호     5\n 8 대한민국 문화       4\n 9 대한민국 영광       4\n10 대한민국 기생충     3\n# ℹ 99 more rows\n\n\n\n# 엔그램으로 네트워크 그래프 만들기\n# 네트워크 그래프 데이터 만들기\ngraph_bigram &lt;- pair_bigram %&gt;%\n  filter(n &gt;= 8) %&gt;%\n  as_tbl_graph()\n\n\n# 네트워크 그래프 만들기\nset.seed(1234)\nword_network(graph_bigram)\n\n\n\n# 유의어 통일하고 네트워크 그래프 다시 만들기\n# 유의어 처리\nbigram_seprated &lt;- bigram_seprated %&gt;%\n  mutate(word1 = ifelse(str_detect(word1, \"대단\"), \"대단\", word1),\n         word2 = ifelse(str_detect(word2, \"대단\"), \"대단\", word2),\n         word1 = ifelse(str_detect(word1, \"자랑\"), \"자랑\", word1),\n         word2 = ifelse(str_detect(word2, \"자랑\"), \"자랑\", word2),\n         word1 = ifelse(str_detect(word1, \"짝짝짝\"), \"짝짝짝\", word1),\n         word2 = ifelse(str_detect(word2, \"짝짝짝\"), \"짝짝짝\", word2)) %&gt;%\n  filter(word1 != word2) # 같은 단어 연속 제거\n\n\n# 단어쌍 빈도 구하기\npair_bigram &lt;- bigram_seprated %&gt;%\n  count(word1, word2, sort = T) %&gt;%\n  na.omit()\n\n\n# 네트워크 그래프 데이터 만들기\nset.seed(1234)\ngraph_bigram &lt;- pair_bigram %&gt;%\n  filter(n &gt;= 8) %&gt;%\n  as_tbl_graph(directed = F) %&gt;%\n  mutate(centrality = centrality_degree(), # 중심성\n         group = as.factor(group_infomap())) # 커뮤니티\n\n\n# 네트워크 그래프 만들기\nset.seed(1234)\nggraph(graph_bigram, layout = \"fr\") + # 레이아웃\n  geom_edge_link(color = \"gray50\", # 엣지 색깔\n                 alpha = 0.5) + # 엣지 명암\n  geom_node_point(aes(size = centrality, # 노드 크기\n                      color = group), # 노드 색깔\n                  show.legend = F) + # 범례 삭제\n  scale_size(range = c(4, 8)) + # 노드 크기 범위\n  geom_node_text(aes(label = name), # 텍스트 표시\n                 repel = T, # 노드밖 표시\n                 size = 5, # 텍스트 크기\n                 family = \"nanumgothic\") + # 폰트\n  theme_graph() # 배경 삭제"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_5.html",
    "href": "teaching/media_ds/about/NLP_5.html",
    "title": "Text Pre-processing & Basic Analysis",
    "section": "",
    "text": "한류 & K-pop\n\nK-pop Top 아이돌 가사(t-1)와 중국 아이돌 가사(번역)(t)의 비교를 통해 한류가 중국 아이돌 가사에 어떤 영향을 주는지?"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_5.html#가능한-연구-주제-또는-연구-질문들",
    "href": "teaching/media_ds/about/NLP_5.html#가능한-연구-주제-또는-연구-질문들",
    "title": "Text Pre-processing & Basic Analysis",
    "section": "",
    "text": "한류 & K-pop\n\nK-pop Top 아이돌 가사(t-1)와 중국 아이돌 가사(번역)(t)의 비교를 통해 한류가 중국 아이돌 가사에 어떤 영향을 주는지?"
  },
  {
    "objectID": "teaching/media_ds/about/tidyverse.html",
    "href": "teaching/media_ds/about/tidyverse.html",
    "title": "Data Wranggling with tidyverse in R",
    "section": "",
    "text": "Object\n\n\n\nLearn how to clean, manipulate, and transform data using the tidyverse package in R\n\n\n\nIntroduction\nData wrangling is the process of cleaning, manipulating, and transforming raw data into a more suitable format for data analysis. In R, the tidyverse package is an essential tool for data wrangling. It is a collection of R packages designed for data science, including dplyr, tidyr, ggplot2, and readr. In this tutorial, we will focus on using dplyr and tidyr for data wrangling tasks.\n\nDataset: For this tutorial, we will use the mtcars dataset, which is a built-in dataset in R. It contains data about 32 cars, including their miles per gallon (mpg), number of cylinders (cyl), and horsepower (hp), among other variables.\n\nLet’s get started!\n\nLoad the required packages and dataset\nFirst, we will install and load the tidyverse package and then load the mtcars dataset.\n\n# Install the tidyverse package if not already installed\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the mtcars dataset\ndata(mtcars)\n\n\n\n\nAbout Tidyverse\nThe tidyverse is a collection of R packages designed to make data science tasks more efficient, consistent, and enjoyable. The main philosophy behind the tidyverse is the concept of “tidy data,” which emphasizes a consistent structure for data manipulation and visualization. Tidy data has a simple structure, where each variable forms a column, each observation forms a row, and each cell contains a single value.\nMotivation:\nThe motivation for the tidyverse came from the need for a coherent set of tools for data manipulation and analysis that adhere to a consistent grammar and data structure. This consistency reduces the cognitive load for users, making it easier to learn and apply various tools across different stages of the data science pipeline.\nThe tidyverse was created by Hadley Wickham and is now maintained by a team of developers. The main goals of the tidyverse are:\n\nEncourage a consistent and efficient workflow for data manipulation and visualization.\nMake data analysis more intuitive, reducing the learning curve for newcomers.\nProvide a comprehensive set of tools for common data science tasks.\nPromote best practices in data science, including reproducibility and clear communication.\n\nMain packages:\nSome of the key packages included in the tidyverse are:\n\ndplyr: A package for data manipulation that provides a set of functions for filtering, sorting, selecting, and mutating data, as well as performing grouped operations and joining datasets. You can find a cheatsheet for this package.\nggplot2: A powerful package for data visualization based on the Grammar of Graphics, which allows for the creation of complex and customizable plots using a layered approach. You can find a cheatsheet for this package.\ntidyr: A package for cleaning and reshaping data, with functions for pivoting data between wide and long formats, handling missing values, and more. Please see this cheatsheet for this package.\nreadr: A package for reading and writing data that provides functions for importing and exporting data in various formats, including CSV, TSV, and fixed-width files.\ntibble: A modern take on data frames that offers a more consistent and user-friendly interface, with enhanced printing and subsetting capabilities.\nstringr: A package for working with strings, offering a consistent set of functions for common string manipulation tasks, such as concatenation, splitting, and pattern matching.\nforcats: A package for working with categorical data, providing functions for creating, modifying, and summarizing factors.\n\n\n\nExploring the dataset\nBefore diving into data wrangling, let’s explore the mtcars dataset.\n\n# View the first few rows of the dataset\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Get a summary of the dataset\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nThe mtcars dataset is a classic built-in dataset in R, often used for teaching and demonstrating various data analysis and visualization techniques. It originates from the 1974 Motor Trend US magazine and contains information about 32 different car models, primarily from the 1973-1974 model years.\nThe dataset consists of 11 variables, which include various specifications and performance metrics for each car. The variables in the mtcars dataset are:\n\nmpg: Miles per gallon (fuel efficiency)\ncyl: Number of cylinders in the engine\ndisp: Engine displacement, measured in cubic inches\nhp: Gross horsepower\ndrat: Rear axle ratio\nwt: Weight of the car, in thousands of pounds\nqsec: 1/4 mile time, a measure of acceleration\nvs: Engine type (0 = V-shaped, 1 = straight)\nam: Transmission type (0 = automatic, 1 = manual)\ngear: Number of forward gears\ncarb: Number of carburetors\n\n\nGlimpse!\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n\nData Wrangling tasks\nSelecting columns\nLet’s say you’re only interested in analyzing the mpg, hp, and wt (weight) columns. You can use dplyr’s select() function to choose specific columns.\n\n# Select mpg, hp, and wt columns\nselected_data &lt;- mtcars %&gt;%\n  select(mpg, hp, wt)\n\nhead(selected_data)\n\n                   mpg  hp    wt\nMazda RX4         21.0 110 2.620\nMazda RX4 Wag     21.0 110 2.875\nDatsun 710        22.8  93 2.320\nHornet 4 Drive    21.4 110 3.215\nHornet Sportabout 18.7 175 3.440\nValiant           18.1 105 3.460\n\n\nSometimes, we need to pull() only a vector from the dataset.\n\n# pull mpg as a vector\nmtcars %&gt;%\n  pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nUse sample_frac() to select a random fraction of the dataset:\n\n# Select a random 50% of the dataset\nsampled_frac &lt;- mtcars %&gt;%\n  sample_frac(0.5)\n\nglimpse(sampled_frac)\n\nRows: 16\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 30.4, 24.4, 14.7, 15.2, 17.3, 19.2, 10.4, 15.5, 22.8, 16.4,…\n$ cyl  &lt;dbl&gt; 6, 4, 4, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 4, 4, 8\n$ disp &lt;dbl&gt; 160.0, 75.7, 146.7, 440.0, 304.0, 275.8, 400.0, 472.0, 318.0, 108…\n$ hp   &lt;dbl&gt; 110, 52, 62, 230, 150, 180, 175, 205, 150, 93, 180, 245, 264, 109…\n$ drat &lt;dbl&gt; 3.90, 4.93, 3.69, 3.23, 3.15, 3.07, 3.08, 2.93, 2.76, 3.85, 3.07,…\n$ wt   &lt;dbl&gt; 2.875, 1.615, 3.190, 5.345, 3.435, 3.730, 3.845, 5.250, 3.520, 2.…\n$ qsec &lt;dbl&gt; 17.02, 18.52, 20.00, 17.42, 17.30, 17.60, 17.05, 17.98, 16.87, 18…\n$ vs   &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0\n$ am   &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 5, 4, 4, 3\n$ carb &lt;dbl&gt; 4, 2, 2, 4, 2, 3, 2, 4, 2, 1, 3, 4, 4, 2, 1, 3\n\n\nUse sample_n() to select a random number of rows from the dataset:\n\n# Select a random 10 rows from the dataset\nsampled_n &lt;- mtcars %&gt;%\n  sample_n(10)\n\nglimpse(sampled_n)\n\nRows: 10\nColumns: 11\n$ mpg  &lt;dbl&gt; 22.8, 15.8, 27.3, 15.5, 13.3, 33.9, 21.0, 19.2, 17.3, 21.4\n$ cyl  &lt;dbl&gt; 4, 8, 4, 8, 8, 4, 6, 6, 8, 4\n$ disp &lt;dbl&gt; 140.8, 351.0, 79.0, 318.0, 350.0, 71.1, 160.0, 167.6, 275.8, 121.0\n$ hp   &lt;dbl&gt; 95, 264, 66, 150, 245, 65, 110, 123, 180, 109\n$ drat &lt;dbl&gt; 3.92, 4.22, 4.08, 2.76, 3.73, 4.22, 3.90, 3.92, 3.07, 4.11\n$ wt   &lt;dbl&gt; 3.150, 3.170, 1.935, 3.520, 3.840, 1.835, 2.875, 3.440, 3.730, 2.…\n$ qsec &lt;dbl&gt; 22.90, 14.50, 18.90, 16.87, 15.41, 19.90, 17.02, 18.30, 17.60, 18…\n$ vs   &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 1, 0, 1\n$ am   &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 0, 0, 1\n$ gear &lt;dbl&gt; 4, 5, 4, 3, 3, 4, 4, 4, 3, 4\n$ carb &lt;dbl&gt; 2, 4, 1, 2, 4, 1, 4, 4, 3, 2\n\n\nUse slice() to select specific rows by index:\n\n# Select rows 1, 5, and 10 from the dataset\nsliced_rows &lt;- mtcars %&gt;%\n  slice(c(1, 5, 10))\n\nhead(sliced_rows)\n\n                   mpg cyl  disp  hp drat   wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.62 16.46  0  1    4    4\nHornet Sportabout 18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2\nMerc 280          19.2   6 167.6 123 3.92 3.44 18.30  1  0    4    4\n\n\nUse top_n() to select the top n rows based on a specific variable:\n\n# Select the top 5 cars with the highest mpg\ntop_cars &lt;- mtcars %&gt;%\n  top_n(5, wt = mpg)\n\nhead(top_cars)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nFiat 128       32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4 79.0  66 4.08 1.935 18.90  1  1    4    1\nLotus Europa   30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\nThese functions provide various ways to sample, select, and filter rows in the mtcars dataset, allowing you to explore and analyze the data more effectively.\nFiltering rows\nYou might want to analyze cars with specific characteristics. For instance, let’s filter the dataset to include only cars with mpg greater than 20 and hp less than 150.\n\n# Filter cars with mpg &gt; 20 and hp &lt; 150\nfiltered_data &lt;- mtcars %&gt;%\n  filter(mpg &gt; 20, hp &lt; 150)\n\nhead(filtered_data)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n\n\n\nSorting data\nTo sort the dataset by a specific column, use the arrange() function. Let’s sort the filtered_data by mpg in descending order.\n\n# Sort filtered_data by mpg in descending order\nsorted_data &lt;- filtered_data %&gt;%\n  arrange(desc(mpg))\n\nhead(sorted_data)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\n\n\nCreating new columns\nYou might want to create a new column based on existing columns. For instance, let’s create a column called “performance” calculated as hp / wt.\n\n# Create a new column 'performance'\nwith_performance &lt;- mtcars %&gt;%\n  mutate(performance = hp / wt)\n\nglimpse(with_performance)\n\nRows: 32\nColumns: 12\n$ mpg         &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2…\n$ cyl         &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4…\n$ disp        &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 14…\n$ hp          &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 1…\n$ drat        &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92…\n$ wt          &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.…\n$ qsec        &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22…\n$ vs          &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1…\n$ am          &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1…\n$ gear        &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4…\n$ carb        &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1…\n$ performance &lt;dbl&gt; 41.98473, 38.26087, 40.08621, 34.21462, 50.87209, 30.34682…\n\n\nFunctions that are too convenient to use with select(): contains(), ends_with(), starts_with()\nFirstly, we add new columns to the mtcars dataset:\n\n# Add new columns to the mtcars dataset\nmtcars_new &lt;- mtcars %&gt;%\n  mutate(hp_disp_ratio = hp / disp,\n         disp_hp_ratio = disp / hp,\n         wt_mpg_ratio = wt / mpg)\n\nglimpse(mtcars_new)\n\nRows: 32\nColumns: 14\n$ mpg           &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19…\n$ cyl           &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4,…\n$ disp          &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, …\n$ hp            &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180,…\n$ drat          &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.…\n$ wt            &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, …\n$ qsec          &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, …\n$ vs            &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,…\n$ am            &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,…\n$ gear          &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ carb          &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2,…\n$ hp_disp_ratio &lt;dbl&gt; 0.6875000, 0.6875000, 0.8611111, 0.4263566, 0.4861111, 0…\n$ disp_hp_ratio &lt;dbl&gt; 1.454545, 1.454545, 1.161290, 2.345455, 2.057143, 2.1428…\n$ wt_mpg_ratio  &lt;dbl&gt; 0.12476190, 0.13690476, 0.10175439, 0.15023364, 0.183957…\n\n\nUse contains() to select columns that contain a specific string:\n\n# Select columns containing the string 'ratio'\nratio_columns &lt;- mtcars_new %&gt;%\n  select(contains(\"ratio\"))\n\nhead(ratio_columns)\n\n                  hp_disp_ratio disp_hp_ratio wt_mpg_ratio\nMazda RX4             0.6875000      1.454545    0.1247619\nMazda RX4 Wag         0.6875000      1.454545    0.1369048\nDatsun 710            0.8611111      1.161290    0.1017544\nHornet 4 Drive        0.4263566      2.345455    0.1502336\nHornet Sportabout     0.4861111      2.057143    0.1839572\nValiant               0.4666667      2.142857    0.1911602\n\n\nUse ends_with() to select columns that end with a specific string:\n\n# Select columns ending with the string 'ratio'\nending_ratio_columns &lt;- mtcars_new %&gt;%\n  select(ends_with(\"ratio\"))\n\nhead(ending_ratio_columns)\n\n                  hp_disp_ratio disp_hp_ratio wt_mpg_ratio\nMazda RX4             0.6875000      1.454545    0.1247619\nMazda RX4 Wag         0.6875000      1.454545    0.1369048\nDatsun 710            0.8611111      1.161290    0.1017544\nHornet 4 Drive        0.4263566      2.345455    0.1502336\nHornet Sportabout     0.4861111      2.057143    0.1839572\nValiant               0.4666667      2.142857    0.1911602\n\n\nUse starts_with() to select columns that start with a specific string:\n\n# Select columns starting with the string 'hp'\nstarting_hp_columns &lt;- mtcars_new %&gt;%\n  select(starts_with(\"hp\"))\n\nhead(starting_hp_columns)\n\n                   hp hp_disp_ratio\nMazda RX4         110     0.6875000\nMazda RX4 Wag     110     0.6875000\nDatsun 710         93     0.8611111\nHornet 4 Drive    110     0.4263566\nHornet Sportabout 175     0.4861111\nValiant           105     0.4666667\n\n\nThese functions are helpful when you need to select multiple columns that share a specific naming pattern, such as a common prefix or suffix. They can simplify your code and make it more readable, especially when working with large datasets containing numerous columns.\n\nUse transmute() to create a new dataset with transformed columns:\n\n# Create a new dataset with the mpg to km/l conversion, and engine displacement from cubic inches to liters\nmtcars_transformed &lt;- mtcars %&gt;%\n  transmute(mpg_to_kml = mpg * 0.425144, # 1 mile per gallon is approximately 0.425144 km/l\n            liters = disp * 0.0163871)   # 1 cubic inch is approximately 0.0163871 liters\n\nhead(mtcars_transformed)\n\n                  mpg_to_kml   liters\nMazda RX4           8.928024 2.621936\nMazda RX4 Wag       8.928024 2.621936\nDatsun 710          9.693283 1.769807\nHornet 4 Drive      9.098082 4.227872\nHornet Sportabout   7.950193 5.899356\nValiant             7.695106 3.687098\n\n\nUse rename() to rename columns in the mtcars dataset:\n\n# Rename the 'mpg' column to 'miles_per_gallon' and the 'disp' column to 'displacement'\nmtcars_renamed &lt;- mtcars %&gt;%\n  rename(miles_per_gallon = mpg,\n         displacement = disp)\n\nhead(mtcars_renamed)\n\n                  miles_per_gallon cyl displacement  hp drat    wt  qsec vs am\nMazda RX4                     21.0   6          160 110 3.90 2.620 16.46  0  1\nMazda RX4 Wag                 21.0   6          160 110 3.90 2.875 17.02  0  1\nDatsun 710                    22.8   4          108  93 3.85 2.320 18.61  1  1\nHornet 4 Drive                21.4   6          258 110 3.08 3.215 19.44  1  0\nHornet Sportabout             18.7   8          360 175 3.15 3.440 17.02  0  0\nValiant                       18.1   6          225 105 2.76 3.460 20.22  1  0\n                  gear carb\nMazda RX4            4    4\nMazda RX4 Wag        4    4\nDatsun 710           4    1\nHornet 4 Drive       3    1\nHornet Sportabout    3    2\nValiant              3    1\n\n\n\n\nGrouping and summarizing data\nGrouping data can help you understand it better. Let’s group the dataset by the number of cylinders (cyl) and calculate the average mpg for each group.\n\n# Group data by 'cyl' and calculate average 'mpg'\ngrouped_data &lt;- mtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(avg_mpg = mean(mpg))\n\ngrouped_data\n\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\nCalculate the median, minimum, and maximum mpg for each number of cylinders:\n\ngrouped_data_2 &lt;- mtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(median_mpg = median(mpg),\n            min_mpg = min(mpg),\n            max_mpg = max(mpg))\n\ngrouped_data_2\n\n# A tibble: 3 × 4\n    cyl median_mpg min_mpg max_mpg\n  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     4       26      21.4    33.9\n2     6       19.7    17.8    21.4\n3     8       15.2    10.4    19.2\n\n\nGroup data by the number of gears and calculate the average horsepower and engine displacement:\n\ngrouped_data_3 &lt;- mtcars %&gt;%\n  group_by(gear) %&gt;%\n  summarize(avg_hp = mean(hp),\n            avg_disp = mean(disp))\n\ngrouped_data_3\n\n# A tibble: 3 × 3\n   gear avg_hp avg_disp\n  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1     3  176.      326.\n2     4   89.5     123.\n3     5  196.      202.\n\n\nGroup data by transmission type (automatic or manual) and calculate the average mpg, total horsepower, and number of cars in each group:\n\ngrouped_data_4 &lt;- mtcars %&gt;%\n  group_by(am) %&gt;%\n  summarize(avg_mpg = mean(mpg),\n            total_hp = sum(hp),\n            num_cars = n())\n\ngrouped_data_4\n\n# A tibble: 2 × 4\n     am avg_mpg total_hp num_cars\n  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1     0    17.1     3045       19\n2     1    24.4     1649       13\n\n\nHandling missing values\nIf your dataset has missing values, you can use tidyr’s drop_na() function to remove rows with missing values or replace_na() function to replace missing values with a specified value.\nFor this tutorial, let’s artificially introduce missing values in the mtcars dataset and then perform missing value handling.\n\n# Introduce missing values to mtcars dataset\nset.seed(42)\nmtcars_with_na &lt;- mtcars %&gt;%\n  mutate(mpg = ifelse(runif(n()) &lt; 0.1, NA, mpg))\n\n# Check for missing values\nsummary(mtcars_with_na)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.35   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.12   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n NA's   :1                                                      \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n                                                                 \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n                                                 \n\n\nRemoving rows with missing values\n\n# Remove rows with missing values in 'mpg' column\nno_na_rows &lt;- mtcars_with_na %&gt;%\n  drop_na(mpg)\n\nsummary(no_na_rows)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.35   1st Qu.:4.000   1st Qu.:120.7   1st Qu.: 96.0  \n Median :19.20   Median :6.000   Median :167.6   Median :123.0  \n Mean   :20.12   Mean   :6.129   Mean   :225.3   Mean   :145.8  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:311.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.115   1st Qu.:2.542   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.700   Median :3.215   Median :17.82   Median :0.0000  \n Mean   :3.613   Mean   :3.197   Mean   :17.87   Mean   :0.4516  \n 3rd Qu.:3.920   3rd Qu.:3.570   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear           carb      \n Min.   :0.0000   Min.   :3.00   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.00   1st Qu.:2.000  \n Median :0.0000   Median :4.00   Median :2.000  \n Mean   :0.4194   Mean   :3.71   Mean   :2.839  \n 3rd Qu.:1.0000   3rd Qu.:4.00   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.00   Max.   :8.000  \n\n\n\n\nReplacing missing values\nIf you want to replace missing values with a specific value or the mean value of the column, use the replace_na() function.\n\n# Replace missing values in 'mpg' column with the mean value\nmean_mpg &lt;- mean(mtcars$mpg, na.rm = TRUE)\nreplaced_na &lt;- mtcars_with_na %&gt;%\n  mutate(mpg = replace_na(mpg, mean_mpg))\n\nsummary(replaced_na)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.45   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.12   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\n\n\nQZs\n\nUsing the mtcars dataset, filter the data to only include cars with a manual transmission (am = 1) and sort the result by miles per gallon (mpg) in descending order. What are the top 3 cars in terms of mpg?\n\n\nMazda RX4, Mazda RX4 Wag, Datsun 710\nFiat 128, Honda Civic, Toyota Corolla\nLotus Europa, Porsche 914-2, Ford Pantera L\n\n\n\nUsing the mtcars dataset, group the data by the number of cylinders (cyl) and calculate the average miles per gallon (mpg) for each group. Which group of cars has the highest average mpg?\n\n\n4 cylinders\n6 cylinders\n8 cylinders\n\n\n\nUsing the mtcars dataset, create a new column called “mpg_kml” that converts the mpg values to km/l (1 mile per gallon is approximately 0.425144 km/l). Then, rename the “mpg” column to “miles_per_gallon”.\n\n\n\nLet’s use gapminder dataset. Calculate the average GDP per capita for each continent and display the results in descending order.\n\n\nif (!requireNamespace(\"gapminder\", quietly = TRUE)) {\n  install.packages(\"gapminder\")\n}\n\nlibrary(gapminder)\ngapminder %&gt;% head\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\n\n\nFilter the dataset for the year 2007 and randomly select 100 countries. Then, arrange these countries in descending order of life expectancy and display the top 5 countries with the highest life expectancy.\n\n\n\n\n\nCombining datasets\nSometimes, you may need to combine datasets. You can use dplyr’s bind_rows() function to combine datasets vertically (by rows) and bind_cols() function to combine datasets horizontally (by columns).\nFor this tutorial, let’s create two datasets and then combine them.\n\n# Create two datasets\ndataset_1 &lt;- mtcars %&gt;%\n  filter(cyl == 4) %&gt;% \n  head(3)\n\ndataset_1\n\n            mpg cyl  disp hp drat   wt  qsec vs am gear carb\nDatsun 710 22.8   4 108.0 93 3.85 2.32 18.61  1  1    4    1\nMerc 240D  24.4   4 146.7 62 3.69 3.19 20.00  1  0    4    2\nMerc 230   22.8   4 140.8 95 3.92 3.15 22.90  1  0    4    2\n\ndataset_2 &lt;- mtcars %&gt;%\n  filter(cyl == 6) %&gt;% \n  head(3)\n\ndataset_2\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n# Combine datasets vertically\ncombined_data &lt;- bind_rows(dataset_1, dataset_2)\n\n# Check the combined data\ncombined_data\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n\n\n\nTable Joining\nIn this section, we’ll demonstrate how to join tables using dplyr’s join functions. We will use a new dataset called “car_brands” that contains information about each car’s brand.\n\n# Create a car_brands dataset\ncar_brands &lt;- tibble(\n  car_name = rownames(mtcars),\n  brand = c(rep(\"Mazda\", 2), rep(\"Datsun\", 1), rep(\"Hornet\", 2), \n            rep(\"Valiant\", 1), rep(\"Duster\", 1), rep(\"Merc\", 7), \n            rep(\"Cadillac\", 1), rep(\"Lincoln\", 1), rep(\"Chrysler\", 1), \n            rep(\"Fiat\", 2), rep(\"Honda\", 1), rep(\"Toyota\", 2), \n            rep(\"Dodge\", 1), rep(\"AMC\", 1), rep(\"Camaro\", 1), \n            rep(\"Pontiac\", 1), rep(\"Porsche\", 1), rep(\"Lotus\", 1), \n            rep(\"Ford\", 1), rep(\"Ferrari\", 1), rep(\"Maserati\", 1), \n            rep(\"Volvo\", 1))\n)\n\ncar_brands\n\n# A tibble: 32 × 2\n   car_name          brand  \n   &lt;chr&gt;             &lt;chr&gt;  \n 1 Mazda RX4         Mazda  \n 2 Mazda RX4 Wag     Mazda  \n 3 Datsun 710        Datsun \n 4 Hornet 4 Drive    Hornet \n 5 Hornet Sportabout Hornet \n 6 Valiant           Valiant\n 7 Duster 360        Duster \n 8 Merc 240D         Merc   \n 9 Merc 230          Merc   \n10 Merc 280          Merc   \n# ℹ 22 more rows\n\n# Convert rownames of mtcars to a column called \"car_name\"\nmtcars &lt;- mtcars %&gt;%\n  rownames_to_column(var = \"car_name\")\n\nmtcars %&gt;% glimpse\n\nRows: 32\nColumns: 12\n$ car_name &lt;chr&gt; \"Mazda RX4\", \"Mazda RX4 Wag\", \"Datsun 710\", \"Hornet 4 Drive\",…\n$ mpg      &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 1…\n$ cyl      &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4…\n$ disp     &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8…\n$ hp       &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180,…\n$ drat     &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3…\n$ wt       &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150…\n$ qsec     &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90…\n$ vs       &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1…\n$ am       &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0…\n$ gear     &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3…\n$ carb     &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1…\n\n\nInner join\nInner join returns only the rows with matching keys in both tables.\nSee the types of joins below.\n\n\n# Perform inner join\ninner_joined &lt;- mtcars %&gt;%\n  inner_join(car_brands, by = \"car_name\")\n\nhead(inner_joined)\n\n           car_name  mpg cyl disp  hp drat    wt  qsec vs am gear carb   brand\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   Mazda\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   Mazda\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  Datsun\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  Hornet\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2  Hornet\n6           Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 Valiant\n\n\nLeft join\nLeft join returns all the rows from the left table and the matched rows from the right table. If no match is found, NA values are returned for right table columns.\n\n# Perform left join\nleft_joined &lt;- mtcars %&gt;%\n  left_join(car_brands, by = \"car_name\")\n\nhead(left_joined)\n\n           car_name  mpg cyl disp  hp drat    wt  qsec vs am gear carb   brand\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   Mazda\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   Mazda\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  Datsun\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  Hornet\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2  Hornet\n6           Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 Valiant\n\n\n\n\n\nPivoting\n\npivot_longer\npivot_longer() is used to transform a dataset from “wide” to “long” format. It takes multiple columns and collapses them into key-value pairs.\n\nFor this example, we’ll create a dataset called “car_data” to demonstrate the pivot_longer() function.\n\n# Create a car_data dataset\ncar_data &lt;- tibble(\n  car_name = c(\"Mazda RX4\", \"Mazda RX4 Wag\", \"Datsun 710\"),\n  mpg = c(21.0, 21.0, 22.8),\n  hp = c(110, 110, 93),\n  wt = c(2.620, 2.875, 2.320)\n)\ncar_data\n\n# A tibble: 3 × 4\n  car_name        mpg    hp    wt\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4      21     110  2.62\n2 Mazda RX4 Wag  21     110  2.88\n3 Datsun 710     22.8    93  2.32\n\n\nUse pivot_longer to collapse the ‘mpg’, ‘hp’, and ‘wt’ columns into key-value pairs.\n\n# Transform car_data from wide to long format\nlong_car_data &lt;- car_data %&gt;%\n  pivot_longer(cols = c(mpg, hp, wt),\n               names_to = \"variable\",\n               values_to = \"value\")\n\nlong_car_data\n\n# A tibble: 9 × 3\n  car_name      variable  value\n  &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;\n1 Mazda RX4     mpg       21   \n2 Mazda RX4     hp       110   \n3 Mazda RX4     wt         2.62\n4 Mazda RX4 Wag mpg       21   \n5 Mazda RX4 Wag hp       110   \n6 Mazda RX4 Wag wt         2.88\n7 Datsun 710    mpg       22.8 \n8 Datsun 710    hp        93   \n9 Datsun 710    wt         2.32\n\n\n\n\npivot_wider\npivot_wider() is used to transform a dataset from “long” to “wide” format. It spreads key-value pairs across multiple columns.\n\nFor this example, we’ll use the “long_car_data” dataset created in the previous step.\n\n# Transform long_car_data back to wide format\nwide_car_data &lt;- long_car_data %&gt;%\n  pivot_wider(names_from = variable,\n              values_from = value)\n\nprint(wide_car_data)\n\n# A tibble: 3 × 4\n  car_name        mpg    hp    wt\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4      21     110  2.62\n2 Mazda RX4 Wag  21     110  2.88\n3 Datsun 710     22.8    93  2.32\n\n\n\n\n\nAdditional data wrangling tasks\n\nCounting occurrences\nTo count the occurrences of a specific value or group of values in a dataset, use the count() function.\n\n# Count the occurrences of each brand in the car_brands dataset\nbrand_counts &lt;- car_brands %&gt;%\n  count(brand, sort = TRUE)\n\nbrand_counts\n\n# A tibble: 22 × 2\n   brand        n\n   &lt;chr&gt;    &lt;int&gt;\n 1 Merc         7\n 2 Fiat         2\n 3 Hornet       2\n 4 Mazda        2\n 5 Toyota       2\n 6 AMC          1\n 7 Cadillac     1\n 8 Camaro       1\n 9 Chrysler     1\n10 Datsun       1\n# ℹ 12 more rows\n\n\n\n\nNesting and unnesting\nNesting and unnesting data can be useful for performing operations on grouped data. Let’s demonstrate this by calculating the mean mpg for each brand.\n\n# Nest the mtcars dataset by brand\nhead(left_joined)\n\n           car_name  mpg cyl disp  hp drat    wt  qsec vs am gear carb   brand\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   Mazda\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   Mazda\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  Datsun\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  Hornet\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2  Hornet\n6           Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 Valiant\n\nnested_data &lt;- left_joined %&gt;%\n  group_by(brand) %&gt;%\n  nest()\n\nnested_data\n\n# A tibble: 22 × 2\n# Groups:   brand [22]\n   brand    data             \n   &lt;chr&gt;    &lt;list&gt;           \n 1 Mazda    &lt;tibble [2 × 12]&gt;\n 2 Datsun   &lt;tibble [1 × 12]&gt;\n 3 Hornet   &lt;tibble [2 × 12]&gt;\n 4 Valiant  &lt;tibble [1 × 12]&gt;\n 5 Duster   &lt;tibble [1 × 12]&gt;\n 6 Merc     &lt;tibble [7 × 12]&gt;\n 7 Cadillac &lt;tibble [1 × 12]&gt;\n 8 Lincoln  &lt;tibble [1 × 12]&gt;\n 9 Chrysler &lt;tibble [1 × 12]&gt;\n10 Fiat     &lt;tibble [2 × 12]&gt;\n# ℹ 12 more rows\n\n# Calculate mean mpg for each brand\nmean_mpg_by_brand &lt;- nested_data %&gt;%\n  mutate(mean_mpg = map_dbl(data, ~ mean(.x$mpg, na.rm = TRUE)))\nmean_mpg_by_brand\n\n# A tibble: 22 × 3\n# Groups:   brand [22]\n   brand    data              mean_mpg\n   &lt;chr&gt;    &lt;list&gt;               &lt;dbl&gt;\n 1 Mazda    &lt;tibble [2 × 12]&gt;     21  \n 2 Datsun   &lt;tibble [1 × 12]&gt;     22.8\n 3 Hornet   &lt;tibble [2 × 12]&gt;     20.0\n 4 Valiant  &lt;tibble [1 × 12]&gt;     18.1\n 5 Duster   &lt;tibble [1 × 12]&gt;     14.3\n 6 Merc     &lt;tibble [7 × 12]&gt;     19.0\n 7 Cadillac &lt;tibble [1 × 12]&gt;     10.4\n 8 Lincoln  &lt;tibble [1 × 12]&gt;     10.4\n 9 Chrysler &lt;tibble [1 × 12]&gt;     14.7\n10 Fiat     &lt;tibble [2 × 12]&gt;     31.4\n# ℹ 12 more rows\n\n# Unnest the nested data\nunnested_data &lt;- mean_mpg_by_brand %&gt;%\n  unnest(cols = data)\n\nhead(unnested_data)\n\n# A tibble: 6 × 14\n# Groups:   brand [4]\n  brand   car_name     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda   Mazda RX4   21       6   160   110  3.9   2.62  16.5     0     1     4\n2 Mazda   Mazda RX4…  21       6   160   110  3.9   2.88  17.0     0     1     4\n3 Datsun  Datsun 710  22.8     4   108    93  3.85  2.32  18.6     1     1     4\n4 Hornet  Hornet 4 …  21.4     6   258   110  3.08  3.22  19.4     1     0     3\n5 Hornet  Hornet Sp…  18.7     8   360   175  3.15  3.44  17.0     0     0     3\n6 Valiant Valiant     18.1     6   225   105  2.76  3.46  20.2     1     0     3\n# ℹ 2 more variables: carb &lt;dbl&gt;, mean_mpg &lt;dbl&gt;\n\n\n\n\nWindow functions\nWindow functions perform calculations across a set of rows related to the current row. Let’s calculate the rank of each car within its brand based on mpg.\n\n# Calculate rank within brand by mpg\nranking_by_mpg &lt;- left_joined %&gt;%\n  group_by(brand) %&gt;%\n  mutate(rank = dense_rank(desc(mpg)))\n\nhead(ranking_by_mpg)\n\n# A tibble: 6 × 14\n# Groups:   brand [4]\n  car_name       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6 Valiant       18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n# ℹ 2 more variables: brand &lt;chr&gt;, rank &lt;int&gt;"
  },
  {
    "objectID": "teaching/media_ds/index.html",
    "href": "teaching/media_ds/index.html",
    "title": "Media & Data Science",
    "section": "",
    "text": "a &lt;- \"미디어와\"\nb &lt;- \"데이터사이언스\"\npaste0(\"Welcome to \", a,\" \",b, \" 수업\")\n\n[1] \"Welcome to 미디어와 데이터사이언스 수업\"\n\n\n\n\n\n\n\n\n\n\n\nNotice\n(공지)\n\n\n\nPre-class videos\n(주차별 선행 학습)\n\n\n\nSyllabus\n(주차별 수업 자료)"
  },
  {
    "objectID": "teaching/media_ds/preclass/index.html",
    "href": "teaching/media_ds/preclass/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Week\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/ml101/icpbl/domestic.html",
    "href": "teaching/ml101/icpbl/domestic.html",
    "title": "IC-PBL",
    "section": "",
    "text": "Team Assignment\n\nRandom assignment (See the method)\n\n\nThursday classTuesday class\n\n\n\n\n\nTeam 1\nTeam 2\nTeam 3\n\n\n\n\n박승연\n김상경\n김재엽\n\n\n박지원\n박종현\n김찬우\n\n\n윤지성\n이윤진\n정재윤\n\n\n이유정\n정지윤\n정혜림\n\n\n\n\n\n\n\n\nTeam 1\nTeam 2\nTeam 3\n\n\n\n\n김숭기\n김민지\n김가영\n\n\n김정환\n김원\n박은서\n\n\n문하윤\n노솔\n이정헌\n\n\n조민석\n임예빈\n최지희\n\n\n\n\n\n\n\n\n시나리오\n독서어플 리더스에서 데이터를 제공 받아 주어진 문제를 풀게 됨.\n\n리더스 소개\n\n소개 블로그\n소개 기사\n\n리더스 다운로드\n\nAndroid\niOS\n\n\n\n\n\n\n\n\n문제\n\n\n\n독서 앱에서 사용하는 고객의 독서 관련 데이터를 받아 수업시간에 배운 기계학습 모델과 텍스트 분석 방법을 활용하여 새로운 서비스를 제안한다.\n\n\n\n\n\n지난 수업(2022-1) 결과물 모음(참고용)\n\n\n\n오전 1조\n오전 2조\n오전 3조\n오전 4조\n\n\n오후 1조\n오후 2조\n오후 3조\n오후 4조\n\n\n\n\n\n\n리더스 제공 데이터\n\n데이터 다운로드 [Here]\n\n한양대 계정으로 로그인해야 다운 받을 수 있습니다.\n\n\nR 에서 제공 데이터 불러오기\n\n#####\n# Import readers DB\n#####\n\n# Sys.setlocale(\"LC_ALL\", locale=\"Korean\")\n\nlibrary(readxl)\n\nuser &lt;- read_excel(\"readers/01_user.xlsx\")\nuser_cat &lt;- read_excel(\"readers/02_user_cat.xlsx\")\nfollow  &lt;- read_excel(\"readers/03_follow.xlsx\")\nuser_book  &lt;- read_excel(\"readers/04_user_book.xlsx\")\nbook  &lt;- read_excel(\"readers/05_book.xlsx\")\nbook_cat  &lt;- read_excel(\"readers/06_book_cat.xlsx\")\ncat  &lt;- read_excel(\"readers/07_cat.xlsx\")\nscrap  &lt;- read_excel(\"readers/08_scrap.xlsx\")\n\n\n\n\nEDA in brief\n\n\nuser: 유저 정보\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nbirth_year\ngender\ncreated_at\n직업\n직종\n\n\n\n\n10936\n1986\nM\n2019-07-20 11:16:15+00\n직장인\n디자인\n\n\n22042\n1988\nM\n2019-09-09 12:49:27+00\n직장인\n경영지원\n\n\n29446\n1987\nF\n2019-09-09 13:19:59+00\n직장인\n경영지원\n\n\n73870\n1985\nF\n2019-09-09 14:10:13+00\n직장인\n경영지원\n\n\n414454\n1991\nF\n2019-09-09 15:12:12+00\n직장인\n영업/고객상담\n\n\n532918\n1989\nF\n2019-09-09 15:44:42+00\n직장인\n디자인\n\n\n536620\n1985\nF\n2019-09-09 15:45:02+00\n직장인\n마케팅/광고/홍보\n\n\n592150\n1989\nF\n2019-09-09 15:50:59+00\n직장인\nNA\n\n\n629170\n1991\nF\n2019-09-09 15:54:22+00\n직장인\nNA\n\n\n917926\n1984\nF\n2019-09-24 12:54:10+00\n직장인\n의료\n\n\n\n\n\n\nuser_id: 유저 고유번호\nbirth_year: 생년\ngender: 성별\ncreated_at: 가입 날짜와 시간\n직업\n직종\n\n\n\nuser_cat: 유저의 관심 책 카테고리\n\n\n\n\n\n\nuser_id\ntitle\n\n\n\n\n10936\n언어\n\n\n10936\n에세이\n\n\n10936\n습관\n\n\n10936\n시\n\n\n10936\n다이어트\n\n\n10936\n부동산\n\n\n10936\n교육\n\n\n10936\n예술\n\n\n10936\n건강\n\n\n10936\n디자인\n\n\n\n\n\n\nuser_id: 유저 고유번호\ntitle: 해당 유저가 관심 있는 책의 카테고리\n\n\n한 유저가 복수의 카테고리를 선택 가능함. 위의 10936 유저는 언어, 에세이, 습관, 시, 다이어트, 부동산 카테고리 등에 관심이 있음.\n\n\n\nfollow: 유저의 팔로우 팔로워 관계\n\n\n\n\n\n\nuser_id\ntarget_user_id\n\n\n\n\n69249442\n13230778\n\n\n69249442\n48855124\n\n\n69249442\n21104932\n\n\n344337658\n48118426\n\n\n344337658\n34169290\n\n\n344337658\n13038274\n\n\n283176916\n48288718\n\n\n154550926\n423038476\n\n\n154550926\n271563742\n\n\n154550926\n71648338\n\n\n\n\n\n\nuser_id\ntarget_user_id: user_id의 유저가 팔로우 하는 유저의 고유 번호\n\n\n\nuser_book: 유저의 책장에 담긴 책 고유 번호와 현재 독서 상태\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nbook_id\nrate\nread_status\nmodified_at\n\n\n\n\n69249442\n2515\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:08:02.451517\n\n\n69249442\n39950\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:07:16.139095\n\n\n69249442\n35001\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:06:38.229688\n\n\n69249442\n25786\n0\nREAD_STATUS_DONE\n2022-03-20 11:05:44.171380\n\n\n69249442\n38623\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:05:23.039321\n\n\n69249442\n67901\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:05:03.298657\n\n\n69249442\n69673\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:04:39.875480\n\n\n69249442\n451816\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:03:50.586800\n\n\n69249442\n451815\nNA\nREAD_STATUS_PAUSE\n2022-03-20 11:03:35.242273\n\n\n69249442\n115635\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:03:15.823352\n\n\n\n\n\n\nuser_id\nbook_id: 책의 고유 번호\nrate: 해당 책에 유저가 준 평점 (0~5)\nread_status: 최종 수정 시각의 독서 상태\n\nREAD_STATUS_BEFORE → 읽고 싶은 책 (하지만 아직 읽기 전)\nREAD_STATUS_DONE → 읽기 완료 (최종 수정 시각에)\nREAD_STATUS_ING → 읽는 중\nREAD_STATUS_PAUSE → 독서 (일시) 중단\nREAD_STATUS_STOP → 독서 중단\n\nmodified_at: read_status 최종 수정 시각: 이 수정 시각에 최종으로 읽기 상태를 수정한 것\n\n\n69249442 유저는 2515 책을 읽고 싶고(BEFORE) 이 레코드를 남긴 최종 시각은 2022년 3월 20일 11시 8분 2초경이다.\n\n\n\nbook: 책에 대한 정보\n\n\n\n# A tibble: 10 × 14\n   ...1       id title           sub_title isbn13 pub_date description publisher\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;    \n 1 1     1124370 무엇이든 마녀…  &lt;NA&gt;      &lt;NA&gt;   2017-02… &lt;NA&gt;        예림당   \n 2 2       86580 새벽 거리에서   &lt;NA&gt;      97889… 2011-09… 히가시노 …  재인     \n 3 3      262921 세계사의 구조   &lt;NA&gt;      97889… 2012-12… '가라타니 … 비(도서… \n 4 4     2015776 모래그릇 1      &lt;NA&gt;      97889… 2017-12… 일본 사회…  문학동네 \n 5 5       56903 가끔은 웅크리…  복잡다단… 97911… 2018-06… 어린 시절 … 빌리버튼 \n 6 6       47114 머릿속 생각을 … 그러니까… 97889… 2018-01… 화려한 말…  시그마북…\n 7 7     1310946 세컨드 라이프 … 인생을 …  97889… 2019-08… 투자금융사… 문학동네 \n 8 8      289094 돈 카를로스 (…  &lt;NA&gt;      97889… 2014-03… '문학동네 … 문학동네 \n 9 9      232305 즐거운 북아트…  현직 교…  97889… 2010-10… 초등학교 …  우리책   \n10 10    1142843 만화로 읽는 조… &lt;NA&gt;      97889… 2008-09… &lt;NA&gt;        태동출판…\n# ℹ 6 more variables: category_name_aladin &lt;chr&gt;, page &lt;dbl&gt;,\n#   full_description &lt;chr&gt;, full_description_2 &lt;chr&gt;, toc &lt;chr&gt;, author &lt;chr&gt;\n\n\n\nid: 책의 고유 번호 (주의: 위 테이블의 book_id와 동일)\ntitle: 책의 제목\nsub_title: 책의 부제목\nisbn13: 책의 isbn(출판 번호)\npub_date: 책이 출간된 날짜\ndescription: 책에 대한 간략한 설명\npublisher: 출판사\ncategory_name_aladin: 해당 책의 알라딘 카테고리\n\n첫 번째 책은 → 국내도서&gt;전집/중고전집&gt;창작동화\n\npage: 책의 총 페이지\nfull_description: 책에 대한 설명 (조금 더 긴 버전)\nfull_description_2: 다른 source 에서 가져온 책의 설명\ntoc: 책의 목차\n\n예를 들어, 4번째 책은 아래 목차\n\n\n\n1권 1장 토리스 바의 손님 2장 가메다 3장 누보 그룹 4장 미해결 5장 종이 날리는 여자  6장 방언 분포 7장 혈흔 8장 변사  2권  9장 모색 10장 에미코 11장 그녀의 죽음 12장 혼미 13장 실마리 14장 무성無聲 15장 항적 16장 어떤 호적 17장 방송   해설 | 일본 근대사회의 집합적 무의식, 그 터부를 비평하다 마쓰모토 세이초 연보\n\n\n\n\nauthor: 책의 저자\n\n\n\nbook_cat: 책과 카테고리의 연결 테이블\n\n\n\n\n\n\nbook_id\nbook_category_id\n\n\n\n\n796\n2326\n\n\n704\n4597\n\n\n704\n4723\n\n\n704\n4725\n\n\n704\n4653\n\n\n704\n4669\n\n\n866\n121\n\n\n866\n162\n\n\n866\n163\n\n\n866\n122\n\n\n\n\n\n\nbook_id: 책의 고유 번호\nbook_category_id: 카테고리의 고유 번호\n\n\n\ncat: 카테고리 고유 번호에 대한 상세 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbook_category_id\nname\ndepth_1\ndepth_2\ndepth_3\ndepth_4\ndepth_5\n\n\n\n\n8239\n산업연구\n일본 도서\n경제/경영\n경제학\n산업연구\nNA\n\n\n8403\n정치\n일본 도서\n인문/사회/논픽션\n정치/외교/국제관계\n정치\nNA\n\n\n8254\n문학/평론\n일본 도서\n문고/신서\n문학/평론\nNA\nNA\n\n\n8255\n일본문학\n일본 도서\n문고/신서\n문학/평론\n일본문학\nNA\n\n\n8252\n경제/경영\n일본 도서\n문고/신서\n경제/경영\nNA\nNA\n\n\n8253\n문고기타\n일본 도서\n문고/신서\n문고기타\nNA\nNA\n\n\n8250\n라이트노벨\n일본 도서\n라이트노벨\nNA\nNA\nNA\n\n\n8251\n문고/신서\n일본 도서\n문고/신서\nNA\nNA\nNA\n\n\n8248\n재테크/투자\n일본 도서\n경제/경영\n재테크/투자\nNA\nNA\n\n\n8249\nIT/e-commerce\n일본 도서\n경제/경영\nIT/e-commerce\nNA\nNA\n\n\n\n\n\n\nbook_category_id: 카테고리의 고유 번호\nname: 카테고리 이름\ndepth_1: 카테고리 대분류\ndepth_2: 카테고리 중분류\ndepth_3: 카테고리 소분류_1\ndepth_4: 카테고리 소분류_2\ndepth_5: 카테고리 소분류_3\n\n\n\nscrap: 유저가 책에서 스크랩한 문구\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\ncontent\nbook_id\npage\ncreated_at\n\n\n\n\n69249442\n분노가 가라앉으니 남는 건 부끄러움과 당혹스러움밖에 없었다.\n1511719\n408\n2021-09-01 12:47:40.171670\n\n\n69249442\n슬픔은 그 나름의 수명과 생명력을 가지게 마련이니까.\n1511719\n315\n2021-08-31 13:16:26.898560\n\n\n69249442\n세상에 완벽한 사람은 없어. 그런 사람하고만 친구가 되라는 법도 없고.\n1511719\n219\n2021-08-31 08:04:58.755703\n\n\n69249442\n다들 알아시피, 결말은 두려워하는 사람이 아니라 두려움을 유발하는 사람이 정하는 법이다.\n1560481\n288\n2021-08-29 11:54:53.155436\n\n\n69249442\n불은 아무것도 소유하지 않은 사람만 정화할 뿐이에요. 타오르는 것들 안에는 슬픔과 적막함이 깃들어 있어요.\n1560481\n267\n2021-08-29 11:38:52.528101\n\n\n69249442\n후회할 때가 아니야, 나 자신에게 말했다. 지나간 일은 지나간 일일 뿐이야.\n\n\n\n\n\n내 의무는 살아남는 것이었다.\n1560481\n247\n2021-08-29 10:41:01.534785\n\n\n\n69249442\n그 시간 동안 나는 현실이 언제나 확신을 깨부순다는 사실을 깨달았다.\n1560481\n178\n2021-08-28 16:26:24.889925\n\n\n69249442\n나라 전체가 그랬듯, 우리는 서로에게 남이 되는 형을 선고빋았다.\n1560481\n65\n2021-08-28 15:08:01.429808\n\n\n69249442\n나는 온몸으로 울었다.\n1560481\n35\n2021-08-27 12:31:43.595596\n\n\n69249442\n엄마의 묘비명을 작성하는 동안 첫 번째 죽음은 언어 안에서, 주어를 현재에서 끌어내 과거에 세워두는 과정에서 발생한다는 사실을 깨달았다.\n1560481\n12\n2021-08-26 12:52:51.500924\n\n\n\n\n\n\nuser_id: 유저 고유 번호\ncontent: 스크랩 문구\nbook_id: 스크랩한 책의 고유 번호\npage: 스크랩한 책의 페이지\ncreated_at: 스크랩한 날짜와 시간 (중요한점: 날짜와 시간은 UDT 기준입니다.)"
  },
  {
    "objectID": "teaching/ml101/icpbl/index.html",
    "href": "teaching/ml101/icpbl/index.html",
    "title": "IC-PBL",
    "section": "",
    "text": "Domestic students (한국 학생)\n\n\n\nExchange students (외국인 학생, 교환 학생)"
  },
  {
    "objectID": "teaching/ml101/index.html",
    "href": "teaching/ml101/index.html",
    "title": "Cultural Data & Machine Learning",
    "section": "",
    "text": "a &lt;- \"Machine\"\nb &lt;- \"Learning\"\npaste0(\"Welcome to \", a,\" \",b,\" \",100+1)\n\n[1] \"Welcome to Machine Learning 101\"\n\n\n\n\n\n\n\n\n\n\n\nAbout course\n\n\n\nWeekly design\n\n\n\nPBL"
  },
  {
    "objectID": "teaching/ml101/weekly/index.html",
    "href": "teaching/ml101/weekly/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nWeek\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n1\n\n\nCourse Intro\n\n\n \n\n\n\n\n2\n\n\nAbout ML & Modelling\n\n\nEDA Review\n\n\n\n\n3\n\n\nClassification\n\n\nDecision Tree\n\n\n\n\n4\n\n\nClassification\n\n\nRandom Forest\n\n\n\n\n5\n\n\nClassification\n\n\nNaive Bayes\n\n\n\n\n6\n\n\nClassification\n\n\nK-Nearest Neighbors\n\n\n\n\n7\n\n\nRegression\n\n\nLinear Regression\n\n\n\n\n8\n\n\nQZ #1\n\n\n19 April (Wed) 13:00-15:00\n\n\n\n\n9\n\n\nRegression\n\n\nNon-linear Regression\n\n\n\n\n10\n\n\nUnsupervised Learning\n\n\nClustering\n\n\n\n\n11\n\n\nUnsupervised Learning\n\n\nPattern Finding\n\n\n\n\n12\n\n\nModel Improvement\n\n\nEnhancing Predictive Performance with Model Evaluation\n\n\n\n\n13\n\n\nNatural Language Process\n\n\nTidytext in R\n\n\n\n\n14\n\n\nQZ #2\n\n\n31 May (Wed) 13:00-15:00\n\n\n\n\n15\n\n\nProject presentation\n\n\n14 June (Wed) 13:00-15:00\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html",
    "href": "teaching/ml101/weekly/posts/02_week.html",
    "title": "About ML & Modelling",
    "section": "",
    "text": "Weekly design\nBefore attending class for Week 2, please complete the following tasks:"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#introduction",
    "href": "teaching/ml101/weekly/posts/02_week.html#introduction",
    "title": "About ML & Modelling",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nIt contains the analysis process of ‘Titanic’, one of the representative ‘Competition’ of ‘Kaggle’."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#prepare-to-work",
    "href": "teaching/ml101/weekly/posts/02_week.html#prepare-to-work",
    "title": "About ML & Modelling",
    "section": "2. Prepare to work",
    "text": "2. Prepare to work\n\n\n2.1 Packages\nsee “What is a package in R”\n\nThis is the process of loading (loading) the Packages I used for analysis, in addition to the representative Packages of R, such as tidyverse (including ggplot2 and dplyr).\n\n\n# Data input, assesment \nlibrary(titanic)\nlibrary(readr)           # Data input with readr::read_csv()\nlibrary(descr)           # descr::CrossTable() - Frequency by category, check with ratio figures\n\n# Visualization\nlibrary(VIM)             # Missing values assesment used by VIM::aggr()\nlibrary(RColorBrewer)    # Plot color setting\nlibrary(scales)          # plot setting - x, y axis\n\n# Feature engineering, Data Pre-processing\nlibrary(tidyverse)     # dplyr, ggplot2, purrr, etc..      # Feature Engineering & Data Pre-processing\nlibrary(ggpubr)\n\nlibrary(randomForest)\n# Model validation \nlibrary(caret)           # caret::confusionMatrix()\nlibrary(ROCR)            # Plotting ROC Curve\n\n\n\n\n2.2 Raw data import\n\nIn titanic competition, train data used to create Model and test data used for actual prediction (estimation) are separated.\nHere, we will load those two data and combine them into one. The reason for tying the separate data together is to work the same when feature engineering and pre-processing the input variables used in modeling.\nPlease see this link if you want to know about the story of Titanic.\n\n\ntitanic_train %&gt;% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\ntitanic_test %&gt;% glimpse\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;int&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"B45\", \"\",…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\ntrain &lt;- titanic_train\ntest  &lt;- titanic_test\n\nfull &lt;- dplyr::bind_rows(train, test)\nfull %&gt;% glimpse\n\nRows: 1,309\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nThe reason why rbind() was not used even when combining the two data into full is that Survived, the dependent variable (target variable, Y) of Titanic competition, does not exist in test. Therefore, the dimensions (dimension) of the two data do not match, so they are not merged with rbind(). However, if you use dplyr::bind_rows(), Survived in test is treated as NA and merged into one.\n\n\n2.3 variable meaning explanation\n\n\n\n\n\n\n\n\nvariable name\nInterpretation (meaning)\nType\n\n\n\n\nPassengerID\nUnique ID number that identifies the passenger\nInt\n\n\nSurvived\nIndicates whether or not the passenger survived. Survival is 1 and death is 0.\nFactor\n\n\nPclass\nThe class of the cabin, with 3 categories from 1st class (1) to 3rd class (3).\nOrd.Factor\n\n\nName\nPassenger’s name\nFactor\n\n\nSex\nPassenger’s gender\nFactor\n\n\nAge\nAge of passenger\nNumeric\n\n\nSibSp\nVariable describing the number of siblings or spouses accompanying each passenger. It can range from 0 to 8.\nInteger\n\n\nParch\nVariable describing the number of parents or children accompanying each passenger, from 0 to 9.\nInteger\n\n\nTicket\nString variable for the ticket the passenger boarded\nFactor\n\n\nFare\nVariable for how much the passenger has paid for the trip so far\nNumeric\n\n\nCabin\nVariable that distinguishes each passenger’s cabin, with too many categories and missing values.\nFactor\n\n\nEmbarked\nIndicates the boarding port and departure port, and consists of three categories: C, Q, and S.\nFactor\n\n\n\n\n\n\n2.4 Change the variables type\n\nBefore the full-scale EDA and feature engineering, let’s transform some variable properties. For example, Pclass is treated as numeric, but actually 1, 2, 3 are factors representing 1st, 2nd, and 3rd grades.\n\n\nfull &lt;- full %&gt;%\n  dplyr::mutate(Survived = factor(Survived),\n                Pclass   = factor(Pclass, ordered = T),\n                Name     = factor(Name),\n                Sex      = factor(Sex),\n                Ticket   = factor(Ticket),\n                Cabin    = factor(Cabin),\n                Embarked = factor(Embarked))"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#eda-exploratory-data-analysis",
    "href": "teaching/ml101/weekly/posts/02_week.html#eda-exploratory-data-analysis",
    "title": "About ML & Modelling",
    "section": "3. EDA : Exploratory data analysis",
    "text": "3. EDA : Exploratory data analysis\n\nIt is the process of exploring and understanding raw data, such as how data is structured and whether there are missing values or outliers in it.\nWe will use various functions and visualizations here.\n\n\n3.1 Data confirmation using numerical values\nFirst of all, let’s check the data through the output of various functions such as head() and summary().\n\n\n3.1.1 head()\n\n\nhead(full, 10)\n\n   PassengerId Survived Pclass\n1            1        0      3\n2            2        1      1\n3            3        1      3\n4            4        1      1\n5            5        0      3\n6            6        0      3\n7            7        0      1\n8            8        0      3\n9            9        1      3\n10          10        1      2\n                                                  Name    Sex Age SibSp Parch\n1                              Braund, Mr. Owen Harris   male  22     1     0\n2  Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                               Heikkinen, Miss. Laina female  26     0     0\n4         Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                             Allen, Mr. William Henry   male  35     0     0\n6                                     Moran, Mr. James   male  NA     0     0\n7                              McCarthy, Mr. Timothy J   male  54     0     0\n8                       Palsson, Master. Gosta Leonard   male   2     3     1\n9    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female  27     0     2\n10                 Nasser, Mrs. Nicholas (Adele Achem) female  14     1     0\n             Ticket    Fare Cabin Embarked\n1         A/5 21171  7.2500              S\n2          PC 17599 71.2833   C85        C\n3  STON/O2. 3101282  7.9250              S\n4            113803 53.1000  C123        S\n5            373450  8.0500              S\n6            330877  8.4583              Q\n7             17463 51.8625   E46        S\n8            349909 21.0750              S\n9            347742 11.1333              S\n10           237736 30.0708              C\n\n\n\nLooking at the result of head(), we can see that there is a missing value (NA) in Age.\nIf so, is there only Age missing in the entire data?\nFor the answer, please refer to 3.2 Missing values.\n\n\n\n3.1.2 str()\n\n\nstr(full)\n\n'data.frame':   1309 obs. of  12 variables:\n $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass     : Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : Factor w/ 1307 levels \"Abbing, Mr. Anthony\",..: 156 287 531 430 23 826 775 922 613 855 ...\n $ Sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : Factor w/ 929 levels \"110152\",\"110413\",..: 721 817 915 66 650 374 110 542 478 175 ...\n $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 108 1 72 1 1 165 1 1 1 ...\n $ Embarked   : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 2 4 4 4 3 4 4 4 2 ...\n\n\n\nBy combining the train and test data, the total number of observations (record, row, row) is 1309 (train: 891, test: 418), and the number of variables (column, feature, variable, column) is 12.\nIn addition, you can find out what the attributes of each variable are and how many categories there are for variables that are factor attributes.\nIn addition, in head(), it can be seen that the missing value (NA), which was thought to exist only in Age, also exists in other variables including Cabin.\n\n\n\n3.1.3 summary()\n\n\nsummary(full)\n\n  PassengerId   Survived   Pclass                                Name     \n Min.   :   1   0   :549   1:323   Connolly, Miss. Kate            :   2  \n 1st Qu.: 328   1   :342   2:277   Kelly, Mr. James                :   2  \n Median : 655   NA's:418   3:709   Abbing, Mr. Anthony             :   1  \n Mean   : 655                      Abbott, Master. Eugene Joseph   :   1  \n 3rd Qu.: 982                      Abbott, Mr. Rossmore Edward     :   1  \n Max.   :1309                      Abbott, Mrs. Stanton (Rosa Hunt):   1  \n                                   (Other)                         :1301  \n     Sex           Age            SibSp            Parch            Ticket    \n female:466   Min.   : 0.17   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n male  :843   1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n              Median :28.00   Median :0.0000   Median :0.000   CA 2144 :   8  \n              Mean   :29.88   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n              3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n              Max.   :80.00   Max.   :8.0000   Max.   :9.000   347082  :   7  \n              NA's   :263                                      (Other) :1261  \n      Fare                     Cabin      Embarked\n Min.   :  0.000                  :1014    :  2   \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270   \n Median : 14.454   B57 B59 B63 B66:   5   Q:123   \n Mean   : 33.295   G6             :   5   S:914   \n 3rd Qu.: 31.275   B96 B98        :   4           \n Max.   :512.329   C22 C26        :   4           \n NA's   :1         (Other)        : 271           \n\n\n\nsummary() provides a lot of information about the data.\nThe representative values of quantitative variables (Integer, Numeric), the number of categories of categorical (Factor) variables, and the number of observations belonging to each category are all shown as numerical values.\nHere are the things to check and move on:\n\nSurvived: This is the target variable for this competition, and 418 missing values are due to the test data.\nPclass: There are three categories of 1st class, 2nd class, and 3rd class, and 3rd class passengers are the most.\nName: There are people with similar names. So you can see that some passengers are traveling alone, while others are traveling with their families.\nSex: There are almost twice as many males as females.\nAge: It ranges from 0.17 to 80 years old, but it seems necessary to check whether it is an outlier that incorrectly entered 17, and there are 263 missing values.\nSibSp: From 0 to 8, and the 3rd quartile is 1, so it can be seen that you boarded the Titanic with a couple or siblings.\nParch: It ranges from 0 to 9, but the fact that the 3rd quartile is 0 indicates that there are very few passengers with parents and children.\nBoth SibSp and Parch are variables representing family relationships. Through this, we will find out the total number of people in the family, although we do not know who was on board, and based on that, we will create a categorical derived variable called FamilySized that represents the size of the family.\nTicket: Looking at the result of 3.1.2 str(), you can see that some passengers have exactly the same ticket, some passengers have tickets overlapping only a certain part, and some passengers have completely different tickets. We plan to use this to create a derived variable called ticket.size.\nFare: 0 to 512, with 1 missing value. I care that the 3rd quartile is 31.275 and the max is 512.\nCabin: It has the most (1014) missing values among a total of 12 features. It’s a variable that represents the ship’s area, but if there’s no way to use it, I think it should be discarded.\nEmbarked: It consists of a total of 3 categories, S is the most, and there are 2 missing values.\n\nWhen performing a basic exploration of the data, please look at the outputs of various functions besides summary() and str() while comparing them.\n\n\n\n\n3.2 Missing values\n\nThis is the process of checking which variables have missing values mentioned above and how many of them exist.\nI’m going to check it numerically and visually at the same time using the dplyr, ggplot2, and VIM packages.\nYou don’t have to use all the code I’ve run, you can use only the parts you think you need or like as you read.\n\n\n3.2.1 VIM packages\n\n\nVIM::aggr(full, prop = FALSE, combined = TRUE, numbers = TRUE,\n          sortVars = TRUE, sortCombs = TRUE)\n\n\n\n\n\n Variables sorted by number of missings: \n    Variable Count\n    Survived   418\n         Age   263\n        Fare     1\n PassengerId     0\n      Pclass     0\n        Name     0\n         Sex     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n       Cabin     0\n    Embarked     0\n\n\n\n\n\n3.2.2 tidyverse packages\n\nIn addition to checking missing values at once using the VIM package, these are methods for checking missing values using various packages that exist in the tidyverse.\nFirst, find the proportion of missing values for each variable with dplyr.\n\n\nfull %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n\n\nThere is a way to check the proportion of missing values that exist in variables, but it can also be checked using visual data.\nPlease see the two bar plots below.\n\n\n# Calculate the missing value ratio of each feature -&gt; Data Frame property but has a structure of 1 row and 12 columns.\nmissing_values &lt;- full %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nmissing_values %&gt;% head\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n# Generate the missing_values obtained above as a 12X2 data frame\nmissing_values &lt;- tidyr::gather(missing_values,\n                                key = \"feature\", value = \"missing_pct\")\n\nmissing_values %&gt;% head(12)\n\n       feature  missing_pct\n1  PassengerId 0.0000000000\n2     Survived 0.3193277311\n3       Pclass 0.0000000000\n4         Name 0.0000000000\n5          Sex 0.0000000000\n6          Age 0.2009167303\n7        SibSp 0.0000000000\n8        Parch 0.0000000000\n9       Ticket 0.0000000000\n10        Fare 0.0007639419\n11       Cabin 0.0000000000\n12    Embarked 0.0000000000\n\n# Visualization with missing_values\nmissing_values %&gt;% \n  # Aesthetic setting : missing_pct 내림차순으로 정렬  \n  ggplot(aes(x = reorder(feature, missing_pct), y = missing_pct)) +\n  # Bar plot \n  geom_bar(stat = \"identity\", fill = \"red\") +\n  # Title generation \n  ggtitle(\"Rate of missing values in each features\") +\n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = \"Feature names\", y = \"Rate\") + \n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nIf you look at the bar graph above, you can check the percentage of missing values for all features.\nHowever, what we are actually curious about is which variables have missing values and how many missing values exist in them.\nTherefore, after calculating the proportion of missing values using the purrr package, I extracted only the variables that had at least one and visualized them.\n\n\n# 변수별 결측치 비율 계산\nmiss_pct &lt;- purrr::map_dbl(full, function(x){round((sum(is.na(x))/length(x)) * 100, 1) })\n\n# 결측치 비율이 0%보다 큰 변수들만 선택\nmiss_pct &lt;- miss_pct[miss_pct &gt; 0]\n\n# Data Frame 생성 \ndata.frame(miss = miss_pct, var = names(miss_pct), row.names = NULL) %&gt;%\n  # Aesthetic setting : miss 내림차순으로 정렬 \n  ggplot(aes(x = reorder(var, miss), y = miss)) + \n  # Bar plot \n  geom_bar(stat = 'identity', fill = 'red') +\n  # Plot title setting \n  ggtitle(\"Rate of missing values\") + \n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = 'Feature names', y = 'Rate of missing values') +\n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nThrough this, only 4 variables out of a total of 12 variables have missing values (except Survived because it is due to test data), and there are many missing values in the order of Cabin, Age, Embarked, and Fare.\n\n\n\nNow, it is the process of analyzing and exploring feature through visualization.\n\n\n\n3.3 Age\n\n\nage.p1 &lt;- full %&gt;% \n  ggplot(aes(Age)) + \n  geom_histogram(breaks = seq(0, 80, by = 1), # interval setting \n                 col    = \"red\",              # bar border color\n                 fill   = \"green\",            # bar inner color\n                 alpha  = .5) +               # Bar Transparency = 50%\n  \n  # Plot title\n  ggtitle(\"All Titanic passengers age hitogram\") +\n  theme(plot.title = element_text(face = \"bold\",     \n                                  hjust = 0.5,      # Horizon (horizontal ratio) = 0.5\n                                  size = 15, color = \"darkblue\"))\n\nage.p2 &lt;- full %&gt;% \n# Exclude values where Survived == NA in the test dataset  \n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Age, fill = Survived)) + \n  geom_density(alpha = .5) +\n  ggtitle(\"Titanic passengers age density plot\") + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5,\n                                  size = 15, color = \"darkblue\"))\n\n# Display the two graphs created above on one screen\nggarrange(age.p1, age.p2, ncol=2)\n\n\n\n\n\n\n\n3.4 Pclass\n\nLet’s visualize the frequency of passengers for each Pclass.\nAfter grouping (grouping) by Pclass using dplyr package, Data Frame representing frequency by category was created and visualized with ggplot.\n\n\nfull %&gt;% \n  # Get Pclass frequencies using dplyr::group_by(), summarize()\n  group_by(Pclass) %&gt;% \n  summarize(N = n()) %&gt;% \n  # Aesthetic setting \n  ggplot(aes(Pclass, N)) +\n  geom_col() +\n  geom_text(aes(label = N),       \n            size = 5,             \n            vjust = 1.2,           \n            color = \"#FFFFFF\") +  \n  # Plot title \n  ggtitle(\"Number of each Pclass's passengers\") + \n  # Title setting \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15)) +\n  # x, y axis name change  \n  labs(x = \"Pclass\", y = \"Count\")\n\n\n\n\n\nIt can be seen that the largest number of passengers boarded in the 3-class cabin.\n\n\n\n3.5 Fare\n\nThis is a visualization of the ‘Fare’ variable, which represents the amount paid by the passenger.\nTwo histograms and boxplots were used.\n\n# Histogram \nFare.p1 &lt;- full %&gt;%\n  ggplot(aes(Fare)) + \n  geom_histogram(col    = \"yellow\",\n                 fill   = \"blue\", \n                 alpha  = .5) +\n  ggtitle(\"Histogram of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\n# Boxplot \nFare.p2 &lt;- full %&gt;%\n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Survived, Fare)) + \n  # Observations are drawn as gray dots, but overlapping areas are spread out.  \n  geom_jitter(col = \"gray\") + \n  # Boxplot: 50% transparency\n  geom_boxplot(alpha = .5) + \n  ggtitle(\"Boxplot of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\nggarrange(Fare.p1, Fare.p2, ncol=2)\n\n\n\n\n\nYou can see that the survivors have a higher ‘Fare’ than the deceased passengers, but not by much.\n\n\n\n3.6 Sex\nAre there differences in survival rates between men and women? See the plot below.\n\n\nsex.p1 &lt;- full %&gt;% \n  dplyr::group_by(Sex) %&gt;% \n  summarize(N = n()) %&gt;% \n  ggplot(aes(Sex, N)) +\n  geom_col() +\n  geom_text(aes(label = N), size = 5, vjust = 1.2, color = \"#FFFFFF\") + \n  ggtitle(\"Bar plot of Sex\") +\n  labs(x = \"Sex\", y = \"Count\")\n  \nsex.p2 &lt;- full[1:891, ] %&gt;% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  ggtitle(\"Survival Rate by Sex\") + \n  labs(x = \"Sex\", y = \"Rate\")\n\nggarrange(sex.p1, sex.p2, ncol = 2)\n\n\n\nmosaicplot(Survived ~ Sex,\n           data = full[1:891, ], col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\nIf you interpret the graph, you can see that the survival rate is higher for female passengers, while there are far more males than females."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#feature-engineering-data-pre-processing",
    "href": "teaching/ml101/weekly/posts/02_week.html#feature-engineering-data-pre-processing",
    "title": "About ML & Modelling",
    "section": "4. Feature engineering & Data Pre-processing",
    "text": "4. Feature engineering & Data Pre-processing\n\nThis is the process of filling in missing values (‘NA’) based on the contents of ‘Chapter 3 EDA’ and creating derived variables at the same time.\n\n\n4.1 Age -&gt; Age.Group\n\n\nfull &lt;- full %&gt;%\n# The missing value (NA) is filled in first, and the average of the values excluding the missing value is filled.\n  mutate(Age = ifelse(is.na(Age), mean(full$Age, na.rm = TRUE), Age),\n# Create a categorical derived variable Age.Group based on Age values\n        Age.Group = case_when(Age &lt; 13             ~ \"Age.0012\",\n                               Age &gt;= 13 & Age &lt; 18 ~ \"Age.1317\",\n                               Age &gt;= 18 & Age &lt; 60 ~ \"Age.1859\",\n                               Age &gt;= 60            ~ \"Age.60inf\"),\n# Convert Chr attribute to Factor\n        Age.Group = factor(Age.Group))\n\n\n\n\n4.3 SibSp & Parch -&gt; FamilySized\n\n\nfull &lt;- full %&gt;% \n # First create a derived variable called FamilySize by adding SibSp, Parch and 1 (self)\n  mutate(FamilySize = .$SibSp + .$Parch + 1,\n        # Create a categorical derived variable FamilySized according to the value of FamilySize\n         FamilySized = dplyr::case_when(FamilySize == 1 ~ \"Single\",\n                                        FamilySize &gt;= 2 & FamilySize &lt; 5 ~ \"Small\",\n                                        FamilySize &gt;= 5 ~ \"Big\"),\n        # Convert the Chr property FamilySized to a factor\n        # Assign new levels according to the size of the group size\n         FamilySized = factor(FamilySized, levels = c(\"Single\", \"Small\", \"Big\")))\n\n\nCeated FamilySized using SibSp and Parch.\nReducing these two variables to one has the advantage of simplifying the model.\nA similar use case is to combine height and weight into a BMI index.\n\n\n\n4.4 Name & Sex -&gt; title\n\nWhen looking at the results of ‘Chapter 3.6 Sex’, it was confirmed that the survival rate of women was higher than that of men.\nTherefore, in Name, “Wouldn’t it be useful to extract only names related to gender and categorize them?” I think it is.\nFirst, extract only the column vector named Name from full data and save it as title.\n\n\n# First, extract only the Name column vector and store it in the title vector\ntitle &lt;- full$Name\ntitle %&gt;% head(20)\n\n [1] Braund, Mr. Owen Harris                                \n [2] Cumings, Mrs. John Bradley (Florence Briggs Thayer)    \n [3] Heikkinen, Miss. Laina                                 \n [4] Futrelle, Mrs. Jacques Heath (Lily May Peel)           \n [5] Allen, Mr. William Henry                               \n [6] Moran, Mr. James                                       \n [7] McCarthy, Mr. Timothy J                                \n [8] Palsson, Master. Gosta Leonard                         \n [9] Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      \n[10] Nasser, Mrs. Nicholas (Adele Achem)                    \n[11] Sandstrom, Miss. Marguerite Rut                        \n[12] Bonnell, Miss. Elizabeth                               \n[13] Saundercock, Mr. William Henry                         \n[14] Andersson, Mr. Anders Johan                            \n[15] Vestrom, Miss. Hulda Amanda Adolfina                   \n[16] Hewlett, Mrs. (Mary D Kingcome)                        \n[17] Rice, Master. Eugene                                   \n[18] Williams, Mr. Charles Eugene                           \n[19] Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\n[20] Masselmani, Mrs. Fatima                                \n1307 Levels: Abbing, Mr. Anthony ... Zimmerman, Mr. Leo\n\n# Using regular expression and gsub(), extract only names that are highly related to gender and save them as title vectors\ntitle &lt;- gsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title)\ntitle %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n# Save the title vector saved above to full again, but save it as a title derived variable\nfull$title &lt;- title\n\nfull$title %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n\n\ngsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title) In short, this code uses gsub to search for and replace a pattern in the title string, where the pattern is defined by the regular expression \"^.*, (.*?)\\\\..*$\" and the replacement is defined by the string \"\\\\1\". If you want to understand more about regular expression. Please see my blog post: What are Regular Expressions and How to Use Them in R\nThen check what are the Unique titles.\n\n\nunique(full$title)\n\n [1] \"Mr\"           \"Mrs\"          \"Miss\"         \"Master\"       \"Don\"         \n [6] \"Rev\"          \"Dr\"           \"Mme\"          \"Ms\"           \"Major\"       \n[11] \"Lady\"         \"Sir\"          \"Mlle\"         \"Col\"          \"Capt\"        \n[16] \"the Countess\" \"Jonkheer\"     \"Dona\"        \n\n\n\nYou can see that there are 18 categories in total.\nIf you use this derived variable called ‘title’ as it is, the complexity of the model (especially the tree based model) increases considerably, so you need to reduce the category.\nBefore that, let’s check the frequency and rate for each category using the descr package.\n\n\n# Check frequency, ratio by category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|         Capt |          Col |          Don |         Dona |           Dr |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            4 |            1 |            1 |            8 |\n|        0.001 |        0.003 |        0.001 |        0.001 |        0.006 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|     Jonkheer |         Lady |        Major |       Master |         Miss |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            1 |            2 |           61 |          260 |\n|        0.001 |        0.001 |        0.002 |        0.047 |        0.199 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|         Mlle |          Mme |           Mr |          Mrs |           Ms |\n|--------------|--------------|--------------|--------------|--------------|\n|            2 |            1 |          757 |          197 |            2 |\n|        0.002 |        0.001 |        0.578 |        0.150 |        0.002 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|          Rev |          Sir | the Countess |\n|--------------|--------------|--------------|\n|            8 |            1 |            1 |\n|        0.006 |        0.001 |        0.001 |\n|--------------|--------------|--------------|\n\n\n\nThe frequencies and proportions of the 18 categories are very different.\nSo let’s narrow these down to a total of five categories.\n\n\n# Simplify into 5 categories\nfull &lt;- full %&gt;%\n# If you use \"==\" instead of \"%in%\", it won't work as you want because of Recyling Rule.\n  mutate(title = ifelse(title %in% c(\"Mlle\", \"Ms\", \"Lady\", \"Dona\"), \"Miss\", title),\n         title = ifelse(title == \"Mme\", \"Mrs\", title),\n         title = ifelse(title %in% c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\", \"Don\",\n                                     \"Sir\", \"the Countess\", \"Jonkheer\"), \"Officer\", title),\n         title = factor(title))\n\n# After creating the derived variable, check the frequency and ratio for each category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|  Master |    Miss |      Mr |     Mrs | Officer |\n|---------|---------|---------|---------|---------|\n|      61 |     266 |     757 |     198 |      27 |\n|   0.047 |   0.203 |   0.578 |   0.151 |   0.021 |\n|---------|---------|---------|---------|---------|\n\n\n\n\n\n4.5 Ticket -&gt; ticket.size\n\nAs we saw in Chapter 3.1.3 Summary(), the number of passengers (train and test together) is 1309. However, all passengers’ tickets are not different.\nSee the results of summary() and unique() below.\n\n\n# We used length() to get only the number of unique categories.\nlength(unique(full$Ticket))\n\n[1] 929\n\n# Printing all of them was too messy, so only 10 were printed.\nhead(summary(full$Ticket), 10)\n\n    CA. 2343         1601      CA 2144      3101295       347077       347082 \n          11            8            8            7            7            7 \n    PC 17608 S.O.C. 14879       113781        19950 \n           7            7            6            6 \n\n\n\nWhy are there 929 unique tickets when there are no missing values in feature?\nEven the ticket is CA. There are 11 exactly the same number of people as 2343.\nLet’s see who the passengers are.\n\n\nfull %&gt;% \n# Filter only 11 passengers with matching tickets\n  filter(Ticket == \"CA. 2343\") %&gt;% \n  # We don't need to check for all variables, so we only want to look at the variables below.\n  select(Pclass, Name, Age, FamilySized)\n\n   Pclass                              Name      Age FamilySized\n1       3        Sage, Master. Thomas Henry 29.88114         Big\n2       3      Sage, Miss. Constance Gladys 29.88114         Big\n3       3               Sage, Mr. Frederick 29.88114         Big\n4       3          Sage, Mr. George John Jr 29.88114         Big\n5       3           Sage, Miss. Stella Anna 29.88114         Big\n6       3          Sage, Mr. Douglas Bullen 29.88114         Big\n7       3 Sage, Miss. Dorothy Edith \"Dolly\" 29.88114         Big\n8       3                   Sage, Miss. Ada 29.88114         Big\n9       3             Sage, Mr. John George 29.88114         Big\n10      3       Sage, Master. William Henry 14.50000         Big\n11      3    Sage, Mrs. John (Annie Bullen) 29.88114         Big\n\n\n\nYou can see that the 11 passengers above are all from the same family, brothers.\nWhile there are passengers whose tickets are exactly the same, there are also passengers whose tickets are partially matched.\nCreate a ticket.unique derived variable that represents the number of unique numbers (number of characters) of such a ticket.\nLet’s create a derived variable ticket.size with 3 categories based on ticket.unique.\n\n\n# First of all, ticket.unique is saved as all 0\nticket.unique &lt;- rep(0, nrow(full))\n\n# Extract only the unique ones from ticket features and store them in the tickets vector\ntickets &lt;- unique(full$Ticket)\n\n# After extracting only passengers with the same ticket by using overlapping loops, extract and store the length (number of characters) of each ticket.\n\nfor (i in 1:length(tickets)) {\n  current.ticket &lt;- tickets[i]\n  party.indexes &lt;- which(full$Ticket == current.ticket)\n    # For loop 중첩 \n    for (k in 1:length(party.indexes)) {\n    ticket.unique[party.indexes[k]] &lt;- length(party.indexes)\n    }\n  }\n\n# Save ticket.unique calculated above as a derived variable\nfull$ticket.unique &lt;- ticket.unique\n\n# Create ticket.size variable by dividing it into three categories according to ticket.unique\n\nfull &lt;- full %&gt;% \n  mutate(ticket.size = case_when(ticket.unique == 1 ~ 'Single',\n                                 ticket.unique &lt; 5 & ticket.unique &gt;= 2 ~ \"Small\",\n                                 ticket.unique &gt;= 5 ~ \"Big\"),\n         ticket.size = factor(ticket.size,\n                              levels = c(\"Single\", \"Small\", \"Big\")))\n\n\n\n\n4.6 Embarked\n\nThis is feature with two missing values (NA). In the case of Embarked, replace it with S, which is the most frequent value among the three categories.\n\n\nfull$Embarked &lt;- replace(full$Embarked,               # Specify Data$feature to replace\n                         which(is.na(full$Embarked)), # Find only missing values\n                         'S')                        # specify the value to replace\n\n\n\n\n4.7 Fare\n\nFor Fare, there was only one missing value.\nBased on the histogram seen above (Chapter 3.5 Fare), missing values are replaced with 0.\n\n\nfull$Fare &lt;- replace(full$Fare, which(is.na(full$Fare)), 0)\n\n\nAt this point, data preprocessing is complete.\nThe following is the process of selecting the variables to be used for model creation while exploring the derived variables created so far.\nIn other words, Feature selection."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#relationship-to-target-feature-survived-feature-selection",
    "href": "teaching/ml101/weekly/posts/02_week.html#relationship-to-target-feature-survived-feature-selection",
    "title": "About ML & Modelling",
    "section": "5. Relationship to target feature Survived & Feature selection",
    "text": "5. Relationship to target feature Survived & Feature selection\n\nPrior to full-scale visualization, since the purpose here is to see how well each variable correlates with the survival rate, we did not use the entire full data, but only the train data set that can determine survival and death.\nAlso, please note that the plot used above may be duplicated as it is.\n\n\n5.0 Data set split\nFirst, use the code below to split preprocessed full data into train and test.\n\n# Before feature selection, select all variables first.\n\ntrain &lt;- full[1:891, ]\n\ntest &lt;- full[892:1309, ]\n\n\n\n\n5.1 Pclass\n\n\ntrain %&gt;% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(position = \"fill\") +\n# Set plot theme: Converts to a more vivid color.\n  scale_fill_brewer(palette = \"Set1\") +\n  # Y axis setting \n  scale_y_continuous(labels = percent) +\n# Set x, y axis names and plot main title, sub title\n  labs(x = \"Pclass\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Pclass?\")\n\n\n\n\n\n\n\n5.2 Sex\n\nSame as Chapter 3.6 Sex.\n\n\nmosaicplot(Survived ~ Sex,\n           data = train, col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\n\n\n5.3 Embarked\n\n\ntrain %&gt;% \n  ggplot(aes(Embarked, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Embarked\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Embarked?\")\n\n\n\n\n\n\n\n5.4 FamilySized\n\n\ntrain %&gt;% \n  ggplot(aes(FamilySized, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"FamilySized\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by FamilySized\")\n\n\n\n\n\nIt can be seen that there is a difference in survival rate depending on the number of people on board, and that ‘FamilySized’ and ‘Survived’ have a non-linear relationship.\n\n\n5.5 Age.Group\n\n\ntrain %&gt;% \n  ggplot(aes(Age.Group, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"Age group\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by Age group\")\n\n\n\n\n\n\n\n5.6 title\n\n\ntrain %&gt;% \n  ggplot(aes(title, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"title\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by passengers title\")\n\n\n\n\n\n\n\n5.7 ticket.size\n\n\ntrain %&gt;% \n  ggplot(aes(ticket.size, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"ticket.size\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by ticket.size\")\n\n\n\n\n\n\n\n5.8 Description of actual used features\n\nNow that all the derived variables created so far have been found to be useful, select and save only the variables you will actually use.\nThe table below is a brief description of the actual selected variables.\n\n\n\n\n\n\n\n\nvariable name\nType\nDescription\n\n\n\n\nSurvived\nfactor\nTarget feature, survival == 1, death == 0\n\n\nSex\nfactor\ngender, male or female\n\n\nPclass\nfactor\nCabin Class, First Class (1), Second Class (2), Third Class (3)\n\n\nEmbarked\nfactor\nPort of embarkation, Southampton (S), Cherbourg (C), Queenstown (Q)\n\n\nFamilySized\nfactor\nFamily size, a derived variable created using SibSp and Parch, with 3 categories\n\n\nAge.Group\nfactor\nAge group, a derived variable created using Age, with 4 categories\n\n\ntitle\nfactor\nA part of the name, a derived variable made using Name, and 5 categories\n\n\nticket.size\nfactor\nThe length of the unique part of the ticket, a derived variable created using ticket, with 3 categories\n\n\n\n\n\n# Excluding ID number, select and save 7 input variables and 1 target variable to actually use\n\ntrain &lt;- train %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\", \"Survived\")\n\n# For Submit, extract the Id column vector and store it in ID\n\nID &lt;- test$PassengerId\n\n# Select and save the remaining 6 variables except for Id and Survived\n\ntest &lt;- test %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\")"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#machine-learning-model-generation",
    "href": "teaching/ml101/weekly/posts/02_week.html#machine-learning-model-generation",
    "title": "About ML & Modelling",
    "section": "6. Machine learning model generation",
    "text": "6. Machine learning model generation\n\nNow is the time to create a machine learning model using the train data set.\nOriginally, it is correct to create train, validation, test data sets first, create various models, and then select the final model through cross validation (CV, Cross Validation), but these processes are omitted here and RandomForest After creating only, we will predict (estimate) the test data and even create data to Submit to competition.\n\n\n6.1 Random Forest model generation\n\n\n# Set the seed number for reproducibility.\nset.seed(1901)\n\ntitanic.rf &lt;- randomForest(Survived ~ ., data = train, importance = T, ntree = 2000)\n\n\n\n\n6.2 Feature importance check\n\n\nimportance(titanic.rf)\n\n                    0        1 MeanDecreaseAccuracy MeanDecreaseGini\nPclass      47.442449 53.94070             64.73724        36.807804\nSex         54.250630 37.30378             58.66109        57.223102\nEmbarked    -6.328112 38.10930             27.32587         9.632958\nFamilySized 32.430898 31.24383             50.13349        18.086894\nAge.Group   15.203313 26.72696             29.36321        10.201187\ntitle       48.228450 41.60124             57.02653        73.146999\nticket.size 39.544367 37.80849             59.59915        22.570142\n\nvarImpPlot(titanic.rf)\n\n\n\n\n\n\n6.3 Predict test data and create submit data\n\n\n# Prediction \npred.rf &lt;- predict(object = titanic.rf, newdata = test, type = \"class\")\n\n# Data frame generation \nsubmit &lt;- data.frame(PassengerID = ID, Survived = pred.rf)\n\n# Write the submit data frame to file : csv is created in the folder designated by setwd().\n\nwrite.csv(submit, file = './titanic_submit.csv', row.names = F)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/04_week.html",
    "href": "teaching/ml101/weekly/posts/04_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #3\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nClass\n\nA large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n\n The Random Forest algorithm has become an essential tool in the world of machine learning and data science due to its remarkable performance in solving complex classification and regression problems. The popularity of this algorithm stems from its ability to create a multitude of decision trees, each contributing to the final output, thus providing a robust and accurate model. Let’s explore the ins and outs of the Random Forest algorithm, its benefits, and its applications in various industries.\n\n\nWhat is the Random Forest Algorithm?\nThe Random Forest algorithm is an ensemble learning method that combines multiple decision trees, each trained on different subsets of the dataset. The final prediction is generated by aggregating the results from each tree, typically through a majority vote for classification or averaging for regression problems.\n\n\n\nHow Does the Random Forest Algorithm Work?\n\nThe random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.\n\nThe Random Forest algorithm works through the following steps:\n\nBootstrapping: Random samples with replacement are drawn from the original dataset, creating several smaller datasets called bootstrap samples.\nDecision tree creation: A decision tree is built for each bootstrap sample. During the construction of each tree, a random subset of features is chosen to split the data at each node. This randomness ensures that each tree is diverse and less correlated with others.\nAggregation: Once all the decision trees are built, they are combined to make the final prediction. For classification tasks, this is done by taking the majority vote from all the trees, while for regression tasks, the average prediction is used.\n\n\n\n\nAdvantages of the Random Forest Algorithm\nThe Random Forest algorithm offers several benefits:\n\nHigh accuracy: By combining multiple decision trees, the algorithm minimizes the risk of overfitting and produces more accurate predictions than a single tree.\nHandling missing data: The algorithm can handle missing data efficiently, as it uses information from other trees when making predictions.\nFeature importance: The Random Forest algorithm can rank the importance of features, providing valuable insights into which variables contribute the most to the model’s predictive power.\nVersatility: The algorithm is applicable to both classification and regression tasks, making it a versatile choice for various problem types.\n\n\n\n\nReal-World Applications of Random Forest\n\nHealthcare: In medical diagnosis, the algorithm can predict diseases based on patient data, improving the accuracy and efficiency of healthcare professionals.\nFinance: The algorithm is used in credit scoring, fraud detection, and stock market predictions, enhancing the decision-making process for financial institutions.\nMarketing: The algorithm helps in customer segmentation, identifying potential customers, and predicting customer churn, allowing businesses to make informed marketing decisions.\nEnvironment: The Random Forest algorithm is used in remote sensing, climate modeling, and species distribution modeling, assisting researchers and policymakers in environmental conservation and management.\n\n\n\n\nHands-on practice\nFirst, make sure to install and load the randomForest package:\n\n# Install the package if you haven't already\n# if (!requireNamespace(\"randomForest\", quietly = TRUE)) {\n#   install.packages(\"randomForest\")\n# }\n\n# Load the package\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n Now, let’s create a random forest model using the iris dataset:\n\nSplitting the dataset\n\n\n# Load the iris dataset\ndata(iris)\n\n# Split the dataset into training (70%) and testing (30%) sets\nset.seed(42) # Set the seed for reproducibility\nsample_size &lt;- floor(0.7 * nrow(iris))\ntrain_index &lt;- sample(seq_len(nrow(iris)), \n                      size = sample_size)\ntrain_index\n\n  [1]  49  65  74 146 122 150 128  47  24  71 100  89 110  20 114 111 131  41\n [19] 139  27 109   5  84  34  92 104   3  58  97  42 142  30  43  15  22 123\n [37]   8  36  68  86  18 130 126  69   4  98  50  99  88  87 145  26   6 105\n [55]   2 124  21  96 115  10  40 129  33 140  73  29  76   9  35  16 107  93\n [73] 120 138  80  55  90  94  57 121  77  13  53  54  32  60  85  17  44  83\n [91]  72 135 118 149  48 136  64  38   1 144  14 132  61  81 103\n\niris_train &lt;- iris[train_index, ]\niris_test &lt;- iris[-train_index, ]\n\nhead(iris_train)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n49           5.3         3.7          1.5         0.2     setosa\n65           5.6         2.9          3.6         1.3 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n146          6.7         3.0          5.2         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\nhead(iris_test)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n7           4.6         3.4          1.4         0.3  setosa\n11          5.4         3.7          1.5         0.2  setosa\n12          4.8         3.4          1.6         0.2  setosa\n19          5.7         3.8          1.7         0.3  setosa\n23          4.6         3.6          1.0         0.2  setosa\n25          4.8         3.4          1.9         0.2  setosa\n\n\n\nBuilding a model\n\n\n# Create the random forest model\nrf_model &lt;- randomForest(Species ~ ., data = iris_train, ntree = 500, mtry = 2, importance = TRUE)\n\n# Print the model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris_train, ntree = 500,      mtry = 2, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 5.71%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         38          0         0  0.00000000\nversicolor      0         33         2  0.05714286\nvirginica       0          4        28  0.12500000\n\n\nThis code creates a random forest model with 500 trees (ntree = 500) and 2 variables tried at each split (mtry = 2). The importance = TRUE argument calculates variable importance.\nPlease see the details below.\n\nCall: This is the function call that was used to create the random forest model. It shows the formula used (Species ~ .), the dataset (iris_train), the number of trees (ntree = 500), the number of variables tried at each split (mtry = 2), and whether variable importance was calculated (importance = TRUE).\nType of random forest: This indicates the type of problem the random forest model is built for. In this case, it’s a classification problem since we are predicting the species of iris flowers.\nNumber of trees: This is the number of decision trees that make up the random forest model. In this case, there are 500 trees.\nNo. of variables tried at each split: This is the number of variables (features) that are randomly selected at each node for splitting. Here, 2 variables are tried at each split.\nOOB estimate of error rate: The Out-of-Bag (OOB) error rate is an estimate of the model’s classification error based on the observations that were not included in the bootstrap sample (i.e., left out) for each tree during the training process. In this case, the OOB error rate is 5.71%, which means that the model misclassified about 5.71% of the samples in the training data.\nConfusion matrix: The confusion matrix shows the number of correct and incorrect predictions made by the random forest model for each class in the training data. The diagonal elements represent correct predictions, and the off-diagonal elements represent incorrect predictions. In this case, the model correctly predicted 38 setosa, 33 versicolor, and 28 virginica samples. It misclassified 2 versicolor samples as virginica and 4 virginica samples as versicolor.\nclass.error: This column displays the error rate for each class. The error rate for setosa is 0% (no misclassifications), 5.71% for versicolor (2 misclassifications), and 12.5% for virginica (4 misclassifications).\n\nIn a Random Forest algorithm, hyperparameters are adjustable settings that control the learning process and model’s behavior. Selecting the right hyperparameters is critical to achieving optimal performance. Here, we discuss some common hyperparameters in Random Forest and how to adjust them using the R programming language.\n\n# Train the Random Forest model with custom hyperparameters\nmodel &lt;- randomForest(\n  Species ~ ., \n  data = iris_train,\n  ntree = 100,                # n_estimators\n  mtry = 6,                   # max_features\n  maxnodes = 30,              # max_depth (use maxnodes instead)\n  min.node.size = 10,         # min_samples_split\n  nodesize = 5,               # min_samples_leaf\n  replace = TRUE              # bootstrap\n)\n\nWarning in randomForest.default(m, y, ...): invalid mtry: reset to within valid\nrange\n\n\n\nn_estimators: The number of decision trees in the forest. A higher value typically results in better performance but increases computation time.\nmax_features: The maximum number of features considered at each split in a decision tree. Some common values are ‘auto’, ‘sqrt’, or a float representing a percentage of the total features.\nmax_depth: The maximum depth of each decision tree. A deeper tree captures more complex patterns but may also lead to overfitting.\nmin_samples_split: The minimum number of samples required to split an internal node. Higher values reduce overfitting by limiting the depth of the tree.\nmin_samples_leaf: The minimum number of samples required to be at a leaf node. This parameter prevents the creation of very small leaves, which can help avoid overfitting.\nbootstrap: A boolean value indicating whether bootstrap samples are used when building trees. If set to FALSE, the whole dataset is used to build each tree.\n\nTo find the best hyperparameters, you can perform a grid search or use other optimization techniques such as random search or Bayesian optimization. The caret package in R can help you with hyperparameter tuning. We will learn this later on.\n\nTo make predictions using the model and evaluate its accuracy, use the following code:\n\n# Make predictions using the testing dataset\npredictions &lt;- predict(rf_model, iris_test)\npredictions\n\n         7         11         12         19         23         25         28 \n    setosa     setosa     setosa     setosa     setosa     setosa     setosa \n        31         37         39         45         46         51         52 \n    setosa     setosa     setosa     setosa     setosa versicolor versicolor \n        56         59         62         63         66         67         70 \nversicolor versicolor versicolor versicolor versicolor versicolor versicolor \n        75         78         79         82         91         95        101 \nversicolor  virginica versicolor versicolor versicolor versicolor  virginica \n       102        106        108        112        113        116        117 \n virginica  virginica  virginica  virginica  virginica  virginica  virginica \n       119        125        127        133        134        137        141 \n virginica  virginica  virginica  virginica versicolor  virginica  virginica \n       143        147        148 \n virginica  virginica  virginica \nLevels: setosa versicolor virginica\n\n# Calculate the accuracy\naccuracy &lt;- sum(predictions == iris_test$Species) /  length(predictions)\ncat(\"Accuracy:\", accuracy)\n\nAccuracy: 0.9555556\n\n\n Finally, you can visualize the variable importance using the following code:\n\n# Plot variable importance\nimportance(rf_model)\n\n                setosa versicolor virginica MeanDecreaseAccuracy\nSepal.Length  7.496191  6.2012298  7.691861            10.430372\nSepal.Width   5.126713 -0.5597552  1.662978             3.209728\nPetal.Length 22.072402 28.0181030 28.358114            31.140032\nPetal.Width  21.082980 27.3286061 29.256384            29.796881\n             MeanDecreaseGini\nSepal.Length         7.081252\nSepal.Width          2.116113\nPetal.Length        30.519101\nPetal.Width         29.393582\n\nvarImpPlot(rf_model)\n\n\n\n\nThere are two measures of variable importance presented: Mean Decrease in Accuracy and Mean Decrease in Gini Impurity. Both measures provide an indication of how important a given feature is for the model’s performance.\n\nMean Decrease in Accuracy: This measure calculates the decrease in model accuracy when the values of a specific feature are randomly permuted (keeping all other features the same). A higher value indicates that the feature is more important for the model’s performance. In this case, the order of importance is: Petal.Length (31.14), Petal.Width (29.80), Sepal.Length (10.43), and Sepal.Width (3.21).\nMean Decrease in Gini Impurity: This measure calculates the average decrease in Gini impurity for a specific feature when it is used in trees of the random forest model. The Gini impurity is a measure of how “mixed” the classes are in a given node, with a lower impurity indicating better separation. A higher Mean Decrease in Gini indicates that the feature is more important for the model’s performance. In this case, the order of importance is: Petal.Length (30.52), Petal.Width (29.39), Sepal.Length (7.08), and Sepal.Width (2.12).\n\nBoth measures of variable importance agree on the ranking of the features. Petal.Length and Petal.Width are the most important features for predicting the species of iris flowers, while Sepal.Length and Sepal.Width are less important.\n\n\n\nAdvanced study\nThe AdaBoost algorithm is a type of ensemble learning algorithm that combines multiple “weak” classifiers to create a “strong” classifier. A weak classifier is one that performs only slightly better than random guessing (i.e., its accuracy is slightly better than 50%). In contrast, a strong classifier is one that performs well on the classification task. There are many similarities with Random Forest but different in the way of giving weights. If you want to know more about the adaboost algorithm then click (here)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/06_week.html",
    "href": "teaching/ml101/weekly/posts/06_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #5\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nClass\n\nIntroduction\nOverview of k-nearest neighbor algorithm\nThe k-nearest neighbor algorithm is a non-parametric algorithm that works by finding the k closest data points in the training set to a new, unseen data point, and then predicting the class or value of that data point based on the labels of its k-nearest neighbors. The algorithm is simple to implement and has a wide range of applications, including image recognition, text classification, and recommendation systems.\nFor example, let’s say you want to predict whether a new flower is a setosa, versicolor, or virginica based on its sepal length and width. You can use the k-nearest neighbor algorithm to find the k closest flowers in the training set to the new flower, and then predict the most common species among those k flowers.\n\nApplications of k-nearest neighbor algorithm\nThe k-nearest neighbor algorithm has a wide range of applications, including:\n\nImage recognition: identifying the content of an image based on its features\nText classification: categorizing text documents based on their content\nRecommendation systems: suggesting products or services based on the preferences of similar users\nBioinformatics: identifying similar genes or proteins based on their expression patterns\nAnomaly detection: identifying unusual data points based on their distance from other data points\n\n\nAdvantages and disadvantages of k-nearest neighbor algorithm\nThe k-nearest neighbor algorithm has several advantages, including:\n\nIntuitive and easy to understand\nNo assumption about the distribution of the data\nNon-parametric: can work with any type of data\nCan handle multi-class classification problems\n\nHowever, the k-nearest neighbor algorithm also has some disadvantages, including:\n\nCan be computationally expensive for large data sets\nSensitive to irrelevant features and noisy data\nRequires a good distance metric for accurate predictions\nChoosing the right value of k can be challenging\n\nDespite these limitations, the k-nearest neighbor algorithm remains a popular and effective machine learning algorithm that is widely used in various fields.\n\n\n\nTheory\nLet’s dive deeper into the theory behind the k-nearest neighbor algorithm and explore different distance metrics used in the algorithm.\nDistance metrics: Euclidean distance, Manhattan distance, etc.\nOne of the key components of the k-nearest neighbor algorithm is the distance metric used to measure the similarity between two data points. The most commonly used distance metrics are:\n\nFind out more (here)\n\nEuclidean distance: this is the straight-line distance between two points in Euclidean space. The formula for Euclidean distance between two points, x and y, is:\n\\[\nd(x,y) = \\sqrt {\\sum(x_i - y_i)^2}\n\\]\nd(x, y) = sqrt(sum((xi - yi)^2))\nManhattan distance: this is the distance between two points measured along the axes at right angles. The formula for Manhattan distance between two points, x and y, is:\n\\[\nd(x,y) = \\sum|x_i - y_i|\n\\]\nd(x, y) = sum(|xi - yi|)\nMinkowski distance: this is a generalization of Euclidean and Manhattan distance that allows us to control the “shape” of the distance metric. The formula for Minkowski distance between two points, x and y, is:\n\\[\nd(x,y) = (\\sum|x_i - y_i|^p)^\\frac{1}{p}\n\\]\nd(x, y) = (sum(|xi - yi|^p))^(1/p)\nwhere p is a parameter that controls the “shape” of the distance metric. When p=1, the Minkowski distance is equivalent to Manhattan distance, and when p=2, the Minkowski distance is equivalent to Euclidean distance.\n\nChoosing the right distance metric is important for accurate predictions in the k-nearest neighbor algorithm. You should choose a distance metric that is appropriate for your data and the problem you are trying to solve.\n\nChoosing the value of k\nAnother important component of the k-nearest neighbor algorithm is the value of k, which represents the number of nearest neighbors used to make the prediction. Choosing the right value of k is crucial for the performance of the algorithm.\n\nIf k is too small, the algorithm may be too sensitive to noise and outliers in the data, leading to overfitting. On the other hand, if k is too large, the algorithm may be too general and fail to capture the nuances of the data, leading to underfitting.\n\nOne common approach to choosing the value of k is to use cross-validation to evaluate the performance of the algorithm on different values of k and choose the value that gives the best performance.\n\nWeighted versus unweighted k-nearest neighbor algorithm\nIn the basic k-nearest neighbor algorithm, all k nearest neighbors are treated equally when making the prediction. However, in some cases, it may be more appropriate to assign different weights to the nearest neighbors based on their distance from the new data point.\nFor example, you may want to give more weight to the nearest neighbors that are closer to the new data point and less weight to the neighbors that are farther away. This can be done by using a weighted k-nearest neighbor algorithm, where the weights are inversely proportional to the distance between the neighbors and the new data point.\n\nHandling ties in k-nearest neighbor algorithm\nIn some cases, there may be a tie in the labels of the k nearest neighbors, making it difficult to make a prediction. For example, if k=4 and two neighbors are labeled as class A and also two neighbors are labeled as class B, there is a tie between class A and class B.\nThere are several ways to handle ties in the k-nearest neighbor algorithm. One common approach is to assign the new data point to the class that has the nearest neighbor among the tied classes.\n\n\n\nImplementation\nWe introduced the k-nearest neighbor algorithm and discussed its theory, including distance metrics, choosing the value of k, weighted versus unweighted k-nearest neighbor algorithm, and handling ties in k-nearest neighbor algorithm. Let’s learn how to implement the k-nearest neighbor algorithm in R, step by step.\n\nLoading data into R\nThe first step in implementing the k-nearest neighbor algorithm is to load the data into R. In this example, we’ll use the iris dataset, which contains measurements of iris flowers.\n\ndata(iris)\n\nSplitting data into training and testing sets\nThe next step is to split the data into training and testing sets. We’ll use 70% of the data for training and 30% of the data for testing.\n\nset.seed(123)\ntrain_index &lt;- sample(nrow(iris), 0.7 * nrow(iris))\ntrain_data &lt;- iris[train_index, ]\ntest_data &lt;- iris[-train_index, ]\n\nPreprocessing data: scaling and centering\nBefore applying the k-nearest neighbor algorithm, it’s important to preprocess the data by scaling and centering the features. We’ll use the scale function in R to scale and center the data.\n\ntrain_data_scaled &lt;- scale(train_data[, -5])\ntest_data_scaled &lt;- scale(test_data[, -5])\n\n\nhead(train_data_scaled)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n14   -1.76664631  -0.1186139   -1.4793538  -1.4471163\n50   -0.96560680   0.5607204   -1.3126884  -1.3157877\n118   2.12411700   1.6929443    1.6317336   1.3107847\n43   -1.65221209   0.3342756   -1.3682435  -1.3157877\n150   0.06430113  -0.1186139    0.7428515   0.7854702\n148   0.75090642  -0.1186139    0.7984066   1.0481275\n\nsummary(train_data_scaled)\n\n  Sepal.Length       Sepal.Width       Petal.Length      Petal.Width     \n Min.   :-1.76665   Min.   :-1.9302   Min.   :-1.5349   Min.   :-1.4471  \n 1st Qu.:-0.85117   1st Qu.:-0.5715   1st Qu.:-1.2016   1st Qu.:-1.1845  \n Median :-0.05013   Median :-0.1186   Median : 0.3540   Median : 0.2602  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.63647   3rd Qu.: 0.7872   3rd Qu.: 0.7429   3rd Qu.: 0.7855  \n Max.   : 2.35299   Max.   : 3.0516   Max.   : 1.7428   Max.   : 1.7048  \n\n\nWhen working with machine learning algorithms, especially distance-based methods such as k-nearest neighbors (kNN), it is crucial to preprocess the data by scaling and centering the features. This ensures that all features contribute equally to the model’s performance and prevents features with larger magnitudes from dominating the algorithm.\n\nScaling and centering involve transforming the data such that the features have a mean of 0 and a standard deviation of 1. The transformation is performed using the following equations:\n\nCentering (Mean subtraction):\nX_centered = X - mean(X)\nThis step involves subtracting the mean of the feature from each data point, effectively centering the data around 0.\nScaling (Divide by standard deviation):\nX_scaled = X_centered / sd(X)\nIn this step, we divide the centered data by the standard deviation, resulting in a transformed feature with a standard deviation of 1.\n\nIn R, we can use the scale() function to perform both centering and scaling in one step. Here’s how to apply it to the train_data and test_data.\n\nMin-max normalization is another preprocessing technique used to scale the features within a specific range, usually [0, 1]. This method can be particularly useful when working with algorithms sensitive to feature magnitudes or when we want to maintain the same unit of measurement across features.\nMin-max normalization is performed using the following equation:\nX_normalized = (X - min(X)) / (max(X) - min(X))\nThis transformation scales the data linearly between the minimum and maximum values of each feature.\n\n# Define the min-max normalization function\nmin_max_normalize &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ntrain_data_normalized &lt;- min_max_normalize(train_data[, -5])\n\nhead(train_data_normalized)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n14     0.5384615   0.3717949    0.1282051  0.00000000\n50     0.6282051   0.4102564    0.1666667  0.01282051\n118    0.9743590   0.4743590    0.8461538  0.26923077\n43     0.5512821   0.3974359    0.1538462  0.01282051\n150    0.7435897   0.3717949    0.6410256  0.21794872\n148    0.8205128   0.3717949    0.6538462  0.24358974\n\nsummary(train_data_normalized)\n\n  Sepal.Length     Sepal.Width      Petal.Length     Petal.Width     \n Min.   :0.5385   Min.   :0.2692   Min.   :0.1154   Min.   :0.00000  \n 1st Qu.:0.6410   1st Qu.:0.3462   1st Qu.:0.1923   1st Qu.:0.02564  \n Median :0.7308   Median :0.3718   Median :0.5513   Median :0.16667  \n Mean   :0.7364   Mean   :0.3785   Mean   :0.4696   Mean   :0.14127  \n 3rd Qu.:0.8077   3rd Qu.:0.4231   3rd Qu.:0.6410   3rd Qu.:0.21795  \n Max.   :1.0000   Max.   :0.5513   Max.   :0.8718   Max.   :0.30769  \n\n\nWriting a function to calculate distances\nNext, we’ll write a function to calculate distances between two data points using the Euclidean distance metric.\n\neuclidean_distance &lt;- function(x, y) {\n  sqrt(sum((x - y)^2))\n}\n\nImplementing k-nearest neighbor algorithm using class package\nWe’ll use the class package in R to implement the k-nearest neighbor algorithm. We’ll use the knn function in the class package to make predictions based on the k nearest neighbors.\n\nlibrary(class)\nk &lt;- 5\npredicted_classes &lt;- knn(train = train_data_scaled, \n                         test = test_data_scaled, \n                         cl = train_data[, 5], \n                         k = k, \n                         prob = TRUE)\n\nIn the code above, we set k to 5, which means we’ll use the 5 nearest neighbors to make the prediction. The knn function returns the predicted classes of the test data, based on the labels of the nearest neighbors in the training data. The option cl means class, prop means that the result comes with the probability.\n\nEvaluating model performance using confusion matrix, accuracy, precision, recall, and F1-score\nFinally, we’ll evaluate the performance of the k-nearest neighbor algorithm using a confusion matrix, accuracy, precision, recall, and F1-score.\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nconfusion_matrix &lt;- confusionMatrix(predicted_classes, test_data[, 5])\nconfusion_matrix\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0         17         0\n  virginica       0          1        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9778          \n                 95% CI : (0.8823, 0.9994)\n    No Information Rate : 0.4             \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9664          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9444           1.0000\nSpecificity                 1.0000            1.0000           0.9688\nPos Pred Value              1.0000            1.0000           0.9286\nNeg Pred Value              1.0000            0.9643           1.0000\nPrevalence                  0.3111            0.4000           0.2889\nDetection Rate              0.3111            0.3778           0.2889\nDetection Prevalence        0.3111            0.3778           0.3111\nBalanced Accuracy           1.0000            0.9722           0.9844\n\nconfusion_matrix$byClass[,\"Precision\"]\n\n    Class: setosa Class: versicolor  Class: virginica \n        1.0000000         1.0000000         0.9285714 \n\nconfusion_matrix$byClass[,\"Recall\"]\n\n    Class: setosa Class: versicolor  Class: virginica \n        1.0000000         0.9444444         1.0000000 \n\nconfusion_matrix$byClass[,\"F1\"]\n\n    Class: setosa Class: versicolor  Class: virginica \n        1.0000000         0.9714286         0.9629630 \n\n\nIn the code above, we use the confusionMatrix function in the caret package to generate a confusion matrix based on the predicted classes and the true labels of the test data. We then extract the overall accuracy and the precision, recall, and F1-score for each class from the confusion matrix.\n\nWhat is F1 score?\nF1 score is a measure of a machine learning algorithm’s accuracy that combines precision and recall. It is the harmonic mean of precision and recall, and ranges from 0 to 1, with higher values indicating better performance.\nF1 score is calculated using the following formula:\n\\[\nF1 = 2 \\times \\frac{Precision \\times Recall}{Precesion + Recall}\n\\]\nF1 score = 2 * (precision * recall) / (precision + recall)\nwhere precision is the number of true positives divided by the total number of positive predictions, and recall is the number of true positives divided by the total number of actual positives.\nWhy use F1 score?\nF1 score is useful when the dataset is imbalanced, meaning that the number of positive and negative examples is not equal. In such cases, accuracy alone is not a good measure of the algorithm’s performance, as a high accuracy can be achieved by simply predicting the majority class all the time.\nInstead, we need a metric that takes into account both precision and recall, as precision measures the algorithm’s ability to make correct positive predictions, and recall measures the algorithm’s ability to find all positive examples in the dataset.\nHow to interpret F1 score?\nF1 score ranges from 0 to 1, with higher values indicating better performance. An F1 score of 1 means perfect precision and recall, while an F1 score of 0 means that either the precision or recall is 0.\nIn practice, we aim to achieve a high F1 score while balancing precision and recall based on the problem and its requirements. For example, in medical diagnosis, we may want to prioritize recall over precision to avoid missing any positive cases, while in fraud detection, we may want to prioritize precision over recall to avoid false positives.\n\n\n\nQZs\n\nWhich of the following is a distance metric commonly used in the k-nearest neighbor algorithm?\n\nCorrelation distance\nChebyshev distance\nHamming distance\nAll of the above\n\nHow do you choose the value of k in the k-nearest neighbor algorithm?\n\nChoose a small value of k to avoid overfitting\nChoose a large value of k to avoid overfitting\nUse cross-validation to evaluate the performance of the algorithm on different values of k and choose the value that gives the best performance\nNone of the above\n\nWhat is the difference between weighted and unweighted k-nearest neighbor algorithm?\n\nWeighted k-nearest neighbor algorithm gives more weight to the nearest neighbors that are farther away\nUnweighted k-nearest neighbor algorithm gives more weight to the nearest neighbors that are closer\nWeighted k-nearest neighbor algorithm assigns different weights to the nearest neighbors based on their distance from the new data point\nUnweighted k-nearest neighbor algorithm assigns different weights to the nearest neighbors based on their distance from the new data point\n\nWhat is F1 score?\n\nA measure of a machine learning algorithm’s accuracy that combines precision and recall\nThe average of precision and recall\nThe harmonic mean of precision and recall\nNone of the above\n\nWhat is the formula for calculating Euclidean distance between two points in Euclidean space?\n\nd(x, y) = sqrt(sum((xi - yi)^2))\nd(x, y) = sum(|xi - yi|)\nd(x, y) = (sum(|xi - yi|^p))^(1/p)\nNone of the above\n\nWhat is the purpose of scaling and centering the features in the k-nearest neighbor algorithm?\n\nTo make the features easier to interpret\nTo make the features more accurate\nTo make the features more comparable\nNone of the above\n\nHow can you handle ties in the k-nearest neighbor algorithm?\n\nAssign the new data point to the class that has the nearest neighbor among the tied classes\nAssign the new data point to the class that has the farthest neighbor among the tied classes\nAssign the new data point to the class that has the most neighbors among the tied classes\nNone of the above\n\n\nAns) dcccaca\n\nIn this week, we learned about the k-nearest neighbor algorithm, a popular machine learning algorithm used for classification and regression problems. We started with an overview of the algorithm and its applications, and discussed the advantages and disadvantages of the algorithm.\nWe then delved into the theory behind the k-nearest neighbor algorithm, including distance metrics such as Euclidean distance and Manhattan distance, choosing the value of k, weighted versus unweighted k-nearest neighbor algorithm, and handling ties in k-nearest neighbor algorithm.\nWe also showed you how to implement the k-nearest neighbor algorithm in R step by step, including loading data into R, splitting data into training and testing sets, preprocessing data, writing a function to calculate distances, and evaluating model performance using confusion matrix, accuracy, precision, recall, and F1-score.\nFinally, we gave you the opportunity to practice implementing the k-nearest neighbor algorithm on your own data set and evaluate the model’s performance.\nBy understanding the k-nearest neighbor algorithm and its theory, as well as its implementation in R, you can apply this algorithm to your own machine learning problems and make informed decisions about your data. Keep practicing and exploring different machine learning algorithms to expand your knowledge and skills in data science.\n\n\n\n\n\nModel Comparison with tidymodels in R\nLet’s learn how to compare four different machine learning models using the tidymodels package in R. We will be using the classic Iris dataset to showcase this comparison. The models we will compare are Decision Trees, Random Forests, Naive Bayes, and k-Nearest Neighbors (kNN).\n First, let’s load the necessary libraries and the Iris dataset.\n\n# Load the required libraries\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ dplyr        1.1.3     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()         masks scales::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ dplyr::lag()             masks stats::lag()\n✖ purrr::lift()            masks caret::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ purrr::lift()       masks caret::lift()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(rpart.plot)\nlibrary(discrim)\n\n\nAttaching package: 'discrim'\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n\nlibrary(naivebayes)\n\nnaivebayes 0.9.7 loaded\n\nlibrary(kknn)\n\n\nAttaching package: 'kknn'\n\nThe following object is masked from 'package:caret':\n\n    contr.dummy\n\nlibrary(yardstick)\n\n# Load the Iris dataset\ndata(iris)\n\nBefore we begin modeling, we need to preprocess the data. We will split the dataset into training (75%) and testing (25%) sets.\n\n# Split the data into training and testing sets\nset.seed(42)\ndata_split &lt;- initial_split(iris, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nNow, we will create the models using tidymodels’ parsnip package. Each model will be created using a similar structure, specifying the model type and the mode (classification in this case).\n\n# Decision Tree\ndecision_tree &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Random Forest\nrandom_forest &lt;- rand_forest() %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"classification\")\n\n# Naive Bayes\nnaive_bayes &lt;- naive_Bayes() %&gt;%\n  set_engine(\"naivebayes\") %&gt;%\n  set_mode(\"classification\")\n\n# k-Nearest Neighbors (kNN)\nknn &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\n\nNext, we will create a workflow for each model. In this example, we don’t require any preprocessing steps, so we will directly specify the model in the workflow.\n\n# Decision Tree Workflow\nworkflow_dt &lt;- workflow() %&gt;%\n  add_model(decision_tree) %&gt;%\n  add_formula(Species ~ .)\n\n\n# Random Forest Workflow\nworkflow_rf &lt;- workflow() %&gt;%\n  add_model(random_forest) %&gt;%\n  add_formula(Species ~ .)\n\n# Naive Bayes Workflow\nworkflow_nb &lt;- workflow() %&gt;%\n  add_model(naive_bayes) %&gt;%\n  add_formula(Species ~ .)\n\n# kNN Workflow\nworkflow_knn &lt;- workflow() %&gt;%\n  add_model(knn) %&gt;%\n  add_formula(Species ~ .)\n\n\nWe will now fit each model using the training data and make predictions on the test data.\n\n# Fit the models\nfit_dt &lt;- fit(workflow_dt, data = train_data)\nfit_rf &lt;- fit(workflow_rf, data = train_data)\nfit_nb &lt;- fit(workflow_nb, data = train_data)\nfit_knn &lt;- fit(workflow_knn, data = train_data)\n\n\nFinally, we will evaluate the performance of each model using accuracy as the metric.\n\npredict(fit_dt, test_data) %&gt;% \n  bind_cols(predict(fit_dt, test_data, type = \"prob\")) %&gt;% \n  # Add the true outcome data back in\n  bind_cols(test_data %&gt;% \n              select(Species))\n\n# A tibble: 38 × 5\n   .pred_class .pred_setosa .pred_versicolor .pred_virginica Species\n   &lt;fct&gt;              &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;  \n 1 setosa                 1                0               0 setosa \n 2 setosa                 1                0               0 setosa \n 3 setosa                 1                0               0 setosa \n 4 setosa                 1                0               0 setosa \n 5 setosa                 1                0               0 setosa \n 6 setosa                 1                0               0 setosa \n 7 setosa                 1                0               0 setosa \n 8 setosa                 1                0               0 setosa \n 9 setosa                 1                0               0 setosa \n10 setosa                 1                0               0 setosa \n# ℹ 28 more rows\n\npredict(fit_dt, test_data) %&gt;% \n  bind_cols(predict(fit_dt, test_data, type = \"prob\")) %&gt;% \n  bind_cols(test_data %&gt;% \n              select(Species)) %&gt;% \n  # Add accuracy function from yardstick\n  accuracy(truth = Species, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.947\n\n# All together \naccuracy_dt &lt;- \n  predict(fit_dt, test_data) %&gt;% \n  bind_cols(predict(fit_dt, test_data, type = \"prob\")) %&gt;% \n  bind_cols(test_data %&gt;% \n              select(Species)) %&gt;% \n  accuracy(truth = Species, .pred_class)\n\nDo the same things for the other models\n\naccuracy_rf &lt;- \n  predict(fit_rf, test_data) %&gt;% \n  bind_cols(predict(fit_rf, test_data, type = \"prob\")) %&gt;% \n  bind_cols(test_data %&gt;% \n              select(Species)) %&gt;% \n  accuracy(truth = Species, .pred_class)\n\naccuracy_nb &lt;- \n  predict(fit_nb, test_data) %&gt;% \n  bind_cols(predict(fit_nb, test_data, type = \"prob\")) %&gt;% \n  bind_cols(test_data %&gt;% \n              select(Species)) %&gt;% \n  accuracy(truth = Species, .pred_class)\n\naccuracy_knn &lt;- \n  predict(fit_knn, test_data) %&gt;% \n  bind_cols(predict(fit_knn, test_data, type = \"prob\")) %&gt;% \n  bind_cols(test_data %&gt;% \n              select(Species)) %&gt;% \n  accuracy(truth = Species, .pred_class)\n\ncalculates the accuracy for each model and displays the results in a sorted data frame.\n\naccuracy_dt %&gt;% \n  bind_rows(accuracy_rf, accuracy_nb, accuracy_knn) %&gt;% \n  mutate(models = c(\"Decision Tree\", \"Random Forest\", \"Naive Bayes\", \"kNN\")) %&gt;% \n  arrange(desc(.estimate))\n\n# A tibble: 4 × 4\n  .metric  .estimator .estimate models       \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        \n1 accuracy multiclass     0.974 kNN          \n2 accuracy multiclass     0.947 Decision Tree\n3 accuracy multiclass     0.947 Random Forest\n4 accuracy multiclass     0.947 Naive Bayes"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/08_week.html",
    "href": "teaching/ml101/weekly/posts/08_week.html",
    "title": "QZ #1",
    "section": "",
    "text": "summary(my_data)\n\n    Class               Score      \n Length:47          Min.   : 8.00  \n Class :character   1st Qu.:21.50  \n Mode  :character   Median :29.00  \n                    Mean   :29.11  \n                    3rd Qu.:39.00  \n                    Max.   :45.00  \n\n\n\nMax score: 45 / 47\nMin score: 8 / 47\nAVG score: 29.11 / 47\n\nHistogram\n\nggplot(my_data, aes(x = Score)) + \n  geom_histogram(binwidth = 3, color = \"white\", fill = \"#56B4E9\", alpha = 0.8) +\n  # Add vertical line for mean\n  geom_vline(aes(xintercept = mean(Score)), \n             color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_text(aes(x = 29.11, y = 6.5, label = \"AVG: 29.11 / 47\")) +\n  # Add x and y axis labels\n  labs(x = \"Score\", y = \"Frequency\", \n       title = \"Score distribution\") +\n  # Set a simple theme\n  theme_minimal()\n\n\n\n\nBox-plot by classes\n\n# Calculate average scores for each class\nclass_averages &lt;- my_data %&gt;%\n  group_by(Class) %&gt;%\n  summarize(avg_score = mean(Score))\n\nggplot(my_data, aes(x = Class, y = Score, fill = Class)) +\n  geom_boxplot(alpha = 0.8, color = \"black\", size = 1) +\n  # Add individual data points with jitter\n  geom_jitter(alpha = 0.2) +\n  # Add text labels for average values\n  geom_text(data = class_averages, \n            aes(x = Class, y = 15, \n                label = paste0(\"Group AVG: \", round(avg_score, 1))), \n            color = \"black\", \n            size = 4, \n            fontface = \"bold\") +\n  # Remove legend\n  guides(fill = FALSE) +\n  # Choose color palette\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  # Add x and y axis labels\n  labs(x = \"Class\", y = \"Score\") +\n  # Set a simple theme\n  theme_minimal()"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/08_week.html#score-distribution",
    "href": "teaching/ml101/weekly/posts/08_week.html#score-distribution",
    "title": "QZ #1",
    "section": "",
    "text": "summary(my_data)\n\n    Class               Score      \n Length:47          Min.   : 8.00  \n Class :character   1st Qu.:21.50  \n Mode  :character   Median :29.00  \n                    Mean   :29.11  \n                    3rd Qu.:39.00  \n                    Max.   :45.00  \n\n\n\nMax score: 45 / 47\nMin score: 8 / 47\nAVG score: 29.11 / 47\n\nHistogram\n\nggplot(my_data, aes(x = Score)) + \n  geom_histogram(binwidth = 3, color = \"white\", fill = \"#56B4E9\", alpha = 0.8) +\n  # Add vertical line for mean\n  geom_vline(aes(xintercept = mean(Score)), \n             color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_text(aes(x = 29.11, y = 6.5, label = \"AVG: 29.11 / 47\")) +\n  # Add x and y axis labels\n  labs(x = \"Score\", y = \"Frequency\", \n       title = \"Score distribution\") +\n  # Set a simple theme\n  theme_minimal()\n\n\n\n\nBox-plot by classes\n\n# Calculate average scores for each class\nclass_averages &lt;- my_data %&gt;%\n  group_by(Class) %&gt;%\n  summarize(avg_score = mean(Score))\n\nggplot(my_data, aes(x = Class, y = Score, fill = Class)) +\n  geom_boxplot(alpha = 0.8, color = \"black\", size = 1) +\n  # Add individual data points with jitter\n  geom_jitter(alpha = 0.2) +\n  # Add text labels for average values\n  geom_text(data = class_averages, \n            aes(x = Class, y = 15, \n                label = paste0(\"Group AVG: \", round(avg_score, 1))), \n            color = \"black\", \n            size = 4, \n            fontface = \"bold\") +\n  # Remove legend\n  guides(fill = FALSE) +\n  # Choose color palette\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  # Add x and y axis labels\n  labs(x = \"Class\", y = \"Score\") +\n  # Set a simple theme\n  theme_minimal()"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/08_week.html#date-and-location",
    "href": "teaching/ml101/weekly/posts/08_week.html#date-and-location",
    "title": "QZ #1",
    "section": "Date and Location",
    "text": "Date and Location\n\nDate: 19 April (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/08_week.html#notice",
    "href": "teaching/ml101/weekly/posts/08_week.html#notice",
    "title": "QZ #1",
    "section": "Notice",
    "text": "Notice\n\nQuiz will be administered through Google Forms.\nPlease bring your laptop for the quiz.\nYou are allowed to access any information through the Internet\nHowever, communication with others is strictly prohibited.\nDo not use any messaging apps (e.g., KakaoTalk, TikTok, Line, WeChat, etc.) during the quiz.\nUpon completion of the quiz, you are required to submit your code."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/08_week.html#data",
    "href": "teaching/ml101/weekly/posts/08_week.html#data",
    "title": "QZ #1",
    "section": "Data",
    "text": "Data\n\nWe are going to use a dataset named ‘penguins’ from the ‘palmerpenguins’ package. The dataset contains different body measurements for three species of penguins from three islands in the Palmer Archipelago, Antarctica. The penguins dataset is useful for learning R, because it contains multiple kinds of data (both categorical and numeric variables).\n\n\nPlease see the link below if you want to know more about this data. https://allisonhorst.github.io/palmerpenguins/articles/intro.html"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/08_week.html#qz",
    "href": "teaching/ml101/weekly/posts/08_week.html#qz",
    "title": "QZ #1",
    "section": "QZ",
    "text": "QZ\n\nPart I. Data Import & Exploration\nLet’s import “Palmerspenguins” data (Use the code below)\n\n# Install palmerpenguins pacakge (if required)\nif(!require(palmerpenguins)){\n  install.packages(\"palmerpenguins\")\n}\n\nLoading required package: palmerpenguins\n\n# import libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\ndata(penguins)\n\nLet’s have a glimpse of the data\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nThe penguins dataset consists of 344 observations and the following 8 variables:\n\nspecies: A factor with three levels - Adelie, Chinstrap, and Gentoo. These are the three penguin species under study.\nisland: A factor with three levels - Biscoe, Dream, and Torgersen. These are the islands in the Palmer Archipelago where the penguins were observed.\nbill_length_mm: A numeric variable representing the length of the penguin’s culmen (bill) in millimeters.\nbill_depth_mm: A numeric variable representing the depth of the penguin’s culmen (bill) in millimeters.\nflipper_length_mm: A numeric variable representing the length of the penguin’s flipper in millimeters.\nbody_mass_g: A numeric variable representing the penguin’s body mass in grams.\nsex: A factor with two levels - male and female.\nyear: An integer variable representing the year of observation (2007, 2008, or 2009).\n\n\nThere are three species in palmerspenguins: Chinstrap / Gentoo / Adelie\n\n\nThe data table looks like ..\n\nknitr::kable(penguins %&gt;% head(10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n2007\n\n\nAdelie\nTorgersen\n34.1\n18.1\n193\n3475\nNA\n2007\n\n\nAdelie\nTorgersen\n42.0\n20.2\n190\n4250\nNA\n2007\n\n\n\n\n\n\n# Glimpse of the penguins dataset.\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n# 344 observations\n# 8 vars\n\n############################3\n# EDA\n############################3\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\ntable(penguins$year)\n\n\n2007 2008 2009 \n 110  114  120 \n\ntable(penguins$sex)\n\n\nfemale   male \n   165    168 \n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\ntable(penguins$island)\n\n\n   Biscoe     Dream Torgersen \n      168       124        52 \n\ntable(penguins$species, penguins$island)\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n\n# All penguins in Torgersen island is 'Adelie'\ntable(penguins$sex, penguins$island)\n\n        \n         Biscoe Dream Torgersen\n  female     80    61        24\n  male       83    62        23\n\nplot(penguins$bill_length_mm, \n     penguins$bill_depth_mm)\n\n\n\n\n\n\nPart II. Data wrangling\n\n#####\n# missing values\nlibrary(VIM)\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, were retired in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\naggr(penguins)\n\n\n\n# Variable with the most missing values: sex\nsum(is.na(penguins$bill_length_mm))\n\n[1] 2\n\nsum(is.na(penguins$bill_depth_mm))\n\n[1] 2\n\nsum(is.na(penguins$flipper_length_mm))\n\n[1] 2\n\nsum(is.na(penguins$body_mass_g))\n\n[1] 2\n\n# Filter out when missings in 'bill_length_mm'\npenguins %&gt;% \n  filter(!is.na(bill_length_mm)) -&gt; penguins_new\n\n# Check the missing values again\naggr(penguins_new)\n\n\n\n# Missing value handling with mice library and check again\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\npenguins_new&lt;-mice(penguins_new, method='rf', seed=1234)\n\n\n iter imp variable\n  1   1  sex\n  1   2  sex\n  1   3  sex\n  1   4  sex\n  1   5  sex\n  2   1  sex\n  2   2  sex\n  2   3  sex\n  2   4  sex\n  2   5  sex\n  3   1  sex\n  3   2  sex\n  3   3  sex\n  3   4  sex\n  3   5  sex\n  4   1  sex\n  4   2  sex\n  4   3  sex\n  4   4  sex\n  4   5  sex\n  5   1  sex\n  5   2  sex\n  5   3  sex\n  5   4  sex\n  5   5  sex\n\npenguins_imputed&lt;-complete(penguins_new, 1)\n\naggr(penguins_imputed)\n\n\n\ntable(penguins$sex)\n\n\nfemale   male \n   165    168 \n\ntable(penguins_imputed$sex)\n\n\nfemale   male \n   172    170 \n\nplot(penguins_imputed$bill_length_mm, \n     penguins_imputed$bill_depth_mm)\n\n\n\npenguins_imputed %&gt;% names\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\npenguins_imputed %&gt;% \n  ggplot() +\n  geom_point(aes(x=bill_length_mm, y=bill_depth_mm, color=species))\n\n\n\npenguins_imputed %&gt;% \n  group_by(sex) %&gt;% \n  summarise(bill_len=mean(bill_length_mm),\n            bill_dep=mean(bill_depth_mm),\n            mass=mean(body_mass_g),\n            flipper_len=mean(flipper_length_mm)\n            )\n\n# A tibble: 2 × 5\n  sex    bill_len bill_dep  mass flipper_len\n  &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 female     42.0     16.4 3863.        197.\n2 male       45.8     17.9 4545.        204.\n\npenguins_imputed %&gt;% \n  group_by(island) %&gt;% \n  summarise(bill_len=mean(bill_length_mm),\n            bill_dep=mean(bill_depth_mm),\n            mass=mean(body_mass_g),\n            flipper_len=mean(flipper_length_mm)\n  )\n\n# A tibble: 3 × 5\n  island    bill_len bill_dep  mass flipper_len\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Biscoe        45.3     15.9 4716.        210.\n2 Dream         44.2     18.3 3713.        193.\n3 Torgersen     39.0     18.4 3706.        191.\n\npenguins_imputed %&gt;% \n  group_by(species) %&gt;% \n  summarise(bill_len=mean(bill_length_mm),\n            bill_dep=mean(bill_depth_mm),\n            mass=mean(body_mass_g),\n            flipper_len=mean(flipper_length_mm)\n  )\n\n# A tibble: 3 × 5\n  species   bill_len bill_dep  mass flipper_len\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie        38.8     18.3 3701.        190.\n2 Chinstrap     48.8     18.4 3733.        196.\n3 Gentoo        47.5     15.0 5076.        217.\n\n\n\n\nPart III. Train the model (Modeling)\n\n############################\n# Modeling\n\nglimpse(penguins_imputed)\n\nRows: 342\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n# 1. Decision Tree\npenguins_imputed %&gt;% select(-c(\"island\", \"sex\", \"year\")) -&gt; train\n\nlibrary(rpart)\ndt_model&lt;-rpart(species~., data = train, method = \"class\")\n\nsummary(dt_model)\n\nCall:\nrpart(formula = species ~ ., data = train, method = \"class\")\n  n= 342 \n\n          CP nsplit  rel error    xerror       xstd\n1 0.62827225      0 1.00000000 1.0000000 0.04807937\n2 0.28272251      1 0.37172775 0.3769634 0.03947314\n3 0.02617801      2 0.08900524 0.1151832 0.02375417\n4 0.01000000      3 0.06282723 0.1204188 0.02425006\n\nVariable importance\nflipper_length_mm    bill_length_mm     bill_depth_mm       body_mass_g \n               28                28                24                20 \n\nNode number 1: 342 observations,    complexity param=0.6282723\n  predicted class=Adelie     expected loss=0.5584795  P(node) =1\n    class counts:   151    68   123\n   probabilities: 0.442 0.199 0.360 \n  left son=2 (213 obs) right son=3 (129 obs)\n  Primary splits:\n      flipper_length_mm &lt; 206.5 to the left,  improve=114.04630, (0 missing)\n      bill_length_mm    &lt; 42.35 to the left,  improve=106.39810, (0 missing)\n      bill_depth_mm     &lt; 16.45 to the right, improve=100.54170, (0 missing)\n      body_mass_g       &lt; 4525  to the left,  improve= 85.54889, (0 missing)\n  Surrogate splits:\n      bill_depth_mm  &lt; 16.35 to the right, agree=0.933, adj=0.822, (0 split)\n      body_mass_g    &lt; 4525  to the left,  agree=0.906, adj=0.752, (0 split)\n      bill_length_mm &lt; 43.25 to the left,  agree=0.789, adj=0.442, (0 split)\n\nNode number 2: 213 observations,    complexity param=0.2827225\n  predicted class=Adelie     expected loss=0.3004695  P(node) =0.622807\n    class counts:   149    63     1\n   probabilities: 0.700 0.296 0.005 \n  left son=4 (150 obs) right son=5 (63 obs)\n  Primary splits:\n      bill_length_mm    &lt; 43.35 to the left,  improve=71.131460, (0 missing)\n      flipper_length_mm &lt; 192.5 to the left,  improve= 9.130740, (0 missing)\n      body_mass_g       &lt; 3225  to the left,  improve= 2.352691, (0 missing)\n      bill_depth_mm     &lt; 20.05 to the right, improve= 1.037547, (0 missing)\n  Surrogate splits:\n      flipper_length_mm &lt; 195.5 to the left,  agree=0.746, adj=0.143, (0 split)\n\nNode number 3: 129 observations,    complexity param=0.02617801\n  predicted class=Gentoo     expected loss=0.05426357  P(node) =0.377193\n    class counts:     2     5   122\n   probabilities: 0.016 0.039 0.946 \n  left son=6 (7 obs) right son=7 (122 obs)\n  Primary splits:\n      bill_depth_mm     &lt; 17.65 to the right, improve=10.5382100, (0 missing)\n      body_mass_g       &lt; 4325  to the left,  improve= 3.4127400, (0 missing)\n      flipper_length_mm &lt; 212.5 to the left,  improve= 1.4479800, (0 missing)\n      bill_length_mm    &lt; 50.75 to the right, improve= 0.6684581, (0 missing)\n  Surrogate splits:\n      body_mass_g &lt; 4050  to the left,  agree=0.961, adj=0.286, (0 split)\n\nNode number 4: 150 observations\n  predicted class=Adelie     expected loss=0.03333333  P(node) =0.4385965\n    class counts:   145     5     0\n   probabilities: 0.967 0.033 0.000 \n\nNode number 5: 63 observations\n  predicted class=Chinstrap  expected loss=0.07936508  P(node) =0.1842105\n    class counts:     4    58     1\n   probabilities: 0.063 0.921 0.016 \n\nNode number 6: 7 observations\n  predicted class=Chinstrap  expected loss=0.2857143  P(node) =0.02046784\n    class counts:     2     5     0\n   probabilities: 0.286 0.714 0.000 \n\nNode number 7: 122 observations\n  predicted class=Gentoo     expected loss=0  P(node) =0.3567251\n    class counts:     0     0   122\n   probabilities: 0.000 0.000 1.000 \n\nlibrary(rpart.plot)\nrpart.plot(dt_model, type=4, extra=100, box.palette =\"-YlGnBl\", branch.lty = 2)\n\n\n\npredict(dt_model, penguins_imputed[104,])\n\n       Adelie  Chinstrap Gentoo\n104 0.9666667 0.03333333      0\n\n# 2. Random Forest\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrf_model &lt;- randomForest(species~.,\n                         data = train, \n                         mtry = 3,\n                         ntree = 200)\nrf_model\n\n\nCall:\n randomForest(formula = species ~ ., data = train, mtry = 3, ntree = 200) \n               Type of random forest: classification\n                     Number of trees: 200\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 2.92%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie       146         4      1 0.033112583\nChinstrap      4        64      0 0.058823529\nGentoo         0         1    122 0.008130081\n\nvarImpPlot(rf_model)\n\n\n\n# 3. Naive Bayes\n\nlibrary(naivebayes)\n\nnaivebayes 0.9.7 loaded\n\nnb_model &lt;- naive_bayes(species ~ ., data=train)\nsummary(nb_model)\n\n\n================================== Naive Bayes ================================== \n \n- Call: naive_bayes.formula(formula = species ~ ., data = train) \n- Laplace: 0 \n- Classes: 3 \n- Samples: 342 \n- Features: 4 \n- Conditional distributions: \n    - Gaussian: 4\n- Prior probabilities: \n    - Adelie: 0.4415\n    - Chinstrap: 0.1988\n    - Gentoo: 0.3596\n\n--------------------------------------------------------------------------------- \n\ntable(train$species)\n\n\n   Adelie Chinstrap    Gentoo \n      151        68       123 \n\n# 4. kNN\n\n# Normalization\n\nnor &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) }\n\ntrain %&gt;% mutate(bill_length_mm=nor(bill_length_mm),\n                 bill_depth_mm=nor(bill_depth_mm),\n                 flipper_length_mm=nor(flipper_length_mm),\n                 body_mass_g=nor(body_mass_g)) %&gt;% \n  select(-species)-&gt; train_nor\n\nhead(train_nor)\n\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1      0.2545455     0.6666667         0.1525424   0.2916667\n2      0.2690909     0.5119048         0.2372881   0.3055556\n3      0.2981818     0.5833333         0.3898305   0.1527778\n4      0.1672727     0.7380952         0.3559322   0.2083333\n5      0.2618182     0.8928571         0.3050847   0.2638889\n6      0.2472727     0.5595238         0.1525424   0.2569444\n\nlibrary(class)\n\nkn_model &lt;- knn(train_nor, \n                train_nor[101:120,], \n                cl=train$species, k=13)\ntab &lt;- table(kn_model,train[101:120,\"species\"])\ntab\n\n           \nkn_model    Adelie Chinstrap Gentoo\n  Adelie        20         0      0\n  Chinstrap      0         0      0\n  Gentoo         0         0      0\n\n\n\n\nPart IV. Model score & Prediction\n\n######################\n# Score & Prediction \n\n# create a train dataset\ntest &lt;- train[seq(1,300,3),]\n\n# Prediction by using trained models\npred_dt &lt;- predict(dt_model, test, type='class')\npred_rf &lt;- predict(rf_model, test, type='class')\npred_nb &lt;- predict(nb_model, test, type='class')\n\nWarning: predict.naive_bayes(): more features in the newdata are provided as\nthere are probability tables in the object. Calculation is performed based on\nfeatures to be found in the tables.\n\npred_kn &lt;- knn(train_nor, train_nor[seq(1,300,3),], cl=train$species, k=13)\n\n\ndata.frame(truth=train[seq(1,300,3),\"species\"],\n           dt=pred_dt, \n           rf=pred_rf, \n           nb=pred_nb, \n           kn=pred_kn) %&gt;% \n  mutate(dt=ifelse(dt==truth, 1, 0),\n         rf=ifelse(rf==truth, 1, 0),\n         nb=ifelse(nb==truth, 1, 0),\n         kn=ifelse(kn==truth, 1, 0)) -&gt; score\n\napply(score[-1], 2, sum)\n\n dt  rf  nb  kn \n 95 100  96  98 \n\nscore %&gt;% \n  mutate(ts=dt+rf+nb+kn) %&gt;% \n  mutate(low=ifelse(ts==1, 1, 0)) %&gt;% \n  filter(low==1)\n\n        truth dt rf nb kn ts low\n295 Chinstrap  0  1  0  0  1   1\n\npenguins_imputed[295,]\n\n      species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n295 Chinstrap  Dream           42.4          17.3               181        3600\n       sex year\n295 female 2007\n\n\n\n\nPart V. Code submission (Upload your R file)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/10_week.html",
    "href": "teaching/ml101/weekly/posts/10_week.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #8\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\n\nClass\nK-Means Clustering in R\n\nThe basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (Hartigan and Wong 1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid\n\n\nLet’s experiment with this simple shiny app\nhttps://jjallaire.shinyapps.io/shiny-k-means/\n\nHands-on practice\nData we’ll use is USArrests. The data must contains only continuous variables, as the k-means algorithm uses variable means.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"USArrests\")      # Loading the data set\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\n\nThe USArrests dataset is a built-in dataset in R that contains data on crime rates (number of arrests per 100,000 residents) in the United States in 1973. The dataset has 50 observations, corresponding to the 50 US states, and 4 variables:\n\nMurder: Murder arrests (number of arrests for murder per 100,000 residents).\nAssault: Assault arrests (number of arrests for assault per 100,000 residents).\nUrbanPop: Urban population (percentage of the population living in urban areas).\nRape: Rape arrests (number of arrests for rape per 100,000 residents).\n\n\nVisualize the data\nSee the link for the detail (in Korean)\n\nLet’s create a new column for the state name\n\n\n#change row names to column (variable)\ncrime &lt;- rownames_to_column(USArrests, var=\"state\") \nhead(crime)\n\n       state Murder Assault UrbanPop Rape\n1    Alabama   13.2     236       58 21.2\n2     Alaska   10.0     263       48 44.5\n3    Arizona    8.1     294       80 31.0\n4   Arkansas    8.8     190       50 19.5\n5 California    9.0     276       91 40.6\n6   Colorado    7.9     204       78 38.7\n\n\n\n#change the upper letter to lower character in state variable\ncrime$state &lt;- tolower(crime$state) \nhead(crime)\n\n       state Murder Assault UrbanPop Rape\n1    alabama   13.2     236       58 21.2\n2     alaska   10.0     263       48 44.5\n3    arizona    8.1     294       80 31.0\n4   arkansas    8.8     190       50 19.5\n5 california    9.0     276       91 40.6\n6   colorado    7.9     204       78 38.7\n\n\nThe states_map &lt;- map_data(\"state\") code is used to create a dataframe that contains map data for the 50 states in the United States.\nThe map_data() function is from the ggplot2 package, and it returns a dataframe that contains latitude and longitude coordinates for the boundaries of each state, along with additional information that can be used to plot the map.\nThe argument to the map_data() function is the name of the region for which to retrieve the map data. In this case, the argument is \"state\", which indicates that we want map data for the 50 states in the US.\nThe resulting states_map dataframe contains the following columns:\n\nlong: A vector of longitudes representing the boundaries of the state.\nlat: A vector of latitudes representing the boundaries of the state.\ngroup: An integer indicating the group to which each point belongs. This is used to group the points together when plotting the map.\norder: An integer indicating the order in which the points should be plotted.\nregion: A character string indicating the name of the state.\nsubregion: A character string indicating the name of a subregion within the state, if applicable. This is usually NA for the state maps.\n\n\nstates_map &lt;- map_data(\"state\")\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nThe code library(ggiraphExtra) loads the ggiraphExtra package, which extends the functionality of the ggplot2 package to allow for interactive graphics in R.\nThe ggChoropleth() function is from the ggiraphExtra package and is used to create a choropleth map in which each state is colored according to its value of a specified variable.\nThe first argument to ggChoropleth() is the data frame containing the data to be plotted, which is crime in this case.\nThe second argument is the aes() function, which is used to map variables in the data frame to visual properties of the plot. The fill aesthetic is used to specify that the color of each state should be determined by the value of the Murder variable in the crime data frame. The map_id aesthetic is used to specify that each state should be identified by its name, which is found in the state variable in the crime data frame.\nThe third argument is the map argument, which specifies the data frame containing the map data. In this case, the states_map data frame is used, which was created earlier using the map_data() function.\n\n# install.packages(\"ggiraphExtra\")\nlibrary(ggiraphExtra)\n\nggChoropleth(data=crime, aes(fill=Murder, map_id=state), map=states_map)\n\n\n\n\n\nRemove legend and change background color\nMurder rate by state\n\n\nlibrary(ggthemes)\n\n\nAttaching package: 'ggthemes'\n\n\nThe following object is masked from 'package:ggiraphExtra':\n\n    theme_clean\n\nggChoropleth(data=crime, aes(fill=Murder, map_id=state), map=states_map) + \n  theme_map() + theme(legend.position=\"right\")\n\n\n\n\n\nUrban pop by state\n\n\nggChoropleth(data=crime, aes(fill=UrbanPop, map_id=state), map=states_map) + \n  theme_map() + theme(legend.position=\"right\")\n\n\n\n\n\nAssault rate by state\n\n\nggChoropleth(data=crime, aes(fill=Assault, map_id=state), map=states_map) + \n  theme_map() + theme(legend.position=\"right\")\n\n\n\n\n\nRape rate by state\n\n\nggChoropleth(data=crime, aes(fill=Rape, map_id=state), map=states_map) + \n  theme_map() + theme(legend.position=\"right\")\n\n\n\n\nRequired R packages and functions The standard R function for k-means clustering is kmeans() [stats package], which simplified format is as follow:\nkmeans(x, centers, iter.max = 10, nstart = 1)\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\n\nZ-score Standardization for k-means clustering\n\n\ndf &lt;- scale(USArrests) # Scaling the data\nhead(df)\n\n               Murder   Assault   UrbanPop         Rape\nAlabama    1.24256408 0.7828393 -0.5209066 -0.003416473\nAlaska     0.50786248 1.1068225 -1.2117642  2.484202941\nArizona    0.07163341 1.4788032  0.9989801  1.042878388\nArkansas   0.23234938 0.2308680 -1.0735927 -0.184916602\nCalifornia 0.27826823 1.2628144  1.7589234  2.067820292\nColorado   0.02571456 0.3988593  0.8608085  1.864967207\n\nsummary(df)\n\n     Murder           Assault           UrbanPop             Rape        \n Min.   :-1.6044   Min.   :-1.5090   Min.   :-2.31714   Min.   :-1.4874  \n 1st Qu.:-0.8525   1st Qu.:-0.7411   1st Qu.:-0.76271   1st Qu.:-0.6574  \n Median :-0.1235   Median :-0.1411   Median : 0.03178   Median :-0.1209  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7949   3rd Qu.: 0.9388   3rd Qu.: 0.84354   3rd Qu.: 0.5277  \n Max.   : 2.2069   Max.   : 1.9948   Max.   : 1.75892   Max.   : 2.6444  \n\n\nTo create a beautiful graph of the clusters generated with the kmeans() function, will use the factoextra package.\n\n# install.packages(\"factoextra\")\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\nHow many clusters (k) is the best?\n\n\ng1&lt;-fviz_nbclust(df, kmeans, method = \"wss\")\ng1\n\n\n\n\n\ng2&lt;-fviz_nbclust(df, kmeans, method = \"silhouette\")\ng2\n\n\n\n\nThis code uses the fviz_nbclust() function from the factoextra package in R to determine the optimal number of clusters to use in a K-means clustering analysis.\nThe first argument of fviz_nbclust() is the data frame df that contains the variables to be used in the clustering analysis.\nThe second argument is the clustering function kmeans that specifies the algorithm to be used for clustering.\nThe third argument method specifies the method to be used to determine the optimal number of clusters. In this case, two methods are used:\n\n\"wss\": Within-cluster sum of squares. This method computes the sum of squared distances between each observation and its assigned cluster center, and then adds up these values across all clusters. The goal is to find the number of clusters that minimize the within-cluster sum of squares.\n\"silhouette\": Silhouette width. This method computes a silhouette width for each observation, which measures how similar the observation is to its own cluster compared to other clusters. The goal is to find the number of clusters that maximize the average silhouette width across all observations.\n\nLet’s run unsupervised clustering given k=4\n\n# Compute k-means with k = 4\nset.seed(123)\nkm.res &lt;- kmeans(df, 4, nstart = 25)\n\nAs the final result of k-means clustering result is sensitive to the random starting assignments, we specify nstart = 25. This means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation. The default value of nstart in R is one. But, it’s strongly recommended to compute k-means clustering with a large value of nstart such as 25 or 50, in order to have a more stable result.\n\nPrint the result\n\n\n# Print the results\nprint(km.res)\n\nK-means clustering with 4 clusters of sizes 8, 13, 16, 13\n\nCluster means:\n      Murder    Assault   UrbanPop        Rape\n1  1.4118898  0.8743346 -0.8145211  0.01927104\n2 -0.9615407 -1.1066010 -0.9301069 -0.96676331\n3 -0.4894375 -0.3826001  0.5758298 -0.26165379\n4  0.6950701  1.0394414  0.7226370  1.27693964\n\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             1              4              4              1              4 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             4              3              3              4              1 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             3              2              4              3              2 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             3              2              1              2              4 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             3              4              2              1              4 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             2              2              4              2              3 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             4              4              1              2              3 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             3              3              3              3              1 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             2              1              4              3              2 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             3              3              2              2              3 \n\nWithin cluster sum of squares by cluster:\n[1]  8.316061 11.952463 16.212213 19.922437\n (between_SS / total_SS =  71.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nThe printed output displays: the cluster means or centers: a matrix, which rows are cluster number (1 to 4) and columns are variables the clustering vector: A vector of integers (from 1:k) indicating the cluster to which each point is allocated\n\nIf you want to add the point classifications to the original data, use this:\n\nstr(km.res)\n\nList of 9\n $ cluster     : Named int [1:50] 1 4 4 1 4 4 3 3 4 1 ...\n  ..- attr(*, \"names\")= chr [1:50] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ centers     : num [1:4, 1:4] 1.412 -0.962 -0.489 0.695 0.874 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:4] \"Murder\" \"Assault\" \"UrbanPop\" \"Rape\"\n $ totss       : num 196\n $ withinss    : num [1:4] 8.32 11.95 16.21 19.92\n $ tot.withinss: num 56.4\n $ betweenss   : num 140\n $ size        : int [1:4] 8 13 16 13\n $ iter        : int 2\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\nkm.res$cluster\n\n       Alabama         Alaska        Arizona       Arkansas     California \n             1              4              4              1              4 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             4              3              3              4              1 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             3              2              4              3              2 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             3              2              1              2              4 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             3              4              2              1              4 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             2              2              4              2              3 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             4              4              1              2              3 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             3              3              3              3              1 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             2              1              4              3              2 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             3              3              2              2              3 \n\n\n\nCreate a new data frame including cluster information\n\n\ndd &lt;- data.frame(USArrests, cluster = km.res$cluster)\nhead(dd)\n\n           Murder Assault UrbanPop Rape cluster\nAlabama      13.2     236       58 21.2       1\nAlaska       10.0     263       48 44.5       4\nArizona       8.1     294       80 31.0       4\nArkansas      8.8     190       50 19.5       1\nCalifornia    9.0     276       91 40.6       4\nColorado      7.9     204       78 38.7       4\n\n\n\nCheck groups’ characteristics\n\n\ndd %&gt;% \n  group_by(cluster) %&gt;% \n  summarize_all(mean)\n\n# A tibble: 4 × 5\n  cluster Murder Assault UrbanPop  Rape\n    &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1       1  13.9    244.      53.8  21.4\n2       2   3.6     78.5     52.1  12.2\n3       3   5.66   139.      73.9  18.8\n4       4  10.8    257.      76    33.2\n\n\n\nCluster number for each of the observations\n\n\ntable(dd$cluster)\n\n\n 1  2  3  4 \n 8 13 16 13 \n\n\n\nReshape the dataset to visualize\n\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\ndd %&gt;% \n  melt(id.vars=\"cluster\") %&gt;% \n  mutate(cluster=as.factor(cluster)) %&gt;% \n  head(20)\n\n   cluster variable value\n1        1   Murder  13.2\n2        4   Murder  10.0\n3        4   Murder   8.1\n4        1   Murder   8.8\n5        4   Murder   9.0\n6        4   Murder   7.9\n7        3   Murder   3.3\n8        3   Murder   5.9\n9        4   Murder  15.4\n10       1   Murder  17.4\n11       3   Murder   5.3\n12       2   Murder   2.6\n13       4   Murder  10.4\n14       3   Murder   7.2\n15       2   Murder   2.2\n16       3   Murder   6.0\n17       2   Murder   9.7\n18       1   Murder  15.4\n19       2   Murder   2.1\n20       4   Murder  11.3\n\n\n\nCheck groups’ characteristics with ggplot\n\n\ndd %&gt;% \n  melt(id.vars=\"cluster\") %&gt;% \n  mutate(cluster=as.factor(cluster)) %&gt;% \n  ggplot(aes(x=cluster, y=value))+\n  geom_boxplot()+\n  facet_wrap(~variable, scale=\"free_y\")\n\n\n\n\n\ncluster 1: Rural area with high murder, assault\ncluster 2: Peaceful rural areas\ncluster 3: Good city with low crime\ncluster 4: City Gotham…?\n\nLet’s see clusters are well made\n\nfviz_cluster(km.res, data = df)\n\n\n\n\nLet’s play with different k (number of clusters)\n\nk2 &lt;- kmeans(df, centers = 2, nstart = 25)\nk3 &lt;- kmeans(df, centers = 3, nstart = 25)\nk4 &lt;- kmeans(df, centers = 4, nstart = 25)\nk5 &lt;- kmeans(df, centers = 5, nstart = 25)\n\n# plots to compare\np1 &lt;- fviz_cluster(k2, geom = \"point\", data = df) + ggtitle(\"k = 2\")\np2 &lt;- fviz_cluster(k3, geom = \"point\",  data = df) + ggtitle(\"k = 3\")\np3 &lt;- fviz_cluster(k4, geom = \"point\",  data = df) + ggtitle(\"k = 4\")\np4 &lt;- fviz_cluster(k5, geom = \"point\",  data = df) + ggtitle(\"k = 5\")\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/12_week.html",
    "href": "teaching/ml101/weekly/posts/12_week.html",
    "title": "Model Improvement",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\nDiscussion\nDiscussion #10\n\n&lt;p&gt;Loading…&lt;/p&gt;\n\n\n\nClass\n\nTraining data to fit diverse models using caret package\n\nRegardless of the technique or algorithm used in ML, the common process required is learning.\nThe object of learning is the training dataset, and there are numerous ways to learn from the training data.\n\nEach method has distinct principles, characteristics, and nuances, and there are various approaches. Knowing all algorithms has its limits.\n\nWhen faced with a practical problem, one must consider what to adopt and determine the appropriate parameter values to create a suitable model.\nThe caret package offers convenient functions for training data to develop predictive models.\n\nA standardized interface allows for testing around 300 machine learning algorithms.\nEasy tuning is possible by configuring different parameter scenarios and measuring variable importance.\nThrough convenient training data learning, you can receive assistance in making an informed algorithm selection decision.\n\n\n\n\ncaret package: Classification And REgression Training\n\nClassification if the dependent variable (predictor) is a nominal (categorical) variable\nRegression if it is a continuous variable\n\n\nlibrary(tidyverse) # for tidy tools (pipe operation, tibble, etc..)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\nData: Sonar: Mines Vs. Rocks\n\n\n\nThis is the data set used by Gorman and Sejnowski in their study of the classification of sonar signals using a neural network. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and a roughly cylindrical rock. Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The label associated with each record contains the letter R if the object is a rock and M if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n\n\n# install.packages(\"mlbench\")\n\nset.seed(1234) # for reproducibility\n\ndata(Sonar, package = \"mlbench\")\n\nSonar %&gt;% glimpse\n\nRows: 208\nColumns: 61\n$ V1    &lt;dbl&gt; 0.0200, 0.0453, 0.0262, 0.0100, 0.0762, 0.0286, 0.0317, 0.0519, …\n$ V2    &lt;dbl&gt; 0.0371, 0.0523, 0.0582, 0.0171, 0.0666, 0.0453, 0.0956, 0.0548, …\n$ V3    &lt;dbl&gt; 0.0428, 0.0843, 0.1099, 0.0623, 0.0481, 0.0277, 0.1321, 0.0842, …\n$ V4    &lt;dbl&gt; 0.0207, 0.0689, 0.1083, 0.0205, 0.0394, 0.0174, 0.1408, 0.0319, …\n$ V5    &lt;dbl&gt; 0.0954, 0.1183, 0.0974, 0.0205, 0.0590, 0.0384, 0.1674, 0.1158, …\n$ V6    &lt;dbl&gt; 0.0986, 0.2583, 0.2280, 0.0368, 0.0649, 0.0990, 0.1710, 0.0922, …\n$ V7    &lt;dbl&gt; 0.1539, 0.2156, 0.2431, 0.1098, 0.1209, 0.1201, 0.0731, 0.1027, …\n$ V8    &lt;dbl&gt; 0.1601, 0.3481, 0.3771, 0.1276, 0.2467, 0.1833, 0.1401, 0.0613, …\n$ V9    &lt;dbl&gt; 0.3109, 0.3337, 0.5598, 0.0598, 0.3564, 0.2105, 0.2083, 0.1465, …\n$ V10   &lt;dbl&gt; 0.2111, 0.2872, 0.6194, 0.1264, 0.4459, 0.3039, 0.3513, 0.2838, …\n$ V11   &lt;dbl&gt; 0.1609, 0.4918, 0.6333, 0.0881, 0.4152, 0.2988, 0.1786, 0.2802, …\n$ V12   &lt;dbl&gt; 0.1582, 0.6552, 0.7060, 0.1992, 0.3952, 0.4250, 0.0658, 0.3086, …\n$ V13   &lt;dbl&gt; 0.2238, 0.6919, 0.5544, 0.0184, 0.4256, 0.6343, 0.0513, 0.2657, …\n$ V14   &lt;dbl&gt; 0.0645, 0.7797, 0.5320, 0.2261, 0.4135, 0.8198, 0.3752, 0.3801, …\n$ V15   &lt;dbl&gt; 0.0660, 0.7464, 0.6479, 0.1729, 0.4528, 1.0000, 0.5419, 0.5626, …\n$ V16   &lt;dbl&gt; 0.2273, 0.9444, 0.6931, 0.2131, 0.5326, 0.9988, 0.5440, 0.4376, …\n$ V17   &lt;dbl&gt; 0.3100, 1.0000, 0.6759, 0.0693, 0.7306, 0.9508, 0.5150, 0.2617, …\n$ V18   &lt;dbl&gt; 0.2999, 0.8874, 0.7551, 0.2281, 0.6193, 0.9025, 0.4262, 0.1199, …\n$ V19   &lt;dbl&gt; 0.5078, 0.8024, 0.8929, 0.4060, 0.2032, 0.7234, 0.2024, 0.6676, …\n$ V20   &lt;dbl&gt; 0.4797, 0.7818, 0.8619, 0.3973, 0.4636, 0.5122, 0.4233, 0.9402, …\n$ V21   &lt;dbl&gt; 0.5783, 0.5212, 0.7974, 0.2741, 0.4148, 0.2074, 0.7723, 0.7832, …\n$ V22   &lt;dbl&gt; 0.5071, 0.4052, 0.6737, 0.3690, 0.4292, 0.3985, 0.9735, 0.5352, …\n$ V23   &lt;dbl&gt; 0.4328, 0.3957, 0.4293, 0.5556, 0.5730, 0.5890, 0.9390, 0.6809, …\n$ V24   &lt;dbl&gt; 0.5550, 0.3914, 0.3648, 0.4846, 0.5399, 0.2872, 0.5559, 0.9174, …\n$ V25   &lt;dbl&gt; 0.6711, 0.3250, 0.5331, 0.3140, 0.3161, 0.2043, 0.5268, 0.7613, …\n$ V26   &lt;dbl&gt; 0.6415, 0.3200, 0.2413, 0.5334, 0.2285, 0.5782, 0.6826, 0.8220, …\n$ V27   &lt;dbl&gt; 0.7104, 0.3271, 0.5070, 0.5256, 0.6995, 0.5389, 0.5713, 0.8872, …\n$ V28   &lt;dbl&gt; 0.8080, 0.2767, 0.8533, 0.2520, 1.0000, 0.3750, 0.5429, 0.6091, …\n$ V29   &lt;dbl&gt; 0.6791, 0.4423, 0.6036, 0.2090, 0.7262, 0.3411, 0.2177, 0.2967, …\n$ V30   &lt;dbl&gt; 0.3857, 0.2028, 0.8514, 0.3559, 0.4724, 0.5067, 0.2149, 0.1103, …\n$ V31   &lt;dbl&gt; 0.1307, 0.3788, 0.8512, 0.6260, 0.5103, 0.5580, 0.5811, 0.1318, …\n$ V32   &lt;dbl&gt; 0.2604, 0.2947, 0.5045, 0.7340, 0.5459, 0.4778, 0.6323, 0.0624, …\n$ V33   &lt;dbl&gt; 0.5121, 0.1984, 0.1862, 0.6120, 0.2881, 0.3299, 0.2965, 0.0990, …\n$ V34   &lt;dbl&gt; 0.7547, 0.2341, 0.2709, 0.3497, 0.0981, 0.2198, 0.1873, 0.4006, …\n$ V35   &lt;dbl&gt; 0.8537, 0.1306, 0.4232, 0.3953, 0.1951, 0.1407, 0.2969, 0.3666, …\n$ V36   &lt;dbl&gt; 0.8507, 0.4182, 0.3043, 0.3012, 0.4181, 0.2856, 0.5163, 0.1050, …\n$ V37   &lt;dbl&gt; 0.6692, 0.3835, 0.6116, 0.5408, 0.4604, 0.3807, 0.6153, 0.1915, …\n$ V38   &lt;dbl&gt; 0.6097, 0.1057, 0.6756, 0.8814, 0.3217, 0.4158, 0.4283, 0.3930, …\n$ V39   &lt;dbl&gt; 0.4943, 0.1840, 0.5375, 0.9857, 0.2828, 0.4054, 0.5479, 0.4288, …\n$ V40   &lt;dbl&gt; 0.2744, 0.1970, 0.4719, 0.9167, 0.2430, 0.3296, 0.6133, 0.2546, …\n$ V41   &lt;dbl&gt; 0.0510, 0.1674, 0.4647, 0.6121, 0.1979, 0.2707, 0.5017, 0.1151, …\n$ V42   &lt;dbl&gt; 0.2834, 0.0583, 0.2587, 0.5006, 0.2444, 0.2650, 0.2377, 0.2196, …\n$ V43   &lt;dbl&gt; 0.2825, 0.1401, 0.2129, 0.3210, 0.1847, 0.0723, 0.1957, 0.1879, …\n$ V44   &lt;dbl&gt; 0.4256, 0.1628, 0.2222, 0.3202, 0.0841, 0.1238, 0.1749, 0.1437, …\n$ V45   &lt;dbl&gt; 0.2641, 0.0621, 0.2111, 0.4295, 0.0692, 0.1192, 0.1304, 0.2146, …\n$ V46   &lt;dbl&gt; 0.1386, 0.0203, 0.0176, 0.3654, 0.0528, 0.1089, 0.0597, 0.2360, …\n$ V47   &lt;dbl&gt; 0.1051, 0.0530, 0.1348, 0.2655, 0.0357, 0.0623, 0.1124, 0.1125, …\n$ V48   &lt;dbl&gt; 0.1343, 0.0742, 0.0744, 0.1576, 0.0085, 0.0494, 0.1047, 0.0254, …\n$ V49   &lt;dbl&gt; 0.0383, 0.0409, 0.0130, 0.0681, 0.0230, 0.0264, 0.0507, 0.0285, …\n$ V50   &lt;dbl&gt; 0.0324, 0.0061, 0.0106, 0.0294, 0.0046, 0.0081, 0.0159, 0.0178, …\n$ V51   &lt;dbl&gt; 0.0232, 0.0125, 0.0033, 0.0241, 0.0156, 0.0104, 0.0195, 0.0052, …\n$ V52   &lt;dbl&gt; 0.0027, 0.0084, 0.0232, 0.0121, 0.0031, 0.0045, 0.0201, 0.0081, …\n$ V53   &lt;dbl&gt; 0.0065, 0.0089, 0.0166, 0.0036, 0.0054, 0.0014, 0.0248, 0.0120, …\n$ V54   &lt;dbl&gt; 0.0159, 0.0048, 0.0095, 0.0150, 0.0105, 0.0038, 0.0131, 0.0045, …\n$ V55   &lt;dbl&gt; 0.0072, 0.0094, 0.0180, 0.0085, 0.0110, 0.0013, 0.0070, 0.0121, …\n$ V56   &lt;dbl&gt; 0.0167, 0.0191, 0.0244, 0.0073, 0.0015, 0.0089, 0.0138, 0.0097, …\n$ V57   &lt;dbl&gt; 0.0180, 0.0140, 0.0316, 0.0050, 0.0072, 0.0057, 0.0092, 0.0085, …\n$ V58   &lt;dbl&gt; 0.0084, 0.0049, 0.0164, 0.0044, 0.0048, 0.0027, 0.0143, 0.0047, …\n$ V59   &lt;dbl&gt; 0.0090, 0.0052, 0.0095, 0.0040, 0.0107, 0.0051, 0.0036, 0.0048, …\n$ V60   &lt;dbl&gt; 0.0032, 0.0044, 0.0078, 0.0117, 0.0094, 0.0062, 0.0103, 0.0053, …\n$ Class &lt;fct&gt; R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R…\n\n\n\ntable(Sonar$Class)\n\n\n  M   R \n111  97 \n\n\nScatterplot Matrix\nA scatterplot matrix shows a grid of scatterplots where each attribute is plotted against all other attributes. It can be read by column or row, and each plot appears twice, allowing you to consider the spatial relationships from two perspectives. An improvement of just plotting the scatterplots, is to further include class information. This is commonly done by coloring dots in each scatterplot by their class value.\n\nfig &lt;- function(width, heigth){\n     options(repr.plot.width = width, \n             repr.plot.height = heigth)\n}\n\nfig(10,10)\n\nfeaturePlot(x=Sonar[,1:4], \n            y=Sonar[,61], \n            plot=\"pairs\", \n            auto.key=list(columns=2))\n\n\n\n\nFor example, in Iris dataset,\n\nfig(10,10)\nfeaturePlot(x=iris[,1:4], \n            y=iris[,5], \n            plot=\"pairs\",\n            auto.key=list(columns=3))\n\n\n\n\nDensity Plots\nDensity estimation plots (density plots for short) summarize the distribution of the data. Like a histogram, the relationship between the attribute values and number of observations is summarized, but rather than a frequency, the relationship is summarized as a continuous probability density function (PDF). This is the probability that a given observation has a given value. The density plots can further be improved by separating each attribute by their class value for the observation. This can be useful to understand the single-attribute relationship with the class values and highlight useful structures like linear separability of attribute values into classes.\n\nfig(10, 5)\nfeaturePlot(x=Sonar[,1:4], \n            y=Sonar[,61], \n            plot=\"density\", \n            scales=list(x=list(relation=\"free\"), \n                        y=list(relation=\"free\")), \n            auto.key=list(columns=2))\n\n\n\n\nFor example, in Iris dataset,\n\nfig(10, 5)\ncaret::featurePlot(x=iris[,1:4], \n            y=iris[,5], \n            plot=\"density\", \n            scales=list(x=list(relation=\"free\"), \n                        y=list(relation=\"free\")), \n            auto.key=list(columns=3))\n\n\n\n\nHold out method: (7:3 rule)\n\n# Without package\nindexTrain &lt;- sample(1:nrow(Sonar), \n                     round(nrow(Sonar) * .7))\ntraining &lt;- Sonar[ indexTrain, ]\ntesting  &lt;- Sonar[-indexTrain, ]\n\n\ntable(training$Class)\n\n\n M  R \n75 71 \n\ntable(testing$Class)\n\n\n M  R \n36 26 \n\n\nPartitioning using createDataPartition()\n\nThere is a factor, making it convenient for partitioning based on a specific ratio.\n\np = .7 means 70% training 30% test\n\nWhen using the sample() function, it performs complete random sampling, disregarding the factor ratio of the dependent variable. However, when using the createDataPartition() function, it supports stratified random sampling based on the factor ratio of the dependent variable, which is more effective.\nBy default, the returned type is a list. If the list argument value is set to FALSE, a vector is output.\n\n\nindexTrain &lt;- createDataPartition(Sonar$Class, p = .7, list = F)\ntraining &lt;- Sonar[ indexTrain, ]\ntesting  &lt;- Sonar[-indexTrain, ]\n\n\ntable(training$Class)\n\n\n M  R \n78 68 \n\ntable(testing$Class)\n\n\n M  R \n33 29 \n\n\nFor the best parameter tuning\n\nThe parameters must be set accordingly, such as k in KNN, mtry and ntre in RF\nWith caret, the “Tuning parameters” feature helps identify optimal parameters based on data using methods like LOOCV, K-fold cross-validation, and more.\nThe number of tuning parameters varies for each algorithm. For instance, in the case of the number of parameters is \\(p\\), candidate models are tested by searching a grid of \\(3^p\\).\nFor example, in KNN, which has a single parameter K, three K values (\\(3^1 = 3\\)) are used as candidates, and the models are compared.\nFor a model with two parameters, there are 9 combinations (\\(3^2 = 9\\)) of parameter values used as candidates, and the models are compared.\nIf K-fold cross-validation is chosen as the comparison method, determining the number of folds (K) is also necessary.\nThe trainControl() function helps evaluate by consistently applying a uniform comparison method to each candidate.\n\nThe following code creates a configuration that repeats 10-fold cross-validation five times to find the best candidate’s parameter grid. The fitControl object, which contains information on how to compare models, is later utilized in the learning process.\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nNow let’s train with the training data set!\nThe standardized interface for learning is the train() function: You can implement the desired learning model algorithm by changing the method argument in the function\n\nrf_fit &lt;- train(Class ~ ., \n                data = training, \n                method = \"rf\", \n                trControl = fitControl, \n                verbose = F)\nrf_fit\n\nRandom Forest \n\n146 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 131, 131, 132, 131, 132, 131, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.8025714  0.5995588\n  31    0.7763810  0.5476392\n  60    0.7608571  0.5146801\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\n\nmtry candidates are automatically set to 2, 31, and 60\nOf these, mtry = 2 was finally selected based on Kappa statistics and accuracy.\nIf you want to see the selection process in detail, set verbose = F and run\n\n\nplot(rf_fit)\n\n\n\n\nLet’s summarize various performance evaluation indicators\n\nAccuracy: Overall correctness of the model’s predictions. Measures the proportion of correct predictions.\n\\[ \\frac{TP + TN}{TP + TN + FP + FN} \\]\n\n\n\n\n\nPrecision: Proportion of true positive predictions among all positive predictions. High precision means fewer false positives.\n\\[ \\frac{TP}{TP + FP} \\]\nRecall: Proportion of true positive predictions among all actual positive instances. Also known as sensitivity or true positive rate.\n\\[ \\frac{TP}{TP + FN} \\]\n\n\n\n\n\nSensitivity is the same index with Recall\nSpecificity: Proportion of true negative predictions among all actual negative instances. Also known as true negative rate.\n\\[ \\frac{TN}{TN + FP} \\]\nFP Rate: Proportion of false positive predictions among all actual negative instances. Also known as false positive rate.\n\\[ \\frac{FP}{FP + TN} \\]\nF1 Score: Harmonic mean of precision and recall. Provides a balanced measure between the two.\n\\[ \\frac{2 \\cdot (Precision \\cdot Recall)}{Precision + Recall} \\]\nKappa: Measures the agreement between the model’s predictions and the expected outcomes, taking into account the possibility of agreement by chance.\n\\[ \\frac{Accuracy - RandomAccuracy}{1 - RandomAccuracy} \\]\n\nModel Validation\nLet’s compete fairly among the models using various indicators\n\nmodelLookup() %&gt;% head(10)\n\n         model parameter            label forReg forClass probModel\n1          ada      iter           #Trees  FALSE     TRUE      TRUE\n2          ada  maxdepth   Max Tree Depth  FALSE     TRUE      TRUE\n3          ada        nu    Learning Rate  FALSE     TRUE      TRUE\n4       AdaBag    mfinal           #Trees  FALSE     TRUE      TRUE\n5       AdaBag  maxdepth   Max Tree Depth  FALSE     TRUE      TRUE\n9     adaboost     nIter           #Trees  FALSE     TRUE      TRUE\n10    adaboost    method           Method  FALSE     TRUE      TRUE\n6  AdaBoost.M1    mfinal           #Trees  FALSE     TRUE      TRUE\n7  AdaBoost.M1  maxdepth   Max Tree Depth  FALSE     TRUE      TRUE\n8  AdaBoost.M1 coeflearn Coefficient Type  FALSE     TRUE      TRUE\n\n\n\nmodelLookup(\"rpart\")\n\n  model parameter                label forReg forClass probModel\n1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE\n\n\nLet’s train the same data fitting to four different models (DT, RF, KNN, NB)\n\ndt_fit &lt;- train(Class ~ ., \n                data = training, \n                method = \"rpart\", \n                trControl = fitControl)\n\nrf_fit &lt;- train(Class ~ ., \n                data = training, \n                method = \"rf\", \n                trControl = fitControl)\n\nknn_fit &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"knn\", \n                 trControl = fitControl)\n\nnb_fit &lt;- train(Class ~ ., \n                data = training, \n                method = \"nb\", \n                trControl = fitControl)\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 10\n\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 11\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 11\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 11\n\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 10\n\n\n\nresamp=resamples(list(DecisionTree=dt_fit,\n                      RandomForest=rf_fit, \n                      kNN=knn_fit, \n                      NaiveBayes=nb_fit))\n\ndotplot() shows Accuracy and Kappa for all models included in resamples()\n\ndotplot(resamp)\n\n\n\n\nUse predict() which is a generic function of Testing models\n\npredict(rf_fit, newdata = testing)\n\n [1] M R R R R R M M R R R R R R R M R R R R R R R R M R R R R M M M R M R M M M\n[39] M M M M M M M M M M M M M M M R M M M M M M M M\nLevels: M R\n\n\n\ntable(predict(rf_fit, newdata = testing),\n      testing$Class)\n\n   \n     M  R\n  M 30  5\n  R  3 24\n\n\nIf you add confusionMatrix() of the caret package, you can output various statistics including confusion matrix and accuracy.\n\npredict(rf_fit, newdata = testing) %&gt;% \n  confusionMatrix(testing$Class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  M  R\n         M 30  5\n         R  3 24\n                                          \n               Accuracy : 0.871           \n                 95% CI : (0.7615, 0.9426)\n    No Information Rate : 0.5323          \n    P-Value [Acc &gt; NIR] : 1.5e-08         \n                                          \n                  Kappa : 0.7398          \n                                          \n Mcnemar's Test P-Value : 0.7237          \n                                          \n            Sensitivity : 0.9091          \n            Specificity : 0.8276          \n         Pos Pred Value : 0.8571          \n         Neg Pred Value : 0.8889          \n             Prevalence : 0.5323          \n         Detection Rate : 0.4839          \n   Detection Prevalence : 0.5645          \n      Balanced Accuracy : 0.8683          \n                                          \n       'Positive' Class : M               \n                                          \n\n\nCustom search grid: Grid adjustment of tuning parameters\n\nWhen selecting the optimal parameters, the search range and grid can be manually adjusted\nThe candidates for mtry, which are automatically determined by the 3P formula: 2, 31, and 60\nIf you want to compare with more candidates, you set the candidates yourself\nThe code below changes the mtry candidates to 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10 and sets them up.\n\n\ncustomGrid &lt;- expand.grid(mtry = 1:10)\n\nrf_fit2 &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"rf\", \n                 trControl = fitControl, \n                 tuneGrid = customGrid, \n                 verbose = F)\n\nrf_fit2\n\nRandom Forest \n\n146 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 131, 132, 131, 132, 131, 131, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   1    0.7942784  0.5805762\n   2    0.8014505  0.5967626\n   3    0.8041978  0.6031289\n   4    0.8038022  0.6023856\n   5    0.7965348  0.5875659\n   6    0.8005201  0.5949173\n   7    0.8029963  0.5991999\n   8    0.8018681  0.5975455\n   9    0.7996117  0.5931190\n  10    0.7993114  0.5929248\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 3.\n\n\nGeneric plot() function shows the Accuracy change depending on mtry grid\n\nplot(rf_fit2)\n\n\n\n\nRandom Search Grid: random selection of tuning parameter combinations\n\nAs the number of tuning parameters increases, the number of search grids increases exponentially, and the search process may become inefficient due to the grid configuration with equal intervals\nLet’s train through RDA (Regularized Discriminant Analysis) with two tuning parameters\n\n\nrda_fit &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"rda\", \n                 trControl = fitControl, \n                 verbose = F)\n\nrda_fit\n\nRegularized Discriminant Analysis \n\n146 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 131, 132, 131, 132, 132, 131, ... \nResampling results across tuning parameters:\n\n  gamma  lambda  Accuracy   Kappa     \n  0.0    0.0     0.5604762  0.07758973\n  0.0    0.5     0.7583810  0.51039188\n  0.0    1.0     0.6845714  0.35894233\n  0.5    0.0     0.7892381  0.56724332\n  0.5    0.5     0.7835238  0.55523021\n  0.5    1.0     0.7415238  0.47753750\n  1.0    0.0     0.6467619  0.28747059\n  1.0    0.5     0.6508571  0.29649203\n  1.0    1.0     0.6607619  0.31605195\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were gamma = 0.5 and lambda = 0.\n\n\n\nYou can see a total of 9 parameter combinations being compared\nThe user search grid introduced just above can also be searched for equally spaced grids using the expand.grid() function, but this time, let’s configure a parameter combination that is not equally spaced using a random search grid.\nsearch = \"random\" in the trainControl() function, change the search type to random.\n\n\nplot(rda_fit)\n\n\n\n\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", \n                           number = 10, \n                           repeats = 5, \n                           search = \"random\")\n\n\nrda_fit2 &lt;- train(Class ~ ., data = training, \n                  method = \"rda\", \n                  trControl = fitControl, \n                  verbose = F)\nrda_fit2\n\nRegularized Discriminant Analysis \n\n146 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 131, 131, 132, 131, 132, 131, ... \nResampling results across tuning parameters:\n\n  gamma      lambda     Accuracy   Kappa    \n  0.1028932  0.7653744  0.7679267  0.5290507\n  0.2431458  0.1114396  0.7786886  0.5494464\n  0.5760511  0.1579374  0.7845128  0.5593490\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were gamma = 0.5760511 and lambda\n = 0.1579374.\n\n\n\nThere are two tuning parameters, gamma and lambda, but the moment you change the search type to random, you can see that the candidate group is not set with the 3P formula.\nTo manually increase the number of tuning parameter combinations, use the tuneLength argument of the train() function.\n\n\nrda_fit2 &lt;- train(Class ~ ., \n                  data = training, \n                  method = \"rda\", \n                  trControl = fitControl, \n                  tuneLength = 50, \n                  verbose = F)\nrda_fit2\n\nRegularized Discriminant Analysis \n\n146 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 132, 131, 131, 132, 131, 131, ... \nResampling results across tuning parameters:\n\n  gamma         lambda      Accuracy   Kappa    \n  0.0007485631  0.74191082  0.7589084  0.5144843\n  0.0414835601  0.01287166  0.7601758  0.5158588\n  0.0783394738  0.52302887  0.7627179  0.5177754\n  0.0981407929  0.34136666  0.7629084  0.5187975\n  0.1215299605  0.93627195  0.7394799  0.4679774\n  0.1290835007  0.63893257  0.7604176  0.5107336\n  0.1291675451  0.37643517  0.7682418  0.5286955\n  0.1332780691  0.60682541  0.7659560  0.5224079\n  0.1338704182  0.74118555  0.7476703  0.4841626\n  0.1602785466  0.97047875  0.7409084  0.4712820\n  0.1618139972  0.20392341  0.7891136  0.5719495\n  0.1777139497  0.42049942  0.7629084  0.5168299\n  0.1794137117  0.58848017  0.7699560  0.5298930\n  0.1810493851  0.02484183  0.7931136  0.5803734\n  0.2031416909  0.18328472  0.7928938  0.5790880\n  0.2145218640  0.93273889  0.7490989  0.4860594\n  0.2352795987  0.96551055  0.7435751  0.4758013\n  0.2794060404  0.48901564  0.7685128  0.5254606\n  0.3231490329  0.99972542  0.7489084  0.4872199\n  0.3873158591  0.12334491  0.7833700  0.5562390\n  0.3908691760  0.19369314  0.7863223  0.5619009\n  0.3929732225  0.53632580  0.7654505  0.5172339\n  0.4569480976  0.22765241  0.7836557  0.5550322\n  0.4665695983  0.46639755  0.7724029  0.5308909\n  0.4692936756  0.78518915  0.7545275  0.4967273\n  0.4850399913  0.50985965  0.7695458  0.5249073\n  0.5070618326  0.96435422  0.7541465  0.4979233\n  0.5164007600  0.14748781  0.7810842  0.5493820\n  0.5246395844  0.43766934  0.7764029  0.5389038\n  0.6293819949  0.65953016  0.7559267  0.4989787\n  0.6405648568  0.36828794  0.7764982  0.5381764\n  0.6644365741  0.40821422  0.7710696  0.5275948\n  0.6681491192  0.75535534  0.7518608  0.4919304\n  0.6803096321  0.66944697  0.7518315  0.4908615\n  0.7028437410  0.76722496  0.7503370  0.4883800\n  0.7059634458  0.47948041  0.7710696  0.5283761\n  0.7216346122  0.79049471  0.7544322  0.4969646\n  0.7362087339  0.85754167  0.7490989  0.4878771\n  0.7754983467  0.03475408  0.7658315  0.5161669\n  0.8180513082  0.68751588  0.7495604  0.4872603\n  0.8333342015  0.05593821  0.7646886  0.5140673\n  0.8594429099  0.86730980  0.7274652  0.4452287\n  0.8617005984  0.50849207  0.7491648  0.4837251\n  0.8719446701  0.77311713  0.7345275  0.4579433\n  0.8930286658  0.75939288  0.7207985  0.4307518\n  0.8987579306  0.63752126  0.7274505  0.4421286\n  0.9058322918  0.19435659  0.7534505  0.4908708\n  0.9212931057  0.34698021  0.7396410  0.4637153\n  0.9602838892  0.64811097  0.6875458  0.3651033\n  0.9788019210  0.24244529  0.6807692  0.3499658\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were gamma = 0.1810494 and lambda\n = 0.02484183.\n\n\n\nBy randomly setting 50 parameters, we consider the optimal parameter tuning method that is slightly more flexible.\nIn comparison, it can be seen that the value of the adopted parameter has more decimal points and has naturally become more precise.\n\n\nplot(rda_fit2)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/14_week.html",
    "href": "teaching/ml101/weekly/posts/14_week.html",
    "title": "QZ #2",
    "section": "",
    "text": "summary(my_data)\n\n    Class               Score      \n Length:47          Min.   :15.00  \n Class :character   1st Qu.:24.00  \n Mode  :character   Median :33.00  \n                    Mean   :33.53  \n                    3rd Qu.:43.00  \n                    Max.   :49.00  \n\n\n\nMax score: 49 / 50\nMin score: 15 / 50\nAVG score: 33.53 / 50\n\nHistogram\n\nggplot(my_data, aes(x = Score)) + \n  geom_histogram(binwidth = 3, color = \"white\", fill = \"#56B4E9\", alpha = 0.8) +\n  # Add vertical line for mean\n  geom_vline(aes(xintercept = mean(Score)), \n             color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_text(aes(x = 33.53, y = 6.5, label = \"AVG: 33.53 / 50\")) +\n  # Add x and y axis labels\n  labs(x = \"Score\", y = \"Frequency\", \n       title = \"Score distribution\") +\n  # Set a simple theme\n  theme_minimal()\n\n\n\n\nBox-plot by classes\n\n# Calculate average scores for each class\nclass_averages &lt;- my_data %&gt;%\n  group_by(Class) %&gt;%\n  summarize(avg_score = mean(Score))\n\nggplot(my_data, aes(x = Class, y = Score, fill = Class)) +\n  geom_boxplot(alpha = 0.8, color = \"black\", size = 1) +\n  # Add individual data points with jitter\n  geom_jitter(alpha = 0.2) +\n  # Add text labels for average values\n  geom_text(data = class_averages, \n            aes(x = Class, y = 15, \n                label = paste0(\"Group AVG: \", round(avg_score, 1))), \n            color = \"black\", \n            size = 4, \n            fontface = \"bold\") +\n  # Remove legend\n  guides(fill = FALSE) +\n  # Choose color palette\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  # Add x and y axis labels\n  labs(x = \"Class\", y = \"Score\") +\n  # Set a simple theme\n  theme_minimal()"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/14_week.html#score-distribution",
    "href": "teaching/ml101/weekly/posts/14_week.html#score-distribution",
    "title": "QZ #2",
    "section": "",
    "text": "summary(my_data)\n\n    Class               Score      \n Length:47          Min.   :15.00  \n Class :character   1st Qu.:24.00  \n Mode  :character   Median :33.00  \n                    Mean   :33.53  \n                    3rd Qu.:43.00  \n                    Max.   :49.00  \n\n\n\nMax score: 49 / 50\nMin score: 15 / 50\nAVG score: 33.53 / 50\n\nHistogram\n\nggplot(my_data, aes(x = Score)) + \n  geom_histogram(binwidth = 3, color = \"white\", fill = \"#56B4E9\", alpha = 0.8) +\n  # Add vertical line for mean\n  geom_vline(aes(xintercept = mean(Score)), \n             color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_text(aes(x = 33.53, y = 6.5, label = \"AVG: 33.53 / 50\")) +\n  # Add x and y axis labels\n  labs(x = \"Score\", y = \"Frequency\", \n       title = \"Score distribution\") +\n  # Set a simple theme\n  theme_minimal()\n\n\n\n\nBox-plot by classes\n\n# Calculate average scores for each class\nclass_averages &lt;- my_data %&gt;%\n  group_by(Class) %&gt;%\n  summarize(avg_score = mean(Score))\n\nggplot(my_data, aes(x = Class, y = Score, fill = Class)) +\n  geom_boxplot(alpha = 0.8, color = \"black\", size = 1) +\n  # Add individual data points with jitter\n  geom_jitter(alpha = 0.2) +\n  # Add text labels for average values\n  geom_text(data = class_averages, \n            aes(x = Class, y = 15, \n                label = paste0(\"Group AVG: \", round(avg_score, 1))), \n            color = \"black\", \n            size = 4, \n            fontface = \"bold\") +\n  # Remove legend\n  guides(fill = FALSE) +\n  # Choose color palette\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  # Add x and y axis labels\n  labs(x = \"Class\", y = \"Score\") +\n  # Set a simple theme\n  theme_minimal()"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/14_week.html#date-and-location",
    "href": "teaching/ml101/weekly/posts/14_week.html#date-and-location",
    "title": "QZ #2",
    "section": "Date and Location",
    "text": "Date and Location\n\nDate: 31 May (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/14_week.html#notice",
    "href": "teaching/ml101/weekly/posts/14_week.html#notice",
    "title": "QZ #2",
    "section": "Notice",
    "text": "Notice\n\nQuiz will be administered through Google Forms.\nPlease bring your laptop for the quiz.\nYou are allowed to access any information through the Internet\nHowever, communication with others is strictly prohibited.\nDo not use any messaging apps (e.g., KakaoTalk, TikTok, Line, WeChat, etc.) during the quiz.\nUpon completion of the quiz, you are required to submit your code."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/14_week.html#qz",
    "href": "teaching/ml101/weekly/posts/14_week.html#qz",
    "title": "QZ #2",
    "section": "QZ",
    "text": "QZ\n\nPART I. Linear Regression\nWe will be utilizing the USArrests data, which was previously discussed in class, for our regression analysis. Omitting the data description as it has already been covered in class.\nLet’s import USArrests data (Use the code below)\n\ndata(\"USArrests\")\nsummary(USArrests)\n\n     Murder          Assault         UrbanPop          Rape      \n Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30  \n 1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50   1st Qu.:15.07  \n Median : 7.250   Median :159.0   Median :66.00   Median :20.10  \n Mean   : 7.788   Mean   :170.8   Mean   :65.54   Mean   :21.23  \n 3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75   3rd Qu.:26.18  \n Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00  \n\n\n\nHow many observations in the dataset?\n\n50\n\nHow many variables in the dataset?\n\n4\n\n\nCrimes like Assault, Murder, and Rape are more likely to happen in cities. Linear regression analysis was performed to find out which variable among Assault, Murder, and Rape was the most Urban-population dependent variable. (See the code below)\n\nm1 &lt;- lm(Assault ~ UrbanPop, data=USArrests)\nm2 &lt;- lm(Murder ~ UrbanPop, data=USArrests)\nm3 &lt;- lm(Rape ~ UrbanPop, data=USArrests)\n\nsummary(m1)\n\n\nCall:\nlm(formula = Assault ~ UrbanPop, data = USArrests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-150.78  -61.85  -18.68   58.05  196.85 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  73.0766    53.8508   1.357   0.1811  \nUrbanPop      1.4904     0.8027   1.857   0.0695 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 81.33 on 48 degrees of freedom\nMultiple R-squared:  0.06701,   Adjusted R-squared:  0.04758 \nF-statistic: 3.448 on 1 and 48 DF,  p-value: 0.06948\n\nsummary(m2)\n\n\nCall:\nlm(formula = Murder ~ UrbanPop, data = USArrests)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.537 -3.736 -0.779  3.332  9.728 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  6.41594    2.90669   2.207   0.0321 *\nUrbanPop     0.02093    0.04333   0.483   0.6312  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.39 on 48 degrees of freedom\nMultiple R-squared:  0.00484,   Adjusted R-squared:  -0.01589 \nF-statistic: 0.2335 on 1 and 48 DF,  p-value: 0.6312\n\nsummary(m3)\n\n\nCall:\nlm(formula = Rape ~ UrbanPop, data = USArrests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.644  -5.476  -1.216   5.885  27.937 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  3.78707    5.71128   0.663    0.510   \nUrbanPop     0.26617    0.08513   3.127    0.003 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.626 on 48 degrees of freedom\nMultiple R-squared:  0.1692,    Adjusted R-squared:  0.1519 \nF-statistic: 9.776 on 1 and 48 DF,  p-value: 0.003001\n\n\n\nWhich dependent variable has the most significant relationship with the urban population?\n\nRape\n\nChoose a model in which only the intercept is a statistically significant coefficient.\n\nm2\n\nAccording to the result from summary, choose the model that fits the most.\n\nm3\n\n\nThis time, I thought that Murder was influenced by Assault, Rape, and UrbanPop, so I performed the following regression analysis. (Multiple regression, see the code below).\n\nm4 &lt;- lm(Murder ~ Assault+Rape+UrbanPop, data=USArrests)\nsummary(m4)\n\n\nCall:\nlm(formula = Murder ~ Assault + Rape + UrbanPop, data = USArrests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3990 -1.9127 -0.3444  1.2557  7.4279 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.276639   1.737997   1.885   0.0657 .  \nAssault      0.039777   0.005912   6.729 2.33e-08 ***\nRape         0.061399   0.055740   1.102   0.2764    \nUrbanPop    -0.054694   0.027880  -1.962   0.0559 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.574 on 46 degrees of freedom\nMultiple R-squared:  0.6721,    Adjusted R-squared:  0.6507 \nF-statistic: 31.42 on 3 and 46 DF,  p-value: 3.322e-11\n\n\n\nWhich variable has a statistically significant effect on Y (dependent variable) among X (independent variables)?\n\nAssault\n\nUse m4 (above model) to predict the Murder of a new state. New state is Assault=100, Rape=20, UrbanPop=60. Choose the correct predicted Murder of the new state.\n\nnew_state &lt;- data.frame(Assault=100, Rape=20, UrbanPop=60)\n\npredict(m4, newdata = new_state)\n\n       1 \n5.200726 \n\n\n\n5.20\n\n\n\n\nPART II. Non-linear Regression\nThe code below creates a factor-type variable ‘Murder_high’ that is 1 when Murder is greater than 10 (Zero if not), and stores it in USArrests_new.\n\nUSArrests %&gt;% \n  mutate(Murder_high=as.factor(ifelse(Murder &gt; 10, 1, 0))) -&gt; USArrests_new\n\nsummary(USArrests_new)\n\n     Murder          Assault         UrbanPop          Rape       Murder_high\n Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30   0:35       \n 1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50   1st Qu.:15.07   1:15       \n Median : 7.250   Median :159.0   Median :66.00   Median :20.10              \n Mean   : 7.788   Mean   :170.8   Mean   :65.54   Mean   :21.23              \n 3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75   3rd Qu.:26.18              \n Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00              \n\n\n\nHow many observations have Murder_high equal to 1?\n\n15\n\n\nFollowing code is about fitting the new data to Logit model by using ‘glm’ function. See the result and answer the questions below.\n\nm5 &lt;- glm(Murder_high~Assault+Rape+UrbanPop, \n          data=USArrests_new, \n          family='binomial')\nsummary(m5)\n\n\nCall:\nglm(formula = Murder_high ~ Assault + Rape + UrbanPop, family = \"binomial\", \n    data = USArrests_new)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.65955    2.88481  -1.962 0.049781 *  \nAssault      0.03682    0.01094   3.366 0.000762 ***\nRape        -0.04290    0.05858  -0.732 0.463976    \nUrbanPop    -0.02252    0.03647  -0.618 0.536865    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.086  on 49  degrees of freedom\nResidual deviance: 29.252  on 46  degrees of freedom\nAIC: 37.252\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nChoose the significant X variables related to Murder_high.\n\nAssault\n\n\nDefine 3 new states as shown in the code below, and when the m5 model predicts the probability that Murder_high is 1 (use the type=‘response’ option),\n\nnew_state_1 &lt;- data.frame(Assault=100, Rape=70, UrbanPop=60)\nnew_state_2 &lt;- data.frame(Assault=200, Rape=20, UrbanPop=30)\nnew_state_3 &lt;- data.frame(Assault=250, Rape=0, UrbanPop=10)\n\n\nChoose all states with a predicted probability of being Murder_high is 1 equals 0.5 or greater. (Murder_high가 1일 확률이 0.5보다 큰 것을 고르시오).\n\npredict(m5, newdata = new_state_1, type = \"response\")\n\n          1 \n0.001775415 \n\npredict(m5, newdata = new_state_2, type = \"response\")\n\n        1 \n0.5425262 \n\npredict(m5, newdata = new_state_3, type = \"response\")\n\n        1 \n0.9651047 \n\n\n\nnew_state_2\nnew_state_3\n\nAssault’s coefficient in the ‘m5’ model is the log odds ratio. Choose Assault’s Odds ratio.\n\nexp(m5$coefficients)\n\n(Intercept)     Assault        Rape    UrbanPop \n0.003484092 1.037504921 0.958010049 0.977727163 \n\n\n\n1.038\n\n\n\n\nPART III. Clustering\nLet’s use iris dataset. First thing we need to do for Clustering is the code below.\n\ndf &lt;- scale(iris[-5])\nsummary(df)\n\n  Sepal.Length       Sepal.Width       Petal.Length      Petal.Width     \n Min.   :-1.86378   Min.   :-2.4258   Min.   :-1.5623   Min.   :-1.4422  \n 1st Qu.:-0.89767   1st Qu.:-0.5904   1st Qu.:-1.2225   1st Qu.:-1.1799  \n Median :-0.05233   Median :-0.1315   Median : 0.3354   Median : 0.1321  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.67225   3rd Qu.: 0.5567   3rd Qu.: 0.7602   3rd Qu.: 0.7880  \n Max.   : 2.48370   Max.   : 3.0805   Max.   : 1.7799   Max.   : 1.7064  \n\n\n\nChoose the best explanation the reason why we use ‘scale’ before clustering.\n\nto minimize the bias caused by different units\n\n\nThis is the second step for the k-means clustering.\n\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_nbclust(df, kmeans, method = \"wss\")\n\n\n\n\n\nSee the result, and choose the incorrect explanation of this step.\n\nIn this step, we can get a recommendation about the number of clusters ‘k’\nk is bigger the better (Incorrect)\nThe appropriate k is 3, but 2 or 4 is also Ok.\nTotal Within Sum of Square is the smallest at k = 10\n\n\nThe code below is k-means clustering with k=3. Then, I created ‘iris_cluster’ by merging the original iris dataset and the clustering result. See the result of the table(iris_cluster$Species, iris_cluster$cluster), and answer the questions.\n\n# Compute k-means with k = 3\nset.seed(123)\nkm.res &lt;- kmeans(df, 3, nstart = 25)\n\niris_cluster &lt;- data.frame(iris, cluster = km.res$cluster)\n\ntable(iris_cluster$Species, iris_cluster$cluster)\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 39 11\n  virginica   0 14 36\n\n\n\nAs a result of clustering, which species are best seperated?\n\nSetosa\n\n\n\n\nPART IV. Apriori\nThe code below is about Apriori algorithm for items {A, B, C, D, E, ...} to find association patterns.  \n\n# Apriori\n\nitemList&lt;-c(\"A, B, C\", \n            \"A, C\",\n            \"B, D\",\n            \"D, E, A\",\n            \"B, F\",\n            \"E, F\", \n            \"A, F\",\n            \"C, E, F\",\n            \"A, B, E\", \n            \"B, E, F, A, C\",\n            \"E, F, G, H, D\",\n            \"A, B, C\")\nwrite.csv(itemList,\"ItemList.csv\", quote = FALSE, row.names = TRUE)\n\nlibrary(arules)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'arules'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\nlibrary(arulesViz)\n\ntxn = read.transactions(file=\"ItemList.csv\", \n                        rm.duplicates= TRUE, \n                        format=\"basket\",sep=\",\",cols=1);\n\nbasket_rules &lt;- apriori(txn, \n                        parameter = list(minlen=2, \n                                         sup = 0.2, \n                                         conf = 0.1, \n                                         target=\"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.1    0.1    1 none FALSE            TRUE       5     0.2      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 2 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[9 item(s), 13 transaction(s)] done [0.00s].\nsorting and recoding items ... [6 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [13 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nsummary(basket_rules)\n\nset of 13 rules\n\nrule length distribution (lhs + rhs):sizes\n 2  3 \n10  3 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.000   2.000   2.231   2.000   3.000 \n\nsummary of quality measures:\n    support         confidence        coverage           lift       \n Min.   :0.2308   Min.   :0.4286   Min.   :0.2308   Min.   :0.9286  \n 1st Qu.:0.2308   1st Qu.:0.5714   1st Qu.:0.3846   1st Qu.:1.2381  \n Median :0.2308   Median :0.6667   Median :0.4615   Median :1.4444  \n Mean   :0.2663   Mean   :0.6516   Mean   :0.4260   Mean   :1.4020  \n 3rd Qu.:0.3077   3rd Qu.:0.7500   3rd Qu.:0.4615   3rd Qu.:1.4857  \n Max.   :0.3077   Max.   :1.0000   Max.   :0.5385   Max.   :1.9500  \n     count      \n Min.   :3.000  \n 1st Qu.:3.000  \n Median :3.000  \n Mean   :3.462  \n 3rd Qu.:4.000  \n Max.   :4.000  \n\nmining info:\n data ntransactions support confidence\n  txn            13     0.2        0.1\n                                                                                       call\n apriori(data = txn, parameter = list(minlen = 2, sup = 0.2, conf = 0.1, target = \"rules\"))\n\ninspect(basket_rules)\n\n     lhs       rhs support   confidence coverage  lift      count\n[1]  {C}    =&gt; {B} 0.2307692 0.6000000  0.3846154 1.3000000 3    \n[2]  {B}    =&gt; {C} 0.2307692 0.5000000  0.4615385 1.3000000 3    \n[3]  {C}    =&gt; {A} 0.3076923 0.8000000  0.3846154 1.4857143 4    \n[4]  {A}    =&gt; {C} 0.3076923 0.5714286  0.5384615 1.4857143 4    \n[5]  {B}    =&gt; {A} 0.3076923 0.6666667  0.4615385 1.2380952 4    \n[6]  {A}    =&gt; {B} 0.3076923 0.5714286  0.5384615 1.2380952 4    \n[7]  {F}    =&gt; {E} 0.3076923 0.6666667  0.4615385 1.4444444 4    \n[8]  {E}    =&gt; {F} 0.3076923 0.6666667  0.4615385 1.4444444 4    \n[9]  {A}    =&gt; {E} 0.2307692 0.4285714  0.5384615 0.9285714 3    \n[10] {E}    =&gt; {A} 0.2307692 0.5000000  0.4615385 0.9285714 3    \n[11] {B, C} =&gt; {A} 0.2307692 1.0000000  0.2307692 1.8571429 3    \n[12] {A, C} =&gt; {B} 0.2307692 0.7500000  0.3076923 1.6250000 3    \n[13] {A, B} =&gt; {C} 0.2307692 0.7500000  0.3076923 1.9500000 3    \n\n\n\nHow many rules in basket_rules?\n\n13\n\nChoose the incorrect explanations for the result above\n\nThe highest lift rule is {A,B} =&gt; {C}\nThe highest confidence rule is {B,C} =&gt; {A}\nThe minimum value of the support is 0.2308 (round to 5 decimal places)\nItem A and E are highly associated each other\nlift is the only index we consider to find a good pattern (Incorrect)\n\nTo increase the sales of item ‘F’, which items should be attached and sold?\n\nE\n\n\n\n\nPART V. Model Comparison and Validation\n\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nUSArrests_new %&gt;%\n  select(-Murder) -&gt; US\n\nindexTrain &lt;- createDataPartition(US$Murder_high, p = .9, list = F)\ntraining &lt;- US[ indexTrain, ]\ntesting  &lt;- US[-indexTrain, ]\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\ndt_fit &lt;- train(Murder_high ~ ., data = training, method = \"rpart\", trControl = fitControl)\nrf_fit &lt;- train(Murder_high ~ ., data = training, method = \"rf\", trControl = fitControl)\n\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n\nknn_fit &lt;- train(Murder_high ~ ., data = training, method = \"knn\", trControl = fitControl)\nnb_fit &lt;- train(Murder_high ~ ., data = training, method = \"nb\", trControl = fitControl)\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 5\n\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 4\n\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 2\n\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 4\n\nWarning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\nobservation 4\n\nresamp=resamples(list(DecisionTree=dt_fit,\n                      RandomForest=rf_fit,\n                      kNN=knn_fit,\n                      NaiveBayes=nb_fit))\nsummary(resamp)\n\n\nCall:\nsummary.resamples(object = resamp)\n\nModels: DecisionTree, RandomForest, kNN, NaiveBayes \nNumber of resamples: 50 \n\nAccuracy \n             Min. 1st Qu.    Median      Mean 3rd Qu. Max. NA's\nDecisionTree 0.50    0.75 0.9166667 0.8733333       1    1    0\nRandomForest 0.25    0.75 0.8333333 0.8523333       1    1    0\nkNN          0.75    0.80 1.0000000 0.9043333       1    1    0\nNaiveBayes   0.50    0.75 1.0000000 0.8916667       1    1    0\n\nKappa \n                   Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's\nDecisionTree  0.0000000 0.5000000 0.8333333 0.7063866       1    1    0\nRandomForest -0.5000000 0.5000000 0.6410256 0.6246370       1    1    0\nkNN           0.0000000 0.5454545 1.0000000 0.7784515       1    1    0\nNaiveBayes   -0.3333333 0.5000000 1.0000000 0.7329787       1    1    0\n\ndotplot(resamp)\n\n\n\n\n\nChoose the incorrect explanation about the code above.\n\nThe createDataPartition function is used to divide the dataset into training and testing dataset.\nThe ratio of training and test data is 7:3. (9:1)\nThe Murder_high ratio in the train and test sets remains almost the same.\n\nChoose all incorrect explanations of the code above.\n\nTo calculate the accuracy and kappa, the repeated cross validation method is used\n“repeatedcv” divides the training dataset into 5 sections, and validates the model 10 times\nThe validation is repeated 5 times overall.\nUse test dataset for validation.\nFour models are compared in terms of accuracy and kappa index\n\nChoose the best model according to the graph above\n\nDecision Tree in the QZ (the result is different with the one above)\n\nThe simplest model, decision tree, performed much better than the most complex model, such as random forest. Which of the following is appropriate for that reason?\n\nThe number of observations is too small to fit to the complicated model like random forest.\nThis is because the proportion of Y (the dependent variable) = 1 was too small.\nIt’s just a coincidence.\n\n\nThe following code is to predict with dt_fit, the best model, using testing dataset. Fill (1) and (2).\n\npredict(dt_fit, testing) %&gt;% \n  confusionMatrix(testing$Murder_high)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 0 1\n         0 1 0\n         1 2 1\n                                          \n               Accuracy : 0.5             \n                 95% CI : (0.0676, 0.9324)\n    No Information Rate : 0.75            \n    P-Value [Acc &gt; NIR] : 0.9492          \n                                          \n                  Kappa : 0.2             \n                                          \n Mcnemar's Test P-Value : 0.4795          \n                                          \n            Sensitivity : 0.3333          \n            Specificity : 1.0000          \n         Pos Pred Value : 1.0000          \n         Neg Pred Value : 0.3333          \n             Prevalence : 0.7500          \n         Detection Rate : 0.2500          \n   Detection Prevalence : 0.2500          \n      Balanced Accuracy : 0.6667          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "teaching/network/index.html",
    "href": "teaching/network/index.html",
    "title": "Network Analysis in Social Science",
    "section": "",
    "text": "Introduction\nSocial scientists are increasingly turning to network analysis as a way to gain insights into the structure and dynamics of social systems. Network analysis allows researchers to examine how social actors are connected to one another, how information and resources flow through social networks, and how social networks shape individual behavior and collective outcomes. In this course, students will learn the fundamental concepts and techniques of network analysis and explore how they can be applied to social science research using the R programming language.\nHands-on practice with R will be an integral part of the course. Students will use the R programming language to manipulate, visualize, and analyze network data. R packages such as igraph, statnet, vizNetwork, and networkD3 will be used for network visualization, analysis, and modeling. Students will be provided with R code and examples to practice the concepts covered in class. Assignments and projects will require students to apply their knowledge of network analysis to real-world social science problems using R.\nThis course is designed for graduate students and researchers in social sciences who want to expand their research methods and explore new avenues for analysis using R. Prior experience with R is not required, but students should be comfortable with basic statistical analysis and have some familiarity with programming concepts. By the end of this course, students will have a solid understanding of network analysis in social science research and the ability to apply it using R. They will be able to use their new skills to explore social systems and gain insights into how social networks shape individual behavior and collective outcomes.\n\n\n\nSyllabus\nWeek 1: Introduction to network analysis in social science\n\nOverview of social network theory and concepts\nIntroduction to network data collection and analysis\nBasic network measures and visualization techniques using R\n\nWeek 2: Network data collection and preparation in R\n\nSurvey and interview techniques for network data collection\nData cleaning and preparation for network analysis in R\nEthical considerations in network data collection\n\nWeek 3: Network visualization and exploration in R\n\nNetwork visualization and layout techniques using R\nNetwork exploration and analysis using R software tools\nInteractive network visualization for exploration and presentation in R\n\nWeek 4: Measures of centrality and power in networks using R\n\nDegree centrality, betweenness centrality, and closeness centrality in R\nEigenvalue centrality and PageRank algorithm in R\nHubs and authorities and other measures of power in R\n\nWeek 5: Network clustering and community detection in R\n\nClustering algorithms and techniques for detecting communities in R\nModularity optimization and other community detection measures in R\nVisualization of network clusters and communities in R\n\nWeek 6: Network dynamics and change over time in R\n\nModels for network growth and evolution in R\nNetwork diffusion models and spread of influence in R\nLongitudinal network analysis and visualization in R\n\nWeek 7: Multiplex networks and multilevel analysis in R\n\nMultiplex networks and their analysis in R\nMultilevel network analysis and its applications in R\nNetwork-based models for social systems in R\n\nWeek 8: Network models for social contagion and influence in R\n\nDiffusion models and the spread of information and behavior in R\nContagion models and epidemics in social networks in R\nModels for social influence and persuasion in R\n\nWeek 9: Network models for social support and health in R\n\nSocial support and its measurement in network analysis in R\nSocial network analysis of health and illness in R\nNetwork models for health interventions and prevention in R\n\nWeek 10: Political networks and power relations in R\n\nNetwork analysis of power and influence in politics in R\nPolitical alliances and coalitions in networks in R\nNetwork models for predicting elections and voting behavior in R\n\nWeek 11: Economic networks and market dynamics in R\n\nNetwork analysis of economic systems and markets in R\nSocial networks and their influence on economic outcomes in R\nModels for network-based entrepreneurship and innovation in R\n\nWeek 12: Cultural networks and artistic production in R\n\nCultural networks and their analysis in R\nSocial networks in the creative industries in R\nNetwork models for artistic collaboration and production in R\n\nWeek 13: Social network interventions and applications in R\n\nNetwork-based interventions in social systems in R\nNetwork approaches to community building and development in R\nNetwork analysis and social policy in R\n\nWeek 14: Advanced network analysis and future directions in R\n\nAdvanced topics in network analysis in R\nFuture directions and trends in network analysis research in R\nStudent project presentations and feedback in R\n\nWeek 15: Final project and wrap-up\n\nStudents will work on a final project applying network analysis techniques to a social science research question using R.\nThe instructor will provide guidance and feedback to the students throughout the project.\nStudents will present their final project to the class and receive feedback from their peers.\nThe final class will be a wrap-up and future directions in network analysis research using R."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "CJ’s Blogs",
    "section": "",
    "text": "Technical reports, research, conference talks, and thoughts.\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n헬스커뮤니케이션2024\n\n\nGreen Exposure and Mental Health\n\n\n\n\n\n\n\n\n15 June 2024\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n학생들이 주는 상의 가치\n\n\nBest Professor Award\n\n\n컬텍전공 2024 BP\n\n\n\n\n\n06 March 2024\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n사람에서 장소로\n\n\n우리 모두가 공유하는 회복탄력성 레시피\n\n\n성대한특강 2023\n\n\n\n\n\n23 October 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n데이터 사이언스, 인공지능, 그리고 진로\n\n\n그래서 나 뭘해야 할까?\n\n\n2023 건국대학교 데이터사이언스 특강\n\n\n\n\n\n07 October 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n메타버스, 게임화, 선택 모형\n\n\n메타버스 서비스의 다양한 게임화 속성에 대한 소비자 선호와 선택\n\n\n2023 한국미디어패널 학술대회\n\n\n\n\n\n22 September 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n디지털전환 스타트업 생태계\n\n\n투자유치 주요 요인들에 대한 탐색적 분석\n\n\n2023 정보통신정책학회 글로벌 워크샵\n\n\n\n\n\n23 June 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nOTT Movie Launching Strategy\n\n\nwith Movie-Country Relatedness\n\n\n2023 한국언론학회 봄철 정기학술대회 미디어 경제·경영 연구회\n\n\n\n\n\n19 May 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n메타버스 속 멀티 페르소나 경향이 태도, 몰입, 지속사용의도에 끼치는 영향\n\n\n2023 봄철 3학회 공동학술대회\n\n\n미디어경영학회, 정보사회학회, 사이버커뮤니케이션학회\n\n\n\n\n\n12 May 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining in Academic Research\n\n\nMethods, Applications, and Challenges\n\n\nInvited Talk @서강대학교 메타버스전문대학원\n\n\n\n\n\n09 May 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Modeling in R\n\n\n\n\n\nUnveiling Hidden Structures in Text Data\n\n\n\n\n\n06 May 2023\n\n\nChangjun Lee\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nMake Your Graph Interactive! Transform ggplot Graphs into Interactive Visualizations with Plotly\n\n\n살아 숨쉬는 그래프 그리기\n\n\n\n\n\n\n\n\n12 April 2023\n\n\nChangjun Lee\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the AdaBoost\n\n\n오답노트의 힘\n\n\n\n\n\n\n\n\n29 March 2023\n\n\nChangjun Lee\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Confusion Matrix and ROC Curve in R\n\n\n혼동이 찾아오는 혼돈행렬\n\n\n\n\n\n\n\n\n21 March 2023\n\n\nChangjun Lee\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are Regular Expressions and How to Use Them in R\n\n\n외계어가 아니에요!\n\n\n\n\n\n\n\n\n08 March 2023\n\n\nChangjun Lee\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Package in R\n\n\nStand on the shoulders of giants\n\n\n\n\n\n\n\n\n08 March 2023\n\n\nChangjun Lee\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto, the game changer\n\n\n날개 달고 날아가는 디지털 퍼블리싱\n\n\n누구나 퍼블리싱 하는 시대가 온다\n\n\n\n\n\n22 February 2023\n\n\nChangjun Lee\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Changjun LEE",
    "section": "",
    "text": "Sungkyunkwan University (SKKU)\nCollege of Computing and Informatics\n\nSchool of Convergence.\nDep. of Culture & Technology (Head)\n\nGraduate Schools (I affiliated in):\n\nDep. of Interaction Science\nDep. of Immersive Media Engineering\n(Metaverse Graduate School)\n\n✉ changjunlee@skku.edu\n\n\n\nBio\n\nAs a computational social scientist, I bring a unique interdisciplinary perspective to the fields of economics, innovation studies, and convergence technologies. With a background in natural sciences, including a bachelor’s degree in biology and chemistry, I went on to earn a Ph.D. in technology management, economics, and policy. My research focuses on utilizing computational methods to tackle a wide range of social phenomena, including technology evolution & regional growth, knowledge management, and technology & convergence innovation. I am passionate about using technology and data to drive innovation and solve real-world problems.\n\n\nResearch #Innovation #Media #Culture&Tech #Public_Policy\nTeaching #DataScience #Computational_Social_Science #Culture&Tech\n\n\n\n\nEmployment\n\nSungkyunkwan University (SKKU), Seoul, Republic of Korea\nCollege of Computing and Informatics\nSchool of Convergence\nDep. of Culture & Technology (Head)\n\nAssociate Professor (2023 ~ present)\n\nHanyang University (ERICA), Ansan, Republic of Korea\nDepartment of Media & Social Informatics\n\nAssociate Professor (2022 ~ 2023)\nAssistant Professor (2019 ~ 2022)\n\nUniversity College Dublin, Republic of Ireland\nSpatial Dynamics Lab\n\nSenior Postdoctoral Research Fellow (2017 ~ 2019)\n\nNational University of Singapore, Singapore\nLee Kuan Yew School of Public Policy\n\nPostdoctoral Research Associate (2015 ~ 2017)\n\n\n\n\n\nEducation\n\nPh.D. in Economics (Technology Management)\n\nSeoul National University, Seoul, Republic of Korea (2010 - 2015)\nTechnology Management, Economics, and Policy Program\nDissertation: Research on the Growth Mechanism of Platform Ecosystem\n\nB.Sc. in Biology & Chemistry\n\nYonsei University, Seoul, Republic of Korea (2004 - 2010)\n\n\n\n\n\nTeachings\nGo to Course page\n\nCulture & Technology: 2024-S, 2023-F\nData-Science101: 2024-S, 2022-F, 2021-F, 2020-F/S, 2019-F\nMachine Learning 101: 2024-F, 2023-S, 2022-S, 2021-S, 2020-F\nDatabase101: 2022-S, 2021-S, 2020-S\nBasic Math & Statistics for Data Science: 2021-S\n(Grad) Media & Data-Science: 2023-S, 2022-S, 2021-S\n(Grad) Media User Data Analysis: 2022-F\n(Grad) Statistics for Interaction Science: 2024-F\n(Grad) Imersive Media Culture & Technology: 2024-F\nCapstone Design: 2021-S\n\n\n\n\nInternal and External Positions\n\nEditors\n\n한국방송학보 편집위원 (2023~현재)\n한국언론학보 편집위원 (2023~현재)\n한국혁신학회지 부편집위원장 (2021~현재)\n정보사회와미디어 편집위원 (2023~현재)\n정보통신정책학회지 편집위원 (2022~현재)\n\nAssociations\n\n한국언론학회 (The Korean Society for Journalism & communication Studies)\n\n총무이사 (2025)\n연구이사 (2023)\n\n한국방송학회 (Korean Association for Broadcasting & Telecommunication Studies)\n\n연구이사 (2022-2024)\n\n미디어경영학회 (Korea Media Management Association)\n\n총무이사 (2023-2024)\n연구이사 (2022)\n\n사이버커뮤니케이션학회 (Cyber Communication Academic Society)\n\n총무이사 (2022)\n기획이사 (2023-2024)\n\n한국IT서비스학회\n\n운영이사 (2024)\n\n\n자문 위원 활동\n\n정보통신정책연구원 (2022 ~ 현재)\n한국과학기술기획평가원 (2023 ~ 현재)\n한국생산기술연구원 (2021 ~ 2023)\n\n\n\n\n\nAchievements\n\n2023. 성균관대학교 글로벌융합학부 컬처앤테크놀로지 Best Professor.\n2022. 한국미디어경영학회 우수 논문 학술상.\n2022. 한양대학교 우수업적 특별 승진.\n2021. 한양대학교 에리카 국제연구부문 학술상 (신진교원상).\n2021. IC-PBL 루키 부문 우수상 수상.\n\n\n\n\nMedia\n\n한국방송학회, ‘대한민국 온라인 콘텐츠 산업 진단’ 기획 세미나 개최\n온라인 플랫폼 규제…“사회적 논의 반영할 수 있는 방안에 대해 고민해야”\n한국언론학회 정기학술대회, 네이버 특별세션\n[이데일리] 빅테크, M&A로 엔터산업 공략…국내기업 글로벌 확장 지원 필요\n[성대의 성대한 특강] 사람에서 장소로: 우리 모두가 공유하는 회복탄력성 레시피\n[매일경제] 인공지능 시대, 언론은 살아남을 수 있을까\n[현장]언론의 생성AI 도입, 기회일까 위기일까(블루오션테크미디어)\n21세기 우리가 알아야 할 필수 과목, 정보통신(The Science Times)\n“韓클라우드 시장, 美中 공룡기업 놀이터로” (매일경제)\n[매일경제] 이창준 교수, 클라우드 보안인증제도(CSAP) 완화에 대해 코멘트 (NewsH)\n[신문 읽어주는 교수님] 늦은 밤 넷플릭스 시청이 수면에 미치는 영향 (NewsH)\n수면 루틴 깨는 OTT…잠자리 26분 늦어지고 수면시간 30분 줄어 (디지털데일리)\n넷플릭스 유튜브 보느라 얼마나 늦게 잠드나요? (미디어오늘)\n서울대학교 기술경영경제정책 대학원 졸업생 멘토링 세미나\n에퀴즈 온 더 블럭 (사랑한대)\n한양대 슬기로운 새내기 생활(교수님편)\nERICA 방송국 VOH와 취중젠담 1편\nERICA 방송국 VOH와 취중젠담 2편\n알고리즘, 누구냐 넌! (한대신문)\n너란 SNS 광고 (한대신문)\n\n\n\n\nGraduate Alumni\n\n김종화(Ph.D. Student @University of Georgia)\n박지은(연구원 @미디어미래연구소)\n\n\n\n\nPublications\n\nORCID: https://orcid.org/0000-0002-8859-9796\n국가연구자번호: 11291477\n\n\nPublication keywordsCo-author network\n\n\n\n\n\n\n\n\n\n\n\n\nInternational SSCI or SCIE\n\nLee, C., & Ji, S. W. (2024). Strategies for launching streaming content: Assessing movie-country relatedness and its impact on international popularity. Plos one, 19(6), e0305433. https://doi.org/10.1371/journal.pone.0305433\nChoi Y.W. & Lee, C.* (2024). Time-of-Day and Day-of-Week Effects on TV and OTT Media Choices: Evidence from South Korea. Journal of Theoretical and Applied Electronic Commerce Research. 19(1), 1-19. https://doi.org/10.3390/jtaer19010001\nLee, C., Na, C. & Kim, K. (2023). The effect of watching OTT late at night on the sleep pattern of users. Sleep Biol. Rhythms, 21, 395–407. https://doi.org/10.1007/s41105-023-00459-z\nKim, J. & Lee, C.* (2023). The Return of the King: The Importance of Killer Content in a Competitive OTT Market. Journal of Theoretical and Applied Electronic Commerce Research. 18(2), 976-994. https://doi.org/10.3390/jtaer18020050\nNa, C. H., Lee, C. & Kim, E. D.* (2023). The Optimal Open Innovation Strategy with Science-based Partners for Venture Firm’s Innovation Capabilities: Focusing on Innovation Modes. Science, Technology and Society, 28(2), 235-256. https://doi.org/10.1177/09717218231160442\nLiu, H. S., Lee, C., Kim, K., Lee, J., Moon, A., Lee, D., & Park, M. (2023). An Analysis of Factors Influencing the Intention to Use” Untact” Services by Service Type. Sustainability, 15(4), 2870. https://www.mdpi.com/2071-1050/15/4/2870\nShim, D., Lee, C., & Oh, I. (2022). Analysis of OTT Users’ Watching Behavior for Identifying a Profitable Niche: Latent Class Regression Approach. Journal of Theoretical and Applied Electronic Commerce Research, 17(4), 1564-1580. https://www.mdpi.com/0718-1876/17/4/79\nKim, K., Kogler, D. F., Lee, C., & Kang, T.* (2022). Changes in regional knowledge bases and its effect on local labour markets in the midst of transition: Evidence from France over 1985–2015. Applied Spatial Analysis and Policy, 1-22. https://doi.org/10.1007/s12061-022-09444-4\nShon, M., Lee, D. & Lee, C.(2022). Inward or Outward? Direction of Knowledge Flow and Firm Efficiency. International Journal of Technology Management. 90(1-2), 102-121. https://doi.org/10.1504/IJTM.2022.124617\nPark, I., Lee, J., Lee, D., Lee, C., & Chung, W. Y.* (2022). Changes in consumption patterns during the COVID-19 pandemic: Analyzing the revenge spending motivations of different emotional groups. Journal of Retailing and Consumer Services, 65, 102874. https://doi.org/10.1016/j.jretconser.2021.102874\nLee, C., Shin, H., Kim, K., & Kogler, D. F.* (2022). The effects of regional capacity in knowledge recombination on production efficiency. Technological Forecasting and Social Change. 180, 121669. https://doi.org/10.1016/j.techfore.2022.121669\nJung, E., Lee, C.*, & Hwang, J. (2022). Effective strategies to attract crowdfunding investment based on the novelty of business ideas. Technological Forecasting and Social Change. 178, 121558. https://doi.org/10.1016/j.techfore.2022.121558\nJeon, H., & Lee, C.* (2022). Internet of Things Technology: Balancing Privacy Concerns with Convenience. Telematics and Informatics. 70, 101816. https://doi.org/10.1016/j.tele.2022.101816\nKogler, D. F., Davies, R. B., Lee, C., & Kim, K.* (2022). Regional knowledge spaces: the interplay of entry-relatedness and entry-potential for technological change and growth. The Journal of Technology Transfer. 1-24. https://doi.org/10.1007/s10961-022-09924-2\nTóth, G., Elekes, Z., Whittle, A., Lee, C.*, & Kogler, D. F. (2022). Technology network structure conditions the economic resilience of regions. Economic Geography. 98(4), 1-24. https://doi.org/10.1080/00130095.2022.2035715\nJo, H., Park, S., Shin, D., Shin, J.*, & Lee, C. (2022). Estimating cost of fighting against fake news during catastrophic situations. Telematics and Informatics. 66, 101734. https://doi.org/10.1016/j.tele.2021.101734\nPark, I., Shim, H., Kim, J., Lee, C., & Lee, D.* (2022). The Effects of Popularity Metrics in News Comments on the Formation of Public Opinion: Evidence from an Internet Portal Site. The Social Science Journal. doi:https://doi.org/10.1080/03623319.2020.1768485\nKim, K., Lee, J., & Lee, C.(2022). Which innovation type is better for production efficiency? A comparison between product/service, process, organizational, and marketing innovations using stochastic frontier and meta-frontier analysis. Technology Analysis & Strategic Management. doi: https://doi.org/10.1080/09537325.2021.1965979\nRocchetta, S., Mina, A., Lee, C., & Kogler, F. D. (2022). Technological Knowledge Space and the Resilience of European Regions. Journal of Economic Geography. 22(1), 27-51.* doi: https://doi.org/10.1093/jeg/lbab001\nLee, C., Cho, H., & Lee, D.* (2021). The mechanism of innovation spill-over across sub-layers in the ICT industry. Asian Journal of Technology Innovation. 29(2), 159-179. doi:https://doi.org/10.1080/19761597.2020.1796725\nJung, S., Kim, K. & Lee, C.(2021). The nature of ICT in technology convergence: A knowledge-based network analysis. PLOS ONE. 16(7): e0254424. https://doi.org/10.1371/journal.pone.0254424\nNa, C., Lee, D., Hwang, J., & Lee, C.* (2021). Strategic Groups Emerged by Selecting R&D Collaboration Partners and Firms’ Efficiency. Asian Journal of Technology Innovation. 29(1), 109-133. doi:https://doi.org/10.1080/19761597.2020.1788957\nLee, C., Lee, D., & Shon, M.* (2020). Effect of efficient triple-helix collaboration on organizations based on their stage of growth. Journal of Engineering and Technology Management. 58, 101604. https://doi.org/10.1016/j.jengtecman.2020.101604\nLee, C., Kogler, D.F., & Lee, D.* (2019). Capturing Information on Technology Convergence, International Collaboration, and Knowledge Flow from Patent Document: A Case of Information and Communication Technology. Information Processing & Management. 56(4), 1576-1591. doi:1016/j.ipm.2018.09.007\nKim, E.H.W.* & Lee, C. (2019). Does Working Long Hours Cause Marital Dissolution? Evidence from the Reduction in South Korea’s Workweek Standard. Asian Population Studies. 15(1), 87-104. doi:1080/17441730.2019.1565131\nKim E.H.W.*, Lee, C., & Do, Y.K. (2019). The Effect of Adult Children’s Working Times on Visiting with Elderly Parents: A Natural Experiment in Korea. Population Research and Policy Review. 38(1), 53-72. doi:1007/s11113-018-9486-0\n\nFeatured in: Straits Times, Lianhe Zaobao (In Chinese)\n\nLee, C.* & Hwang, J. (2018). The Influence of Giant Platform on Content Diversity. Technological Forecasting and Social Change. 136, 157-165. doi:1016/j.techfore.2016.11.029\nKim, E.H.W.*, Lee, C., & Do Y.K. (2018). The Effect of a Reduced Statutory Workweek on Familial Long-Term Care in Korea. Journal of Aging and Health. 30(10), 1620-1641. doi:1177/0898264318797469\n\nPoster Session Winner, 2018 Population Association of America Conference\n\nLee, C. & Kim, H.* (2018). The Evolutionary Trajectory of ICT Ecosystem: A Network Analysis based on Media User Data. Information & Management. 55(6), 795-805. doi:1016/j.im.2018.03.008\n\nBest Paper Award, 2015 Korea Media Panel Conference\n\nLee, C., Shin, J., & Hong, A.* (2018). Does Social Media Use Really Make People Politically Polarized? Direct and Indirect Effects of Social Media Use on Political Polarization. Telematics and Informatics. 35(1), 245-254. doi:1016/j.tele.2017.11.005\nNa, H.S., Lee, D., Hwang, J., & Lee, C.(2018). Research on the Mutual Relations between ISP and ASP Efficiency Changes for the Sustainable Growth of Internet Industry. Applied Economics. 50(11), 1238-1253. doi:1080/00036846.2017.1358443\nLee, C., Kim, J.H., & Lee, D.* (2017). Intra-industry Innovation, Spillovers, and Industry Evolution: Evidence from the Korean ICT industry. Telematics and Informatics. 34(8), 1503-1513. doi:1016/j.tele.2017.06.013\nLee, C., Jung, S., & Kim, K.O.* (2017). Effect of a Policy Intervention on Handset Subsidies on the Intention to Change Handsets and Households’ Expenses in Mobile Telecommunications. Telematics and Informatics. 34(8), 1524-1531. doi:1016/j.tele.2017.06.017\nLee, C., Kim, H., & Hong, A.* (2017). Ex-post Evaluation of Illegalizing Juvenile Online Game after Midnight: A Case of Shutdown Policy in South Korea. Telematics and Informatics. 34(8), 1597-1606. doi:1016/j.tele.2017.07.006\n\nCited in Nature Editorial http://www.nature.com/news/put-cult-online-games-to-the-test-1.22343\n\nLee, C., Lee, K., & Lee, D.* (2017). Mobile Healthcare Applications and Gamification for Sustained Health Maintenance. 9(5), 772. doi:10.3390/su9050772\nLee, C., Lee, D.*, & Hwang, J. (2015). Platform Openness and the Productivity of Content Providers: A meta-frontier analysis. Telecommunications Policy. 39(7). 553-562. doi:1016/j.telpol.2014.06.010\n\n\n\n\nKorean Citation Index (KCI) Journals\n\nPark, J., Lee, S., & Lee, C.* (2023) Effect of psychological factors and household composition of seniors on active SNS use : Focusing on self-esteem, need for cognition and household composition. Information Society & Media. 24(3), 1-35. https://doi.org/10.52558/ISM.2023.12.24.3.1\nLee, S., Choi, Y., Park, Y., & Lee, C.* (2023) The Effects of Metaverse Users Social and Psychological Characteristics on Avatar Customization. Journal of Cybercommunication Academic Society. 40(2), 1-52. https://doi.org/10.36494/JCAS.2023.06.40.2.5\nChoi, M. & Lee, C.* (2022). The effect of Online Community Activities in Non-face-to-face Situations on Life Satisfaction : Focusing on the Comparison between Before(2017) and After(2021) COVID-19. Information Society & Media. 23(3), 83-124. https://doi.org/52558/ISM.2022.12.23.3.83\nYeom, J., Lee, S., & Lee, C.* (2022). Analysis of the Characteristics of Extreme Patriotism and Psychological Motives by Understanding the Cyber Conflict Between Chinese Fandom Patriotism and Hallyu Fandom. Journal of Cybercommunication Academic Society. 39(4). 5-49. https://doi.org/10.36494/JCAS.2022.12.39.4.5\nKang, S., Seo, Y., & Lee, C.* (2022). A Study of the Development of Sectoral Digital Transformation Conflict Indicator. Information Society & Media. 23(1), 41-68. https://doi.org/52558/ISM.2022.04.23.1.41\n\nBest Paper Award, 2022 Korea Media Management Association\n\nKim, K., Lee, J., & Lee, C.* (2021). Exploratory Analysis of Knowledge Structure and Evolutionary Trajectory in Korean Artificial Intelligence for Effective Technology Policy. Korean Innovation Study, 16(3). DOI:https://doi.org/10.46251/INNOS.2021.8.16.3.139\nJo, H., Oh, M., Shin, J.*, & Lee, C. (2021). Identifying Fake news in the Disastrous situations: Video versus Text. Journal of Cybercommunication Academic Society, 38(2).  DOI: https://doi.org/10.36494/JCAS.2021.06.38.2.83\nRoh, T., Jo, G., & Lee, C.* (2021). Coincidence Analysis with International Patent Classification (IPC) Network of US ICT Companies: Focusing on Technology Similarity and Application. Korean Innovation Study, 16(2).  doi:https://doi.org/10.46251/INNOS.2021.5.16.2.237\nLee, C., Woo, H.J., & Park, S.B. (2020). Factors of newly entering into the purchase on TV home-shopping: Random Forest Analysis based on users’ media repertoire data. Korean Innovation Study, 15(2), 113-149. doi:https://doi.org/10.46251/INNOS.2020.05.15.2.113\nKim, M.K., Lee, C., & Hong, A. (2016). The Analysis of Media Usage Pattern for Effective Diffusion of Information. Information Society & Media. 17(1). 77-113. doi:https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE06667546\n\n\n\n\nBook chapters\n\nLee, C. & Bae, Y. (2019). Development of Information Communication Technology Industry and Public Policy in South Korea, In Ahn, M.J., & Kim, Y. (Eds.), Public Administration and Public Policy in Korea. Springer. Doi: https://doi.org/10.1007/978-3-319-31816-5_3801-1"
  },
  {
    "objectID": "blogs/posts/14_skku_special.html",
    "href": "blogs/posts/14_skku_special.html",
    "title": "사람에서 장소로",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Courses\"&gt;Courses&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:All Courses\"&gt;All Courses&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/index.html\"&gt;/teaching/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:CNT101\"&gt;CNT101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/cul_tech_101/index.html\"&gt;/teaching/cul_tech_101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:CNT102\"&gt;CNT102&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/cul_tech/index.html\"&gt;/teaching/cul_tech/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:DS101\"&gt;DS101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/ds101/index.html\"&gt;/teaching/ds101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:ML101\"&gt;ML101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/ml101/index.html\"&gt;/teaching/ml101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Research\"&gt;Research&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Publications\"&gt;Publications&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/research/index.html\"&gt;/research/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Work in progress\"&gt;Work in progress&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/research/working.html\"&gt;/research/working.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Proj\"&gt;Proj&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/proj/index.html\"&gt;/proj/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Blogs\"&gt;Blogs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/blogs/index.html\"&gt;/blogs/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:LAB\"&gt;LAB&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://ctcl.netlify.app\"&gt;https://ctcl.netlify.app&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;CJL &amp; Lab - 사람에서 장소로&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;CJL &amp; Lab - 사람에서 장소로&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;CJL &amp; Lab - 사람에서 장소로&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;성대한특강 2023&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;성대한특강 2023&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &lt;ul class=\"footer-items list-unstyled\"&gt;\n    &lt;li class=\"nav-item compact\"&gt;\n    &lt;a class=\"nav-link\" href=\"https://github.com/ChangjunChrisLee/\"&gt;\n      &lt;i \n  class=\"bi bi-github\" \n  role=\"img\" \n&gt;\n&lt;/i&gt; \n    &lt;/a&gt;\n  &lt;/li&gt;  \n    &lt;li class=\"nav-item compact\"&gt;\n    &lt;a class=\"nav-link\" href=\"https://twitter.com/Dr_CJLee\"&gt;\n      &lt;i \n  class=\"bi bi-twitter\" \n  role=\"img\" \n&gt;\n&lt;/i&gt; \n    &lt;/a&gt;\n  &lt;/li&gt;  \n&lt;/ul&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "research/published/kci_2023_ism_park_lee_lee.html",
    "href": "research/published/kci_2023_ism_park_lee_lee.html",
    "title": "시니어의 심리적 요인과 가구구성형태가 적극적인 SNS 이용에 미치는 영향",
    "section": "",
    "text": "Abstract\n제4차산업혁명시대, 우버ㆍ카풀에서 시작하여 카카오, 타다 사건으로 절정을 맞았던 도시여객 운수업 갈등은 완전히 해소되지는 않았다. 그간 갈등을 다룬 공공행정 관련 기존 연구에서는 그 양상을 증폭되어 해소되어 가는 선형적인 틀 내에서 이해했으며 정성적 사례 분석이 주류를 이뤘다. 설문 기반 정량적 실증 연구들도 대개 개인이나 조직 내 갈등을 다뤘고, 나아가 디지털 전환 관련 조직 간 갈등을 분석한 연구는 없었다. 본 연구는 산업별 디지털 전환 갈등에 주목하고 갈등 상황의 변화를 측정하기 위한 방법으로 빈도, 강도, 강도 집중도 지표의 개발을 제안한다. 그리고 한국의 6대 산업을 대상으로 2003년 12월~2019년 6월까지 보도된 뉴스 본문을 수집하여 이 지표들을 측정하고 내ㆍ외적 타당성을 검증했다. 분석 결과 갈등 상황은 강도 수준의 변화를 볼 때 세부 단계가 반복, 순환하면서 비선형적으로 진화해갔다. 산업별 디지털 전환 갈등은 단순히 해소되는 대상이 아니라 관리해야 할 대상이다. 본 연구는 기존 연구의 연장선에서 산업별 디지털 전환 갈등을 빈도, 강도, 강도의 집중도만으로 측정했음에도 그 양상의 변화를 식별할 수 있는 가능성을 열었다는 점에서 기여가 있다."
  },
  {
    "objectID": "research/published/2024_Choi_Lee_JTAECR.html",
    "href": "research/published/2024_Choi_Lee_JTAECR.html",
    "title": "Time-of-Day and Day-of-Week Effects on TV and OTT Media Choices",
    "section": "",
    "text": "Abstract\nThe objective of this manuscript is to investigate the determinants influencing the selection of over-the-top (OTT) platforms as opposed to traditional television mediums—cable, Internet protocol television (IPTV), and satellite broadcasting—for the consumption of content such as television shows and films. Employing data extracted from the 2020 Media Panel comprising 423,851 observations garnered from personal media diaries, this study scrutinizes the impacts of individual attributes, environmental conditions, and temporal factors on platform choice. The findings reveal a temporal influence characterized by a “Friday effect” and a heightened preference for OTT platforms during early afternoon (12:00–16:00) and late-night hours (00:00–04:00). Notably, the likelihood of selecting OTT platforms is significantly augmented during the late-night period in comparison to other time frames. In relation to individual characteristics, variables such as male gender, younger age, higher educational attainment, and elevated income levels were positively correlated with a predilection for OTT platforms. Additionally, environmental variables such as possession of an unlimited data plan and ownership of a tablet personal computer also emerged as significant predictors for OTT preference. Furthermore, the presence of a beam projector during late-night hours and residing in a household with multiple occupants during afternoon hours also served as contributing factors for OTT utilization. In conclusion, the study offers critical insights for stakeholders in both traditional television and burgeoning OTT markets, providing data-driven recommendations for the strategic allocation of resources in consideration of day-of-week and time-of-day variables."
  },
  {
    "objectID": "proj/completed/kobaco_1.html",
    "href": "proj/completed/kobaco_1.html",
    "title": "국내 시청기록 분석을 통한 미디어다양성 진단연구",
    "section": "",
    "text": "(최종목표) 방송시청기록 데이터 등을 활용하여 국내 방송미디어의 다양성을 진단하고, 미디어다양성과 관련한 제도 개선방안을 도출함\n(과업1) 방송시청기록을 포함한 각종 미디어데이터를 분석하여 국내 미디어 다양성의 현황 진단\n\n방송시청기록 및 기타 미디어다양성관련 지표 분석을 통한 미디어다양성 변화의 시계열적 진단\n방송산업의 기술적‧정책적‧산업적‧사회적 환경변화를 분석하여 방송시청행위 및 미디어다양성에 영향을 미치는 영향요인을 연역적으로 추론\n시청점유율조사, N-스크린 이용행태 조사, 미디어다양성조사 등 미디어다양성 증진 관련 조사사업의 누적데이터의 활용성 제고 방안 마련\n\n(과업2) 과업1의 진단 결과를 바탕으로 미디어다양성 증진 관련 현행 제도의 개선방안을 도출\n\n미디어 다양성 정책 변화 양상 진단 및 정책 방향성 제언\n미디어 다양성 규제정책 및 거버넌스의 개선 필요성과 당위성 검토\n미디어 다양성 관련 교육, 미디어 이용기록 조사 및 데이터 활용 등 미디어 다양성 증진을 위한 세부 정책사업안 발굴"
  },
  {
    "objectID": "proj/completed/kobaco_1.html#과업-목적-및-범위",
    "href": "proj/completed/kobaco_1.html#과업-목적-및-범위",
    "title": "국내 시청기록 분석을 통한 미디어다양성 진단연구",
    "section": "",
    "text": "(최종목표) 방송시청기록 데이터 등을 활용하여 국내 방송미디어의 다양성을 진단하고, 미디어다양성과 관련한 제도 개선방안을 도출함\n(과업1) 방송시청기록을 포함한 각종 미디어데이터를 분석하여 국내 미디어 다양성의 현황 진단\n\n방송시청기록 및 기타 미디어다양성관련 지표 분석을 통한 미디어다양성 변화의 시계열적 진단\n방송산업의 기술적‧정책적‧산업적‧사회적 환경변화를 분석하여 방송시청행위 및 미디어다양성에 영향을 미치는 영향요인을 연역적으로 추론\n시청점유율조사, N-스크린 이용행태 조사, 미디어다양성조사 등 미디어다양성 증진 관련 조사사업의 누적데이터의 활용성 제고 방안 마련\n\n(과업2) 과업1의 진단 결과를 바탕으로 미디어다양성 증진 관련 현행 제도의 개선방안을 도출\n\n미디어 다양성 정책 변화 양상 진단 및 정책 방향성 제언\n미디어 다양성 규제정책 및 거버넌스의 개선 필요성과 당위성 검토\n미디어 다양성 관련 교육, 미디어 이용기록 조사 및 데이터 활용 등 미디어 다양성 증진을 위한 세부 정책사업안 발굴"
  },
  {
    "objectID": "proj/completed/soscial_idx.html",
    "href": "proj/completed/soscial_idx.html",
    "title": "디지털전환에 따른 사회부문 국가발전지표 진단연구",
    "section": "",
    "text": "사이버커뮤니케이션학회\nhttp://www.cybercom.or.kr/\n\n\n\n\n\n\n빠르게 진화화는 디지털기술이 사회부문에 갖는 영향들은 계속해서 변화하고 있고, 그 변화들도 다양하게 확장되고 있기 때문에, 관련 통계지표들을 기술과 사회변화에 맞게 수정, 보완하고, 새롭게 등장하는 현상들을 이해분석하기 위한 새로운 통계지표들도 도출해야 할 필요성이 있음\n이를 위해서는 디지털전환으로 인한 사회변화를 분석할 수 있는 통계를 작성하는 프레임워크 구축이 필요함\n현재 한국에서 생산되는 디지털전환과 사회변화 관련 통계는 KOSIS 국가통계포털, ICT통계포털 등에서 제공하고 있는 ‘정보화통계조사’, ‘디지털정보격차실태조사’, ‘웹접근성실태조사’, ‘언론수용자 의식조사’, ‘국민여가활동조사’, ‘한국미디어패널조사’ 등이 있음.\n각 통계들은 그동안 한국이 정보사회로 접어들게 되면서 경험한 다양한 현상들을 포착하는 데 유용하게 활용되었으며, 대표적인 통계지표로는 인터넷 이용률, 스마트폰 보급률, 디지털 격차 지수, 전자상거래 거래액, 모바일 결제 이용률, 소셜미디어 이용률, 클라우드 서비스 이용률, 사이버 보안 사고 발생 건수 등이 있음.\n\n\n하지만 이 통계에 있어서 다음과 같은 한계들이 있음.\n\n\n먼저 디지털기술환경은 지속적으로 발전하며 새로운 기술과 플랫폼이 계속해서 등장하며, 이러한 변화는 디지털전환과 사회변화 관련된 통계를 최신 상태로 유지하고 관련성을 유지하는 것이 어렵게 만들고 있음.\n통계는 지속성이 중요한데, 디지털기전환과 사회에 대한 통계들은 시간이 조금만 지나면 의미가 줄어드는 통계들이 많아짐\n또한 새로운 디지털기술은 새로운 사회의 변화를 가져오는 데, 이때 새로운 통계지표들이 필요하기 때문에 기존의 통계들을 과감하게 그리고 지속적으로 수정보완해야 하는 특성이 있음.\n한편 디지털기술은 워낙 다양하며, 그것이 미치는 사회적 영향 역시 가시적이지 않은 것들도 있기 때문에 디지털기술이 미치는 사회적 영향 관련된 통계는 상관 관계를 보여줄 수는 있지만, 인과관계를 확립하는 것은 어려울 수 있음\n관찰된 결과에 기여하는 다른 기저 요인이 있을 수 있어 디지털기술의 정확한 영향을 결정하기 어려울 수 있음\n이에 인과관계까지 찾아낼 수 있는 지표들을 도출하여 그 프레임워크를 활용하는 것이 대단히 중요함.\n또한 디지털전환이 사회에 미치는 영향을 세부적으로 구분하여 자세하게 살펴보는 것이 필요함.\n디지털전환이 노동과 소득 등 경제활동에 미치는 영향, 신뢰, 사회자본 등 사회관계에 미치는 영향, 행정서비스활용 등 공공서비스활동에 미치는 영향, 행복도, 우울증, 건강, 복지 등 삶의 질에 미치는 영향 들을 모두 고려하여 이를 세부적으로 분석하면서도, 서로 연관될 수 있도록 하는 통계지표들과 그 프레임워크가 필요함."
  },
  {
    "objectID": "proj/completed/soscial_idx.html#연구수행주체",
    "href": "proj/completed/soscial_idx.html#연구수행주체",
    "title": "디지털전환에 따른 사회부문 국가발전지표 진단연구",
    "section": "",
    "text": "사이버커뮤니케이션학회\nhttp://www.cybercom.or.kr/"
  },
  {
    "objectID": "proj/completed/soscial_idx.html#개요",
    "href": "proj/completed/soscial_idx.html#개요",
    "title": "디지털전환에 따른 사회부문 국가발전지표 진단연구",
    "section": "",
    "text": "빠르게 진화화는 디지털기술이 사회부문에 갖는 영향들은 계속해서 변화하고 있고, 그 변화들도 다양하게 확장되고 있기 때문에, 관련 통계지표들을 기술과 사회변화에 맞게 수정, 보완하고, 새롭게 등장하는 현상들을 이해분석하기 위한 새로운 통계지표들도 도출해야 할 필요성이 있음\n이를 위해서는 디지털전환으로 인한 사회변화를 분석할 수 있는 통계를 작성하는 프레임워크 구축이 필요함\n현재 한국에서 생산되는 디지털전환과 사회변화 관련 통계는 KOSIS 국가통계포털, ICT통계포털 등에서 제공하고 있는 ‘정보화통계조사’, ‘디지털정보격차실태조사’, ‘웹접근성실태조사’, ‘언론수용자 의식조사’, ‘국민여가활동조사’, ‘한국미디어패널조사’ 등이 있음.\n각 통계들은 그동안 한국이 정보사회로 접어들게 되면서 경험한 다양한 현상들을 포착하는 데 유용하게 활용되었으며, 대표적인 통계지표로는 인터넷 이용률, 스마트폰 보급률, 디지털 격차 지수, 전자상거래 거래액, 모바일 결제 이용률, 소셜미디어 이용률, 클라우드 서비스 이용률, 사이버 보안 사고 발생 건수 등이 있음.\n\n\n하지만 이 통계에 있어서 다음과 같은 한계들이 있음.\n\n\n먼저 디지털기술환경은 지속적으로 발전하며 새로운 기술과 플랫폼이 계속해서 등장하며, 이러한 변화는 디지털전환과 사회변화 관련된 통계를 최신 상태로 유지하고 관련성을 유지하는 것이 어렵게 만들고 있음.\n통계는 지속성이 중요한데, 디지털기전환과 사회에 대한 통계들은 시간이 조금만 지나면 의미가 줄어드는 통계들이 많아짐\n또한 새로운 디지털기술은 새로운 사회의 변화를 가져오는 데, 이때 새로운 통계지표들이 필요하기 때문에 기존의 통계들을 과감하게 그리고 지속적으로 수정보완해야 하는 특성이 있음.\n한편 디지털기술은 워낙 다양하며, 그것이 미치는 사회적 영향 역시 가시적이지 않은 것들도 있기 때문에 디지털기술이 미치는 사회적 영향 관련된 통계는 상관 관계를 보여줄 수는 있지만, 인과관계를 확립하는 것은 어려울 수 있음\n관찰된 결과에 기여하는 다른 기저 요인이 있을 수 있어 디지털기술의 정확한 영향을 결정하기 어려울 수 있음\n이에 인과관계까지 찾아낼 수 있는 지표들을 도출하여 그 프레임워크를 활용하는 것이 대단히 중요함.\n또한 디지털전환이 사회에 미치는 영향을 세부적으로 구분하여 자세하게 살펴보는 것이 필요함.\n디지털전환이 노동과 소득 등 경제활동에 미치는 영향, 신뢰, 사회자본 등 사회관계에 미치는 영향, 행정서비스활용 등 공공서비스활동에 미치는 영향, 행복도, 우울증, 건강, 복지 등 삶의 질에 미치는 영향 들을 모두 고려하여 이를 세부적으로 분석하면서도, 서로 연관될 수 있도록 하는 통계지표들과 그 프레임워크가 필요함."
  },
  {
    "objectID": "proj/completed/gunbo.html",
    "href": "proj/completed/gunbo.html",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "연구 수행 기관: 창의성과인터랙션 연구소 \n\n\n\n\n\n\n\n\n\n연구책임자\n선임연구원\n연구원\n객원연구원\n\n\n\n\n\n\n\n\n\n\n이창준 교수\n이승경 박사과정\n박영주 석사과정\n조한슬 박사과정\n\n\n한양대학교 ERICA\n정보사회미디어\n한양대학교\n미디어인포매틱스\n한양대학교\n미디어인포매틱스\n경희대학교\n빅데이터응용학과\n\n\n\n\n\n\n\n건강보험공단이 긍정적인 공공 이미지를 유지하고 부정적인 이슈에 능동적으로 대응하기 위해서는 효과적인 홍보 전략과 미디어 및 온라인 트렌드에 대한 실시간 모니터링이 필요하다. 이를 위해 소셜빅데이터 분석팀은 매스 미디어와 소셜 미디어에서 건강보험공단과 관련된 텍스트를 수집하고 수면에 드러나지 않는 이슈들을 탐색적으로 도출해보고자 한다. 이를 통해 건강보험공단이 해당 이슈에 대해 체계적으로 대응하고 PR 전략을 설정할 수 있는 지표가 되고자 한다.\n\n\n\n\n\n\n기존의 분류와 같이 매스 미디어와 소셜 미디어로 구분하여 텍스트를 수집하고자 함. 아래 열거한 매체와 소스를 위주로 데이터를 수집해나갈 계획이지만 공급 업체의 웹사이트 운영 방침이나 기술적인 문제 등으로 텍스트 수집이 불가능한 경우가 있을 수 있음. 매스 미디어 중 방송사 관련 뉴스들은 거의 모든 방송사들이 공식 유튜브 계정을 가지고 있기 때문에 공식 유튜브 계정을 중심으로 수집하고자 함. 또한 해당 뉴스에 대한 댓글들은 실명 기반의 계정들의 댓글이기 때문에 포털 등의 뉴스 댓글에 비해 혐오성, 공격성 댓글이 적어 담론 수집에 용이함.\n\n\n전국일간지(11개): 경향신문, 국민일보, 내일신문, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보\n경제일간지(8개): 매일경제, 머니투데이, 서울경제, 아시아경제, 아주경제,\n파이낸셜뉴스, 한국경제, 헤럴드경제\n지역일간지(28개): 강원도민일보, 강원일보, 경기일보, 경남도민일보, 경남신문, 경상일보, 경인일보, 광주매일신문, 광주일보, 국제신문, 대구일보, 대전일보, 매일신문, 무등일보, 부산일보, 영남일보, 울산매일, 전남일보, 전북도민일보, 전북일보, 제민일보, 중도일보, 중부매일, 중부일보, 충북일보, 충청일보,\n충청투데이, 한라일보\n보건의료전문지(25개): 뉴스더보이스헬스케어, 데일리팜, 데일리메디, 데일리메디팜, 메디게이트뉴스, 메디칼업저버, 메디칼타임즈, 메디파나뉴스, 메디팜스투데이, 메디포뉴스, 병원신문, 보건신문, 약사공론, 약사신문, 약업신문, 의료정보, 의사신문, 의약뉴스, 일간보사, 의협신문, 청년의사, 헬스포커스, 현대건강신문, 후생신보, 히트뉴스\n전문지(2개): 디지털타임스, 전자신문\n방송 언론: 공식 유튜브 채널을 통해 수집(30개): 연합뉴스Yonhapnews, 연합뉴스TV, 뉴스1TV, 뉴시스, KBSNews, MBCNEWS, SBS뉴스, YTN, MBNNews, 채널A뉴스, JTBCNews, 뉴스TVCHOSUN, 이런경향, 동아일보, 매일경제TV, MTN 머니투데이방송, 서울경제TV, 서울신문, 세계일보, 아주경제, 조선일보, 중앙일보, tvFN, 한겨레TV, 한국경제TV, 한국일보, 내일신문, 이데일리TV, 아시아투데이,시사포커스TV\n\n\n\n\n\n기존 소셜 미디어의 수집처였던 페이스북과 트위터는 국민건강보험 관련 담론에 대한 담론이 활발하게 생성되지 않고 있고 특히 트위터의 경우 정치적 성향이 한쪽으로 치우친 경향이 있어 적절한 수집처로 보기 어려움. 또한 인스타그램의 경우는 이미지 기반의 소셜네트워크서비스로 텍스트 기반 담론이 형성되고 있지 않음. 따라서 소셜빅데이터 팀은 유튜브 댓글 위주의 소셜 데이터를 수집하고 이후에 추가적으로 온라인 커뮤니티로 담론 수집을 확장하고자 함. 다만 온라인 커뮤니티별로 정치적 성향이 확연히 다르기 때문에 균형적인 담론 수집을 위한 전략이 필요함.\n\n\n블로그/카페(공개게시글)(2개): 네이버, 다음\n커뮤니티(7개): 각 커뮤니티의 성향을 파악하고 균형적인 담론 수집 전략을 세울 예정: 디시인사이드, 네이트판, 뽐뿌, MLB파크, 클리앙, 더쿠, 딴지일보\n\n건강보험 등의 키워드로 관련 담론 활발히 생성되는 추가적인 커뮤니티 탐색이 필요\n폐쇄적 구조와 집단극화 발생 가능성, 정확한 데이터 수집/분석 우려\n\n유튜브 댓글(30개): 매스 미디어 방송사 공식 유튜브 채널과 동일한 소스를 활용하여 기사 내용과 담론을 연결할 예정\n\n\n\n\n\n\n\n\n빅데이터 수집 및 구축\n\n매스 미디어: 빅카인즈에서 수집가능한 언론사는 빅카인즈 API를 활용하여 수집, 그 외에는 해당 언론사 웹페이지에서 크롤링\n소셜 미디어: 각 소셜 미디어 서비스의 API를 활용하거나 온라인 커뮤니티 웹에서 크롤링을 통해 수집\n\n\n\n\n\n\n빈도 분석 → 감성 분석 → 토픽 추출 → 추가 분석\n\n\n건강보험공단 관련 단어(명사)의 빈도 분석\n\n제시된 키워드와의 동시 출현단어(명사) TOP 20 추출\n분석기간 동안 시계열 추이 확인\n\n감성분석\n\n자연어 처리(NLP) 기술을 적용하여 수집된 데이터의 감정을 분석할 수 있음. 기사(또는 포스팅) 내용에 대한 감정 점수를 결정하여 대중의 인식을 측정하고자 함.\n제시된 키워드와의 동시 출현단어(형용사) 추출\n형용사 기반의 감성분석 진행\n\n토픽 모델링\n\n건강보험공단과 관련된 대중의 관심사와 관심사의 우선순위를 이해하기 위해 추출된 단어의 분포와 머신 러닝 알고리즘을 사용하여 주제를 도출하고자 함.\n제시된 키워드로 추출된 기사(또는 포스팅)의 제목으로 토픽(주제)를 도출\n건강보험공단과 관련된 주제들의 1개월 단위 추이 탐색\n\n추가 분석 논의\n\n위 분석들의 결과와 관련하여 추가적으로 탐색이 필요하다고 판단될 경우 추가 분석을 진행할 예정\n(예) 소셜 네트워크 분석: 단어들 간의 연결성을 파악하여 해당 주제에 대한 담론을 구체적으로 살펴볼 수 있음. 또한 기사에서 등장하는 다양한 행위자(개인, 조직, 언론 매체) 간의 관계를 분석할 수 있음."
  },
  {
    "objectID": "proj/completed/gunbo.html#개요",
    "href": "proj/completed/gunbo.html#개요",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "건강보험공단이 긍정적인 공공 이미지를 유지하고 부정적인 이슈에 능동적으로 대응하기 위해서는 효과적인 홍보 전략과 미디어 및 온라인 트렌드에 대한 실시간 모니터링이 필요하다. 이를 위해 소셜빅데이터 분석팀은 매스 미디어와 소셜 미디어에서 건강보험공단과 관련된 텍스트를 수집하고 수면에 드러나지 않는 이슈들을 탐색적으로 도출해보고자 한다. 이를 통해 건강보험공단이 해당 이슈에 대해 체계적으로 대응하고 PR 전략을 설정할 수 있는 지표가 되고자 한다."
  },
  {
    "objectID": "proj/completed/gunbo.html#분석-매체",
    "href": "proj/completed/gunbo.html#분석-매체",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "기존의 분류와 같이 매스 미디어와 소셜 미디어로 구분하여 텍스트를 수집하고자 함. 아래 열거한 매체와 소스를 위주로 데이터를 수집해나갈 계획이지만 공급 업체의 웹사이트 운영 방침이나 기술적인 문제 등으로 텍스트 수집이 불가능한 경우가 있을 수 있음. 매스 미디어 중 방송사 관련 뉴스들은 거의 모든 방송사들이 공식 유튜브 계정을 가지고 있기 때문에 공식 유튜브 계정을 중심으로 수집하고자 함. 또한 해당 뉴스에 대한 댓글들은 실명 기반의 계정들의 댓글이기 때문에 포털 등의 뉴스 댓글에 비해 혐오성, 공격성 댓글이 적어 담론 수집에 용이함.\n\n\n전국일간지(11개): 경향신문, 국민일보, 내일신문, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보\n경제일간지(8개): 매일경제, 머니투데이, 서울경제, 아시아경제, 아주경제,\n파이낸셜뉴스, 한국경제, 헤럴드경제\n지역일간지(28개): 강원도민일보, 강원일보, 경기일보, 경남도민일보, 경남신문, 경상일보, 경인일보, 광주매일신문, 광주일보, 국제신문, 대구일보, 대전일보, 매일신문, 무등일보, 부산일보, 영남일보, 울산매일, 전남일보, 전북도민일보, 전북일보, 제민일보, 중도일보, 중부매일, 중부일보, 충북일보, 충청일보,\n충청투데이, 한라일보\n보건의료전문지(25개): 뉴스더보이스헬스케어, 데일리팜, 데일리메디, 데일리메디팜, 메디게이트뉴스, 메디칼업저버, 메디칼타임즈, 메디파나뉴스, 메디팜스투데이, 메디포뉴스, 병원신문, 보건신문, 약사공론, 약사신문, 약업신문, 의료정보, 의사신문, 의약뉴스, 일간보사, 의협신문, 청년의사, 헬스포커스, 현대건강신문, 후생신보, 히트뉴스\n전문지(2개): 디지털타임스, 전자신문\n방송 언론: 공식 유튜브 채널을 통해 수집(30개): 연합뉴스Yonhapnews, 연합뉴스TV, 뉴스1TV, 뉴시스, KBSNews, MBCNEWS, SBS뉴스, YTN, MBNNews, 채널A뉴스, JTBCNews, 뉴스TVCHOSUN, 이런경향, 동아일보, 매일경제TV, MTN 머니투데이방송, 서울경제TV, 서울신문, 세계일보, 아주경제, 조선일보, 중앙일보, tvFN, 한겨레TV, 한국경제TV, 한국일보, 내일신문, 이데일리TV, 아시아투데이,시사포커스TV\n\n\n\n\n\n기존 소셜 미디어의 수집처였던 페이스북과 트위터는 국민건강보험 관련 담론에 대한 담론이 활발하게 생성되지 않고 있고 특히 트위터의 경우 정치적 성향이 한쪽으로 치우친 경향이 있어 적절한 수집처로 보기 어려움. 또한 인스타그램의 경우는 이미지 기반의 소셜네트워크서비스로 텍스트 기반 담론이 형성되고 있지 않음. 따라서 소셜빅데이터 팀은 유튜브 댓글 위주의 소셜 데이터를 수집하고 이후에 추가적으로 온라인 커뮤니티로 담론 수집을 확장하고자 함. 다만 온라인 커뮤니티별로 정치적 성향이 확연히 다르기 때문에 균형적인 담론 수집을 위한 전략이 필요함.\n\n\n블로그/카페(공개게시글)(2개): 네이버, 다음\n커뮤니티(7개): 각 커뮤니티의 성향을 파악하고 균형적인 담론 수집 전략을 세울 예정: 디시인사이드, 네이트판, 뽐뿌, MLB파크, 클리앙, 더쿠, 딴지일보\n\n건강보험 등의 키워드로 관련 담론 활발히 생성되는 추가적인 커뮤니티 탐색이 필요\n폐쇄적 구조와 집단극화 발생 가능성, 정확한 데이터 수집/분석 우려\n\n유튜브 댓글(30개): 매스 미디어 방송사 공식 유튜브 채널과 동일한 소스를 활용하여 기사 내용과 담론을 연결할 예정"
  },
  {
    "objectID": "proj/completed/gunbo.html#분석-방법",
    "href": "proj/completed/gunbo.html#분석-방법",
    "title": "언론과 소셜 미디어 상의 건강보험공단 관련 이슈 분석",
    "section": "",
    "text": "빅데이터 수집 및 구축\n\n매스 미디어: 빅카인즈에서 수집가능한 언론사는 빅카인즈 API를 활용하여 수집, 그 외에는 해당 언론사 웹페이지에서 크롤링\n소셜 미디어: 각 소셜 미디어 서비스의 API를 활용하거나 온라인 커뮤니티 웹에서 크롤링을 통해 수집\n\n\n\n\n\n\n빈도 분석 → 감성 분석 → 토픽 추출 → 추가 분석\n\n\n건강보험공단 관련 단어(명사)의 빈도 분석\n\n제시된 키워드와의 동시 출현단어(명사) TOP 20 추출\n분석기간 동안 시계열 추이 확인\n\n감성분석\n\n자연어 처리(NLP) 기술을 적용하여 수집된 데이터의 감정을 분석할 수 있음. 기사(또는 포스팅) 내용에 대한 감정 점수를 결정하여 대중의 인식을 측정하고자 함.\n제시된 키워드와의 동시 출현단어(형용사) 추출\n형용사 기반의 감성분석 진행\n\n토픽 모델링\n\n건강보험공단과 관련된 대중의 관심사와 관심사의 우선순위를 이해하기 위해 추출된 단어의 분포와 머신 러닝 알고리즘을 사용하여 주제를 도출하고자 함.\n제시된 키워드로 추출된 기사(또는 포스팅)의 제목으로 토픽(주제)를 도출\n건강보험공단과 관련된 주제들의 1개월 단위 추이 탐색\n\n추가 분석 논의\n\n위 분석들의 결과와 관련하여 추가적으로 탐색이 필요하다고 판단될 경우 추가 분석을 진행할 예정\n(예) 소셜 네트워크 분석: 단어들 간의 연결성을 파악하여 해당 주제에 대한 담론을 구체적으로 살펴볼 수 있음. 또한 기사에서 등장하는 다양한 행위자(개인, 조직, 언론 매체) 간의 관계를 분석할 수 있음."
  },
  {
    "objectID": "proj/completed/kisdi_1.html",
    "href": "proj/completed/kisdi_1.html",
    "title": "디지털 전환 생태계 분석 및 토픽 모델링을 통한 산업 분류 방안 모색",
    "section": "",
    "text": "연구 목적 및 내용\n\n(목적) 디지털 기술이 서비스 산업에 접목되며 서비스 방식의 변화, 서비스 산업 지형의 변화를 초래하고 있어 이러한 변화를 조망할 수 있는 분석 프레임워크 개발을 위한 연구를 수행\n(내용) 디지털 전환을 이끌고 있는 혁신 스타트업을 대상으로한 토픽모델링을 통해 서비스의 융합, 디지털 전환 트렌드 분석\n\n토픽 모델링을 통해 서비스 산업 분류방법 고찰 및 디지털 전환 트렌드 분석\n네트워크 방법론을 활용한 기술-BM 융합 트렌드 분석\n디지털 전환 생태계에서 고용이 일어나는 요인 규명\n\n\n수행방법\n\n토픽 모델링을 통해 서비스 산업 분류방법 고찰 및 디지털 전환 트렌드 분석\n\n토픽 모델링을 통해 디지털 트랜스포메이션이 가져온 서비스 산업의 변화 및 융합 산업 분류 방법에 대한 고찰\n기존 산업 분류 내에서 새로운 토픽의 분포 분석을 통한 새로운 산업 분류 분석 프레임 워크 개발\n\n네트워크 방법론을 활용한 기술-BM 융합 트렌드 분석\n\n스타트업 '혁신의숲' 데이터를 통해 한국의 스타트업 기업들의 기술과 사업 모델에 대한 태그에 대한 공동 출현 정보로 네트워크로 전환\n네트워크 방법론을 활용하여 산업별로 디지털 전환 생태계 내의 기술과 사업 모델 융합이 어떤 식으로 이루어지는지 앞서 얻어진 토픽 별로 분석\n\n디지털 전환을 이끄는 혁신기업의 성장 요인 규명\n\n디지털 전환을 이끄는 혁신기업의 성장 (성장은 고용(신규 인력 채용 등) 등을 활용하여 측정)의 주요 요인(ex. 기술-BM융합, 산업(토픽), 투자 단계, 투자 시점 등의 요인) 분석"
  },
  {
    "objectID": "teaching/cul_tech_101/index.html",
    "href": "teaching/cul_tech_101/index.html",
    "title": "Culture & Technology (Vol.1)",
    "section": "",
    "text": "About Course\n\n\n\nWeekly Design\n\n\n\nPre-class materials"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html",
    "href": "teaching/cul_tech_101/weekly/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Intro\n\n\nWeek 1: Course Intro\n\nDate: 20240305\nClass: Course Introduction\n\nAn overview of the course\n\n\n\n\n\n\nPart I: 콘텐츠 기획과 제작\n\n\nWeek 2: 융합 콘텐츠 기획과 제작\n\nDate: 20240312\nPre-class video\n\n덕후에서 콘텐츠 크리에이터가 되는 방법 | 대도서관\n좋은 컨텐츠를 만드는 법 | 에이틴 제작사\nWhy AI Will Spark Exponential Economic Growth | Cathie Wood | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 강혜원 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\n\nPart II: 문화와 문화 산업\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n소속사 없이 음원 유통하는 법\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 정헌섭 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 4: 패션과 뉴테크\n\nDate: 20240326\nPre-class video\n\nHow data is driving the future of fashion | Steve Brown | TED Institute\nThe New Reality of Fashion is Digital | Gala Marija Vrbanic | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 류현석 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 5: 게임 & 인터랙티브 디자인\n\nDate: 20240402\nPre-class video\n\n게임 과몰입, 게임 잘못일까요? | 한덕현 중앙대학교 병원 정신건강의학과 교수 | | 세바시\nDiscussion\n\nClass\n\nGuest Lecturer: 김수완 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 6: 한류와 팬덤\n\nDate: 20240409\nPre-class video\n\nAll I Really Need To Know, I Learned From KPOP | Donald Lim | TEDxYouth@SJCS\nHas K-pop prepared us for the metaverse? | Alex Karlsson | TEDxBrunkebergstorg\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이종명 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\n\nPart III: 문화 기술\n\n\nWeek 7: 데이터 시각화의 예술\n\nDate: 20240416\nPre-class video\n\nThe beauty of data visualization - David McCandless\nData Visualization Best Practices - Stephanie Evergreen\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 전서연 강사\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 8: 문화콘텐츠와 자연어 처리\n\nDate: 20240423\nPre-class video\n\nText 데이터를 활용한 개인화 음악 추천서비스 웹 데모 구축 (혁신성장청년인재양성사업 프로젝트 발표)\nNLP Project - Emotion In Text Classifier App with Streamlit and Python\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 구영은 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 9: 메타버스와 메타휴먼\n\nDate: 20240430\nPre-class video\n\n버추얼 아이돌 그룹 플레이브의 탄생 과정과 기술 이야기 (언리얼 페스트 2023 서울)\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 원종서 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 10: 인터랙션 사이언스, UX\n\nDate: 20240507\nPre-class video\n\nHuman-Computer Interaction and User Interface Design | Tony Tang\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이대호 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 11: 서비스 디자인, 데이터 드리븐 마케팅\n\nDate: 20240514\nPre-class video\n\nWhy Data Marketing So Important | 세바시\nAge of Experience, Data+Service | 설상훈 교수님\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 설상훈 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 12: 가상/증강현실 콘텐츠 제작\n\nDate: 20240521\nPre-class video\n\n상상이 현실화가 되는 세계, 그 중심에 서서 외치다 | 서동일 오큘러스VR코리아 지사장\nWhat is extended reality? | The Gadget Show\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 김태원 대표(RGB Makers)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\n\nPart IV: 문화 콘텐츠 경영\n\n\nWeek 13: 융합콘텐츠와 창업\n\nDate: 20240528\nPre-class video\n\nGPT 있는데 왜 또 만들어요? 손쉽게 이해하는 AI 개발 트렌드와 비하인드\n세상을 바꾸는 뉴콘텐츠\n콘텐츠 창업에서 가장 중요한 것 | CEO 노정석\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 윤영훈 대표 (주)ASSI\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 14: 엔터테인먼트 경영\n\nDate: 20240604\nPre-class video\n\nHow Netflix changed entertainment – and where it’s headed | Reed Hastings\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이동찬 경영총괄 (TEO Universe)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\n\nConclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "teaching/cul_tech_101/about/index.html",
    "href": "teaching/cul_tech_101/about/index.html",
    "title": "Course description & Communication",
    "section": "",
    "text": "1주차 수업 녹음 (Course introduction)\n\n\n\n\n\nCourse description\n\n문화와 기술1 은 문화, 콘텐츠, 문화 산업, 기술의 융합과 관련된 다양한 주제에 대해 소개하고 주제와 관련된 여러가지 궁금점을 함께 탐구하기 위해 고안된 입문 수준의 학부 수업이다. 본 과정에서는 글로벌융합학부 컬쳐앤테크놀로지융합전공의 세부 트랙인 콘텐츠, 컬처, 테크, 경영의 카테고리에 대해 세부 주제를 가르치는 교강사와 각 분야 전문가를 초청하여 개괄하고 좌담을 통해 토론과 Q&A를 진행한다. 이 과정에서 학생들은 콘텐츠 기획과 제작부터 음악, 패션, 게임, 데이터 과학, 인공지능, 메타버스 등 다양한 문화 산업에 기술이 어떻게 융합되고 있는지 이해하고, 학생 개인 맞춤형 세부 트랙에 대한 구상에 대한 통찰력을 얻게 된다. 컬처앤테크놀로지융합전공으로 (원정공 또는 다중 전공으로) 진입하여 세부 전공 탐색이 필요한 학생들에게 수강을 권한다.\n\n\n\n\nGoal\n\n문화와 기술의 융합을 이해: 학생들은 기술이 어떻게 문화적 표현에 영향을 미칠 뿐만 아니라 필수적인 부분이 되어 콘텐츠가 생성, 소비 및 배포되는 방식을 형성하는지 탐구.\n문화 산업의 동향을 분석: 음악(K-pop 및 엔터테인먼트에 중점), 패션, 게임 및 디지털 콘텐츠 생태계를 포함한 주요 문화 산업의 현재 동향을 비판적으로 분석할 수 있는 도구를 학생들에게 제공.\n신흥 기술 탐구: 중요한 목표 중 하나는 학생들이 데이터 시각화, 자연어 처리, 메타버스, 상호 작용 과학, UX(사용자 경험) 디자인, 가상/증강 현실 콘텐츠 제작과 같은 새로운 기술에 익숙해지는 것.\n콘텐츠 제작의 혁신 촉진: 서비스 디자인과 데이터 중심 마케팅의 원리를 이해함으로써 학생들은 문화 콘텐츠 제작 공간 내에서 혁신하는 방법을 배우게 된다.\n기업가적 사고 장려: 스타트업 생태계와 엔터테인먼트 관리에서 문화와 기술의 교차점을 조사하여 기업가적 사고를 고취시키는 것을 목표로 함.\n\n\n\n\nTime & Location\n\n(1h) 플립러닝 콘텐츠.\n(2h) 화요일 09:20 ~ 10:50 @국제관 첨단강의실[9B217]\n\n\n\n\nWeekly Design\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nGuest\nNote\n\n\n\n\n1\n03/05/2024\nCourse Intro\n\n\n\n\n\n\n콘텐츠 기획과 제작\n\n\n\n\n2\n03/12/2024\n융합 콘텐츠 기획과 제작\n강혜원\n\n\n\n\n\n문화와 문화 산업\n\n\n\n\n3\n03/19/2024\n음악, K-pop, 엔터테인먼트\n정헌섭\n\n\n\n4\n03/26/2024\n패션과 뉴테크\n류현석\n\n\n\n5\n04/02/2024\n게임 & 인터랙티브 디자인\n김수완\n\n\n\n6\n04/09/2024\n한류와 팬덤\n이종명\n\n\n\n\n\n문화 기술\n\n\n\n\n7\n04/16/2024\n데이터 시각화의 예술\n전서연\n\n\n\n8\n04/23/2024\n문화콘텐츠와 자연어 처리\n구영은\n\n\n\n9\n04/30/2024\n메타버스와 메타휴먼\n원종서\n\n\n\n10\n05/07/2024\n인터랙션 사이언스, UX\n이대호\n\n\n\n11\n05/14/2024\n서비스 디자인, 데이터 드리븐 마케팅\n설상훈\n\n\n\n12\n05/21/2024\n가상/증강현실 콘텐츠 제작\n김태원\n\n\n\n\n\n문화 콘텐츠 경영\n\n\n\n\n13\n05/28/2024\n융합콘텐츠와 창업\n윤영훈\n\n\n\n14\n06/04/2024\n엔터테인먼트 경영\n이동찬\n\n\n\n15\n06/11/2024\nWrap-up Exam\n\n\n\n\n\n\n\n\nAbout Lecturer (Modulator)\nChangjun LEE\n\nHome: https://changjunlee.com/\n\nAssociate Professor (Head of Culture & Tech)\nSchool of Convergence. SKKU.\n\nAs a computational social scientist, I bring a unique interdisciplinary perspective to the fields of economics, innovation studies, and convergence technologies. With a background in natural sciences, including a bachelor’s degree in biology and chemistry, I went on to earn a Ph.D. in technology management, economics, and policy. My research focuses on utilizing computational methods to tackle a wide range of social phenomena, including technology evolution & regional growth, knowledge management, and technology & convergence innovation. I am passionate about using technology and data to drive innovation and solve real-world problems.\n\n\nResearch interest: Media & Innovation, Immersive media and users’ behavior, Technology management, Data Science\nTeaching: Culture & Technology | Data Science in CNT\nThings I love\n\nResearch\nChat & Coffee & MBTI\nTravel\nPlay (Things)\n\n\n\n\n\nFlipped Learning\n\n주차별 주제 기초 지식에 대한 내용으로 사전 학습(수업홈 확인)\n사전 학습 내용에 대한 Discussion & Questions: Google Form 제출\n\nSubmission due: 월요일 20:00까지\n\n사전 학습 내용 외에 해당 영역에 대한 질문도 함께 제출\n\n해당 영역의 이슈, 진로, 산업 전망 등 다양한 내용으로 질문 (한 개 이상 제출)\n\n\n\n\n\nLecture\n\n주차별 주제에 대한 Overview\n주차별 주제 수업의 내용과 결과물 소개\n해당 영역의 학습, 산업, 이슈, 그리고 진로에 대한 내용\n\n\n\n\n좌담: 토론과 Q&A\n\n모듈레이터의 기초 질문들\n학생의 사전 질문\n학생의 즉석 질문 (APP을 통해)\n\n\n\n\nEvaluation\n\nAttendance (10 %)\nPreclass Discussion & Questions Submission (20 %)\nParticipation (20 %)\nWrap-up Exam (50 %)\n\n영역별 3문항 * 13영역 + 1 = 40문항\n\n\n\n\n\nCommunication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gHUaD37f\n입장하셔서 이름을 꼭 출석부에 있는 이름으로 설정해주세요.\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "teaching/cul_tech_101/icpbl/index.html",
    "href": "teaching/cul_tech_101/icpbl/index.html",
    "title": "PBL Project",
    "section": "",
    "text": "목표\n협력 기업에서 주어진 문제 시나리오를 수업 시간에 배운 문화 사업과 미디어에 대한 이해를 바탕으로 팀 별로 해결을 위한 아이디어를 고안해본다. 인터뷰, 설문, 데이터 분석 등 모든 방법을 동원하여 실제 기업의 문제를 풀어 봄으로써 문화 산업과 기술에 대한 이해를 높이고 현장의 멘토링과 평가를 통해 현장감을 높이고 실전 경험을 갖춘다.\n\n\n\n협력 기업\n\nTEO 유니버스\n\n담당: 이동찬 경영총괄 (dclee@teouniverse.kr)\n유튜브 계정: https://www.youtube.com/@TEO_universe\n인스타 계정: https://www.instagram.com/teo.universe/\n\n(주)ASSI\n\n담당: 윤영훈 대표이사 (younghoon.yun@my-assi.com)\n독서 어플 리더스: https://withreaders.com/\nGoogle Play Store [download]\nAPPLE App Store [download]\n\n\n\n\n\nTeam Assignment\n\nRandom assignment\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n김근아\n김명진\n김세린\n김재희\n등시우\n왕일범\n윤동주\n이소언\n\n\n임서영\n최지현\n최진영\n강태현\n고재희\n김가희\n김민기\n김민우\n\n\n신소망\n양제니\n염다인\n왕욱경\n윤지민\n이자의\n이채은\n장현군\n\n\n채자선\n최은서\n최혜린\n김근아\n김승연\n김승주\n김지훈\n박지원\n\n\n박혜원\n왕자문\n유사여\n이수연\n임연우\n정서윤\n \n \n\n\n\n\n\n\n\n시나리오\n시나리오 1: 엔터테인먼트 기업\n\nTEO는 많은 채널과 OTT 중 어느 곳과 전략적 관계를 강화하는 것이 유의미할지? 그 이유는?\n넷플릭스의 경우 고정 수익을 게런티해주지만 제작사로서 작품 IP는 확보하지 못하는 한계가 있는데 넷플릭스와 중장기적으로 함께할 경우 사업적 장단점은 무엇일지?\nTEO가 글로벌 시장에서 인정받을 수 있는 콘텐츠를 제작하기 위해서 준비해야할 요소는 무엇일지? 글로벌 흥행과 국내 흥행 중 무엇이 먼저일지?\n\n\n시나리오 2: 문화 기술 스타트업\n\n책의 소비자 입장\n\n좋은 책이란 무엇일까?\n책을 선택하게 되는 방법은 무엇이 있을까?\n좋은 책을 고르는 방법은 어떤 것들이 있을까?\n그 과정을 기술(IT, AI 등)로 풀어낼 수 있을까? (이 과정에서 더 필요한 것들이 있다면 어떤 것이 필요할까?)\n그 과정에 BM을 붙일 수 있을까?\n\n책의 공급자(저자/출판사/유통사) 입장\n\n책을 제작하고 유통하는 이해관계자들의 경제적 해자는 어디로 부터 기인하는가?\n1) 기술의 발전 2) 트렌드 변화가 영향을 끼치는 것들이 있을까?\n어떤 변화와 기회를 잡는 사람이 주도권을 잡게 될 것인가? (책의 공급자에게 중요해지는/필요한 역량은 무엇일까?)\n\n벤치마킹/브레인스토밍: 다른 산업에서 힌트 얻기\n\n리더스 서비스에서, 독서 트레이너(PT)를 출시하기 위해, 트레이너가 있는 운동 산업에서 힌트를 얻어보고자 한다.\n헬스를 할때 PT를 받는 사람과 그렇지 않은 있다. PT를 받는 사람과 그렇지 않은 사람의 의사결정은 어떤 것에 의해 이루어 지는가?\n선생님(트레이너)과 함께하는 것이 일반적인 운동과 그렇지 않은 운동들이 있다. 무엇이 차이를 만드는가?\n위에 조사를 통해 얻은 시사점을 “독서”라는 행위에 접목해서 서비스 요소(feature)를 설계해보기\n\n\n\n\n\nPBL 주차별 진행:\n\nW4. Team assignment (4~5 students in a team)\nW5 ~ W7: Team building\nW8 ~ W9: Special lecture & Problem description (문제 제공 기업)\nW10 ~ W13: Team Activity in class (수업 중 진행)\nW14: Team activity with mentoring (수업 또는 현장)\nW16: Final project presentation (현장 평가 참여)"
  },
  {
    "objectID": "teaching/ds101/about/index.html",
    "href": "teaching/ds101/about/index.html",
    "title": "About the Course",
    "section": "",
    "text": "Course description\n\n\n\nWelcome to Cultural Industry & Data Analytics course (a.k.a. Data Science 101), designed to equip you with the essential skills to analyze, visualize, and communicate data effectively. Over the course of 15 weeks, you will delve into the fundamentals of data science, master the power of R programming, and learn how to create interactive visualizations and websites to showcase your findings.\nThroughout the course, you will learn how to import, manipulate, and explore data using R and the tidyverse. You will gain hands-on experience with data cleaning, transformation, and aggregation techniques. Additionally, you’ll dive deep into data visualization with ggplot2 and learn how to create advanced, interactive plots using Shiny and plotly.\nBy the end of the course, you will have completed a data science project that demonstrates your ability to analyze, visualize, and communicate complex data insights. You will also learn the importance of collaboration, version control, and reproducible research in data science projects. With a solid understanding of the concepts and tools covered, you will be well-prepared to apply your skills in various real-world applications.\n\n\n\n\nWeekly Design\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nPre-class\nClass\nPBL\nNote\n\n\n\n\n1\n03/06/2024\n\nCourse intro\n\n\n\n\n2\n03/13/2024\nVariable & Vector\nBasic Syntax (1)\n\n\n\n\n3\n03/20/2024\nArray\nBasic Syntax (2)\nProblem description\n\n\n\n4\n03/27/2024\nData.frame & List\nBasic Syntax (3)\nData introduction\n\n\n\n5\n04/03/2024\nData import, export, filter\nData Manipulation\nTeam arrangement\n\n\n\n6\n04/10/2024\nRepetition, Function\nData Exploration (1)\n(Recorded Lecture)\n\nPublic holiday (N.A. elections)\n\n\n7\n04/17/2024\nMissing values, Outliers\nData Exploration (2)\nTeam meeting #1\n\n\n\n8\n04/24/2024\nData viz intro\nData Visualization (1)\nTeam meeting #2\n\n\n\n9\n05/01/2024\nData viz Practice\nData Visualization (2)\nTeam meeting #3\n\n\n\n10\n05/08/2024\n\nQZ\n\n\n\n\n11\n05/15/2024\n\n&lt;No class&gt;\nPublic Holiday\n\nPublic holiday (Buddha’s)\n\n\n12\n05/22/2024\nShiny Intro\nInteractive Web: Shiny\nTeam meeting #4\n\n\n\n13\n05/29/2024\nGit, GitHub Basic\nVersion Control and Collaboration\nTeam meeting #5\n\n\n\n14\n06/05/2024\nQuarto Intro\nReproducible Research\nTeam meeting #6\n\n\n\n15\n06/12/2024\n&lt;Team meetings&gt;\n&lt;Team meetings&gt;\n\n\n\n\n16\n06/19/2024\nProject Presentation\nProject Presentation\n\n\n\n\n\n\n\n\nSyllabus\nWeek 1: Introduction to Data Science and R\n\nCourse Orientation\nIntroduction to R and RStudio\nWhat is Data Science?\n\nWeek 2: Basic Syntax (1)\n\nR syntax and basic operations\nData types and structures in R\n\nWeek 3: Basic Syntax (2)\n\nData types and structures in R: Array\n\nWeek 4: Basic Syntax (3)\n\nData types and structures in R: Data.frame & List\n\nWeek 5: Data Manipulation\n\nData import & export\nData filtering\n\nWeek 6: Data Exploration (1): Recorded lecture\n\nRepetition\nFunction\nIntroduction to tidyverse\nData cleaning with dplyr and tidyr\nData filtering and aggregation\nData transformation with dplyr\n\nWeek 7: Data Exploration (2)\n\nMissing values & Outliers\n\n\n\nDescriptive statistics\nGrouping and summarizing data\nJoining datasets\nExploratory data analysis (EDA)\n\nWeek 8: Data Visualization (1)\n\nIntroduction to ggplot2 for data visualization\nGrammar of graphics with ggplot2\nCustomizing plots with themes and scales\nAdding labels, titles, and legends\nCreating different types of plots (scatter plots, bar plots, etc.)\n\nWeek 9: Data Visualization (2)\n\nAdvanced ggplot2 techniques\nVisualizing distributions and relationships\nFaceting and multi-panel plots\n\n\n\nPlotting time series data\nInteractive plots with plotly or ggplotly\n\n\nWeek 10: Mid-term QZ\n\nWeek 11: Officially No Class (Public Holiday)\n\nWeek 12: Interactive Web: Shiny\n\nWhat is Shiny?\nCreating Shiny apps with R\nAdding interactivity to data visualizations\n\nWeek 13: Version Control and Collaboration\n\nIntroduction to Git and GitHub\nCollaborating with others using version control\nBest practices for organizing and documenting data science projects\nWorking with AI (feat. ChatGPT)\n\nWeek 14: Reproducible Research\n\nIntroduction to Quarto\nCreating a Quarto website with R Markdown\nCustomizing the website layout and design\nPublishing and sharing your Quarto website\n\n\nWeek 15: Project Consultation\nWeek 16: Project Presentation\n\n\n\nCourse management\n\n\nLecturer: Changjun Lee (Associate Professor in SKKU School of Convergence)\n\nchangjunlee@skku.edu\n\nTA: Ye Seo Lim (Master Student, SKKU Immersive Media Engineering)\n\nivisy6952@g.skku.edu\n\n\n\n\nTime:\n\n(1h): Flipped learning content\n(2h): Wed 09:00 ~ 10:50\n\nLocation: International Hall High-Tech e+ Lecture Room (9B312)\n\n\n\nClass consists of Pre-class, Class, and PBL project\n\n\nPre-class\n\nStudents will be required to watch the lecturer’s recorded lecture (or other given videos) before the off-line (or online streaming ZOOM) class and learn themselves\nVideo is about the concept of the data science and the programming language\n(Sometimes) Students are required to submit Discussions to check the level of their understanding\n\nClass\n\nLecturer summarize the pre-class lecture and explain more details\n\nAsk students about the pre-class content to check whether they learned themselves\nOK to answer incorrectly, but if you cannot answer at all, it will be reflected in your pre-class discussion score.\n\nStudents will practice with the advanced code\nA Quiz will be in the class to check the level of understanding\n\nPBL project\n\nStudents organize teams that meet several conditions.\n\n4~5 members in a team\nBackground diversity: no homogeneous majors in a team\nException: Allowed if persuasion is possible for sufficient reasons\n\nData will be given. Teams are going to choose the data they want to explore considering their interest\nTeams can offer a zoom meeting with lecturer if they need\n\n\n\n\n\nFinal outputs (An example not limited)\n\nData Preparing (or Collecting)\nExplore data (Descriptive stats)\nSet your hypothesis (or research questions)\nVisualize data to confirm your hypo or RQs\nExplain your findings\nExpanding your findings to implications\n\n\n\n\nTextbooks for the course\n\nR4DS: R for Data Science (written by Hadley Wickham and Garrett Grolemund)\n\nis an excellent resource for learning data science using R, covering data manipulation, visualization, and modeling with R. The book is available as a free online resource.\n\nRC2E: R Cookbook (written by JD Long and Paul Teetor)\n\nis a comprehensive resource for data scientists, statisticians, and programmers who want to explore the capabilities of R programming for data analysis and visualization.\n\nRGC: R Graphic Cookbook (written by Winston Chang)\n\nis a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems\n\nMDR: Statistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\nis a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language.\n\nISR: Introductory Statistics with R (written by Peter Dalgaard)\n\nis a great resource for learning basic statistics with a focus on R programming. This book covers a wide range of statistical concepts, from descriptive statistic\n\n\n\n\n\nScore\nSee Course intro in Week 1\n\nAttendance & Participation (10 %)\nPreclass Discussion Submission (10 %)\nQZ (40 %)\nProject (40 %)\n\n\n\n\nCommunication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gNpAFOcg\nWhen you enter, please make sure to enter your name as it is on the attendance sheet. (입장하셔서 이름을 꼭 출석부에 있는 이름으로 설정해주세요.)\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "teaching/ds101/weekly/index.html",
    "href": "teaching/ds101/weekly/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nWeek\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n1\n\n\nCourse Intro\n\n\n \n\n\n\n\n2\n\n\nBasic Syntax (1)\n\n\nVariable & Vector\n\n\n\n\n3\n\n\nBasic Syntax (2)\n\n\nArray\n\n\n\n\n4\n\n\nBasic Syntax (3)\n\n\nDF & List\n\n\n\n\n5\n\n\nData Manipulation\n\n\nImport, Export, Filter\n\n\n\n\n6\n\n\nData Exploration (1)\n\n\nLoop & Function\n\n\n\n\n7\n\n\nData Exploration (2)\n\n\nData Wrangling with tidyverse\n\n\n\n\n8\n\n\nData Visualization (1)\n\n\nData viz intro\n\n\n\n\n9\n\n\nData Visualization (2)\n\n\nAdvanced topics in viz\n\n\n\n\n10\n\n\nQZ\n\n\n \n\n\n\n\n11\n\n\nNo Class\n\n\nVisualizaiton with Titanic dataset\n\n\n\n\n12\n\n\nInteractive Web: Shiny\n\n\nLet your data Shiny\n\n\n\n\n13\n\n\nVersion Control and Collaboration\n\n\nGit & GitHub\n\n\n\n\n14\n\n\nReproducible Research\n\n\nQuarto\n\n\n\n\n15\n\n\nTeam Consulting\n\n\n \n\n\n\n\n16\n\n\nFinal Presentation\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html",
    "href": "teaching/ds101/weekly/posts/01_week.html",
    "title": "Course Intro",
    "section": "",
    "text": "Weekly design"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/06_week.html",
    "href": "teaching/ds101/weekly/posts/06_week.html",
    "title": "Data Exploration (1)",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nLoop\n\n\n\n\nx = 5\nif(x %% 2 ==0) {\n   print('x is an even number') # Performed when the conditional expression is true\n} else {\n   print('x is odd') # Performed when the conditional expression is false\n}\n\n[1] \"x is odd\"\n\nx = 8\nif(x&gt;0) {\n   print('x is a positive value.') # Print if x is greater than 0\n} else if(x&lt;0) {\n   print('x is a negative value.') # Prints if the above condition is not satisfied and x is less than 0\n} else {\n   print('x is zero.') # Prints if all of the above conditions are not met\n}\n\n[1] \"x is a positive value.\"\n\nx = c(-5:5)\noptions(digits = 3) # Set the number of significant digits to 3 when expressing numbers\nsqrt(x)\n\nWarning in sqrt(x): NaNs produced\n\n\n [1]  NaN  NaN  NaN  NaN  NaN 0.00 1.00 1.41 1.73 2.00 2.24\n\nsqrt(ifelse(x&gt;=0, x, NA)) # Display negative numbers as NA to prevent NaN from occurring\n\n [1]   NA   NA   NA   NA   NA 0.00 1.00 1.41 1.73 2.00 2.24\n\nstudents = read.csv(\"data/students2.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\n\nstudents # Data contains values over 100 and negative values.\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     120   80\n3 박정원     90      95   90\n4 이상훈    100      85 -100\n5 최건우     85     100  100\n\nstudents[, 2] = ifelse(students[, 2]&gt;= 0 & students[, 2]&lt;= 100,\n                        students[, 2], NA)\nstudents[, 3] = ifelse(students[, 3]&gt;= 0 & students[, 3]&lt;= 100,\n                        students[, 3], NA)\nstudents[, 4] = ifelse(students[, 4]&gt;= 0 & students[, 4]&lt;= 100,\n                        students[, 4], NA)\n\n\nstudents \n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\n# ifelse statement, values other than 0 to 100 among the values in columns 2 to 4 are treated as NA.\n\n\n\n\n# Increment numbers from 1 to 10 using the repeat statement\ni = 1 # starting value of i is 1\nrepeat {\n   if(i&gt;10) { # Break repetition if i exceeds 10\n     break\n   } else {\n     print(i)\n     i = i+1 # Increment i by 1.\n   }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n# Increment numbers from 1 to 10 using while statement\ni = 1 # The starting value of i is 1.\nwhile(i &lt; 10){ # repeat as long as i is less than 10\n   print(i)\n   i = i+1 # Increment i by 1.\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n# Create the second column of the multiplication table using the while statement\ni = 1\nwhile(i&lt;10) {\n   print(paste(2, \"X\", i, \"=\", 2*i))\n   i = i+1\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n\n# Incrementing numbers from 1 to 10 using the for statement\nfor(i in 1:10) {\n   print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n# Create the second column of the multiplication table using the for statement\nfor(i in 1:9) {\n   print(paste(2, \"X\", i, \"=\", 2*i))\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n\n# Create multiplication table columns 2 to 9 using the for statement\nfor(i in 2:9) {\n   for(j in 1:9) {\n     print(paste(i, \"X\", j, \"=\", i*j))\n   }\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n[1] \"3 X 1 = 3\"\n[1] \"3 X 2 = 6\"\n[1] \"3 X 3 = 9\"\n[1] \"3 X 4 = 12\"\n[1] \"3 X 5 = 15\"\n[1] \"3 X 6 = 18\"\n[1] \"3 X 7 = 21\"\n[1] \"3 X 8 = 24\"\n[1] \"3 X 9 = 27\"\n[1] \"4 X 1 = 4\"\n[1] \"4 X 2 = 8\"\n[1] \"4 X 3 = 12\"\n[1] \"4 X 4 = 16\"\n[1] \"4 X 5 = 20\"\n[1] \"4 X 6 = 24\"\n[1] \"4 X 7 = 28\"\n[1] \"4 X 8 = 32\"\n[1] \"4 X 9 = 36\"\n[1] \"5 X 1 = 5\"\n[1] \"5 X 2 = 10\"\n[1] \"5 X 3 = 15\"\n[1] \"5 X 4 = 20\"\n[1] \"5 X 5 = 25\"\n[1] \"5 X 6 = 30\"\n[1] \"5 X 7 = 35\"\n[1] \"5 X 8 = 40\"\n[1] \"5 X 9 = 45\"\n[1] \"6 X 1 = 6\"\n[1] \"6 X 2 = 12\"\n[1] \"6 X 3 = 18\"\n[1] \"6 X 4 = 24\"\n[1] \"6 X 5 = 30\"\n[1] \"6 X 6 = 36\"\n[1] \"6 X 7 = 42\"\n[1] \"6 X 8 = 48\"\n[1] \"6 X 9 = 54\"\n[1] \"7 X 1 = 7\"\n[1] \"7 X 2 = 14\"\n[1] \"7 X 3 = 21\"\n[1] \"7 X 4 = 28\"\n[1] \"7 X 5 = 35\"\n[1] \"7 X 6 = 42\"\n[1] \"7 X 7 = 49\"\n[1] \"7 X 8 = 56\"\n[1] \"7 X 9 = 63\"\n[1] \"8 X 1 = 8\"\n[1] \"8 X 2 = 16\"\n[1] \"8 X 3 = 24\"\n[1] \"8 X 4 = 32\"\n[1] \"8 X 5 = 40\"\n[1] \"8 X 6 = 48\"\n[1] \"8 X 7 = 56\"\n[1] \"8 X 8 = 64\"\n[1] \"8 X 9 = 72\"\n[1] \"9 X 1 = 9\"\n[1] \"9 X 2 = 18\"\n[1] \"9 X 3 = 27\"\n[1] \"9 X 4 = 36\"\n[1] \"9 X 5 = 45\"\n[1] \"9 X 6 = 54\"\n[1] \"9 X 7 = 63\"\n[1] \"9 X 8 = 72\"\n[1] \"9 X 9 = 81\"\n\n# Print only even numbers from 1 to 10\nfor(i in 1:10) {\n   if(i%%2 == 0) {\n     print(i)\n   }\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n# Print decimal numbers from 1 to 10\nfor(i in 1:10) {\n   check = 0\n   for(j in 1:i) {\n     if(i%%j ==0) {\n       check = check+1\n     }\n   }\n   if(check ==2) {\n     print(i)\n   }\n}\n\n[1] 2\n[1] 3\n[1] 5\n[1] 7\n\n# data contains values over 100 and negative values\nstudents\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\nfor(i in 2:4) {\n   students[, i] = ifelse(students[, i]&gt;= 0 & students[, i]&lt;= 100,\n                          students[, i], NA)\n}\n\n\nstudents\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\n\n\n\nFunctions & Others\n\n\n\n# 03 User-defined function: Bundle the desired function #\nx=5\nfa = 1 # Variable to store the factorial value\nwhile(x&gt;1) { # loop while x is greater than 1\n  \n   fa = fa*x # Multiply the value of x by fa and store it back in fa\n   x = x-1 # Decrease x value by 1\n   x\n}\nfa\n\n[1] 120\n\nfact = function(x) { # The name of the function is fact, the input is x\n   fa = 1 # Variable to store the factorial value\n   while(x&gt;1) { # loop while x is greater than 1\n     fa = fa*x # Multiply the value of x by fa and store it back in fa\n     x = x-1 # Decrease x value by 1\n   }\n   return(fa) # returns the final calculated fa\n}\nfact(5) # Prints the result of calculating 5!\n\n[1] 120\n\nmy.is.na&lt;-function(x) { # Create a my.is.na function that combines the table(is.na()) functions into one\n   table(is.na(x))\n}\n\nmy.is.na(airquality) # This result is the same as table(is.na(airquality)).\n\n\nFALSE  TRUE \n  874    44 \n\ntable(is.na(airquality))\n\n\nFALSE  TRUE \n  874    44 \n\n# 04 Data Cleaning Example 1: Missing Value Handling #\n\n# Handling missing values using the is.na function\nstr(airquality) # Examine the structure of airquality data.\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n# NA in airquality data is indicated as TRUE, otherwise it is indicated as FALSE. There is a lot of data, so it is selected using the head function.\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nhead(is.na(airquality))\n\n     Ozone Solar.R  Wind  Temp Month   Day\n[1,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[5,]  TRUE    TRUE FALSE FALSE FALSE FALSE\n[6,] FALSE    TRUE FALSE FALSE FALSE FALSE\n\ntable(is.na(airquality)) # There are a total of 44 NAs.\n\n\nFALSE  TRUE \n  874    44 \n\nsum(is.na(airquality)) # There are a total of 44 NAs.\n\n[1] 44\n\ntable(is.na(airquality$Temp)) # Confirms that there is no NA in Temp.\n\n\nFALSE \n  153 \n\ntable(is.na(airquality$Ozone)) # 37 NAs found in Ozone.\n\n\nFALSE  TRUE \n  116    37 \n\nmean(airquality$Temp) # Temp without NA is averaged.\n\n[1] 77.9\n\nmean(airquality$Ozone) # Ozone with NA has an average of NA.\n\n[1] NA\n\nair_narm = airquality[!is.na(airquality$Ozone), ] # Extract only values without NA from the Ozone attribute.\nair_narm\n\n    Ozone Solar.R Wind Temp Month Day\n1      41     190  7.4   67     5   1\n2      36     118  8.0   72     5   2\n3      12     149 12.6   74     5   3\n4      18     313 11.5   62     5   4\n6      28      NA 14.9   66     5   6\n7      23     299  8.6   65     5   7\n8      19      99 13.8   59     5   8\n9       8      19 20.1   61     5   9\n11      7      NA  6.9   74     5  11\n12     16     256  9.7   69     5  12\n13     11     290  9.2   66     5  13\n14     14     274 10.9   68     5  14\n15     18      65 13.2   58     5  15\n16     14     334 11.5   64     5  16\n17     34     307 12.0   66     5  17\n18      6      78 18.4   57     5  18\n19     30     322 11.5   68     5  19\n20     11      44  9.7   62     5  20\n21      1       8  9.7   59     5  21\n22     11     320 16.6   73     5  22\n23      4      25  9.7   61     5  23\n24     32      92 12.0   61     5  24\n28     23      13 12.0   67     5  28\n29     45     252 14.9   81     5  29\n30    115     223  5.7   79     5  30\n31     37     279  7.4   76     5  31\n38     29     127  9.7   82     6   7\n40     71     291 13.8   90     6   9\n41     39     323 11.5   87     6  10\n44     23     148  8.0   82     6  13\n47     21     191 14.9   77     6  16\n48     37     284 20.7   72     6  17\n49     20      37  9.2   65     6  18\n50     12     120 11.5   73     6  19\n51     13     137 10.3   76     6  20\n62    135     269  4.1   84     7   1\n63     49     248  9.2   85     7   2\n64     32     236  9.2   81     7   3\n66     64     175  4.6   83     7   5\n67     40     314 10.9   83     7   6\n68     77     276  5.1   88     7   7\n69     97     267  6.3   92     7   8\n70     97     272  5.7   92     7   9\n71     85     175  7.4   89     7  10\n73     10     264 14.3   73     7  12\n74     27     175 14.9   81     7  13\n76      7      48 14.3   80     7  15\n77     48     260  6.9   81     7  16\n78     35     274 10.3   82     7  17\n79     61     285  6.3   84     7  18\n80     79     187  5.1   87     7  19\n81     63     220 11.5   85     7  20\n82     16       7  6.9   74     7  21\n85     80     294  8.6   86     7  24\n86    108     223  8.0   85     7  25\n87     20      81  8.6   82     7  26\n88     52      82 12.0   86     7  27\n89     82     213  7.4   88     7  28\n90     50     275  7.4   86     7  29\n91     64     253  7.4   83     7  30\n92     59     254  9.2   81     7  31\n93     39      83  6.9   81     8   1\n94      9      24 13.8   81     8   2\n95     16      77  7.4   82     8   3\n96     78      NA  6.9   86     8   4\n97     35      NA  7.4   85     8   5\n98     66      NA  4.6   87     8   6\n99    122     255  4.0   89     8   7\n100    89     229 10.3   90     8   8\n101   110     207  8.0   90     8   9\n104    44     192 11.5   86     8  12\n105    28     273 11.5   82     8  13\n106    65     157  9.7   80     8  14\n108    22      71 10.3   77     8  16\n109    59      51  6.3   79     8  17\n110    23     115  7.4   76     8  18\n111    31     244 10.9   78     8  19\n112    44     190 10.3   78     8  20\n113    21     259 15.5   77     8  21\n114     9      36 14.3   72     8  22\n116    45     212  9.7   79     8  24\n117   168     238  3.4   81     8  25\n118    73     215  8.0   86     8  26\n120    76     203  9.7   97     8  28\n121   118     225  2.3   94     8  29\n122    84     237  6.3   96     8  30\n123    85     188  6.3   94     8  31\n124    96     167  6.9   91     9   1\n125    78     197  5.1   92     9   2\n126    73     183  2.8   93     9   3\n127    91     189  4.6   93     9   4\n128    47      95  7.4   87     9   5\n129    32      92 15.5   84     9   6\n130    20     252 10.9   80     9   7\n131    23     220 10.3   78     9   8\n132    21     230 10.9   75     9   9\n133    24     259  9.7   73     9  10\n134    44     236 14.9   81     9  11\n135    21     259 15.5   76     9  12\n136    28     238  6.3   77     9  13\n137     9      24 10.9   71     9  14\n138    13     112 11.5   71     9  15\n139    46     237  6.9   78     9  16\n140    18     224 13.8   67     9  17\n141    13      27 10.3   76     9  18\n142    24     238 10.3   68     9  19\n143    16     201  8.0   82     9  20\n144    13     238 12.6   64     9  21\n145    23      14  9.2   71     9  22\n146    36     139 10.3   81     9  23\n147     7      49 10.3   69     9  24\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\nmean(air_narm$Ozone) # The mean function operates normally in data with missing values removed.\n\n[1] 42.1\n\n# Handling missing values using the na.omit function\nair_narm1 = na.omit(airquality)\nmean(air_narm1$Ozone)\n\n[1] 42.1\n\n# Handling missing values using the function property na.rm\nmean(airquality$Ozone, na.rm = T)\n\n[1] 42.1\n\nmean(airquality$Ozone, na.rm = F)\n\n[1] NA\n\ntable(is.na(airquality))\n\n\nFALSE  TRUE \n  874    44 \n\ntable(is.na(airquality$Ozone))\n\n\nFALSE  TRUE \n  116    37 \n\ntable(is.na(airquality$Solar.R))\n\n\nFALSE  TRUE \n  146     7 \n\nair_narm = airquality[!is.na(airquality$Ozone) & !is.na(airquality$Solar.R), ]\nmean(air_narm$Ozone)\n\n[1] 42.1\n\n\n\n# 05 Data Cleansing Example 2: Outlier Processing #\n\n# Patient data containing outliers\npatients = data.frame(name = c(\"Patient 1\", \"Patient 2\", \"Patient 3\", \"Patient 4\", \"Patient 5\"), age = c(22, 20, 25, 30, 27) , gender=factor(c(\"M\", \"F\", \"M\", \"K\", \"F\")), blood.type = factor(c(\"A\", \"O\", \"B\", \" AB\", \"C\")))\npatients\n\n       name age gender blood.type\n1 Patient 1  22      M          A\n2 Patient 2  20      F          O\n3 Patient 3  25      M          B\n4 Patient 4  30      K         AB\n5 Patient 5  27      F          C\n\n# Remove outliers from gender\npatients_outrm = patients[patients$gender==\"M\"|patients$gender==\"F\", ]\npatients_outrm\n\n       name age gender blood.type\n1 Patient 1  22      M          A\n2 Patient 2  20      F          O\n3 Patient 3  25      M          B\n5 Patient 5  27      F          C\n\n# Remove outliers from gender and blood type\npatients_outrm1 = patients[(patients$gender == \"M\"|patients$gender == \"F\") &\n                              (patients$blood.type == \"A\" |\n                                 patients$blood.type == \"B\"|\n                                 patients$blood.type == \"O\"|\n                                 patients$blood.type == \"AB\"), ]\npatients_outrm1\n\n       name age gender blood.type\n1 Patient 1  22      M          A\n2 Patient 2  20      F          O\n3 Patient 3  25      M          B\n\n# Patient data containing outliers\npatients = data.frame(name = c(\"Patient 1\", \"Patient 2\", \"Patient 3\", \"Patient 4\", \"Patient 5\"),\n                       age = c(22, 20, 25, 30, 27),\n                       gender = c(1, 2, 1, 3, 2),\n                       blood.type = c(1, 3, 2, 4, 5))\npatients\n\n       name age gender blood.type\n1 Patient 1  22      1          1\n2 Patient 2  20      2          3\n3 Patient 3  25      1          2\n4 Patient 4  30      3          4\n5 Patient 5  27      2          5\n\n# Change outliers in gender to missing values\npatients$gender = ifelse((patients$gender&lt;1|patients$gender&gt;2), NA, patients$gender)\npatients\n\n       name age gender blood.type\n1 Patient 1  22      1          1\n2 Patient 2  20      2          3\n3 Patient 3  25      1          2\n4 Patient 4  30     NA          4\n5 Patient 5  27      2          5\n\n# Change outlier values in the penalty type to missing values\npatients$blood.type = ifelse((patients$blood.type&lt;1|patients$blood.type&gt;4), NA,\n                              patients$blood.type)\npatients\n\n       name age gender blood.type\n1 Patient 1  22      1          1\n2 Patient 2  20      2          3\n3 Patient 3  25      1          2\n4 Patient 4  30     NA          4\n5 Patient 5  27      2         NA\n\n# Remove all missing values\npatients[!is.na(patients$gender)&!is.na(patients$blood.type), ]\n\n       name age gender blood.type\n1 Patient 1  22      1          1\n2 Patient 2  20      2          3\n3 Patient 3  25      1          2\n\nboxplot(airquality[, c(1:4)]) # boxplot for Ozone, Solar.R, Wind, Temp\n\n\n\nboxplot(airquality[, 1])$stats # Calculate Ozone's boxplot statistics\n\n\n\n\n      [,1]\n[1,]   1.0\n[2,]  18.0\n[3,]  31.5\n[4,]  63.5\n[5,] 122.0\n\nair = airquality # Copy airquality data to temporary storage variable\ntable(is.na(air$Ozone)) # Check the current number of NAs in Ozone\n\n\nFALSE  TRUE \n  116    37 \n\n# Change outliers to NA\nair$Ozone = ifelse(air$Ozone&lt;1|air$Ozone&gt;122, NA, air$Ozone)\ntable(is.na(air$Ozone)) # Check the number of NAs after processing outliers (increased by 2)\n\n\nFALSE  TRUE \n  114    39 \n\n# Remove NA\nair_narm = air[!is.na(air$Ozone), ]\nmean(air_narm$Ozone) # By removing two outliers, the value is reduced compared to the result using the is.na function.\n\n[1] 40.2\n\n\n\n\n\n\nClass\n\n\n\n\n\nload(\"data/List_KMP.RData\")\nnames(List.KMP) &lt;- c(\"p17\", \"p18\", \"p19\", \"d19\")\np17_df &lt;- List.KMP[[\"p17\"]]\np18_df &lt;- List.KMP[[\"p18\"]]\np19_df &lt;- List.KMP[[\"p19\"]]\n\n\n\n\n\n\n\n\n\n# create a function of calculating average and standard deviation of a vector\n\ncal_avg_sd&lt;-function(x){\n\n  avg.x&lt;-mean(x, na.rm=T)\n  sd.x &lt;-sd(x, na.rm=T)\n  out.vector&lt;-c(avg=avg.x, sd=sd.x)\n  return(round(out.vector,2))\n\n}\n\nLet’s apply this function to dataset ‘p17’\n\n# Let's try this function\ncal_avg_sd(p17_df$sp.mobile)\n\n avg   sd \n38.9 21.8 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoop, Function, and Data Manipulation in R: A Dive into the Palmer Penguin Dataset\nIn the world of data analysis with R, mastering the basics of loops, functions, and data manipulation is essential for any aspiring data scientist. While the tidyverse collection of packages offers powerful tools for these tasks, it’s crucial to first understand the foundational techniques that underpin effective data analysis. This week, we embark on a journey with the Palmer Penguin dataset, focusing on traditional R approaches, saving the tidyverse exploration for our next installment.\n\nThe Palmer Penguin Dataset: A Brief History\nThe Palmer Penguin dataset, introduced by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, provides a compelling alternative to the classic Iris dataset for data exploration and visualization. Comprising data on 344 penguins across three species (Adélie, Gentoo, and Chinstrap) from the Palmer Archipelago in Antarctica, the dataset includes variables such as species, island, bill length and depth, flipper length, body mass, and sex.\n\nThis dataset not only offers rich insights into the biological diversity of Antarctic penguins but also serves as an excellent resource for teaching data science techniques due to its manageable size and complexity.\n\n\nData Manipulation with Base R\nBefore diving into complex manipulations, let’s start by loading the Palmer Penguin dataset. Although it’s available through the palmerpenguins package, we’ll keep our focus on base R functions for this exploration.\n\n\nLoading the Dataset\n\n# Assuming palmerpenguins is installed\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nWith the data loaded, let’s proceed to some basic manipulations using base R functions.\n\npalmerpenguins::penguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nSee more information on this dataset: https://allisonhorst.github.io/palmerpenguins/\n\n\n\nSubsetting Data\nSubsetting is crucial for isolating parts of the dataset for analysis. In base R, we can use the subset function or the [ operator.\n\n# Subsetting to include only Adélie penguins\nadelie_penguins &lt;- subset(penguins, species == \"Adelie\")\n\n# Alternatively, using the bracket operator\nadelie_penguins &lt;- penguins[penguins$species == \"Adelie\", ]\n\n\n\nHandling Missing Values\nMissing data can skew analysis, making its identification and treatment essential.\n\n# Identifying missing values\nsum(is.na(penguins))\n\n[1] 19\n\n# Removing rows with any missing value\npenguins_clean &lt;- na.omit(penguins)\n\n\n\nCreating Custom Functions\nCustom functions in R amplify the power of data manipulation by encapsulating repetitive tasks.\n\n# A function to summarize penguin measurements\nsummarize_measurements &lt;- function(data) {\n  summary &lt;- data.frame(\n    Mean_FlipperLength = mean(data$flipper_length_mm, na.rm = TRUE),\n    SD_FlipperLength = sd(data$flipper_length_mm, na.rm = TRUE),\n    Mean_BillLength = mean(data$bill_length_mm, na.rm = TRUE),\n    SD_BillLength = sd(data$bill_length_mm, na.rm = TRUE)\n  )\n  return(summary)\n}\n\n# Applying the function to Adélie penguins\nadelie_summary &lt;- summarize_measurements(adelie_penguins)\nadelie_summary\n\n  Mean_FlipperLength SD_FlipperLength Mean_BillLength SD_BillLength\n1                190             6.54            38.8          2.66\n\n\n\n\nLooping Through Data\nLoops are essential for iterative operations. In R, for loops allow us to apply operations across elements, rows, or columns in a dataset.\n\n# Calculating mean body mass for each species\nspecies_list &lt;- unique(penguins$species)\n# Create an empty vector\nmean_mass_by_species &lt;- numeric(0)\n\nfor (i in seq_along(species_list)) {\n  species_data &lt;- subset(penguins, species == species_list[i])\n  mean_mass_by_species[i] &lt;- mean(species_data$body_mass_g, na.rm = TRUE)\n}\n\nnames(mean_mass_by_species) &lt;- species_list\nmean_mass_by_species\n\n   Adelie    Gentoo Chinstrap \n     3701      5076      3733 \n\n\nThis code iterates through each species in the dataset, calculating and storing the mean body mass.\nLet’s use customized function instead.\n\nspecies_list &lt;- unique(penguins$species)\n# Create an empty list\nsummary_by_species &lt;- list(0)\n\nfor (i in seq_along(species_list)) {\n  species_data &lt;- subset(penguins, species == species_list[i])\n  summary_by_species[[i]] &lt;- summarize_measurements(species_data)\n}\n\nnames(summary_by_species) &lt;- species_list\nsummary_by_species\n\n$Adelie\n  Mean_FlipperLength SD_FlipperLength Mean_BillLength SD_BillLength\n1                190             6.54            38.8          2.66\n\n$Gentoo\n  Mean_FlipperLength SD_FlipperLength Mean_BillLength SD_BillLength\n1                217             6.48            47.5          3.08\n\n$Chinstrap\n  Mean_FlipperLength SD_FlipperLength Mean_BillLength SD_BillLength\n1                196             7.13            48.8          3.34\n\n\n\n\n\n\nExploration with tidyverse\nLet’s enhance our exploration of the Palmer Penguin dataset by paralleling our original code snippets with their tidyverse counterparts. The tidyverse is a collection of R packages designed for data science that makes data manipulation, exploration, and visualization easier and more intuitive.\n\nLoading the Dataset with tidyverse\nFirst, let’s ensure we have the tidyverse and palmerpenguins packages loaded. If you haven’t installed these packages, you can do so using install.packages(\"tidyverse\") and install.packages(\"palmerpenguins\").\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\n\n\nSubsetting Data with dplyr\nWith dplyr, subsetting becomes more readable:\n\n# Subsetting to include only Adélie penguins using dplyr\nadelie_penguins &lt;- penguins %&gt;%\n  filter(species == \"Adelie\")\n\n\n\nHandling Missing Values with tidyverse\nThe tidyverse provides a straightforward approach to dealing with missing values:\n\n# Removing rows with any missing value using dplyr\npenguins_clean &lt;- penguins %&gt;%\n  drop_na()\n\n\n\nCreating Custom Functions and Applying them into dataset\nWhile base R functions are powerful, integrating them with tidyverse functionalities can make your workflows even more efficient:\n\n# Using dplyr and purrr to summarize measurements\nsummarize_measurements &lt;- function(data) {\n  data %&gt;%\n    summarise(Mean_FlipperLength = mean(flipper_length_mm, na.rm = TRUE),\n              SD_FlipperLength = sd(flipper_length_mm, na.rm = TRUE),\n              Mean_BillLength = mean(bill_length_mm, na.rm = TRUE),\n              SD_BillLength = sd(bill_length_mm, na.rm = TRUE))\n}\n\n# Applying the function to Adélie penguins\nadelie_summary &lt;- adelie_penguins %&gt;%\n  summarize_measurements()\nadelie_summary\n\n# A tibble: 1 × 4\n  Mean_FlipperLength SD_FlipperLength Mean_BillLength SD_BillLength\n               &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1               190.             6.54            38.8          2.66\n\n\n\n\nLooping Through Data with group_by and summarise\n\n# Calculating mean body mass for each species with dplyr\nmean_mass_by_species &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(MeanBodyMass = mean(body_mass_g, na.rm = TRUE))\n\nmean_mass_by_species\n\n# A tibble: 3 × 2\n  species   MeanBodyMass\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie           3701.\n2 Chinstrap        3733.\n3 Gentoo           5076.\n\n\nBy incorporating tidyverse techniques, we can make our code more concise and readable, especially for those new to programming or R. The tidyverse syntax is designed to be intuitive, allowing you to more easily understand and articulate what your code is doing, which is particularly beneficial when sharing your work with others or when collaborating on data science projects.\n\n\n\n\nUsing purrr for Advanced Data Manipulation\npurrr enhances functional programming within the tidyverse ecosystem, providing tools for working effectively with lists and functional programming paradigms. Here’s how we could use purrr in conjunction with dplyr for a task similar to our mean body mass calculation:\n\nCalculating Mean Body Mass by Species with purrr\nWe can use purrr’s map functions to apply operations across elements in a list, which is particularly useful for more complex or nested operations. While the direct calculation of mean body mass by species is more straightforward with dplyr alone, let’s consider a scenario where purrr demonstrates its utility:\n\nlibrary(tidyverse)\n# Splitting the data by species\nspecies_split &lt;- split(penguins, penguins$species)\n\n# Calculating mean body mass for each species using purrr\nmean_mass_by_species &lt;- map_dfr(species_split, \n                                ~summarise(.x, MeanBodyMass = mean(body_mass_g, na.rm = TRUE)))\n\nmean_mass_by_species\n\n# A tibble: 3 × 1\n  MeanBodyMass\n         &lt;dbl&gt;\n1        3701.\n2        3733.\n3        5076."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/15_week.html",
    "href": "teaching/ds101/weekly/posts/15_week.html",
    "title": "Team Consulting",
    "section": "",
    "text": "Weekly design"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/15_week.html#for-the-domestic-students",
    "href": "teaching/ds101/weekly/posts/15_week.html#for-the-domestic-students",
    "title": "Team Consulting",
    "section": "For the domestic students",
    "text": "For the domestic students\n\n최종 프로젝트 발표\n\nDate: 14 June (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)\n\nFinal Report Submission\n\n제출 링크: https://forms.gle/TmQfALks1cHwQ6P3A\n파일을 올려야 하기 때문에 한양대 계정으로 로그인 해야함.\n파일 이름: 오전_1조.zip (zip으로 압축해서 업로드)\n\n동료평가 제출\n\n14일 자정까지 꼭 제출해주세요\n제출 링크: https://forms.gle/8iXa4sbjGtEVa56q8"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/15_week.html#for-the-exchange-students",
    "href": "teaching/ds101/weekly/posts/15_week.html#for-the-exchange-students",
    "title": "Team Consulting",
    "section": "For the exchange students",
    "text": "For the exchange students\n\nPlease submit your final output by this day (14 June).\nSubmission link: https://forms.gle/TmQfALks1cHwQ6P3A\n\nFile name: your_name.zip (make it zip)\nTo upload a file, you must log in with your Hanyang University account."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#week-1-class",
    "href": "teaching/ds101/weekly/posts/01_week.html#week-1-class",
    "title": "Course Intro",
    "section": "Week 1 Class",
    "text": "Week 1 Class\n\n\nWhy R?\n3 Reasons to learn R programming for data science\nR isn't a general purpose language, but depending on where or how you plan to work, it could offer a lot of perks that aren't available with a general purpose language.\n1. R is built for statistics: Heavy statistical analysis is possible with Python, but you won't get the syntax-specific libraries and functions as you do with R. The language makes it much more intuitive to build and communicate results from these specific types of programs. Statisticians and data analysts use R to manage large datasets more easily using standard machine learning models and data mining.\n2. R is academic: R is almost a default for working in academia. R is well suited for a subfield of machine learning known as statistical learning. Anyone with a formal statistics background should recognize the syntax and construction of R.\n3. R is intuitive for analysis: R may not work with a wide variety of projects, but it is the best choice for analysis and inference work. If you plan to work in a specialized field, you'll want a specialized programming language. R also offers a powerful environment ideally suited to the types of data visualizations data scientists employ.\nSource: https://www.edx.org/resources/r-vs-python-for-data-science-explainer-learning-tips\n\n\n\nInstall gadgets\n\nInstall R, R Studio, & Rtools\n\nR\n\nFor window https://cran.r-project.org/bin/windows/base/\nFor mac https://cran.r-project.org/bin/macosx/\n\nR Studio\n\nhttps://posit.co/download/rstudio-desktop/\n\nRtools (only for window user)\n\nhttps://cran.r-project.org/bin/windows/Rtools/\nTo install R packages containing C/C++ language for window users (no need for Linux and Mac users)\n\n\nThings you need to know\n\nDon’t Use OneDrive.\n\nUse Github instead\nMany people get an error when installing because of OneDrive\n\nSet Windows user name to English\n\nIf Korean characters are mixed in the installation path, there is a high probability of error occurrence\n\n\nInstallation Order\n\nStep 1 ‑ Download the file\n\nDownload R, Rtools, Rstudio installation files\n\nStep 2 - Install R\n\nUnified installation path: All will be installed in the C:/R folder\nRun in administrator mode when running the R installation file\nAfter installing R, grant write permission to the R folder, Right-click and turn off read only\n\nStep 3 ‑ Install Rtools\n\nAdministrator mode execution installation and folder setting as C:\\R\\rtools40\nCreate environment variable RTOOLS40_HOME after installation: Value - C:\\R\\rtools40\\\nAdd %RTOOLS40_HOME%\\usr\\bin\\ to the Path variable.\n\nStep 4 - Install Rstudio\n\nRight-click and run as administrator - installation path C:\\R\\Rstudio\nCheck rtools connection with Sys.which(“make”) command after installation"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html",
    "href": "teaching/ds101/weekly/posts/11_week.html",
    "title": "No Class",
    "section": "",
    "text": "It contains the analysis process of ‘Titanic’, one of the representative ‘Competition’ of ‘Kaggle’."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/16_week.html",
    "href": "teaching/ds101/weekly/posts/16_week.html",
    "title": "Final Presentation",
    "section": "",
    "text": "Weekly design\n\nFinal Output (by team)\n\nSubmission link: https://forms.gle/ALjKSDVWxoTQ58ev5\n\n\nPeer Review & Class Survey (by individual)\n\nSubmission Link: https://forms.gle/z7oMVNLcGf2QA3tVA\n\n\nTeams\n\n윤승현(경영), 최경석(스포츠과학), 박재원(DS), 임서영(미술), 정하경(국국)\n안제민(DS), 이채빈(DS), 김지희(DS), 이윤서(DS), 유선아(AI)\n민범기(CT), 이시연(영상), 김경서(영상), 김지연(소비자), 이지우(CT)\n손채리(한교), 이수아(글경), 최재원(통계), 윤지우(독독), 김서영(CT)\n문정은(경영), 박우혁(DS), 이상훈(DS), 양제니(CT)\n정은채(AI), 강윤경(AI), 이혜연(DS), 우정현(CT), 김민기(유동)\n송준석(CT), 이자의(CT), 최은서(CT), 홍은지(소비자), 이지원(통계)\n\n\nOutput: Data analysis report & Presentation Video\nReport\n\nAny format is possible (PPT, word, notion web page link, pdf, and so on).\n\n\nReport example (but not limited)\n\nData Preparing (or Collecting)\nExplore data (Descriptive stats)\nSet your hypothesis (or research questions)\nVisualize data to confirm your hypo or RQs\nExplain your findings\nExpanding your findings to implications\n\n\n\nPresentation Video\n\nMaking videos for 10 mins presentation (in any language),\n\n\nSubmit a Youtube link"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/16_week.html#for-the-domestic-students",
    "href": "teaching/ds101/weekly/posts/16_week.html#for-the-domestic-students",
    "title": "Final Presentation",
    "section": "For the domestic students",
    "text": "For the domestic students\n\n최종 프로젝트 발표\n\nDate: 14 June (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)\n\nFinal Report Submission\n\n제출 링크: https://forms.gle/TmQfALks1cHwQ6P3A\n파일을 올려야 하기 때문에 한양대 계정으로 로그인 해야함.\n파일 이름: 오전_1조.zip (zip으로 압축해서 업로드)\n\n동료평가 제출\n\n14일 자정까지 꼭 제출해주세요\n제출 링크: https://forms.gle/8iXa4sbjGtEVa56q8"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/16_week.html#for-the-exchange-students",
    "href": "teaching/ds101/weekly/posts/16_week.html#for-the-exchange-students",
    "title": "Final Presentation",
    "section": "For the exchange students",
    "text": "For the exchange students\n\nPlease submit your final output by this day (14 June).\nSubmission link: https://forms.gle/TmQfALks1cHwQ6P3A\n\nFile name: your_name.zip (make it zip)\nTo upload a file, you must log in with your Hanyang University account."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/14_week.html",
    "href": "teaching/ds101/weekly/posts/14_week.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nGet started with Quarto in R\n\n\n\n\n\n\nClass\n\nWe use this material for the class\nhttps://r4ds.hadley.nz/quarto\n\nQuarto?\nQuarto provides a unified authoring framework for data science, combining your code, its results, and your prose. Quarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, presentations, and more.\nQuarto basics\nThis is a Quarto file – a plain text file that has the extension .qmd:\n\nIt contains three important types of content:\n\nAn (optional) YAML header surrounded by ---s.\nChunks of R code surrounded by ```.\nText mixed with simple text formatting like # heading and _italics_.\n\nDiagram of Quarto workflow from qmd, to knitr, to md, to pandoc, to output in PDF, MS Word, or HTML formats.\n\n\n\nSource Editor\n\n\nCode Chunks\n\n\n\n\nFigures and Tables\n\nknitr::kable(mtcars[1:5, ], )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\nCaching\n\n\nYAML Ain’t Markup Language\n\nYou can create … using quarto.\n\na website\na presentation\na pdf document\n\n\nFor a website:\n\nhttps://quarto.org/docs/websites/\n\nFor a presentation:\n\nhttps://quarto.org/docs/presentations/\n\nLet’s explore more galleries using quarto\n\nhttps://quarto.org/docs/gallery/\nhttps://github.com/quarto-dev/quarto-web/tree/main/docs/presentations"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html",
    "href": "teaching/ds101/weekly/posts/12_week.html",
    "title": "Interactive Web: Shiny",
    "section": "",
    "text": "Weekly design"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#introduction",
    "href": "teaching/ds101/weekly/posts/12_week.html#introduction",
    "title": "Interactive Web: Shiny",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nIt contains the analysis process of ‘Titanic’, one of the representative ‘Competition’ of ‘Kaggle’."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#prepare-to-work",
    "href": "teaching/ds101/weekly/posts/12_week.html#prepare-to-work",
    "title": "Interactive Web: Shiny",
    "section": "2. Prepare to work",
    "text": "2. Prepare to work\n\n\n2.1 Packages\nsee “What is a package in R”\n\nThis is the process of loading (loading) the Packages I used for analysis, in addition to the representative Packages of R, such as tidyverse (including ggplot2 and dplyr).\n\n\n# Data input, assesment \nlibrary(titanic)\nlibrary(readr)           # Data input with readr::read_csv()\nlibrary(descr)           # descr::CrossTable() - Frequency by category, check with ratio figures\n\n# Visualization\nlibrary(VIM)             # Missing values assesment used by VIM::aggr()\nlibrary(RColorBrewer)    # Plot color setting\nlibrary(scales)          # plot setting - x, y axis\n\n# Feature engineering, Data Pre-processing\nlibrary(tidyverse)     # dplyr, ggplot2, purrr, etc..      # Feature Engineering & Data Pre-processing\nlibrary(ggpubr)\n\nlibrary(randomForest)\n# Model validation \nlibrary(caret)           # caret::confusionMatrix()\nlibrary(ROCR)            # Plotting ROC Curve\n\n\n\n\n2.2 Raw data import\n\nIn titanic competition, train data used to create Model and test data used for actual prediction (estimation) are separated.\nHere, we will load those two data and combine them into one. The reason for tying the separate data together is to work the same when feature engineering and pre-processing the input variables used in modeling.\nPlease see this link if you want to know about the story of Titanic.\n\n\ntitanic_train %&gt;% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\ntitanic_test %&gt;% glimpse\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;int&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"B45\", \"\",…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\ntrain &lt;- titanic_train\ntest  &lt;- titanic_test\n\nfull &lt;- dplyr::bind_rows(train, test)\nfull %&gt;% glimpse\n\nRows: 1,309\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nThe reason why rbind() was not used even when combining the two data into full is that Survived, the dependent variable (target variable, Y) of Titanic competition, does not exist in test. Therefore, the dimensions (dimension) of the two data do not match, so they are not merged with rbind(). However, if you use dplyr::bind_rows(), Survived in test is treated as NA and merged into one.\n\n\n2.3 variable meaning explanation\n\n\n\n\n\n\n\n\nvariable name\nInterpretation (meaning)\nType\n\n\n\n\nPassengerID\nUnique ID number that identifies the passenger\nInt\n\n\nSurvived\nIndicates whether or not the passenger survived. Survival is 1 and death is 0.\nFactor\n\n\nPclass\nThe class of the cabin, with 3 categories from 1st class (1) to 3rd class (3).\nOrd.Factor\n\n\nName\nPassenger’s name\nFactor\n\n\nSex\nPassenger’s gender\nFactor\n\n\nAge\nAge of passenger\nNumeric\n\n\nSibSp\nVariable describing the number of siblings or spouses accompanying each passenger. It can range from 0 to 8.\nInteger\n\n\nParch\nVariable describing the number of parents or children accompanying each passenger, from 0 to 9.\nInteger\n\n\nTicket\nString variable for the ticket the passenger boarded\nFactor\n\n\nFare\nVariable for how much the passenger has paid for the trip so far\nNumeric\n\n\nCabin\nVariable that distinguishes each passenger’s cabin, with too many categories and missing values.\nFactor\n\n\nEmbarked\nIndicates the boarding port and departure port, and consists of three categories: C, Q, and S.\nFactor\n\n\n\n\n\n\n2.4 Change the variables type\n\nBefore the full-scale EDA and feature engineering, let’s transform some variable properties. For example, Pclass is treated as numeric, but actually 1, 2, 3 are factors representing 1st, 2nd, and 3rd grades.\n\n\nfull &lt;- full %&gt;%\n  dplyr::mutate(Survived = factor(Survived),\n                Pclass   = factor(Pclass, ordered = T),\n                Name     = factor(Name),\n                Sex      = factor(Sex),\n                Ticket   = factor(Ticket),\n                Cabin    = factor(Cabin),\n                Embarked = factor(Embarked))"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#eda-exploratory-data-analysis",
    "href": "teaching/ds101/weekly/posts/12_week.html#eda-exploratory-data-analysis",
    "title": "Interactive Web: Shiny",
    "section": "3. EDA : Exploratory data analysis",
    "text": "3. EDA : Exploratory data analysis\n\nIt is the process of exploring and understanding raw data, such as how data is structured and whether there are missing values or outliers in it.\nWe will use various functions and visualizations here.\n\n\n3.1 Data confirmation using numerical values\nFirst of all, let’s check the data through the output of various functions such as head() and summary().\n\n\n3.1.1 head()\n\n\nhead(full, 10)\n\n   PassengerId Survived Pclass\n1            1        0      3\n2            2        1      1\n3            3        1      3\n4            4        1      1\n5            5        0      3\n6            6        0      3\n7            7        0      1\n8            8        0      3\n9            9        1      3\n10          10        1      2\n                                                  Name    Sex Age SibSp Parch\n1                              Braund, Mr. Owen Harris   male  22     1     0\n2  Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                               Heikkinen, Miss. Laina female  26     0     0\n4         Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                             Allen, Mr. William Henry   male  35     0     0\n6                                     Moran, Mr. James   male  NA     0     0\n7                              McCarthy, Mr. Timothy J   male  54     0     0\n8                       Palsson, Master. Gosta Leonard   male   2     3     1\n9    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female  27     0     2\n10                 Nasser, Mrs. Nicholas (Adele Achem) female  14     1     0\n             Ticket    Fare Cabin Embarked\n1         A/5 21171  7.2500              S\n2          PC 17599 71.2833   C85        C\n3  STON/O2. 3101282  7.9250              S\n4            113803 53.1000  C123        S\n5            373450  8.0500              S\n6            330877  8.4583              Q\n7             17463 51.8625   E46        S\n8            349909 21.0750              S\n9            347742 11.1333              S\n10           237736 30.0708              C\n\n\n\nLooking at the result of head(), we can see that there is a missing value (NA) in Age.\nIf so, is there only Age missing in the entire data?\nFor the answer, please refer to 3.2 Missing values.\n\n\n\n3.1.2 str()\n\n\nstr(full)\n\n'data.frame':   1309 obs. of  12 variables:\n $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass     : Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : Factor w/ 1307 levels \"Abbing, Mr. Anthony\",..: 156 287 531 430 23 826 775 922 613 855 ...\n $ Sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : Factor w/ 929 levels \"110152\",\"110413\",..: 721 817 915 66 650 374 110 542 478 175 ...\n $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 108 1 72 1 1 165 1 1 1 ...\n $ Embarked   : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 2 4 4 4 3 4 4 4 2 ...\n\n\n\nBy combining the train and test data, the total number of observations (record, row, row) is 1309 (train: 891, test: 418), and the number of variables (column, feature, variable, column) is 12.\nIn addition, you can find out what the attributes of each variable are and how many categories there are for variables that are factor attributes.\nIn addition, in head(), it can be seen that the missing value (NA), which was thought to exist only in Age, also exists in other variables including Cabin.\n\n\n\n3.1.3 summary()\n\n\nsummary(full)\n\n  PassengerId   Survived   Pclass                                Name     \n Min.   :   1   0   :549   1:323   Connolly, Miss. Kate            :   2  \n 1st Qu.: 328   1   :342   2:277   Kelly, Mr. James                :   2  \n Median : 655   NA's:418   3:709   Abbing, Mr. Anthony             :   1  \n Mean   : 655                      Abbott, Master. Eugene Joseph   :   1  \n 3rd Qu.: 982                      Abbott, Mr. Rossmore Edward     :   1  \n Max.   :1309                      Abbott, Mrs. Stanton (Rosa Hunt):   1  \n                                   (Other)                         :1301  \n     Sex           Age            SibSp            Parch            Ticket    \n female:466   Min.   : 0.17   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n male  :843   1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n              Median :28.00   Median :0.0000   Median :0.000   CA 2144 :   8  \n              Mean   :29.88   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n              3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n              Max.   :80.00   Max.   :8.0000   Max.   :9.000   347082  :   7  \n              NA's   :263                                      (Other) :1261  \n      Fare                     Cabin      Embarked\n Min.   :  0.000                  :1014    :  2   \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270   \n Median : 14.454   B57 B59 B63 B66:   5   Q:123   \n Mean   : 33.295   G6             :   5   S:914   \n 3rd Qu.: 31.275   B96 B98        :   4           \n Max.   :512.329   C22 C26        :   4           \n NA's   :1         (Other)        : 271           \n\n\n\nsummary() provides a lot of information about the data.\nThe representative values of quantitative variables (Integer, Numeric), the number of categories of categorical (Factor) variables, and the number of observations belonging to each category are all shown as numerical values.\nHere are the things to check and move on:\n\nSurvived: This is the target variable for this competition, and 418 missing values are due to the test data.\nPclass: There are three categories of 1st class, 2nd class, and 3rd class, and 3rd class passengers are the most.\nName: There are people with similar names. So you can see that some passengers are traveling alone, while others are traveling with their families.\nSex: There are almost twice as many males as females.\nAge: It ranges from 0.17 to 80 years old, but it seems necessary to check whether it is an outlier that incorrectly entered 17, and there are 263 missing values.\nSibSp: From 0 to 8, and the 3rd quartile is 1, so it can be seen that you boarded the Titanic with a couple or siblings.\nParch: It ranges from 0 to 9, but the fact that the 3rd quartile is 0 indicates that there are very few passengers with parents and children.\nBoth SibSp and Parch are variables representing family relationships. Through this, we will find out the total number of people in the family, although we do not know who was on board, and based on that, we will create a categorical derived variable called FamilySized that represents the size of the family.\nTicket: Looking at the result of 3.1.2 str(), you can see that some passengers have exactly the same ticket, some passengers have tickets overlapping only a certain part, and some passengers have completely different tickets. We plan to use this to create a derived variable called ticket.size.\nFare: 0 to 512, with 1 missing value. I care that the 3rd quartile is 31.275 and the max is 512.\nCabin: It has the most (1014) missing values among a total of 12 features. It’s a variable that represents the ship’s area, but if there’s no way to use it, I think it should be discarded.\nEmbarked: It consists of a total of 3 categories, S is the most, and there are 2 missing values.\n\nWhen performing a basic exploration of the data, please look at the outputs of various functions besides summary() and str() while comparing them.\n\n\n\n\n3.2 Missing values\n\nThis is the process of checking which variables have missing values mentioned above and how many of them exist.\nI’m going to check it numerically and visually at the same time using the dplyr, ggplot2, and VIM packages.\nYou don’t have to use all the code I’ve run, you can use only the parts you think you need or like as you read.\n\n\n3.2.1 VIM packages\n\n\nVIM::aggr(full, prop = FALSE, combined = TRUE, numbers = TRUE,\n          sortVars = TRUE, sortCombs = TRUE)\n\n\n\n\n\n Variables sorted by number of missings: \n    Variable Count\n    Survived   418\n         Age   263\n        Fare     1\n PassengerId     0\n      Pclass     0\n        Name     0\n         Sex     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n       Cabin     0\n    Embarked     0\n\n\n\n\n\n3.2.2 tidyverse packages\n\nIn addition to checking missing values at once using the VIM package, these are methods for checking missing values using various packages that exist in the tidyverse.\nFirst, find the proportion of missing values for each variable with dplyr.\n\n\nfull %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n\n\nThere is a way to check the proportion of missing values that exist in variables, but it can also be checked using visual data.\nPlease see the two bar plots below.\n\n\n# Calculate the missing value ratio of each feature -&gt; Data Frame property but has a structure of 1 row and 12 columns.\nmissing_values &lt;- full %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nmissing_values %&gt;% head\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n# Generate the missing_values obtained above as a 12X2 data frame\nmissing_values &lt;- tidyr::gather(missing_values,\n                                key = \"feature\", value = \"missing_pct\")\n\nmissing_values %&gt;% head(12)\n\n       feature  missing_pct\n1  PassengerId 0.0000000000\n2     Survived 0.3193277311\n3       Pclass 0.0000000000\n4         Name 0.0000000000\n5          Sex 0.0000000000\n6          Age 0.2009167303\n7        SibSp 0.0000000000\n8        Parch 0.0000000000\n9       Ticket 0.0000000000\n10        Fare 0.0007639419\n11       Cabin 0.0000000000\n12    Embarked 0.0000000000\n\n# Visualization with missing_values\nmissing_values %&gt;% \n  # Aesthetic setting : missing_pct 내림차순으로 정렬  \n  ggplot(aes(x = reorder(feature, missing_pct), y = missing_pct)) +\n  # Bar plot \n  geom_bar(stat = \"identity\", fill = \"red\") +\n  # Title generation \n  ggtitle(\"Rate of missing values in each features\") +\n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = \"Feature names\", y = \"Rate\") + \n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nIf you look at the bar graph above, you can check the percentage of missing values for all features.\nHowever, what we are actually curious about is which variables have missing values and how many missing values exist in them.\nTherefore, after calculating the proportion of missing values using the purrr package, I extracted only the variables that had at least one and visualized them.\n\n\n# 변수별 결측치 비율 계산\nmiss_pct &lt;- purrr::map_dbl(full, function(x){round((sum(is.na(x))/length(x)) * 100, 1) })\n\n# 결측치 비율이 0%보다 큰 변수들만 선택\nmiss_pct &lt;- miss_pct[miss_pct &gt; 0]\n\n# Data Frame 생성 \ndata.frame(miss = miss_pct, var = names(miss_pct), row.names = NULL) %&gt;%\n  # Aesthetic setting : miss 내림차순으로 정렬 \n  ggplot(aes(x = reorder(var, miss), y = miss)) + \n  # Bar plot \n  geom_bar(stat = 'identity', fill = 'red') +\n  # Plot title setting \n  ggtitle(\"Rate of missing values\") + \n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = 'Feature names', y = 'Rate of missing values') +\n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nThrough this, only 4 variables out of a total of 12 variables have missing values (except Survived because it is due to test data), and there are many missing values in the order of Cabin, Age, Embarked, and Fare.\n\n\n\nNow, it is the process of analyzing and exploring feature through visualization.\n\n\n\n3.3 Age\n\n\nage.p1 &lt;- full %&gt;% \n  ggplot(aes(Age)) + \n  geom_histogram(breaks = seq(0, 80, by = 1), # interval setting \n                 col    = \"red\",              # bar border color\n                 fill   = \"green\",            # bar inner color\n                 alpha  = .5) +               # Bar Transparency = 50%\n  \n  # Plot title\n  ggtitle(\"All Titanic passengers age hitogram\") +\n  theme(plot.title = element_text(face = \"bold\",     \n                                  hjust = 0.5,      # Horizon (horizontal ratio) = 0.5\n                                  size = 15, color = \"darkblue\"))\n\nage.p2 &lt;- full %&gt;% \n# Exclude values where Survived == NA in the test dataset  \n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Age, fill = Survived)) + \n  geom_density(alpha = .5) +\n  ggtitle(\"Titanic passengers age density plot\") + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5,\n                                  size = 15, color = \"darkblue\"))\n\n# Display the two graphs created above on one screen\nggarrange(age.p1, age.p2, ncol=2)\n\n\n\n\n\n\n\n3.4 Pclass\n\nLet’s visualize the frequency of passengers for each Pclass.\nAfter grouping (grouping) by Pclass using dplyr package, Data Frame representing frequency by category was created and visualized with ggplot.\n\n\nfull %&gt;% \n  # Get Pclass frequencies using dplyr::group_by(), summarize()\n  group_by(Pclass) %&gt;% \n  summarize(N = n()) %&gt;% \n  # Aesthetic setting \n  ggplot(aes(Pclass, N)) +\n  geom_col() +\n  geom_text(aes(label = N),       \n            size = 5,             \n            vjust = 1.2,           \n            color = \"#FFFFFF\") +  \n  # Plot title \n  ggtitle(\"Number of each Pclass's passengers\") + \n  # Title setting \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15)) +\n  # x, y axis name change  \n  labs(x = \"Pclass\", y = \"Count\")\n\n\n\n\n\nIt can be seen that the largest number of passengers boarded in the 3-class cabin.\n\n\n\n3.5 Fare\n\nThis is a visualization of the ‘Fare’ variable, which represents the amount paid by the passenger.\nTwo histograms and boxplots were used.\n\n# Histogram \nFare.p1 &lt;- full %&gt;%\n  ggplot(aes(Fare)) + \n  geom_histogram(col    = \"yellow\",\n                 fill   = \"blue\", \n                 alpha  = .5) +\n  ggtitle(\"Histogram of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\n# Boxplot \nFare.p2 &lt;- full %&gt;%\n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Survived, Fare)) + \n  # Observations are drawn as gray dots, but overlapping areas are spread out.  \n  geom_jitter(col = \"gray\") + \n  # Boxplot: 50% transparency\n  geom_boxplot(alpha = .5) + \n  ggtitle(\"Boxplot of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\nggarrange(Fare.p1, Fare.p2, ncol=2)\n\n\n\n\n\nYou can see that the survivors have a higher ‘Fare’ than the deceased passengers, but not by much.\n\n\n\n3.6 Sex\nAre there differences in survival rates between men and women? See the plot below.\n\n\nsex.p1 &lt;- full %&gt;% \n  dplyr::group_by(Sex) %&gt;% \n  summarize(N = n()) %&gt;% \n  ggplot(aes(Sex, N)) +\n  geom_col() +\n  geom_text(aes(label = N), size = 5, vjust = 1.2, color = \"#FFFFFF\") + \n  ggtitle(\"Bar plot of Sex\") +\n  labs(x = \"Sex\", y = \"Count\")\n  \nsex.p2 &lt;- full[1:891, ] %&gt;% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  ggtitle(\"Survival Rate by Sex\") + \n  labs(x = \"Sex\", y = \"Rate\")\n\nggarrange(sex.p1, sex.p2, ncol = 2)\n\n\n\nmosaicplot(Survived ~ Sex,\n           data = full[1:891, ], col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\nIf you interpret the graph, you can see that the survival rate is higher for female passengers, while there are far more males than females."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#feature-engineering-data-pre-processing",
    "href": "teaching/ds101/weekly/posts/12_week.html#feature-engineering-data-pre-processing",
    "title": "Interactive Web: Shiny",
    "section": "4. Feature engineering & Data Pre-processing",
    "text": "4. Feature engineering & Data Pre-processing\n\nThis is the process of filling in missing values (‘NA’) based on the contents of ‘Chapter 3 EDA’ and creating derived variables at the same time.\n\n\n4.1 Age -&gt; Age.Group\n\n\nfull &lt;- full %&gt;%\n# The missing value (NA) is filled in first, and the average of the values excluding the missing value is filled.\n  mutate(Age = ifelse(is.na(Age), mean(full$Age, na.rm = TRUE), Age),\n# Create a categorical derived variable Age.Group based on Age values\n        Age.Group = case_when(Age &lt; 13             ~ \"Age.0012\",\n                               Age &gt;= 13 & Age &lt; 18 ~ \"Age.1317\",\n                               Age &gt;= 18 & Age &lt; 60 ~ \"Age.1859\",\n                               Age &gt;= 60            ~ \"Age.60inf\"),\n# Convert Chr attribute to Factor\n        Age.Group = factor(Age.Group))\n\n\n\n\n4.3 SibSp & Parch -&gt; FamilySized\n\n\nfull &lt;- full %&gt;% \n # First create a derived variable called FamilySize by adding SibSp, Parch and 1 (self)\n  mutate(FamilySize = .$SibSp + .$Parch + 1,\n        # Create a categorical derived variable FamilySized according to the value of FamilySize\n         FamilySized = dplyr::case_when(FamilySize == 1 ~ \"Single\",\n                                        FamilySize &gt;= 2 & FamilySize &lt; 5 ~ \"Small\",\n                                        FamilySize &gt;= 5 ~ \"Big\"),\n        # Convert the Chr property FamilySized to a factor\n        # Assign new levels according to the size of the group size\n         FamilySized = factor(FamilySized, levels = c(\"Single\", \"Small\", \"Big\")))\n\n\nCeated FamilySized using SibSp and Parch.\nReducing these two variables to one has the advantage of simplifying the model.\nA similar use case is to combine height and weight into a BMI index.\n\n\n\n4.4 Name & Sex -&gt; title\n\nWhen looking at the results of ‘Chapter 3.6 Sex’, it was confirmed that the survival rate of women was higher than that of men.\nTherefore, in Name, “Wouldn’t it be useful to extract only names related to gender and categorize them?” I think it is.\nFirst, extract only the column vector named Name from full data and save it as title.\n\n\n# First, extract only the Name column vector and store it in the title vector\ntitle &lt;- full$Name\ntitle %&gt;% head(20)\n\n [1] Braund, Mr. Owen Harris                                \n [2] Cumings, Mrs. John Bradley (Florence Briggs Thayer)    \n [3] Heikkinen, Miss. Laina                                 \n [4] Futrelle, Mrs. Jacques Heath (Lily May Peel)           \n [5] Allen, Mr. William Henry                               \n [6] Moran, Mr. James                                       \n [7] McCarthy, Mr. Timothy J                                \n [8] Palsson, Master. Gosta Leonard                         \n [9] Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      \n[10] Nasser, Mrs. Nicholas (Adele Achem)                    \n[11] Sandstrom, Miss. Marguerite Rut                        \n[12] Bonnell, Miss. Elizabeth                               \n[13] Saundercock, Mr. William Henry                         \n[14] Andersson, Mr. Anders Johan                            \n[15] Vestrom, Miss. Hulda Amanda Adolfina                   \n[16] Hewlett, Mrs. (Mary D Kingcome)                        \n[17] Rice, Master. Eugene                                   \n[18] Williams, Mr. Charles Eugene                           \n[19] Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\n[20] Masselmani, Mrs. Fatima                                \n1307 Levels: Abbing, Mr. Anthony ... Zimmerman, Mr. Leo\n\n# Using regular expression and gsub(), extract only names that are highly related to gender and save them as title vectors\ntitle &lt;- gsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title)\ntitle %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n# Save the title vector saved above to full again, but save it as a title derived variable\nfull$title &lt;- title\n\nfull$title %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n\n\ngsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title) In short, this code uses gsub to search for and replace a pattern in the title string, where the pattern is defined by the regular expression \"^.*, (.*?)\\\\..*$\" and the replacement is defined by the string \"\\\\1\". If you want to understand more about regular expression. Please see my blog post: What are Regular Expressions and How to Use Them in R\nThen check what are the Unique titles.\n\n\nunique(full$title)\n\n [1] \"Mr\"           \"Mrs\"          \"Miss\"         \"Master\"       \"Don\"         \n [6] \"Rev\"          \"Dr\"           \"Mme\"          \"Ms\"           \"Major\"       \n[11] \"Lady\"         \"Sir\"          \"Mlle\"         \"Col\"          \"Capt\"        \n[16] \"the Countess\" \"Jonkheer\"     \"Dona\"        \n\n\n\nYou can see that there are 18 categories in total.\nIf you use this derived variable called ‘title’ as it is, the complexity of the model (especially the tree based model) increases considerably, so you need to reduce the category.\nBefore that, let’s check the frequency and rate for each category using the descr package.\n\n\n# Check frequency, ratio by category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|         Capt |          Col |          Don |         Dona |           Dr |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            4 |            1 |            1 |            8 |\n|        0.001 |        0.003 |        0.001 |        0.001 |        0.006 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|     Jonkheer |         Lady |        Major |       Master |         Miss |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            1 |            2 |           61 |          260 |\n|        0.001 |        0.001 |        0.002 |        0.047 |        0.199 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|         Mlle |          Mme |           Mr |          Mrs |           Ms |\n|--------------|--------------|--------------|--------------|--------------|\n|            2 |            1 |          757 |          197 |            2 |\n|        0.002 |        0.001 |        0.578 |        0.150 |        0.002 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|          Rev |          Sir | the Countess |\n|--------------|--------------|--------------|\n|            8 |            1 |            1 |\n|        0.006 |        0.001 |        0.001 |\n|--------------|--------------|--------------|\n\n\n\nThe frequencies and proportions of the 18 categories are very different.\nSo let’s narrow these down to a total of five categories.\n\n\n# Simplify into 5 categories\nfull &lt;- full %&gt;%\n# If you use \"==\" instead of \"%in%\", it won't work as you want because of Recyling Rule.\n  mutate(title = ifelse(title %in% c(\"Mlle\", \"Ms\", \"Lady\", \"Dona\"), \"Miss\", title),\n         title = ifelse(title == \"Mme\", \"Mrs\", title),\n         title = ifelse(title %in% c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\", \"Don\",\n                                     \"Sir\", \"the Countess\", \"Jonkheer\"), \"Officer\", title),\n         title = factor(title))\n\n# After creating the derived variable, check the frequency and ratio for each category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|  Master |    Miss |      Mr |     Mrs | Officer |\n|---------|---------|---------|---------|---------|\n|      61 |     266 |     757 |     198 |      27 |\n|   0.047 |   0.203 |   0.578 |   0.151 |   0.021 |\n|---------|---------|---------|---------|---------|\n\n\n\n\n\n4.5 Ticket -&gt; ticket.size\n\nAs we saw in Chapter 3.1.3 Summary(), the number of passengers (train and test together) is 1309. However, all passengers’ tickets are not different.\nSee the results of summary() and unique() below.\n\n\n# We used length() to get only the number of unique categories.\nlength(unique(full$Ticket))\n\n[1] 929\n\n# Printing all of them was too messy, so only 10 were printed.\nhead(summary(full$Ticket), 10)\n\n    CA. 2343         1601      CA 2144      3101295       347077       347082 \n          11            8            8            7            7            7 \n    PC 17608 S.O.C. 14879       113781        19950 \n           7            7            6            6 \n\n\n\nWhy are there 929 unique tickets when there are no missing values in feature?\nEven the ticket is CA. There are 11 exactly the same number of people as 2343.\nLet’s see who the passengers are.\n\n\nfull %&gt;% \n# Filter only 11 passengers with matching tickets\n  filter(Ticket == \"CA. 2343\") %&gt;% \n  # We don't need to check for all variables, so we only want to look at the variables below.\n  select(Pclass, Name, Age, FamilySized)\n\n   Pclass                              Name      Age FamilySized\n1       3        Sage, Master. Thomas Henry 29.88114         Big\n2       3      Sage, Miss. Constance Gladys 29.88114         Big\n3       3               Sage, Mr. Frederick 29.88114         Big\n4       3          Sage, Mr. George John Jr 29.88114         Big\n5       3           Sage, Miss. Stella Anna 29.88114         Big\n6       3          Sage, Mr. Douglas Bullen 29.88114         Big\n7       3 Sage, Miss. Dorothy Edith \"Dolly\" 29.88114         Big\n8       3                   Sage, Miss. Ada 29.88114         Big\n9       3             Sage, Mr. John George 29.88114         Big\n10      3       Sage, Master. William Henry 14.50000         Big\n11      3    Sage, Mrs. John (Annie Bullen) 29.88114         Big\n\n\n\nYou can see that the 11 passengers above are all from the same family, brothers.\nWhile there are passengers whose tickets are exactly the same, there are also passengers whose tickets are partially matched.\nCreate a ticket.unique derived variable that represents the number of unique numbers (number of characters) of such a ticket.\nLet’s create a derived variable ticket.size with 3 categories based on ticket.unique.\n\n\n# First of all, ticket.unique is saved as all 0\nticket.unique &lt;- rep(0, nrow(full))\n\n# Extract only the unique ones from ticket features and store them in the tickets vector\ntickets &lt;- unique(full$Ticket)\n\n# After extracting only passengers with the same ticket by using overlapping loops, extract and store the length (number of characters) of each ticket.\n\nfor (i in 1:length(tickets)) {\n  current.ticket &lt;- tickets[i]\n  party.indexes &lt;- which(full$Ticket == current.ticket)\n    # For loop 중첩 \n    for (k in 1:length(party.indexes)) {\n    ticket.unique[party.indexes[k]] &lt;- length(party.indexes)\n    }\n  }\n\n# Save ticket.unique calculated above as a derived variable\nfull$ticket.unique &lt;- ticket.unique\n\n# Create ticket.size variable by dividing it into three categories according to ticket.unique\n\nfull &lt;- full %&gt;% \n  mutate(ticket.size = case_when(ticket.unique == 1 ~ 'Single',\n                                 ticket.unique &lt; 5 & ticket.unique &gt;= 2 ~ \"Small\",\n                                 ticket.unique &gt;= 5 ~ \"Big\"),\n         ticket.size = factor(ticket.size,\n                              levels = c(\"Single\", \"Small\", \"Big\")))\n\n\n\n\n4.6 Embarked\n\nThis is feature with two missing values (NA). In the case of Embarked, replace it with S, which is the most frequent value among the three categories.\n\n\nfull$Embarked &lt;- replace(full$Embarked,               # Specify Data$feature to replace\n                         which(is.na(full$Embarked)), # Find only missing values\n                         'S')                        # specify the value to replace\n\n\n\n\n4.7 Fare\n\nFor Fare, there was only one missing value.\nBased on the histogram seen above (Chapter 3.5 Fare), missing values are replaced with 0.\n\n\nfull$Fare &lt;- replace(full$Fare, which(is.na(full$Fare)), 0)\n\n\nAt this point, data preprocessing is complete.\nThe following is the process of selecting the variables to be used for model creation while exploring the derived variables created so far.\nIn other words, Feature selection."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#relationship-to-target-feature-survived-feature-selection",
    "href": "teaching/ds101/weekly/posts/12_week.html#relationship-to-target-feature-survived-feature-selection",
    "title": "Interactive Web: Shiny",
    "section": "5. Relationship to target feature Survived & Feature selection",
    "text": "5. Relationship to target feature Survived & Feature selection\n\nPrior to full-scale visualization, since the purpose here is to see how well each variable correlates with the survival rate, we did not use the entire full data, but only the train data set that can determine survival and death.\nAlso, please note that the plot used above may be duplicated as it is.\n\n\n5.0 Data set split\nFirst, use the code below to split preprocessed full data into train and test.\n\n# Before feature selection, select all variables first.\n\ntrain &lt;- full[1:891, ]\n\ntest &lt;- full[892:1309, ]\n\n\n\n\n5.1 Pclass\n\n\ntrain %&gt;% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(position = \"fill\") +\n# Set plot theme: Converts to a more vivid color.\n  scale_fill_brewer(palette = \"Set1\") +\n  # Y axis setting \n  scale_y_continuous(labels = percent) +\n# Set x, y axis names and plot main title, sub title\n  labs(x = \"Pclass\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Pclass?\")\n\n\n\n\n\n\n\n5.2 Sex\n\nSame as Chapter 3.6 Sex.\n\n\nmosaicplot(Survived ~ Sex,\n           data = train, col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\n\n\n5.3 Embarked\n\n\ntrain %&gt;% \n  ggplot(aes(Embarked, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Embarked\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Embarked?\")\n\n\n\n\n\n\n\n5.4 FamilySized\n\n\ntrain %&gt;% \n  ggplot(aes(FamilySized, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"FamilySized\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by FamilySized\")\n\n\n\n\n\nIt can be seen that there is a difference in survival rate depending on the number of people on board, and that ‘FamilySized’ and ‘Survived’ have a non-linear relationship.\n\n\n5.5 Age.Group\n\n\ntrain %&gt;% \n  ggplot(aes(Age.Group, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"Age group\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by Age group\")\n\n\n\n\n\n\n\n5.6 title\n\n\ntrain %&gt;% \n  ggplot(aes(title, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"title\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by passengers title\")\n\n\n\n\n\n\n\n5.7 ticket.size\n\n\ntrain %&gt;% \n  ggplot(aes(ticket.size, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"ticket.size\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by ticket.size\")\n\n\n\n\n\n\n\n5.8 Description of actual used features\n\nNow that all the derived variables created so far have been found to be useful, select and save only the variables you will actually use.\nThe table below is a brief description of the actual selected variables.\n\n\n\n\n\n\n\n\nvariable name\nType\nDescription\n\n\n\n\nSurvived\nfactor\nTarget feature, survival == 1, death == 0\n\n\nSex\nfactor\ngender, male or female\n\n\nPclass\nfactor\nCabin Class, First Class (1), Second Class (2), Third Class (3)\n\n\nEmbarked\nfactor\nPort of embarkation, Southampton (S), Cherbourg (C), Queenstown (Q)\n\n\nFamilySized\nfactor\nFamily size, a derived variable created using SibSp and Parch, with 3 categories\n\n\nAge.Group\nfactor\nAge group, a derived variable created using Age, with 4 categories\n\n\ntitle\nfactor\nA part of the name, a derived variable made using Name, and 5 categories\n\n\nticket.size\nfactor\nThe length of the unique part of the ticket, a derived variable created using ticket, with 3 categories\n\n\n\n\n\n# Excluding ID number, select and save 7 input variables and 1 target variable to actually use\n\ntrain &lt;- train %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\", \"Survived\")\n\n# For Submit, extract the Id column vector and store it in ID\n\nID &lt;- test$PassengerId\n\n# Select and save the remaining 6 variables except for Id and Survived\n\ntest &lt;- test %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\")"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#machine-learning-model-generation",
    "href": "teaching/ds101/weekly/posts/12_week.html#machine-learning-model-generation",
    "title": "Interactive Web: Shiny",
    "section": "6. Machine learning model generation",
    "text": "6. Machine learning model generation\n\nNow is the time to create a machine learning model using the train data set.\nOriginally, it is correct to create train, validation, test data sets first, create various models, and then select the final model through cross validation (CV, Cross Validation), but these processes are omitted here and RandomForest After creating only, we will predict (estimate) the test data and even create data to Submit to competition.\n\n\n6.1 Random Forest model generation\n\n\n# Set the seed number for reproducibility.\nset.seed(1901)\n\ntitanic.rf &lt;- randomForest(Survived ~ ., data = train, importance = T, ntree = 2000)\n\n\n\n\n6.2 Feature importance check\n\n\nimportance(titanic.rf)\n\n                    0        1 MeanDecreaseAccuracy MeanDecreaseGini\nPclass      47.442449 53.94070             64.73724        36.807804\nSex         54.250630 37.30378             58.66109        57.223102\nEmbarked    -6.328112 38.10930             27.32587         9.632958\nFamilySized 32.430898 31.24383             50.13349        18.086894\nAge.Group   15.203313 26.72696             29.36321        10.201187\ntitle       48.228450 41.60124             57.02653        73.146999\nticket.size 39.544367 37.80849             59.59915        22.570142\n\nvarImpPlot(titanic.rf)\n\n\n\n\n\n\n6.3 Predict test data and create submit data\n\n\n# Prediction \npred.rf &lt;- predict(object = titanic.rf, newdata = test, type = \"class\")\n\n# Data frame generation \nsubmit &lt;- data.frame(PassengerID = ID, Survived = pred.rf)\n\n# Write the submit data frame to file : csv is created in the folder designated by setwd().\n\nwrite.csv(submit, file = './titanic_submit.csv', row.names = F)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/09_week.html",
    "href": "teaching/ds101/weekly/posts/09_week.html",
    "title": "Data Visualization (2)",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nData visualization (2)\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gapminder)\n\n\n# 03 Visualization Tool\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# type=\"p\" is the point plot, main=\"cars\" is the title of the graph\nplot(cars, type = \"p\", main = \"cars\")\n\n\n\n\n\n\n\nplot(cars, type = \"l\", main = \"cars\") # type =\"l\" is a plot using lines\n\n\n\n\n\n\n\nplot(cars, type=\"b\", main=\"cars\") # type=\"b\" is a plot using both points and lines\n\n\n\n\n\n\n\nplot(cars, type = \"h\", main = \"cars\") # type = \"h\" is a bar graph such as a histogram\n\n\n\n\n\n\n\nx = gapminder %&gt;% filter(year == 1952 & continent == \"Asia\") %&gt;% mutate(gdp = gdpPercap*pop) %&gt;% select(country, gdp) %&gt;% arrange(desc(gdp)) %&gt;% head()\npie(x$gdp, x$country)\n\n\n\n\n\n\n\nbarplot(x$gdp, names.arg = x$country)\n\n\n\n\n\n\n\nx = gapminder %&gt;% filter(year == 2007 & continent == \"Asia\") %&gt;% mutate(gdp = gdpPercap*pop) %&gt;% select(country, gdp) %&gt;% arrange(desc(gdp)) %&gt;% head()\npie(x$gdp, x$country)\n\n\n\n\n\n\n\nbarplot(x$gdp, names.arg = x$country)\n\n\n\n\n\n\n\nmatplot(iris[, 1:4], type = \"l\")\nlegend(\"topleft\", names(iris)[1:4], lty = c(1, 2, 3, 4), col = c(1, 2, 3, 4))\n\n\n\n\n\n\n\nhist(cars$speed)\n\n\n\n\n\n\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) + geom_point(alpha = 0.2)\n\n\n\n\n\n\n\ngapminder %&gt;% filter(lifeExp&gt;70) %&gt;%\n   group_by(continent) %&gt;%\n   summarize(n = n_distinct(country)) %&gt;%\n   ggplot(aes(x = continent, y = n)) +\n   geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\ngapminder %&gt;% filter(year == 2007) %&gt;%\n   ggplot(aes(lifeExp, col = continent)) +\n   geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ngapminder %&gt;% filter(year == 2007) %&gt;%\n   ggplot(aes(lifeExp, col = continent)) +\n   geom_histogram(position = \"dodge\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n   filter(year == 2007) %&gt;%\n   ggplot(aes(continent, lifeExp, col = continent)) +\n   geom_boxplot()\n\n\n\n\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) +\n   geom_point(alpha = 0.2)\n\n\n\n\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) +\n   geom_point(alpha = 0.2) + scale_x_log10() # Convert the horizontal axis to log scale.\n\n\n\n\n\n\n\ngapminder %&gt;%\n   filter(continent == \"Africa\") %&gt;%\n   ggplot(aes(country, lifeExp)) +\n   geom_bar(stat = \"identity\") # [Figure 6-35(a)]\n\n\n\n\n\n\n\ngapminder %&gt;%\n   filter(continent == \"Africa\") %&gt;%\n   ggplot(aes(country, lifeExp)) +\n   geom_bar(stat = \"identity\") +\n   coord_flip() # [Figure 6-35(b)] Switches the direction of the plot.\n\n\n\n\n\n\n\n\n\n# install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\n# [Figure 6-37(a)]: Graph with basic palette applied\ngapminder %&gt;% filter(lifeExp&gt;70) %&gt;%\n   group_by(continent) %&gt;%\n   summarize(n = n_distinct(country)) %&gt;%\n   ggplot(aes(x = continent, y = n)) +\n   geom_bar(stat = \"identity\", aes(fill = continent))\n\n\n\n\n\n\n\n# [Figure 6-37(b)]: Graph applying the Spectral palette\ngapminder %&gt;%\n   filter(lifeExp&gt;70) %&gt;%\n   group_by(year, continent) %&gt;%\n   summarize(n = n_distinct(country)) %&gt;%\n   ggplot(aes(x = continent, y = n)) +\n   geom_bar(stat = \"identity\", aes(fill = continent)) + \n  scale_fill_brewer(palette = \"Spectral\")\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nhelp(geom_bar)\n\nstarting httpd help server ... done\n\n# [Figure 6-37(c)] Graph applying the Blues palette\ngapminder %&gt;%\n   filter(lifeExp&gt;70) %&gt;%\n   group_by(continent) %&gt;%\n   summarize(n = n_distinct(country)) %&gt;%\n   ggplot(aes(x = continent, y = n)) +\n   geom_bar(stat = \"identity\", aes(fill = continent)) + scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n\n# [Figure 6-37(d)] Graph applying the Oranges palette\ngapminder %&gt;%\n   filter(lifeExp&gt;70) %&gt;%\n   group_by(continent) %&gt;%\n   summarize(n = n_distinct(country)) %&gt;%\n   ggplot(aes(x = continent, y = n)) +\n   geom_bar(stat = \"identity\", aes(fill = continent)) + scale_fill_brewer(palette = \"Oranges\")\n\n\n\n\n\n\n\n\n\n# reorder(continent, -n) means sort the continents in descending order based on n\ngapminder %&gt;%\n   filter(lifeExp &gt;70) %&gt;%\n   group_by(continent) %&gt;%\n   summarize(n = n_distinct(country)) %&gt;%\n   ggplot(aes(x = reorder(continent, -n), y = n)) +\n   geom_bar(stat = \"identity\", aes(fill = continent)) +\n   scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n\n# Training!!\ngapminder %&gt;%\n   filter(continent == \"Africa\", year==2007) %&gt;%\n   ggplot(aes(reorder(country, lifeExp), lifeExp, fill=lifeExp)) +\n   geom_bar(stat = \"identity\") +\n   coord_flip()\n\n\n\n\n\n\n\n#\ngapminder %&gt;%\n   filter(continent == \"Africa\", year==2007) %&gt;%\n   ggplot(aes(reorder(country, lifeExp), lifeExp, fill=lifeExp)) +\n   geom_bar(stat = \"identity\") +\n   coord_flip() +\n   scale_fill_distiller(palette = \"Oranges\", direction=1)\n\n\n\n\n\n\n\n\n\n# 04 Data exploration using visualization #\n\ngapminder %&gt;% ggplot(aes(gdpPercap, lifeExp, col = continent)) + geom_point(alpha = 0.2) + facet_wrap(~year) + scale_x_log10()\n\n\n\n\n\n\n\ngapminder %&gt;% filter(year == 1952 & gdpPercap &gt; 10000 & continent == \"Asia\")\n\n# A tibble: 1 × 6\n  country continent  year lifeExp    pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;\n1 Kuwait  Asia       1952    55.6 160000   108382.\n\ngapminder %&gt;% filter(country == \"Kuwait\") %&gt;% ggplot(aes(year, gdpPercap)) + geom_point() + geom_line() # [Figure 6-40(a)]\n\n\n\n\n\n\n\ngapminder %&gt;% filter(country == \"Kuwait\") %&gt;% ggplot(aes(year, pop)) + geom_point() + geom_line() # [Figure 6-40(b)]\n\n\n\n\n\n\n\ngapminder %&gt;% filter(country == \"Korea, Rep.\") %&gt;% ggplot(aes(year, gdpPercap)) + geom_point() + geom_line() # [Figure 6-41(a)]\n\n\n\n\n\n\n\ngapminder %&gt;% filter(country == \"Korea, Rep.\") %&gt;% ggplot(aes(year, pop)) + geom_point() + geom_line() # [Figure 6-41(b)]\n\n\n\n\n\n\n\ngapminder %&gt;% filter(country == \"Kuwait\" | country == \"Korea, Rep.\") %&gt;% mutate(gdp = gdpPercap*pop) %&gt;% ggplot(aes(year, gdp, col = country)) + geom_point() + geom_line()\n\n\n\n\n\n\n\n# [Figure 6-43(a)] Comparison of changes in gdpPercap\ngapminder %&gt;% filter(country == \"Kuwait\"|country == \"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China \"|country == \"Japan\") %&gt;% ggplot(aes(year, gdpPercap, col = country)) + geom_point() + geom_line()\n\n\n\n\n\n\n\n# [Figure 6-43(b)] Comparison of changes in pop\ngapminder %&gt;% filter(country == \"Kuwait\"|country==\"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China \"|country == \"Japan\") %&gt;% ggplot(aes(year, pop, col=country)) + geom_point() + geom_line()\n\n\n\n\n\n\n\n# [Figure 6-43(c)] Comparison of changes in gdp\ngapminder %&gt;% filter(country == \"Kuwait\"|country == \"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China \"|country == \"Japan\") %&gt;% mutate(gdp=gdpPercap*pop) %&gt;% ggplot(aes(year, gdp, col = country)) + geom_point() + geom_line() + scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\nClass\n\n\nReview for ggplot2 library\n\n\nlibrary(tidyverse)\n\nmpg %&gt;% select(hwy, cty, cyl)\n\n# A tibble: 234 × 3\n     hwy   cty   cyl\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    29    18     4\n 2    29    21     4\n 3    31    20     4\n 4    30    21     4\n 5    26    16     6\n 6    26    18     6\n 7    27    18     6\n 8    26    18     4\n 9    25    16     4\n10    28    20     4\n# ℹ 224 more rows\n\n\n\nggplot(mpg, aes(hwy, cty)) +\n  geom_point(aes(color = as.factor(cyl)))\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(hwy, cty)) +\n  geom_point(aes(color = as.factor(cyl))) +\n  geom_smooth(method =\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(hwy, cty)) +\n  geom_point(aes(color = as.factor(cyl))) +\n  geom_smooth(method =\"glm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(mpg, aes(hwy, cty)) +\n  geom_point(aes(color = cyl)) +\n  geom_smooth(method =\"lm\") +\n  # coord_cartesian() +\n  # scale_color_gradient() +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Returns the last plot\nlast_plot()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Saves last plot as 5’ x 5’ file named \"plot.png\" in\n# working directory. Matches file type to file extension.\n# ggsave(\"plot.png\", width = 5, height = 5)\n\n\n\nOne variable\n\n# Continuous\na &lt;- ggplot(mpg, aes(hwy))\na\n\n\n\n\n\n\n\n\n\na + geom_area(stat = \"bin\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\na + geom_density(kernel = \"gaussian\")\n\n\n\n\n\n\n\na + geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\na + geom_freqpoly()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\na + geom_histogram(binwidth = 4)\n\n\n\n\n\n\n\n\n\nmpg %&gt;% ggplot()+\n  geom_area(aes(hwy), stat=\"bin\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n# Discrete\nb &lt;- ggplot(mpg, aes(fl))\nb + geom_bar()\n\n\n\n\n\n\n\n\n\n# Two variables\n# Continuous X & Countinuous Y\nf &lt;- ggplot(mpg, aes(cty, hwy))\nf + geom_blank()\n\n\n\n\n\n\n\nf + geom_jitter()\n\n\n\n\n\n\n\nf + geom_point()\n\n\n\n\n\n\n\n\n\n# install.packages(\"quantreg\")\nlibrary(quantreg)\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nf + geom_quantile() + geom_jitter()\n\nSmoothing formula not specified. Using: y ~ x\n\n\n\n\n\n\n\n\nf + geom_rug(sides = \"bl\") + geom_jitter()\n\n\n\n\n\n\n\nf + geom_rug(sides = \"bl\") + geom_point()\n\n\n\n\n\n\n\nf + geom_smooth(model = lm) +  geom_point()\n\nWarning in geom_smooth(model = lm): Ignoring unknown parameters: `model`\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nf + geom_text(aes(label = cty)) + \n  geom_jitter()\n\n\n\n\n\n\n\nf + geom_text(aes(label = fl))\n\n\n\n\n\n\n\n\n\nmpg %&gt;% \n  ggplot(aes(cty, hwy, label = fl, \n             alpha=0.1, col='red')) +\n  geom_text()+\n  geom_jitter()\n\n\n\n\n\n\n\n\n\n# install.packages(\"ggimage\")\nlibrary(ggimage)\n\nimg &lt;- list.files(system.file(\"extdata\", \n                              package=\"ggimage\"),\n                  pattern=\"png\", full.names=TRUE)\n\nimg[2]\n\n[1] \"C:/R/R-4.4.0/library/ggimage/extdata/Rlogo.png\"\n\n\n\nf + geom_image(aes(image=img[2]))\n\n\n\n\n\n\n\n\n\n# Discrete X & Countinuous Y\ng &lt;- ggplot(mpg, aes(class, hwy))\nlevels(as.factor(mpg$class))\n\n[1] \"2seater\"    \"compact\"    \"midsize\"    \"minivan\"    \"pickup\"    \n[6] \"subcompact\" \"suv\"       \n\nstr(mpg$class)\n\n chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nlevels(as.factor(mpg$class))\n\n[1] \"2seater\"    \"compact\"    \"midsize\"    \"minivan\"    \"pickup\"    \n[6] \"subcompact\" \"suv\"       \n\nunique(mpg$class)\n\n[1] \"compact\"    \"midsize\"    \"suv\"        \"2seater\"    \"minivan\"   \n[6] \"pickup\"     \"subcompact\"\n\n\n\nmpg %&gt;% count(class)\n\n# A tibble: 7 × 2\n  class          n\n  &lt;chr&gt;      &lt;int&gt;\n1 2seater        5\n2 compact       47\n3 midsize       41\n4 minivan       11\n5 pickup        33\n6 subcompact    35\n7 suv           62\n\n\n\nmpg %&gt;% select(manufacturer, class, hwy) %&gt;% \n  group_by(class) %&gt;% \n  arrange(desc(hwy)) %&gt;% head(10) -&gt; dkdk\n\n\nmpg %&gt;% count(class)\n\n# A tibble: 7 × 2\n  class          n\n  &lt;chr&gt;      &lt;int&gt;\n1 2seater        5\n2 compact       47\n3 midsize       41\n4 minivan       11\n5 pickup        33\n6 subcompact    35\n7 suv           62\n\n\n\ng\n\n\n\n\n\n\n\ng + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\ng + geom_boxplot()\n\n\n\n\n\n\n\n\n\ng + geom_dotplot(binaxis = \"y\",\n                 stackdir = \"center\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\ng + geom_violin(scale = \"area\")\n\n\n\n\n\n\n\n\n\n# Discrete X & Discrete Y\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\nh &lt;- ggplot(diamonds, aes(cut, color))\nh + geom_jitter()\n\n\n\n\n\n\n\n\n\n# Continuous Bivariate Distribution\n# install.packages(\"ggplot2movies\")\nlibrary(ggplot2movies)\n\nmovies %&gt;% glimpse\n\nRows: 58,788\nColumns: 24\n$ title       &lt;chr&gt; \"$\", \"$1000 a Touchdown\", \"$21 a Day Once a Month\", \"$40,0…\n$ year        &lt;int&gt; 1971, 1939, 1941, 1996, 1975, 2000, 2002, 2002, 1987, 1917…\n$ length      &lt;int&gt; 121, 71, 7, 70, 71, 91, 93, 25, 97, 61, 99, 96, 10, 10, 10…\n$ budget      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ rating      &lt;dbl&gt; 6.4, 6.0, 8.2, 8.2, 3.4, 4.3, 5.3, 6.7, 6.6, 6.0, 5.4, 5.9…\n$ votes       &lt;int&gt; 348, 20, 5, 6, 17, 45, 200, 24, 18, 51, 23, 53, 44, 11, 12…\n$ r1          &lt;dbl&gt; 4.5, 0.0, 0.0, 14.5, 24.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4…\n$ r2          &lt;dbl&gt; 4.5, 14.5, 0.0, 0.0, 4.5, 4.5, 0.0, 4.5, 4.5, 0.0, 0.0, 0.…\n$ r3          &lt;dbl&gt; 4.5, 4.5, 0.0, 0.0, 0.0, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5…\n$ r4          &lt;dbl&gt; 4.5, 24.5, 0.0, 0.0, 14.5, 14.5, 4.5, 4.5, 0.0, 4.5, 14.5,…\n$ r5          &lt;dbl&gt; 14.5, 14.5, 0.0, 0.0, 14.5, 14.5, 24.5, 4.5, 0.0, 4.5, 24.…\n$ r6          &lt;dbl&gt; 24.5, 14.5, 24.5, 0.0, 4.5, 14.5, 24.5, 14.5, 0.0, 44.5, 4…\n$ r7          &lt;dbl&gt; 24.5, 14.5, 0.0, 0.0, 0.0, 4.5, 14.5, 14.5, 34.5, 14.5, 24…\n$ r8          &lt;dbl&gt; 14.5, 4.5, 44.5, 0.0, 0.0, 4.5, 4.5, 14.5, 14.5, 4.5, 4.5,…\n$ r9          &lt;dbl&gt; 4.5, 4.5, 24.5, 34.5, 0.0, 14.5, 4.5, 4.5, 4.5, 4.5, 14.5,…\n$ r10         &lt;dbl&gt; 4.5, 14.5, 24.5, 45.5, 24.5, 14.5, 14.5, 14.5, 24.5, 4.5, …\n$ mpaa        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"R\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ Action      &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0…\n$ Animation   &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Comedy      &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1…\n$ Drama       &lt;int&gt; 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ Documentary &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ Romance     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Short       &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1…\n\n\n\ni &lt;- ggplot(movies, aes(year, rating))\ni + geom_bin2d(binwidth = c(5, 0.5))\n\n\n\n\n\n\n\ni + geom_density2d()\n\n\n\n\n\n\n\n\n\n# install.packages(\"hexbin\")\nlibrary(hexbin)\ni + geom_hex()\n\n\n\n\n\n\n\n\n\n# Continuous Function\nj &lt;- ggplot(economics, aes(date, unemploy))\nj + geom_area()\n\n\n\n\n\n\n\nj + geom_line()\n\n\n\n\n\n\n\nj + geom_step(direction = \"hv\")\n\n\n\n\n\n\n\n\n\n# Visualizing error\ndf &lt;- data.frame(grp = c(\"A\", \"B\"), fit = 4:5, se = 1:2)\nk &lt;- ggplot(df, \n            aes(grp, fit, \n                ymin = fit-se, \n                ymax = fit+se))\n\nk + geom_crossbar(fatten = 2)\n\n\n\n\n\n\n\nk + geom_errorbar(col=\"grey\") +\n  geom_point(aes(col=\"red\")) \n\n\n\n\n\n\n\nk + geom_linerange()\n\n\n\n\n\n\n\nk + geom_pointrange()\n\n\n\n\n\n\n\n\n\n# Three variables\n?seals\nseals$z &lt;- with(seals, sqrt(delta_long^2 + delta_lat^2))\nm &lt;- ggplot(seals, aes(long, lat))\n\nm + geom_tile(aes(fill = z))\n\n\n\n\n\n\n\nm + geom_contour(aes(z = z))\n\n\n\n\n\n\n\nm + geom_raster(aes(fill = z), hjust=0.5,\n                vjust=0.5, interpolate=FALSE)\n\n\n\n\n\n\n\n\n\n# Scales\nn &lt;- b + geom_bar(aes(fill = fl))\nn\n\n\n\n\n\n\n\nn + scale_fill_manual(\n  values = c(\"skyblue\", \"royalblue\", \"blue\", \"navy\"),\n  limits = c(\"d\", \"e\", \"p\", \"r\"), breaks =c(\"d\", \"e\", \"p\", \"r\"),\n  name = \"Fuel\", labels = c(\"D\", \"E\", \"P\", \"R\"))\n\n\n\n\n\n\n\n\n\n# Color and fill scales\nn &lt;- b + geom_bar(aes(fill = fl))\no &lt;- a + geom_dotplot(aes(fill = ..x..))\n\n\n# install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\nn + scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n\n\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\nn + scale_fill_grey(\n  start = 0.2, end = 0.8,\n  na.value = \"red\")\n\n\n\n\n\n\n\n\n\no + scale_fill_gradient(\n  low = \"red\",\n  high = \"yellow\")\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\no + scale_fill_gradientn(\n  colours = terrain.colors(5))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n# Also: rainbow(), heat.colors(),\n# topo.colors(), cm.colors(),\n# RColorBrewer::brewer.pal()\n\n\n# Shape scales\nf\n\n\n\n\n\n\n\np &lt;- f + geom_point(aes(shape = fl))\np\n\n\n\n\n\n\n\n\n\np + scale_shape(solid = FALSE)\n\n\n\n\n\n\n\np + scale_shape_manual(values = c(3:7))\n\n\n\n\n\n\n\n\n\n# Coordinate Systems\nr &lt;- b+geom_bar()\nr + coord_cartesian(xlim = c(0, 5))\n\n\n\n\n\n\n\nr + coord_fixed(ratio = 1/2)\n\n\n\n\n\n\n\nr + coord_fixed(ratio = 1/10)\n\n\n\n\n\n\n\nr + coord_fixed(ratio = 1/100)\n\n\n\n\n\n\n\nr + coord_flip()\n\n\n\n\n\n\n\nr + coord_polar(theta = \"x\", direction=1 )\n\n\n\n\n\n\n\n\n\n# Position Adjustments\n\ns &lt;- ggplot(mpg, aes(fl, fill = drv))\n\ns + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n# Arrange elements side by side\ns + geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n# Stack elements on top of one another, normalize height\ns + geom_bar(position = \"stack\")\n\n\n\n\n\n\n\n# Stack elements on top of one another\nf + geom_point(position = \"jitter\")\n\n\n\n\n\n\n\n# Add random noise to X and Y position of each element to avoid overplotting\n\n\n# Theme\nr + theme_bw()\n\n\n\n\n\n\n\nr + theme_classic()\n\n\n\n\n\n\n\nr + theme_grey()\n\n\n\n\n\n\n\nr + theme_minimal()\n\n\n\n\n\n\n\n\n\n# Faceting\n\nt &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point()\nt + facet_grid(. ~ fl)\n\n\n\n\n\n\n\nt + facet_grid(fl ~ .)\n\n\n\n\n\n\n\n# facet into columns based on fl\nt + facet_grid(year ~ .)\n\n\n\n\n\n\n\n# facet into rows based on year\nt + facet_grid(year ~ fl)\n\n\n\n\n\n\n\n# facet into both rows and columns\nt + facet_wrap(~ fl)\n\n\n\n\n\n\n\n# wrap facets into a rectangular layout\n\n\n# Labels\nt + ggtitle(\"New Plot Title \")\n\n\n\n\n\n\n\n# Add a main title above the plot\nt + xlab(\"New X label\")\n\n\n\n\n\n\n\n# Change the label on the X axis\nt + ylab(\"New Y label\")\n\n\n\n\n\n\n\n# Change the label on the Y axis\nt + labs(title =\" New title\", x = \"New x\", y = \"New y\")\n\n\n\n\n\n\n\n# All of the above\n\n\nmpg %&gt;% \n  group_by(manufacturer) %&gt;% \n  summarise(avg_hwy=mean(hwy)) %&gt;% \n  arrange(desc(avg_hwy)) %&gt;% \n  ggplot(aes(x=reorder(manufacturer, avg_hwy),\n             y=avg_hwy,\n             fill=avg_hwy))+\n  scale_fill_gradient(\n    low = \"red\",\n    high = \"green\")+\n  coord_flip()+\n  geom_bar(stat='identity')+\n  xlab(\"\")+\n  ggtitle(\"제조사별 평균 연비\")"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/07_week.html",
    "href": "teaching/ds101/weekly/posts/07_week.html",
    "title": "Data Exploration (2)",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nData wrangling\n\n\n\n\n# 02 Data processing using Base R #\n\nlibrary(gapminder)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder[, c(\"country\", \"lifeExp\")]\n\n# A tibble: 1,704 × 2\n   country     lifeExp\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 Afghanistan    28.8\n 2 Afghanistan    30.3\n 3 Afghanistan    32.0\n 4 Afghanistan    34.0\n 5 Afghanistan    36.1\n 6 Afghanistan    38.4\n 7 Afghanistan    39.9\n 8 Afghanistan    40.8\n 9 Afghanistan    41.7\n10 Afghanistan    41.8\n# ℹ 1,694 more rows\n\ngapminder[, c(\"country\", \"lifeExp\", \"year\")]\n\n# A tibble: 1,704 × 3\n   country     lifeExp  year\n   &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;\n 1 Afghanistan    28.8  1952\n 2 Afghanistan    30.3  1957\n 3 Afghanistan    32.0  1962\n 4 Afghanistan    34.0  1967\n 5 Afghanistan    36.1  1972\n 6 Afghanistan    38.4  1977\n 7 Afghanistan    39.9  1982\n 8 Afghanistan    40.8  1987\n 9 Afghanistan    41.7  1992\n10 Afghanistan    41.8  1997\n# ℹ 1,694 more rows\n\ngapminder[1:15, ]\n\n# A tibble: 15 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n11 Afghanistan Asia       2002    42.1 25268405      727.\n12 Afghanistan Asia       2007    43.8 31889923      975.\n13 Albania     Europe     1952    55.2  1282697     1601.\n14 Albania     Europe     1957    59.3  1476505     1942.\n15 Albania     Europe     1962    64.8  1728137     2313.\n\n\n\nlibrary(dplyr)\ngapminder %&gt;% filter(country==\"Croatia\") %&gt;% select(year, gdpPercap) %&gt;% plot\n\n\n\ngapminder[gapminder$country == \"Croatia\", ]\n\n# A tibble: 12 × 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Croatia Europe     1952    61.2 3882229     3119.\n 2 Croatia Europe     1957    64.8 3991242     4338.\n 3 Croatia Europe     1962    67.1 4076557     5478.\n 4 Croatia Europe     1967    68.5 4174366     6960.\n 5 Croatia Europe     1972    69.6 4225310     9164.\n 6 Croatia Europe     1977    70.6 4318673    11305.\n 7 Croatia Europe     1982    70.5 4413368    13222.\n 8 Croatia Europe     1987    71.5 4484310    13823.\n 9 Croatia Europe     1992    72.5 4494013     8448.\n10 Croatia Europe     1997    73.7 4444595     9876.\n11 Croatia Europe     2002    74.9 4481020    11628.\n12 Croatia Europe     2007    75.7 4493312    14619.\n\ngapminder[gapminder$country == \"Korea, Rep.\", ]\n\n# A tibble: 12 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Korea, Rep. Asia       1952    47.5 20947571     1031.\n 2 Korea, Rep. Asia       1957    52.7 22611552     1488.\n 3 Korea, Rep. Asia       1962    55.3 26420307     1536.\n 4 Korea, Rep. Asia       1967    57.7 30131000     2029.\n 5 Korea, Rep. Asia       1972    62.6 33505000     3031.\n 6 Korea, Rep. Asia       1977    64.8 36436000     4657.\n 7 Korea, Rep. Asia       1982    67.1 39326000     5623.\n 8 Korea, Rep. Asia       1987    69.8 41622000     8533.\n 9 Korea, Rep. Asia       1992    72.2 43805450    12104.\n10 Korea, Rep. Asia       1997    74.6 46173816    15994.\n11 Korea, Rep. Asia       2002    77.0 47969150    19234.\n12 Korea, Rep. Asia       2007    78.6 49044790    23348.\n\n\n\nlevels(gapminder$country)\n\n  [1] \"Afghanistan\"              \"Albania\"                 \n  [3] \"Algeria\"                  \"Angola\"                  \n  [5] \"Argentina\"                \"Australia\"               \n  [7] \"Austria\"                  \"Bahrain\"                 \n  [9] \"Bangladesh\"               \"Belgium\"                 \n [11] \"Benin\"                    \"Bolivia\"                 \n [13] \"Bosnia and Herzegovina\"   \"Botswana\"                \n [15] \"Brazil\"                   \"Bulgaria\"                \n [17] \"Burkina Faso\"             \"Burundi\"                 \n [19] \"Cambodia\"                 \"Cameroon\"                \n [21] \"Canada\"                   \"Central African Republic\"\n [23] \"Chad\"                     \"Chile\"                   \n [25] \"China\"                    \"Colombia\"                \n [27] \"Comoros\"                  \"Congo, Dem. Rep.\"        \n [29] \"Congo, Rep.\"              \"Costa Rica\"              \n [31] \"Cote d'Ivoire\"            \"Croatia\"                 \n [33] \"Cuba\"                     \"Czech Republic\"          \n [35] \"Denmark\"                  \"Djibouti\"                \n [37] \"Dominican Republic\"       \"Ecuador\"                 \n [39] \"Egypt\"                    \"El Salvador\"             \n [41] \"Equatorial Guinea\"        \"Eritrea\"                 \n [43] \"Ethiopia\"                 \"Finland\"                 \n [45] \"France\"                   \"Gabon\"                   \n [47] \"Gambia\"                   \"Germany\"                 \n [49] \"Ghana\"                    \"Greece\"                  \n [51] \"Guatemala\"                \"Guinea\"                  \n [53] \"Guinea-Bissau\"            \"Haiti\"                   \n [55] \"Honduras\"                 \"Hong Kong, China\"        \n [57] \"Hungary\"                  \"Iceland\"                 \n [59] \"India\"                    \"Indonesia\"               \n [61] \"Iran\"                     \"Iraq\"                    \n [63] \"Ireland\"                  \"Israel\"                  \n [65] \"Italy\"                    \"Jamaica\"                 \n [67] \"Japan\"                    \"Jordan\"                  \n [69] \"Kenya\"                    \"Korea, Dem. Rep.\"        \n [71] \"Korea, Rep.\"              \"Kuwait\"                  \n [73] \"Lebanon\"                  \"Lesotho\"                 \n [75] \"Liberia\"                  \"Libya\"                   \n [77] \"Madagascar\"               \"Malawi\"                  \n [79] \"Malaysia\"                 \"Mali\"                    \n [81] \"Mauritania\"               \"Mauritius\"               \n [83] \"Mexico\"                   \"Mongolia\"                \n [85] \"Montenegro\"               \"Morocco\"                 \n [87] \"Mozambique\"               \"Myanmar\"                 \n [89] \"Namibia\"                  \"Nepal\"                   \n [91] \"Netherlands\"              \"New Zealand\"             \n [93] \"Nicaragua\"                \"Niger\"                   \n [95] \"Nigeria\"                  \"Norway\"                  \n [97] \"Oman\"                     \"Pakistan\"                \n [99] \"Panama\"                   \"Paraguay\"                \n[101] \"Peru\"                     \"Philippines\"             \n[103] \"Poland\"                   \"Portugal\"                \n[105] \"Puerto Rico\"              \"Reunion\"                 \n[107] \"Romania\"                  \"Rwanda\"                  \n[109] \"Sao Tome and Principe\"    \"Saudi Arabia\"            \n[111] \"Senegal\"                  \"Serbia\"                  \n[113] \"Sierra Leone\"             \"Singapore\"               \n[115] \"Slovak Republic\"          \"Slovenia\"                \n[117] \"Somalia\"                  \"South Africa\"            \n[119] \"Spain\"                    \"Sri Lanka\"               \n[121] \"Sudan\"                    \"Swaziland\"               \n[123] \"Sweden\"                   \"Switzerland\"             \n[125] \"Syria\"                    \"Taiwan\"                  \n[127] \"Tanzania\"                 \"Thailand\"                \n[129] \"Togo\"                     \"Trinidad and Tobago\"     \n[131] \"Tunisia\"                  \"Turkey\"                  \n[133] \"Uganda\"                   \"United Kingdom\"          \n[135] \"United States\"            \"Uruguay\"                 \n[137] \"Venezuela\"                \"Vietnam\"                 \n[139] \"West Bank and Gaza\"       \"Yemen, Rep.\"             \n[141] \"Zambia\"                   \"Zimbabwe\"                \n\ngapminder[gapminder$country == \"Croatia\", \"pop\"]\n\n# A tibble: 12 × 1\n       pop\n     &lt;int&gt;\n 1 3882229\n 2 3991242\n 3 4076557\n 4 4174366\n 5 4225310\n 6 4318673\n 7 4413368\n 8 4484310\n 9 4494013\n10 4444595\n11 4481020\n12 4493312\n\ngapminder[gapminder$country == \"Croatia\", c(\"lifeExp\",\"pop\")]\n\n# A tibble: 12 × 2\n   lifeExp     pop\n     &lt;dbl&gt;   &lt;int&gt;\n 1    61.2 3882229\n 2    64.8 3991242\n 3    67.1 4076557\n 4    68.5 4174366\n 5    69.6 4225310\n 6    70.6 4318673\n 7    70.5 4413368\n 8    71.5 4484310\n 9    72.5 4494013\n10    73.7 4444595\n11    74.9 4481020\n12    75.7 4493312\n\ngapminder[gapminder$country == \"Croatia\" & #Croatia extraction\n             gapminder$year &gt; 1990, #1990 after\n           c(\"lifeExp\",\"pop\")] # those variables\n\n# A tibble: 4 × 2\n  lifeExp     pop\n    &lt;dbl&gt;   &lt;int&gt;\n1    72.5 4494013\n2    73.7 4444595\n3    74.9 4481020\n4    75.7 4493312\n\n\n\napply(gapminder[gapminder$country == \"Croatia\",\n                 c(\"lifeExp\",\"pop\")],\n       2, mean)\n\n     lifeExp          pop \n7.005592e+01 4.289916e+06 \n\napply(gapminder[gapminder$country == \"Korea, Rep.\",\n                 c(\"lifeExp\",\"pop\")],\n       2, mean)\n\n     lifeExp          pop \n      65.001 36499386.333 \n\n\n\n# 03 Data processing using the dplyr library #\nselect(gapminder, country, year, lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\nfilter(gapminder, country == \"Croatia\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n 1 Croatia Europe     1952    61.2 3882229     3119.\n 2 Croatia Europe     1957    64.8 3991242     4338.\n 3 Croatia Europe     1962    67.1 4076557     5478.\n 4 Croatia Europe     1967    68.5 4174366     6960.\n 5 Croatia Europe     1972    69.6 4225310     9164.\n 6 Croatia Europe     1977    70.6 4318673    11305.\n 7 Croatia Europe     1982    70.5 4413368    13222.\n 8 Croatia Europe     1987    71.5 4484310    13823.\n 9 Croatia Europe     1992    72.5 4494013     8448.\n10 Croatia Europe     1997    73.7 4444595     9876.\n11 Croatia Europe     2002    74.9 4481020    11628.\n12 Croatia Europe     2007    75.7 4493312    14619.\n\nsummarize(gapminder, pop_avg = mean(pop))\n\n# A tibble: 1 × 1\n    pop_avg\n      &lt;dbl&gt;\n1 29601212.\n\nsummarize(group_by(gapminder, continent), pop_avg = mean(pop))\n\n# A tibble: 5 × 2\n  continent   pop_avg\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Africa     9916003.\n2 Americas  24504795.\n3 Asia      77038722.\n4 Europe    17169765.\n5 Oceania    8874672.\n\nsummarize(group_by(gapminder, continent, country), pop_avg = mean(pop))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 142 × 3\n# Groups:   continent [5]\n   continent country                    pop_avg\n   &lt;fct&gt;     &lt;fct&gt;                        &lt;dbl&gt;\n 1 Africa    Algeria                  19875406.\n 2 Africa    Angola                    7309390.\n 3 Africa    Benin                     4017497.\n 4 Africa    Botswana                   971186.\n 5 Africa    Burkina Faso              7548677.\n 6 Africa    Burundi                   4651608.\n 7 Africa    Cameroon                  9816648.\n 8 Africa    Central African Republic  2560963 \n 9 Africa    Chad                      5329256.\n10 Africa    Comoros                    361684.\n# ℹ 132 more rows\n\n\n\ngapminder %&gt;%\n   group_by(continent, country) %&gt;%\n   summarize(pop_avg = mean(pop))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 142 × 3\n# Groups:   continent [5]\n   continent country                    pop_avg\n   &lt;fct&gt;     &lt;fct&gt;                        &lt;dbl&gt;\n 1 Africa    Algeria                  19875406.\n 2 Africa    Angola                    7309390.\n 3 Africa    Benin                     4017497.\n 4 Africa    Botswana                   971186.\n 5 Africa    Burkina Faso              7548677.\n 6 Africa    Burundi                   4651608.\n 7 Africa    Cameroon                  9816648.\n 8 Africa    Central African Republic  2560963 \n 9 Africa    Chad                      5329256.\n10 Africa    Comoros                    361684.\n# ℹ 132 more rows\n\n\n\ntemp1 = filter(gapminder, country == \"Croatia\")\ntemp2 = select(temp1, country, year, lifeExp)\ntemp3 = apply(temp2[ , c(\"lifeExp\")], 2, mean)\ntemp3\n\n lifeExp \n70.05592 \n\n\n\ngapminder %&gt;%\n   filter(country == \"Croatia\") %&gt;%\n   select(country, year, lifeExp) %&gt;%\n   summarize(lifeExp_avg = mean(lifeExp))\n\n# A tibble: 1 × 1\n  lifeExp_avg\n        &lt;dbl&gt;\n1        70.1\n\n\ndata in need: avocado.csv\n\n# 04 The reality of data processing #\nlibrary(ggplot2)\navocado &lt;- read.csv(\"data/avocado.csv\", header=TRUE, sep = \",\")\n\nstr(avocado)\n\n'data.frame':   18249 obs. of  14 variables:\n $ X           : int  0 1 2 3 4 5 6 7 8 9 ...\n $ Date        : chr  \"2015-12-27\" \"2015-12-20\" \"2015-12-13\" \"2015-12-06\" ...\n $ AveragePrice: num  1.33 1.35 0.93 1.08 1.28 1.26 0.99 0.98 1.02 1.07 ...\n $ Total.Volume: num  64237 54877 118220 78992 51040 ...\n $ X4046       : num  1037 674 795 1132 941 ...\n $ X4225       : num  54455 44639 109150 71976 43838 ...\n $ X4770       : num  48.2 58.3 130.5 72.6 75.8 ...\n $ Total.Bags  : num  8697 9506 8145 5811 6184 ...\n $ Small.Bags  : num  8604 9408 8042 5677 5986 ...\n $ Large.Bags  : num  93.2 97.5 103.1 133.8 197.7 ...\n $ XLarge.Bags : num  0 0 0 0 0 0 0 0 0 0 ...\n $ type        : chr  \"conventional\" \"conventional\" \"conventional\" \"conventional\" ...\n $ year        : int  2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 ...\n $ region      : chr  \"Albany\" \"Albany\" \"Albany\" \"Albany\" ...\n\n(x_avg = avocado %&gt;% group_by(region) %&gt;% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice)))\n\n# A tibble: 54 × 3\n   region                 V_avg P_avg\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n 1 Albany                47538.  1.56\n 2 Atlanta              262145.  1.34\n 3 BaltimoreWashington  398562.  1.53\n 4 Boise                 42643.  1.35\n 5 Boston               287793.  1.53\n 6 BuffaloRochester      67936.  1.52\n 7 California          3044324.  1.40\n 8 Charlotte            105194.  1.61\n 9 Chicago              395569.  1.56\n10 CincinnatiDayton     131722.  1.21\n# ℹ 44 more rows\n\n(x_avg = avocado %&gt;% group_by(region, year) %&gt;% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice)))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 216 × 4\n# Groups:   region [54]\n   region               year   V_avg P_avg\n   &lt;chr&gt;               &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Albany               2015  38749.  1.54\n 2 Albany               2016  50619.  1.53\n 3 Albany               2017  49355.  1.64\n 4 Albany               2018  64249.  1.44\n 5 Atlanta              2015 223382.  1.38\n 6 Atlanta              2016 272374.  1.21\n 7 Atlanta              2017 271841.  1.43\n 8 Atlanta              2018 342976.  1.29\n 9 BaltimoreWashington  2015 390823.  1.37\n10 BaltimoreWashington  2016 393210.  1.59\n# ℹ 206 more rows\n\nx_avg = avocado %&gt;% group_by(region, year, type) %&gt;% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice))\n\n`summarise()` has grouped output by 'region', 'year'. You can override using\nthe `.groups` argument.\n\navocado %&gt;%\n   group_by(region, year, type) %&gt;%\n   summarize(V_avg = mean(Total.Volume),\n             P_avg = mean(AveragePrice)) -&gt; x_avg\n\n`summarise()` has grouped output by 'region', 'year'. You can override using\nthe `.groups` argument.\n\nx_avg %&gt;% filter(region != \"TotalUS\") %&gt;% \n  ggplot(aes(year, V_avg, col = type)) + geom_line() + facet_wrap(~region)\n\n\n\n\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\narrange(x_avg, desc(V_avg))\n\n# A tibble: 432 × 5\n# Groups:   region, year [216]\n   region        year type             V_avg P_avg\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 TotalUS       2018 conventional 42125533. 1.06 \n 2 TotalUS       2016 conventional 34043450. 1.05 \n 3 TotalUS       2017 conventional 33995658. 1.22 \n 4 TotalUS       2015 conventional 31224729. 1.01 \n 5 SouthCentral  2018 conventional  7465557. 0.806\n 6 West          2018 conventional  7451445. 0.981\n 7 California    2018 conventional  6786962. 1.08 \n 8 West          2016 conventional  6404892. 0.916\n 9 West          2017 conventional  6279482. 1.10 \n10 California    2016 conventional  6105539. 1.05 \n# ℹ 422 more rows\n\nx_avg1 = x_avg %&gt;% filter(region != \"TotalUS\")\n\n\n# After excluding TotalUS, you can process it using statistical functions directly.\n\nx_avg1[x_avg1$V_avg == max(x_avg1$V_avg),]\n\n# A tibble: 1 × 5\n# Groups:   region, year [1]\n  region        year type            V_avg P_avg\n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 SouthCentral  2018 conventional 7465557. 0.806\n\n\n\n# install.packages(\"lubridate\")\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n(x_avg = avocado %&gt;% \n    group_by(region, year, month(Date), type) %&gt;% \n    summarize(V_avg = mean(Total.Volume), \n              P_avg = mean(AveragePrice)))\n\n`summarise()` has grouped output by 'region', 'year', 'month(Date)'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 4,212 × 6\n# Groups:   region, year, month(Date) [2,106]\n   region  year `month(Date)` type          V_avg P_avg\n   &lt;chr&gt;  &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1 Albany  2015             1 conventional 42932.  1.17\n 2 Albany  2015             1 organic       1198.  1.84\n 3 Albany  2015             2 conventional 52343.  1.03\n 4 Albany  2015             2 organic       1334.  1.76\n 5 Albany  2015             3 conventional 50659.  1.06\n 6 Albany  2015             3 organic       1444.  1.83\n 7 Albany  2015             4 conventional 48594.  1.17\n 8 Albany  2015             4 organic       1402.  1.89\n 9 Albany  2015             5 conventional 97216.  1.26\n10 Albany  2015             5 organic       1836.  1.94\n# ℹ 4,202 more rows\n\n\n\n\n\n\nClass\n\n\nIntroduction to Tidyverse\nThe tidyverse is a powerful collection  of  R  packages  that are actually  data tools for transforming and visualizing data. All packages of the tidyverse share an underlying philosophy and common APls.\nThe core packages are: \n\nggplot2, which implements the grammar of graphics. You can use it to visualize your data.\ndplyr is a grammar of data You can use it to solve the most common data manipulation challenges.\ntidyr helps you to create tidy data or data where each variable is in a column, each observation is a row end each value is a column, each observation is a row end each value is a cell. \nreadr is a fast and friendly way to read rectangular\npurrr enhances R’s functional programming (FP)toolkit by providing a complete and consistent set of tools for working with functions and vectors. \ntibble is a modern re-imaginging of the data\nstringr provides a cohesive set of functions designed to make working with strings as easy as possible\nforcats provide a suite of useful  tools that solve common problems with factors.  \n\n\nThe introduction of the tidyverse package in R has significantly influenced the way data science is performed using R, impacting coding practices, data analysis methodologies, and the overall approach to data manipulation and visualization.\n\n\n\nBefore Tidyverse\nBefore the tidyverse, R programming was largely centered around base R functions and packages. This included using base R functions for data manipulation (like subset, merge, and apply functions) and visualization (such as plotting with plot and hist). The syntax and methods varied widely across different packages, which often led to inconsistent coding practices and a steeper learning curve for beginners. Each task could be approached in multiple ways without a clear ‘best’ method, leading to fragmented and less readable code.\n\n\nAfter Tidyverse\nThe tidyverse, developed by Hadley Wickham and others, brought a suite of packages designed to work harmoniously together using a consistent syntax and underlying philosophy. Key features and impacts include:\n\nConsistent Syntax: The tidyverse introduced a consistent and readable syntax that leverages chaining operations using the %&gt;% operator from the magrittr package. This has made code more readable and easier to write and understand, especially for newcomers.\nData Manipulation: With dplyr, data manipulation became more intuitive and less verbose. Functions like filter(), arrange(), select(), mutate(), and summarise() allow for straightforward data operations that are both faster and easier to code compared to base R functions.\nData Importing and Tidying: readr for reading data and tidyr for tidying data introduced more efficient data reading and transforming capabilities, making it simpler to convert data into a tidy format. Tidy data, where each column is a variable and each row is an observation, has become a standard for data analysis, facilitating easier manipulation and analysis.\nVisualization: ggplot2 transformed data visualization in R by allowing for the layering of plots intuitively and flexibly, using a system based on the grammar of graphics. This has enabled users to create complex, publication-quality graphs more easily than was possible with base R plotting functions.\nCommunity and Accessibility: The tidyverse has fostered a strong community and has contributed significantly to teaching materials that are user-friendly and accessible to beginners. This has democratized data analysis in R, making it more accessible to non-programmers.\nImpact on Package Development: The tidyverse’s philosophy and popularity have influenced the development of other packages, even those not part of the tidyverse, to adopt tidy principles and interoperate smoothly with tidyverse packages.\n\n\nThe tidyverse has not only changed the syntax and functionality of R coding but also its philosophy towards data analysis. It promotes a workflow that is coherent, transparent, and efficient, which has been widely adopted in academia, industry, and teaching. While some veteran R users prefer the flexibility and control of base R, the tidyverse’s approachable syntax and powerful capabilities have made it a pivotal tool in modern R programming, particularly for data science.\n\n\nPlease check out the homepage of tidyverse: https://www.tidyverse.org/\n\n\nYou can install the complete tidyverse with:\n&gt; install.packages(“tidyverse”)\nThen, load the core tidyverse and make it available in your current R session by running:\n&gt; library(tidyverse)\nPlease see this cheat sheet:\nhttps://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf\n\n\n\n\n\n\n\n\nPipe\n\nSubset Observations\n\n# filter\niris %&gt;% filter(Sepal.Length &gt; 7)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1           7.1         3.0          5.9         2.1 virginica\n2           7.6         3.0          6.6         2.1 virginica\n3           7.3         2.9          6.3         1.8 virginica\n4           7.2         3.6          6.1         2.5 virginica\n5           7.7         3.8          6.7         2.2 virginica\n6           7.7         2.6          6.9         2.3 virginica\n7           7.7         2.8          6.7         2.0 virginica\n8           7.2         3.2          6.0         1.8 virginica\n9           7.2         3.0          5.8         1.6 virginica\n10          7.4         2.8          6.1         1.9 virginica\n11          7.9         3.8          6.4         2.0 virginica\n12          7.7         3.0          6.1         2.3 virginica\n\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\niris %&gt;% filter(Species == \"setosa\")\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n11          5.4         3.7          1.5         0.2  setosa\n12          4.8         3.4          1.6         0.2  setosa\n13          4.8         3.0          1.4         0.1  setosa\n14          4.3         3.0          1.1         0.1  setosa\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n17          5.4         3.9          1.3         0.4  setosa\n18          5.1         3.5          1.4         0.3  setosa\n19          5.7         3.8          1.7         0.3  setosa\n20          5.1         3.8          1.5         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n22          5.1         3.7          1.5         0.4  setosa\n23          4.6         3.6          1.0         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n28          5.2         3.5          1.5         0.2  setosa\n29          5.2         3.4          1.4         0.2  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n32          5.4         3.4          1.5         0.4  setosa\n33          5.2         4.1          1.5         0.1  setosa\n34          5.5         4.2          1.4         0.2  setosa\n35          4.9         3.1          1.5         0.2  setosa\n36          5.0         3.2          1.2         0.2  setosa\n37          5.5         3.5          1.3         0.2  setosa\n38          4.9         3.6          1.4         0.1  setosa\n39          4.4         3.0          1.3         0.2  setosa\n40          5.1         3.4          1.5         0.2  setosa\n41          5.0         3.5          1.3         0.3  setosa\n42          4.5         2.3          1.3         0.3  setosa\n43          4.4         3.2          1.3         0.2  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n46          4.8         3.0          1.4         0.3  setosa\n47          5.1         3.8          1.6         0.2  setosa\n48          4.6         3.2          1.4         0.2  setosa\n49          5.3         3.7          1.5         0.2  setosa\n50          5.0         3.3          1.4         0.2  setosa\n\niris %&gt;% filter(Species == \"setosa\") %&gt;% plot\n\n\n\niris %&gt;% filter(Species == \"setosa\") %&gt;% summary\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n 1st Qu.:4.800   1st Qu.:3.200   1st Qu.:1.400   1st Qu.:0.200  \n Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n Mean   :5.006   Mean   :3.428   Mean   :1.462   Mean   :0.246  \n 3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n       Species  \n setosa    :50  \n versicolor: 0  \n virginica : 0  \n                \n                \n                \n\n# distinct: remove duplication (take only unique values)\niris %&gt;% distinct(Species)\n\n     Species\n1     setosa\n2 versicolor\n3  virginica\n\n# random sampling\niris %&gt;% nrow\n\n[1] 150\n\niris %&gt;% sample_frac(0.5, replace=T)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1           6.0         2.7          5.1         1.6 versicolor\n2           6.6         3.0          4.4         1.4 versicolor\n3           6.3         2.3          4.4         1.3 versicolor\n4           4.8         3.4          1.9         0.2     setosa\n5           5.1         3.7          1.5         0.4     setosa\n6           5.2         2.7          3.9         1.4 versicolor\n7           6.2         3.4          5.4         2.3  virginica\n8           5.4         3.9          1.3         0.4     setosa\n9           7.3         2.9          6.3         1.8  virginica\n10          6.2         3.4          5.4         2.3  virginica\n11          5.9         3.0          5.1         1.8  virginica\n12          6.9         3.1          5.4         2.1  virginica\n13          4.5         2.3          1.3         0.3     setosa\n14          7.7         2.6          6.9         2.3  virginica\n15          5.6         2.8          4.9         2.0  virginica\n16          7.7         3.0          6.1         2.3  virginica\n17          5.0         3.5          1.6         0.6     setosa\n18          5.1         3.8          1.5         0.3     setosa\n19          5.6         2.9          3.6         1.3 versicolor\n20          6.4         3.2          4.5         1.5 versicolor\n21          6.5         3.0          5.5         1.8  virginica\n22          4.9         2.5          4.5         1.7  virginica\n23          6.3         2.5          5.0         1.9  virginica\n24          4.4         2.9          1.4         0.2     setosa\n25          6.3         3.3          4.7         1.6 versicolor\n26          5.4         3.0          4.5         1.5 versicolor\n27          6.9         3.1          4.9         1.5 versicolor\n28          4.4         2.9          1.4         0.2     setosa\n29          5.4         3.9          1.7         0.4     setosa\n30          4.9         3.1          1.5         0.2     setosa\n31          6.1         2.8          4.0         1.3 versicolor\n32          5.4         3.4          1.7         0.2     setosa\n33          5.8         2.7          4.1         1.0 versicolor\n34          5.7         3.0          4.2         1.2 versicolor\n35          6.0         2.7          5.1         1.6 versicolor\n36          6.5         3.2          5.1         2.0  virginica\n37          5.1         2.5          3.0         1.1 versicolor\n38          5.5         2.3          4.0         1.3 versicolor\n39          6.7         3.1          5.6         2.4  virginica\n40          6.7         3.1          5.6         2.4  virginica\n41          6.8         2.8          4.8         1.4 versicolor\n42          5.0         2.0          3.5         1.0 versicolor\n43          5.5         3.5          1.3         0.2     setosa\n44          5.8         2.7          5.1         1.9  virginica\n45          5.2         2.7          3.9         1.4 versicolor\n46          7.0         3.2          4.7         1.4 versicolor\n47          6.3         2.9          5.6         1.8  virginica\n48          4.8         3.4          1.6         0.2     setosa\n49          6.4         2.8          5.6         2.2  virginica\n50          5.9         3.2          4.8         1.8 versicolor\n51          4.9         2.4          3.3         1.0 versicolor\n52          4.9         3.1          1.5         0.2     setosa\n53          4.8         3.0          1.4         0.3     setosa\n54          6.4         2.9          4.3         1.3 versicolor\n55          4.4         3.2          1.3         0.2     setosa\n56          5.7         3.0          4.2         1.2 versicolor\n57          5.7         2.6          3.5         1.0 versicolor\n58          5.5         2.6          4.4         1.2 versicolor\n59          5.6         2.7          4.2         1.3 versicolor\n60          6.5         2.8          4.6         1.5 versicolor\n61          5.1         3.7          1.5         0.4     setosa\n62          6.4         2.8          5.6         2.1  virginica\n63          5.4         3.9          1.3         0.4     setosa\n64          6.4         3.2          4.5         1.5 versicolor\n65          5.5         3.5          1.3         0.2     setosa\n66          5.7         2.9          4.2         1.3 versicolor\n67          6.9         3.2          5.7         2.3  virginica\n68          6.7         3.3          5.7         2.1  virginica\n69          4.6         3.4          1.4         0.3     setosa\n70          6.7         3.0          5.0         1.7 versicolor\n71          5.6         2.5          3.9         1.1 versicolor\n72          5.3         3.7          1.5         0.2     setosa\n73          5.6         2.8          4.9         2.0  virginica\n74          6.0         2.2          4.0         1.0 versicolor\n75          7.2         3.6          6.1         2.5  virginica\n\niris %&gt;% sample_frac(0.5, replace=T) %&gt;% nrow\n\n[1] 75\n\niris %&gt;% sample_n(10, replace=T) \n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1           5.1         3.8          1.6         0.2     setosa\n2           4.5         2.3          1.3         0.3     setosa\n3           7.2         3.6          6.1         2.5  virginica\n4           5.8         2.7          5.1         1.9  virginica\n5           7.2         3.6          6.1         2.5  virginica\n6           6.4         3.2          5.3         2.3  virginica\n7           6.8         3.0          5.5         2.1  virginica\n8           6.3         2.9          5.6         1.8  virginica\n9           6.0         2.7          5.1         1.6 versicolor\n10          6.0         2.9          4.5         1.5 versicolor\n\niris %&gt;% sample_n(10, replace=T) %&gt;% nrow \n\n[1] 10\n\n# slice \niris %&gt;% slice(10:15)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          4.9         3.1          1.5         0.1  setosa\n2          5.4         3.7          1.5         0.2  setosa\n3          4.8         3.4          1.6         0.2  setosa\n4          4.8         3.0          1.4         0.1  setosa\n5          4.3         3.0          1.1         0.1  setosa\n6          5.8         4.0          1.2         0.2  setosa\n\n# Top n in X\niris %&gt;% top_n(5, Sepal.Length)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          7.7         3.8          6.7         2.2 virginica\n2          7.7         2.6          6.9         2.3 virginica\n3          7.7         2.8          6.7         2.0 virginica\n4          7.9         3.8          6.4         2.0 virginica\n5          7.7         3.0          6.1         2.3 virginica\n\niris %&gt;% top_n(5, Sepal.Width)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.4         3.9          1.7         0.4  setosa\n2          5.8         4.0          1.2         0.2  setosa\n3          5.7         4.4          1.5         0.4  setosa\n4          5.4         3.9          1.3         0.4  setosa\n5          5.2         4.1          1.5         0.1  setosa\n6          5.5         4.2          1.4         0.2  setosa\n\n\nSubset Variables\n\n# pull & select\niris %&gt;% pull(Petal.Width)\n\n  [1] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3\n [19] 0.3 0.3 0.2 0.4 0.2 0.5 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.1 0.2 0.2 0.2\n [37] 0.2 0.1 0.2 0.2 0.3 0.3 0.2 0.6 0.4 0.3 0.2 0.2 0.2 0.2 1.4 1.5 1.5 1.3\n [55] 1.5 1.3 1.6 1.0 1.3 1.4 1.0 1.5 1.0 1.4 1.3 1.4 1.5 1.0 1.5 1.1 1.8 1.3\n [73] 1.5 1.2 1.3 1.4 1.4 1.7 1.5 1.0 1.1 1.0 1.2 1.6 1.5 1.6 1.5 1.3 1.3 1.3\n [91] 1.2 1.4 1.2 1.0 1.3 1.2 1.3 1.3 1.1 1.3 2.5 1.9 2.1 1.8 2.2 2.1 1.7 1.8\n[109] 1.8 2.5 2.0 1.9 2.1 2.0 2.4 2.3 1.8 2.2 2.3 1.5 2.3 2.0 2.0 1.8 2.1 1.8\n[127] 1.8 1.8 2.1 1.6 1.9 2.0 2.2 1.5 1.4 2.3 2.4 1.8 1.8 2.1 2.4 2.3 1.9 2.3\n[145] 2.5 2.3 1.9 2.0 2.3 1.8\n\niris %&gt;% select(Petal.Width)\n\n    Petal.Width\n1           0.2\n2           0.2\n3           0.2\n4           0.2\n5           0.2\n6           0.4\n7           0.3\n8           0.2\n9           0.2\n10          0.1\n11          0.2\n12          0.2\n13          0.1\n14          0.1\n15          0.2\n16          0.4\n17          0.4\n18          0.3\n19          0.3\n20          0.3\n21          0.2\n22          0.4\n23          0.2\n24          0.5\n25          0.2\n26          0.2\n27          0.4\n28          0.2\n29          0.2\n30          0.2\n31          0.2\n32          0.4\n33          0.1\n34          0.2\n35          0.2\n36          0.2\n37          0.2\n38          0.1\n39          0.2\n40          0.2\n41          0.3\n42          0.3\n43          0.2\n44          0.6\n45          0.4\n46          0.3\n47          0.2\n48          0.2\n49          0.2\n50          0.2\n51          1.4\n52          1.5\n53          1.5\n54          1.3\n55          1.5\n56          1.3\n57          1.6\n58          1.0\n59          1.3\n60          1.4\n61          1.0\n62          1.5\n63          1.0\n64          1.4\n65          1.3\n66          1.4\n67          1.5\n68          1.0\n69          1.5\n70          1.1\n71          1.8\n72          1.3\n73          1.5\n74          1.2\n75          1.3\n76          1.4\n77          1.4\n78          1.7\n79          1.5\n80          1.0\n81          1.1\n82          1.0\n83          1.2\n84          1.6\n85          1.5\n86          1.6\n87          1.5\n88          1.3\n89          1.3\n90          1.3\n91          1.2\n92          1.4\n93          1.2\n94          1.0\n95          1.3\n96          1.2\n97          1.3\n98          1.3\n99          1.1\n100         1.3\n101         2.5\n102         1.9\n103         2.1\n104         1.8\n105         2.2\n106         2.1\n107         1.7\n108         1.8\n109         1.8\n110         2.5\n111         2.0\n112         1.9\n113         2.1\n114         2.0\n115         2.4\n116         2.3\n117         1.8\n118         2.2\n119         2.3\n120         1.5\n121         2.3\n122         2.0\n123         2.0\n124         1.8\n125         2.1\n126         1.8\n127         1.8\n128         1.8\n129         2.1\n130         1.6\n131         1.9\n132         2.0\n133         2.2\n134         1.5\n135         1.4\n136         2.3\n137         2.4\n138         1.8\n139         1.8\n140         2.1\n141         2.4\n142         2.3\n143         1.9\n144         2.3\n145         2.5\n146         2.3\n147         1.9\n148         2.0\n149         2.3\n150         1.8\n\niris %&gt;% pull(Petal.Width) %&gt;% str\n\n num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n\niris %&gt;% select(Petal.Width) %&gt;% str\n\n'data.frame':   150 obs. of  1 variable:\n $ Petal.Width: num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n\niris %&gt;% select(Petal.Length, Petal.Width) %&gt;% head\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\n# useful helpers: starts_with(), contains()\niris %&gt;% select(starts_with(\"Peta\")) %&gt;% head\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\niris %&gt;% select(contains(\"tal\")) %&gt;% head\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\n\nReshaping & Arrange data\n\n# let's use mtcars dataset\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# About mtcars dataset\n# help(mtcars)\n\n# arrange\nmtcars %&gt;% arrange(mpg) %&gt;% head\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\nMaserati Bora       15.0   8  301 335 3.54 3.570 14.60  0  1    5    8\n\nmtcars %&gt;% add_rownames %&gt;% head\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\n\n# A tibble: 6 × 12\n  rowname        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6 Valiant       18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n\nmtcars %&gt;% add_rownames %&gt;% arrange(mpg) %&gt;% \nhead\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\n\n# A tibble: 6 × 12\n  rowname        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Cadillac Fl…  10.4     8   472   205  2.93  5.25  18.0     0     0     3     4\n2 Lincoln Con…  10.4     8   460   215  3     5.42  17.8     0     0     3     4\n3 Camaro Z28    13.3     8   350   245  3.73  3.84  15.4     0     0     3     4\n4 Duster 360    14.3     8   360   245  3.21  3.57  15.8     0     0     3     4\n5 Chrysler Im…  14.7     8   440   230  3.23  5.34  17.4     0     0     3     4\n6 Maserati Bo…  15       8   301   335  3.54  3.57  14.6     0     1     5     8\n\nmtcars %&gt;% add_rownames %&gt;% arrange(desc(mpg)) %&gt;% select(rowname)\n\nWarning: `add_rownames()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::rownames_to_column()` instead.\n\n\n# A tibble: 32 × 1\n   rowname       \n   &lt;chr&gt;         \n 1 Toyota Corolla\n 2 Fiat 128      \n 3 Honda Civic   \n 4 Lotus Europa  \n 5 Fiat X1-9     \n 6 Porsche 914-2 \n 7 Merc 240D     \n 8 Datsun 710    \n 9 Merc 230      \n10 Toyota Corona \n# ℹ 22 more rows\n\n\nSummarise data\n\n# Summarise\n\niris %&gt;% summarise(avg.PL=mean(Petal.Length))\n\n  avg.PL\n1  3.758\n\niris %&gt;% summarise(sd.PL=sd(Petal.Length))\n\n     sd.PL\n1 1.765298\n\niris %&gt;% summarise(avg.PL=mean(Petal.Length),\n                   sd.PL=sd(Petal.Length),\n                   min.PL=min(Petal.Length))\n\n  avg.PL    sd.PL min.PL\n1  3.758 1.765298      1\n\niris %&gt;% count(Species)\n\n     Species  n\n1     setosa 50\n2 versicolor 50\n3  virginica 50\n\niris %&gt;% sample_frac(0.3) %&gt;% count(Species)\n\n     Species  n\n1     setosa 10\n2 versicolor 17\n3  virginica 18\n\niris %&gt;% summarise_all(mean)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `Species = (function (x, ...) ...`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1     5.843333    3.057333        3.758    1.199333      NA\n\niris %&gt;% summarise_at(\"Petal.Length\", sum)\n\n  Petal.Length\n1        563.7\n\niris %&gt;% summarise_at(c(\"Petal.Length\", \"Petal.Width\"), mean)\n\n  Petal.Length Petal.Width\n1        3.758    1.199333\n\n\nGroup and Summarise data\n\n# Group\niris %&gt;% group_by(Species)\n\n# A tibble: 150 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\niris %&gt;% group_by(Species) %&gt;% \n  summarise_all(mean)\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\niris %&gt;% group_by(Species) %&gt;% \n  select(starts_with(\"Sep\")) %&gt;% \n  summarise_all(mean)\n\nAdding missing grouping variables: `Species`\n\n\n# A tibble: 3 × 3\n  Species    Sepal.Length Sepal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43\n2 versicolor         5.94        2.77\n3 virginica          6.59        2.97\n\nmtcars %&gt;% group_by(am) %&gt;% \n  summarise(hp.avg=mean(hp),\n            hp.sd=sd(hp))\n\n# A tibble: 2 × 3\n     am hp.avg hp.sd\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     0   160.  53.9\n2     1   127.  84.1\n\n\nMake New Variables\n\n# mutate\n# mpg to kml\nmtcars %&gt;% mutate(kml=0.425144*mpg)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                          kml\nMazda RX4            8.928024\nMazda RX4 Wag        8.928024\nDatsun 710           9.693283\nHornet 4 Drive       9.098082\nHornet Sportabout    7.950193\nValiant              7.695106\nDuster 360           6.079559\nMerc 240D           10.373514\nMerc 230             9.693283\nMerc 280             8.162765\nMerc 280C            7.567563\nMerc 450SE           6.972362\nMerc 450SL           7.354991\nMerc 450SLC          6.462189\nCadillac Fleetwood   4.421498\nLincoln Continental  4.421498\nChrysler Imperial    6.249617\nFiat 128            13.774666\nHonda Civic         12.924378\nToyota Corolla      14.412382\nToyota Corona        9.140596\nDodge Challenger     6.589732\nAMC Javelin          6.462189\nCamaro Z28           5.654415\nPontiac Firebird     8.162765\nFiat X1-9           11.606431\nPorsche 914-2       11.053744\nLotus Europa        12.924378\nFord Pantera L       6.717275\nFerrari Dino         8.375337\nMaserati Bora        6.377160\nVolvo 142E           9.098082\n\nmtcars %&gt;% mutate(kml=round(0.425144*mpg, 1))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb  kml\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.7\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  8.0\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.7\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.1\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.4\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.7\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.2\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.6\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  7.0\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.4\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.5\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.8\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.6\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.5\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.7\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.1\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.4\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.1\n\nmtcars %&gt;% mutate(kml=round(0.425144*mpg, 1)) %&gt;% \n  select(mpg, kml) %&gt;% \n  top_n(5, mpg)\n\n                mpg  kml\nFiat 128       32.4 13.8\nHonda Civic    30.4 12.9\nToyota Corolla 33.9 14.4\nFiat X1-9      27.3 11.6\nLotus Europa   30.4 12.9\n\n# transmute\nmtcars %&gt;% transmute(hp/wt)\n\n                       hp/wt\nMazda RX4           41.98473\nMazda RX4 Wag       38.26087\nDatsun 710          40.08621\nHornet 4 Drive      34.21462\nHornet Sportabout   50.87209\nValiant             30.34682\nDuster 360          68.62745\nMerc 240D           19.43574\nMerc 230            30.15873\nMerc 280            35.75581\nMerc 280C           35.75581\nMerc 450SE          44.22604\nMerc 450SL          48.25737\nMerc 450SLC         47.61905\nCadillac Fleetwood  39.04762\nLincoln Continental 39.63864\nChrysler Imperial   43.03087\nFiat 128            30.00000\nHonda Civic         32.19814\nToyota Corolla      35.42234\nToyota Corona       39.35091\nDodge Challenger    42.61364\nAMC Javelin         43.66812\nCamaro Z28          63.80208\nPontiac Firebird    45.51365\nFiat X1-9           34.10853\nPorsche 914-2       42.52336\nLotus Europa        74.68605\nFord Pantera L      83.28076\nFerrari Dino        63.17690\nMaserati Bora       93.83754\nVolvo 142E          39.20863\n\n# rename\nmtcars %&gt;% names\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\nmtcars %&gt;% mutate(new=1) %&gt;% head\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb new\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   1\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   1\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   1\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   1\n\nmtcars %&gt;% mutate(new=1) %&gt;% \nrename(change.name=new) %&gt;% head\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n                  change.name\nMazda RX4                   1\nMazda RX4 Wag               1\nDatsun 710                  1\nHornet 4 Drive              1\nHornet Sportabout           1\nValiant                     1\n\n\nCombine Data Sets\n\n\n\n\n\n\n\n\nJoins!\nThanks to Garrick Aden-Buie (garrickadenbuie.com), we can teach joins very easily with his fantastic diagrams.\n\nLet’s use some example datasets that come pre-loaded in R to demonstrate how joins work using dplyr. We will use the mtcars dataset, splitting it into two separate data frames to illustrate how the different types of joins operate.\nFirst, let’s set up our example data frames from the mtcars dataset:\n\n# Create two data frames from mtcars\ndf1 &lt;- mtcars[1:10, c(\"mpg\", \"cyl\", \"disp\")]  # first 10 rows, select specific columns\ndf2 &lt;- mtcars[5:15, c(\"disp\", \"hp\", \"drat\")]  # rows 5 to 15, select specific columns\n\n# Make sure there's a common key for joining; here, we'll use 'disp'\n# Print out the data frames to see what they contain\nprint(df1)\n\n                   mpg cyl  disp\nMazda RX4         21.0   6 160.0\nMazda RX4 Wag     21.0   6 160.0\nDatsun 710        22.8   4 108.0\nHornet 4 Drive    21.4   6 258.0\nHornet Sportabout 18.7   8 360.0\nValiant           18.1   6 225.0\nDuster 360        14.3   8 360.0\nMerc 240D         24.4   4 146.7\nMerc 230          22.8   4 140.8\nMerc 280          19.2   6 167.6\n\nprint(df2)\n\n                    disp  hp drat\nHornet Sportabout  360.0 175 3.15\nValiant            225.0 105 2.76\nDuster 360         360.0 245 3.21\nMerc 240D          146.7  62 3.69\nMerc 230           140.8  95 3.92\nMerc 280           167.6 123 3.92\nMerc 280C          167.6 123 3.92\nMerc 450SE         275.8 180 3.07\nMerc 450SL         275.8 180 3.07\nMerc 450SLC        275.8 180 3.07\nCadillac Fleetwood 472.0 205 2.93\n\n\n\nInner Join\n\nAll rows from x where there are matching values in y, and all columns from x and y.\n\n\n\ninner_join_result &lt;- inner_join(df1, df2, by = \"disp\")\n\nWarning in inner_join(df1, df2, by = \"disp\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nprint(inner_join_result)\n\n   mpg cyl  disp  hp drat\n1 18.7   8 360.0 175 3.15\n2 18.7   8 360.0 245 3.21\n3 18.1   6 225.0 105 2.76\n4 14.3   8 360.0 175 3.15\n5 14.3   8 360.0 245 3.21\n6 24.4   4 146.7  62 3.69\n7 22.8   4 140.8  95 3.92\n8 19.2   6 167.6 123 3.92\n9 19.2   6 167.6 123 3.92\n\n\n\n\nLeft Join\n\nAll rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns.\n\n\n\nleft_join_result &lt;- left_join(df1, df2, by = \"disp\")\n\nWarning in left_join(df1, df2, by = \"disp\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nprint(left_join_result)\n\n    mpg cyl  disp  hp drat\n1  21.0   6 160.0  NA   NA\n2  21.0   6 160.0  NA   NA\n3  22.8   4 108.0  NA   NA\n4  21.4   6 258.0  NA   NA\n5  18.7   8 360.0 175 3.15\n6  18.7   8 360.0 245 3.21\n7  18.1   6 225.0 105 2.76\n8  14.3   8 360.0 175 3.15\n9  14.3   8 360.0 245 3.21\n10 24.4   4 146.7  62 3.69\n11 22.8   4 140.8  95 3.92\n12 19.2   6 167.6 123 3.92\n13 19.2   6 167.6 123 3.92\n\n\n\n\nLeft Join (Extra Rows in y)\n\n… If there are multiple matches between x and y, all combinations of the matches are returned.\n\n\n\n\nRight Join\n\nAll rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns.\n\n\n\nright_join_result &lt;- right_join(df1, df2, by = \"disp\")\n\nWarning in right_join(df1, df2, by = \"disp\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nprint(right_join_result)\n\n    mpg cyl  disp  hp drat\n1  18.7   8 360.0 175 3.15\n2  18.7   8 360.0 245 3.21\n3  18.1   6 225.0 105 2.76\n4  14.3   8 360.0 175 3.15\n5  14.3   8 360.0 245 3.21\n6  24.4   4 146.7  62 3.69\n7  22.8   4 140.8  95 3.92\n8  19.2   6 167.6 123 3.92\n9  19.2   6 167.6 123 3.92\n10   NA  NA 275.8 180 3.07\n11   NA  NA 275.8 180 3.07\n12   NA  NA 275.8 180 3.07\n13   NA  NA 472.0 205 2.93\n\n\n\n\nFull Join\n\nAll rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing.\n\n\n\nfull_join_result &lt;- full_join(df1, df2, by = \"disp\")\n\nWarning in full_join(df1, df2, by = \"disp\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nprint(full_join_result)\n\n    mpg cyl  disp  hp drat\n1  21.0   6 160.0  NA   NA\n2  21.0   6 160.0  NA   NA\n3  22.8   4 108.0  NA   NA\n4  21.4   6 258.0  NA   NA\n5  18.7   8 360.0 175 3.15\n6  18.7   8 360.0 245 3.21\n7  18.1   6 225.0 105 2.76\n8  14.3   8 360.0 175 3.15\n9  14.3   8 360.0 245 3.21\n10 24.4   4 146.7  62 3.69\n11 22.8   4 140.8  95 3.92\n12 19.2   6 167.6 123 3.92\n13 19.2   6 167.6 123 3.92\n14   NA  NA 275.8 180 3.07\n15   NA  NA 275.8 180 3.07\n16   NA  NA 275.8 180 3.07\n17   NA  NA 472.0 205 2.93\n\n\nFiltering Joins\n\nFiltering joins match observations in the same way as mutating joins, but affect the observations, not the variables. … Semi-joins are useful for matching filtered summary tables back to the original rows. … Anti-joins are useful for diagnosing join mismatches.\nR for Data Science: Filtering Joins\n\n\n\nSemi Join\n\nAll rows from x where there are matching values in y, keeping just columns from x.\n\n\n\nsemi_join_result &lt;- semi_join(df1, df2, by = \"disp\")\nprint(semi_join_result)\n\n                   mpg cyl  disp\nHornet Sportabout 18.7   8 360.0\nValiant           18.1   6 225.0\nDuster 360        14.3   8 360.0\nMerc 240D         24.4   4 146.7\nMerc 230          22.8   4 140.8\nMerc 280          19.2   6 167.6\n\n\n\n\nAnti Join\n\nAll rows from x where there are not matching values in y, keeping just column\n\n\n\n# Assuming df1 and df2 from the previous example\nanti_join_result &lt;- anti_join(df1, df2, by = \"disp\")\nprint(anti_join_result)\n\n                mpg cyl disp\nMazda RX4      21.0   6  160\nMazda RX4 Wag  21.0   6  160\nDatsun 710     22.8   4  108\nHornet 4 Drive 21.4   6  258"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/05_week.html",
    "href": "teaching/ds101/weekly/posts/05_week.html",
    "title": "Data Manipulation",
    "section": "",
    "text": "Weekly design"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/03_week.html",
    "href": "teaching/ds101/weekly/posts/03_week.html",
    "title": "Basic Syntax (2)",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nArray\n\n\n\n\n# Create N-dimensional array\n\n# Assign values 1 to 5 to a 2×4 matrix\nx = array(1:5, c(2, 4)) \n\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    2\n[2,]    2    4    1    3\n\n# Print row 1 element value\nx[1, ] \n\n[1] 1 3 5 2\n\n# Print 2nd column element values\nx[, 2] \n\n[1] 3 4\n\n# Set row and column names\ndimnamex = list(c(\"1st\", \"2nd\"), c(\"1st\", \"2nd\", \"3rd\", \"4th\")) \n\nx = array(1:5, c(2, 4), dimnames = dimnamex)\nx\n\n    1st 2nd 3rd 4th\n1st   1   3   5   2\n2nd   2   4   1   3\n\nx[\"1st\", ]\n\n1st 2nd 3rd 4th \n  1   3   5   2 \n\nx[, \"4th\"]\n\n1st 2nd \n  2   3 \n\n# Create a two-dimensional array\nx = 1:12\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\nmatrix(x, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(x, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# Create an array by combining vectors\nv1 = c(1, 2, 3, 4)\nv2 = c(5, 6, 7, 8)\nv3 = c(9, 10, 11, 12)\n\n# Create an array by binding by column\ncbind(v1, v2, v3) \n\n     v1 v2 v3\n[1,]  1  5  9\n[2,]  2  6 10\n[3,]  3  7 11\n[4,]  4  8 12\n\n# Create array by binding row by row\nrbind(v1, v2, v3) \n\n   [,1] [,2] [,3] [,4]\nv1    1    2    3    4\nv2    5    6    7    8\nv3    9   10   11   12\n\n# Various matrix operations using the operators in [Table 3-7]\n# Store two 2×2 matrices in x and y, respectively\nx = array(1:4, dim = c(2, 2))\ny = array(5:8, dim = c(2, 2))\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\ny\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nx+y\n\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n\nx-y\n\n     [,1] [,2]\n[1,]   -4   -4\n[2,]   -4   -4\n\n# multiplication for each column\nx * y \n\n     [,1] [,2]\n[1,]    5   21\n[2,]   12   32\n\n# mathematical matrix multiplication\nx %*% y \n\n     [,1] [,2]\n[1,]   23   31\n[2,]   34   46\n\n# transpose matrix of x\nt(x) \n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n# inverse of x\nsolve(x) \n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\n# determinant of x\ndet(x) \n\n[1] -2\n\nx = array(1:12, c(3, 4))\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n# If the center value is 1, apply the function row by row\napply(x, 1, mean) \n\n[1] 5.5 6.5 7.5\n\n# If the center value is 2, apply the function to each column\napply(x, 2, mean) \n\n[1]  2  5  8 11\n\nx = array(1:12, c(3, 4))\ndim(x)\n\n[1] 3 4\n\nx = array(1:12, c(3, 4))\n\n# Randomly mix and extract array elements\nsample(x) \n\n [1]  6  7  8 12  9  3  1  5  4 11 10  2\n\n# Select and extract 10 elements from the array\nsample(x, 10) \n\n [1]  2  1  5  9  7 12  3  8  4 11\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# ?sample\n\n# The extraction probability for each element can be varied\nsample(x, 10, prob = c(1:12)/24) \n\n [1] 12  5 11  8 10  4  6  2  3  9\n\n# You can create a sample using just numbers\nsample(10) \n\n [1]  2  3  6  4  8  7  1  5  9 10\n\n\n\n\n\n\nClass\n\n\nCreate a new project\n\n*.Rproj\n*.R\ngetwd()\n\nVariable and Object\n\nAn object in R is a data structure used for storing data: Everything in R is an object, including functions, numbers, character strings, vectors, and lists. Each object has attributes such as its type (e.g., integer, numeric, character), its length, and often its dimensions. Objects can be complex structures, like data frames that hold tabular data, or simpler structures like a single numeric value or vector.\nA variable in R is a name that you assign to an object so that you can refer to it later in your code. When you assign data to a variable, you are effectively labeling that data with a name that you can use to call up the object later on.\n\n\nHere’s a simple example in R:\n\nmy_vector &lt;- c(1, 2, 3)\n\n\nmy_vector is a variable. It’s a symbolic name that we’re using to refer to some data we’re interested in.\nc(1, 2, 3) creates a vector object containing the numbers 1, 2, and 3.\nThis vector is the object, and it’s the actual data structure that R is storing in memory.\n\n\n# remove all objects stored\nrm()\n\n# Create a vector 1 to 10\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# Sampling 10 values from the vector 1:10\nsample(1:10, 10)\n\n [1]  8  4  5  9 10  7  6  1  2  3\n\nX &lt;- sample(1:10, 10)\n# Extract 2nd to 5th elements of X\nX[2:5]\n\n[1]  5 10  3  8\n\n\n\n\nVectorized codes\n\n\nc(1, 2, 4) + c(2, 3, 5)\n\n[1] 3 5 9\n\n\n\n\nX &lt;- c(1,2,4,5)\n\nX * 2\n\n[1]  2  4  8 10\n\n\n\nRecycling rule\n\n\n1:4 + c(1, 2)\n\n[1] 2 4 4 6\n\nX&lt;-c(1,2,4,5)\nX * 2\n\n[1]  2  4  8 10\n\n1:4 + 1:3\n\nWarning in 1:4 + 1:3: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 5\n\n\n\nPop-up Qz\nChoose two if its type cannot be ‘factor’ variable in R\n\nGPA\nBlood type\nGrade (A,B,C,D,F)\nHeight\nGender\n\n\nWhat is the result of the following R code?\n\nmy_vector &lt;- c(3.5, -1.6, TRUE, \"R\")\nclass(my_vector)\n\n[1] \"character\"\n\n\n\n“numeric”\n“logical”\n“character”\n“complex”\n\n\nConsider the following R code. Which of the following is the correct way to access the second element of the my_vector?\n\nmy_vector &lt;- c(10, \"20\", 30)\nsum(as.numeric(my_vector))\n\n[1] 60\n\n\n\n60\n“60”\n40\nAn error\n\n\n\n\n\nUnderstanding Arrays in R: Concepts and Examples\nArrays are a fundamental data structure in R that extend vectors by allowing you to store multi-dimensional data. While a vector has one dimension, arrays in R can have two or more dimensions, making them incredibly versatile for complex data organization.\n\nWhat is an Array in R?\nAn array in R is a collection of elements of the same type arranged in a grid of a specified dimensionality. It is a multi-dimensional data structure that can hold values in more than two dimensions. Arrays are particularly useful in scenarios where operations on multi-dimensional data are required, such as matrix computations, tabulations, and various applications in data analysis and statistics.\n\n\nCreating an Array\nTo create an array in R, you can use the array function. This function takes a vector of data and a vector of dimensions as arguments. For example:\n\n# Create a 2x3 array\nmy_array &lt;- array(1:6, dim = c(2, 3))\nprint(my_array)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nThis code snippet creates a 2x3 array (2 rows and 3 columns) with the numbers 1 to 6.\n\n\nAccessing Array Elements\nElements within an array can be accessed using indices for each dimension in square brackets []. For example:\n\n# Access the element in the 1st row and 2nd column\nelement &lt;- my_array[1, 2]\nprint(element)\n\n[1] 3\n\n\n\n\nModifying Arrays\nJust like vectors, you can modify the elements of an array by accessing them using their indices and assigning new values. For example:\n\n# Modify the element in the 1st row and 2nd column to be 20\nmy_array[1, 2] &lt;- 20\nprint(my_array)\n\n     [,1] [,2] [,3]\n[1,]    1   20    5\n[2,]    2    4    6\n\n\n\n\nOperations on Arrays\nR allows you to perform operations on arrays. These operations can be element-wise or can involve the entire array. For example, you can add two arrays of the same dimensions, and R will perform element-wise addition.\n\n\nExample: Creating and Manipulating a 3D Array\n\n# Create a 3x2x2 array\nmy_3d_array &lt;- array(1:12, dim = c(3, 2, 2))\nprint(my_3d_array)\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n# Access an element (2nd row, 1st column, 2nd matrix)\nelement_3d &lt;- my_3d_array[2, 1, 2]\nprint(element_3d)\n\n[1] 8\n\n\n\n\n\nQuiz: Test Your Understanding of Arrays in R\n\nQuestion 1: What is the output when accessing the third element in the second row and first column of a 3x3x3 array filled with elements from 1 to 27?\nA) 3\nB) 12\nC) 21\nD) 9\n\nQuestion 2: Which of the following statements creates a 2x2x3 array containing the numbers 1 through 12 in R?\nA) array(1:12, dim = c(2, 2, 3))\nB) matrix(1:12, nrow = 2, ncol = 2)\nC) c(1:12)\nD) array(1:12, dim = c(3, 2, 2))\n\nQuestion 3: How do you modify the element at position [1, 1, 1] in a 3-dimensional array named ‘arr’ to have a value of 100?\nA) arr[1] &lt;- 100\nB) arr[1, 1, 1] &lt;- 100\nC) arr[c(1, 1, 1)] &lt;- 100\nD) Both B and C are correct.\n\n\n\nAnswers:\nAnswer 1: B) 12\nExplanation: Arrays in R are filled column-wise, so the third element in the second row and first column of the second matrix would be 12.\nAnswer 2: A) array(1:12, dim = c(2, 2, 3))\nExplanation: The array function with dimension argument c(2, 2, 3) will create a 2x2x3 array, filling the elements from 1 to 12 across the dimensions.\nAnswer 3: B) arr[1, 1, 1] &lt;- 100\nExplanation: To modify a specific element in an array, you need to specify all its indices. The correct way is arr[1, 1, 1] &lt;- 100.\n\n\n\n\nCOV19 matrix and visualization\n\nData import\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ ggplot2   3.4.4     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(stringdist)\n\n\nAttaching package: 'stringdist'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nImport Johns-Hopkins covid19 data\n\nclean_jhd_to_long &lt;- function(df) {\n  df_str &lt;- deparse(substitute(df))\n  var_str &lt;- substr(df_str, 1, str_length(df_str) - 4)\n  \n  df %&gt;% group_by(`Country/Region`) %&gt;%\n    filter(`Country/Region` != \"Cruise Ship\") %&gt;%\n    select(-`Province/State`, -Lat, -Long) %&gt;%\n    mutate_at(vars(-group_cols()), sum) %&gt;% \n    distinct() %&gt;%\n    ungroup() %&gt;%\n    rename(country = `Country/Region`) %&gt;%\n    pivot_longer(\n      -country, \n      names_to = \"date_str\", \n      values_to = var_str\n    ) %&gt;%\n    mutate(date = mdy(date_str)) %&gt;%\n    select(country, date, !! sym(var_str)) \n}\n\nconfirmed_raw &lt;- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndeaths_raw &lt;- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSee the data\n\nhead(confirmed_raw, 10)\n\n# A tibble: 10 × 1,147\n   `Province/State`  `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;              Afghanistan       33.9  67.7          0         0         0\n 2 &lt;NA&gt;              Albania           41.2  20.2          0         0         0\n 3 &lt;NA&gt;              Algeria           28.0   1.66         0         0         0\n 4 &lt;NA&gt;              Andorra           42.5   1.52         0         0         0\n 5 &lt;NA&gt;              Angola           -11.2  17.9          0         0         0\n 6 &lt;NA&gt;              Antarctica       -71.9  23.3          0         0         0\n 7 &lt;NA&gt;              Antigua and Bar…  17.1 -61.8          0         0         0\n 8 &lt;NA&gt;              Argentina        -38.4 -63.6          0         0         0\n 9 &lt;NA&gt;              Armenia           40.1  45.0          0         0         0\n10 Australian Capit… Australia        -35.5 149.           0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\nCountries in data\n\nunique(confirmed_raw$`Country/Region`)\n\n  [1] \"Afghanistan\"                      \"Albania\"                         \n  [3] \"Algeria\"                          \"Andorra\"                         \n  [5] \"Angola\"                           \"Antarctica\"                      \n  [7] \"Antigua and Barbuda\"              \"Argentina\"                       \n  [9] \"Armenia\"                          \"Australia\"                       \n [11] \"Austria\"                          \"Azerbaijan\"                      \n [13] \"Bahamas\"                          \"Bahrain\"                         \n [15] \"Bangladesh\"                       \"Barbados\"                        \n [17] \"Belarus\"                          \"Belgium\"                         \n [19] \"Belize\"                           \"Benin\"                           \n [21] \"Bhutan\"                           \"Bolivia\"                         \n [23] \"Bosnia and Herzegovina\"           \"Botswana\"                        \n [25] \"Brazil\"                           \"Brunei\"                          \n [27] \"Bulgaria\"                         \"Burkina Faso\"                    \n [29] \"Burma\"                            \"Burundi\"                         \n [31] \"Cabo Verde\"                       \"Cambodia\"                        \n [33] \"Cameroon\"                         \"Canada\"                          \n [35] \"Central African Republic\"         \"Chad\"                            \n [37] \"Chile\"                            \"China\"                           \n [39] \"Colombia\"                         \"Comoros\"                         \n [41] \"Congo (Brazzaville)\"              \"Congo (Kinshasa)\"                \n [43] \"Costa Rica\"                       \"Cote d'Ivoire\"                   \n [45] \"Croatia\"                          \"Cuba\"                            \n [47] \"Cyprus\"                           \"Czechia\"                         \n [49] \"Denmark\"                          \"Diamond Princess\"                \n [51] \"Djibouti\"                         \"Dominica\"                        \n [53] \"Dominican Republic\"               \"Ecuador\"                         \n [55] \"Egypt\"                            \"El Salvador\"                     \n [57] \"Equatorial Guinea\"                \"Eritrea\"                         \n [59] \"Estonia\"                          \"Eswatini\"                        \n [61] \"Ethiopia\"                         \"Fiji\"                            \n [63] \"Finland\"                          \"France\"                          \n [65] \"Gabon\"                            \"Gambia\"                          \n [67] \"Georgia\"                          \"Germany\"                         \n [69] \"Ghana\"                            \"Greece\"                          \n [71] \"Grenada\"                          \"Guatemala\"                       \n [73] \"Guinea\"                           \"Guinea-Bissau\"                   \n [75] \"Guyana\"                           \"Haiti\"                           \n [77] \"Holy See\"                         \"Honduras\"                        \n [79] \"Hungary\"                          \"Iceland\"                         \n [81] \"India\"                            \"Indonesia\"                       \n [83] \"Iran\"                             \"Iraq\"                            \n [85] \"Ireland\"                          \"Israel\"                          \n [87] \"Italy\"                            \"Jamaica\"                         \n [89] \"Japan\"                            \"Jordan\"                          \n [91] \"Kazakhstan\"                       \"Kenya\"                           \n [93] \"Kiribati\"                         \"Korea, North\"                    \n [95] \"Korea, South\"                     \"Kosovo\"                          \n [97] \"Kuwait\"                           \"Kyrgyzstan\"                      \n [99] \"Laos\"                             \"Latvia\"                          \n[101] \"Lebanon\"                          \"Lesotho\"                         \n[103] \"Liberia\"                          \"Libya\"                           \n[105] \"Liechtenstein\"                    \"Lithuania\"                       \n[107] \"Luxembourg\"                       \"MS Zaandam\"                      \n[109] \"Madagascar\"                       \"Malawi\"                          \n[111] \"Malaysia\"                         \"Maldives\"                        \n[113] \"Mali\"                             \"Malta\"                           \n[115] \"Marshall Islands\"                 \"Mauritania\"                      \n[117] \"Mauritius\"                        \"Mexico\"                          \n[119] \"Micronesia\"                       \"Moldova\"                         \n[121] \"Monaco\"                           \"Mongolia\"                        \n[123] \"Montenegro\"                       \"Morocco\"                         \n[125] \"Mozambique\"                       \"Namibia\"                         \n[127] \"Nauru\"                            \"Nepal\"                           \n[129] \"Netherlands\"                      \"New Zealand\"                     \n[131] \"Nicaragua\"                        \"Niger\"                           \n[133] \"Nigeria\"                          \"North Macedonia\"                 \n[135] \"Norway\"                           \"Oman\"                            \n[137] \"Pakistan\"                         \"Palau\"                           \n[139] \"Panama\"                           \"Papua New Guinea\"                \n[141] \"Paraguay\"                         \"Peru\"                            \n[143] \"Philippines\"                      \"Poland\"                          \n[145] \"Portugal\"                         \"Qatar\"                           \n[147] \"Romania\"                          \"Russia\"                          \n[149] \"Rwanda\"                           \"Saint Kitts and Nevis\"           \n[151] \"Saint Lucia\"                      \"Saint Vincent and the Grenadines\"\n[153] \"Samoa\"                            \"San Marino\"                      \n[155] \"Sao Tome and Principe\"            \"Saudi Arabia\"                    \n[157] \"Senegal\"                          \"Serbia\"                          \n[159] \"Seychelles\"                       \"Sierra Leone\"                    \n[161] \"Singapore\"                        \"Slovakia\"                        \n[163] \"Slovenia\"                         \"Solomon Islands\"                 \n[165] \"Somalia\"                          \"South Africa\"                    \n[167] \"South Sudan\"                      \"Spain\"                           \n[169] \"Sri Lanka\"                        \"Sudan\"                           \n[171] \"Summer Olympics 2020\"             \"Suriname\"                        \n[173] \"Sweden\"                           \"Switzerland\"                     \n[175] \"Syria\"                            \"Taiwan*\"                         \n[177] \"Tajikistan\"                       \"Tanzania\"                        \n[179] \"Thailand\"                         \"Timor-Leste\"                     \n[181] \"Togo\"                             \"Tonga\"                           \n[183] \"Trinidad and Tobago\"              \"Tunisia\"                         \n[185] \"Turkey\"                           \"Tuvalu\"                          \n[187] \"US\"                               \"Uganda\"                          \n[189] \"Ukraine\"                          \"United Arab Emirates\"            \n[191] \"United Kingdom\"                   \"Uruguay\"                         \n[193] \"Uzbekistan\"                       \"Vanuatu\"                         \n[195] \"Venezuela\"                        \"Vietnam\"                         \n[197] \"West Bank and Gaza\"               \"Winter Olympics 2022\"            \n[199] \"Yemen\"                            \"Zambia\"                          \n[201] \"Zimbabwe\"                        \n\n\n\n\nCreate conf.case data.frame\n\nconfirmed_raw %&gt;% \n  filter(`Country/Region` %in% c(\"China\", \"Italy\", \"Japan\", \"United Kingdom\", \"US\", \"Korea, South\",\n                                 \"Spain\")) %&gt;%\n  select(-c(`Province/State`, Lat, Long)) %&gt;% \n  group_by(`Country/Region`) %&gt;% summarise_all(sum) -&gt; test\n\nnames(test)[1]&lt;-\"country\"\n\nmelt(data = test, id.vars = \"country\", measure.vars = names(test)[-1]) %&gt;% \n  separate(variable, into = c(\"mon\", \"day\", \"year\"), sep='/', extra = \"merge\") %&gt;% \n  filter(day %in% c(1)) %&gt;%\n  arrange(mon, day) %&gt;% \n  mutate(date=as.Date(with(.,paste(mon, day, year, sep=\"/\")), format = \"%m/%d/%y\")) %&gt;% \n  dcast(country ~ date) -&gt; df.conf.case\n\n\ndf.conf.case\n\n         country 2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\n1          China      11891      79932      84002      86850      87520\n2          Italy          2       1694     110574     207428     233197\n3          Japan         20        259       2535      14558      16778\n4   Korea, South         12       3736       9887      10780      11541\n5          Spain          1         84     104118     215216     239638\n6 United Kingdom          2         94      43755     183500     258979\n7             US          8         32     227903    1115972    1809384\n  2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01 2020-12-01 2021-01-01\n1      88344      91690      94363      95568      97250      99336     102649\n2     240760     247832     270189     317409     709335    1620901    2129376\n3      18732      37790      69018      84212     101936     150857     239005\n4      12904      14366      20449      23952      26732      35163      62593\n5     249659     288522     470973     778607    1185678    1656444    1928265\n6     285276     305558     339403     462780    1038056    1647165    2549671\n7    2698127    4605921    6088458    7292562    9254490   13866746   20397398\n  2021-02-01 2021-03-01 2021-04-01 2021-05-01 2021-06-01 2021-07-01 2021-08-01\n1     107902     109034     110169     111325     112329     113614     115473\n2    2560957    2938371    3607083    4035617    4220304    4260788    4355348\n3     392533     433334     477691     598754     749126     801337     936852\n4      78844      90372     104194     123240     141476     158549     201002\n5    2822805    3204531    3291394    3524077    3682778    3821305    4447044\n6    3846807    4194287    4364544    4434156    4506331    4844879    5907641\n7   26482919   28814420   30656330   32516226   33407540   33797251   35152818\n  2021-09-01 2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01 2022-03-01\n1     117991     119790     121477     123725     128132     134564     456582\n2    4546487    4675758    4774783    5043620    6266939   11116422   12829972\n3    1514400    1706516    1722427    1726913    1733835    2825414    5078276\n4     255401     316020     367974     457612     639083     884310    3492686\n5    4861883    4961128    5011148    5174720    6294745   10039126   11036085\n6    6856890    7878555    9140352   10333452   13174530   17543963   19120746\n7   39585475   43694428   46163201   48743340   55099948   75570589   79228450\n  2022-04-01 2022-05-01 2022-06-01 2022-07-01 2022-08-01 2022-09-01 2022-10-01\n1    1400358    2024284    2097282    2137169    2265424    2510703    2762150\n2   14719394   16504791   17440232   18610011   21059545   21888255   22500346\n3    6614278    7910179    8876113    9355427   12935010   19116684   21329519\n4   13639915   17295733   18129313   18379552   19932439   23417425   24819611\n5   11551574   11896152   12360256   12818184   13226579   13342530   13422984\n6   21379545   22214004   22492903   22941360   23515928   23738035   23893496\n7   80252748   81483804   84556267   87832253   91515236   94659072   96369625\n  2022-11-01 2022-12-01 2023-01-01 2023-02-01 2023-03-01\n1    2959481    3764783    4612203    4903498    4903524\n2   23531023   24260660   25143705   25453789   25576852\n3   22389872   24933509   29321601   32610584   33241180\n4   25670407   27208800   29139535   30213928   30533573\n5   13511768   13595504   13684258   13731478   13763336\n6   24122922   24251636   24365688   24507372   24603450\n7   97540736   98903928  100769628  102479379  103533872\n\n\n\n\nCreate death.case data.frame\n\ndeaths_raw %&gt;% \n  filter(`Country/Region` %in% c(\"China\", \"Italy\", \"Japan\", \"United Kingdom\", \"US\", \"Korea, South\",\n                                 \"Spain\")) %&gt;%\n  select(-c(`Province/State`, Lat, Long)) %&gt;% \n  group_by(`Country/Region`) %&gt;% summarise_all(sum) -&gt; test\n\nnames(test)[1]&lt;-\"country\"\n\nmelt(data = test, id.vars = \"country\", measure.vars = names(test)[-1]) %&gt;% \n  separate(variable, into = c(\"mon\", \"day\", \"year\"), sep='/', extra = \"merge\") %&gt;% \n  filter(day %in% c(1)) %&gt;%\n  arrange(mon, day) %&gt;% \n  mutate(date=as.Date(with(.,paste(mon, day, year, sep=\"/\")), format = \"%m/%d/%y\")) %&gt;% \n  dcast(country ~ date) -&gt; df.death.case\n\n\ndf.death.case\n\n         country 2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\n1          China        259       2872       3332       4698       4708\n2          Italy          0         34      13155      28236      33475\n3          Japan          0          6         72        510        900\n4   Korea, South          0         17        165        250        272\n5          Spain          0          0       9387      24543      27127\n6 United Kingdom          1          3       6070      39849      52768\n7             US          0          1       6996      68518     108624\n  2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01 2020-12-01 2021-01-01\n1       4713       4737       4797       4813       4814       4830       4884\n2      34788      35146      35491      35918      38826      56361      74621\n3        976       1013       1314       1583       1776       2193       3541\n4        282        301        326        416        468        526        942\n5      28364      28445      29152      31973      35878      45511      50837\n6      56338      57454      57995      58946      64667      78184      95917\n7     128134     155059     183855     206852     231054     273099     352844\n  2021-02-01 2021-03-01 2021-04-01 2021-05-01 2021-06-01 2021-07-01 2021-08-01\n1       4966       5011       5023       5031       5031       5033       5047\n2      88845      97945     109847     121033     126221     127587     128068\n3       5833       7948       9194      10326      13160      14808      15198\n4       1435       1606       1737       1833       1965       2024       2099\n5      59081      69609      75541      78216      79983      80883      81486\n6     132799     148935     153012     154085     154509     155010     156941\n7     448381     513045     549448     572904     590904     600972     609715\n  2021-09-01 2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01 2022-03-01\n1       5056       5069       5081       5089       5103       5119       5901\n2     129290     130973     132120     133931     137513     146925     155000\n3      16138      17685      18274      18361      18392      18885      23908\n4       2303       2504       2874       3705       5694       6787       8266\n5      84472      86463      87368      88080      89405      93633      99883\n6     160317     164780     169438     173903     178046     184840     188681\n7     639812     699021     746135     781422     825870     892252     952086\n  2022-04-01 2022-05-01 2022-06-01 2022-07-01 2022-08-01 2022-09-01 2022-10-01\n1      12869      14697      14899      14928      15052      15251      15719\n2     159537     163612     166756     168425     172207     175663     177130\n3      28202      29605      30659      31302      32707      40245      45023\n4      16929      22958      24212      24562      25084      26940      28489\n5     102541     104456     106493     108111     110719     112600     114179\n6     193232     198276     200347     201869     205574     207875     209346\n7     983972     996109    1007741    1017872    1030654    1046956    1059542\n  2022-11-01 2022-12-01 2023-01-01 2023-02-01 2023-03-01\n1      15965      16001      17167      97668     101051\n2     179101     181098     184642     186833     188094\n3      46817      49834      57521      68407      72494\n4      29239      30621      32272      33522      34003\n5     115078     115901     117095     118434     119380\n6     212435     214234     217175     220129     220721\n7    1070821    1081153    1092779    1109996    1120897\n\n\n\n\nCreate Matrix for confirmed and death cases\n\n# country.name&lt;-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \ncountry.name &lt;- unlist(df.conf.case[c(1)])\n\n#str(df.conf.case)\nm.conf.case &lt;- as.matrix(df.conf.case[-1])\nrow.names(m.conf.case) &lt;- country.name\n\nm.death.case &lt;- as.matrix(df.death.case[-1])\nrow.names(m.death.case) &lt;- country.name\n\nm.death.rate &lt;- round(m.death.case/m.conf.case, 2)\n\n\nMatrix for confirmed case: m.conf.case\nMatrix for death case: m.death.case\n\n\nm.conf.case\n\n               2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\nChina               11891      79932      84002      86850      87520\nItaly                   2       1694     110574     207428     233197\nJapan                  20        259       2535      14558      16778\nKorea, South           12       3736       9887      10780      11541\nSpain                   1         84     104118     215216     239638\nUnited Kingdom          2         94      43755     183500     258979\nUS                      8         32     227903    1115972    1809384\n               2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01\nChina               88344      91690      94363      95568      97250\nItaly              240760     247832     270189     317409     709335\nJapan               18732      37790      69018      84212     101936\nKorea, South        12904      14366      20449      23952      26732\nSpain              249659     288522     470973     778607    1185678\nUnited Kingdom     285276     305558     339403     462780    1038056\nUS                2698127    4605921    6088458    7292562    9254490\n               2020-12-01 2021-01-01 2021-02-01 2021-03-01 2021-04-01\nChina               99336     102649     107902     109034     110169\nItaly             1620901    2129376    2560957    2938371    3607083\nJapan              150857     239005     392533     433334     477691\nKorea, South        35163      62593      78844      90372     104194\nSpain             1656444    1928265    2822805    3204531    3291394\nUnited Kingdom    1647165    2549671    3846807    4194287    4364544\nUS               13866746   20397398   26482919   28814420   30656330\n               2021-05-01 2021-06-01 2021-07-01 2021-08-01 2021-09-01\nChina              111325     112329     113614     115473     117991\nItaly             4035617    4220304    4260788    4355348    4546487\nJapan              598754     749126     801337     936852    1514400\nKorea, South       123240     141476     158549     201002     255401\nSpain             3524077    3682778    3821305    4447044    4861883\nUnited Kingdom    4434156    4506331    4844879    5907641    6856890\nUS               32516226   33407540   33797251   35152818   39585475\n               2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01\nChina              119790     121477     123725     128132     134564\nItaly             4675758    4774783    5043620    6266939   11116422\nJapan             1706516    1722427    1726913    1733835    2825414\nKorea, South       316020     367974     457612     639083     884310\nSpain             4961128    5011148    5174720    6294745   10039126\nUnited Kingdom    7878555    9140352   10333452   13174530   17543963\nUS               43694428   46163201   48743340   55099948   75570589\n               2022-03-01 2022-04-01 2022-05-01 2022-06-01 2022-07-01\nChina              456582    1400358    2024284    2097282    2137169\nItaly            12829972   14719394   16504791   17440232   18610011\nJapan             5078276    6614278    7910179    8876113    9355427\nKorea, South      3492686   13639915   17295733   18129313   18379552\nSpain            11036085   11551574   11896152   12360256   12818184\nUnited Kingdom   19120746   21379545   22214004   22492903   22941360\nUS               79228450   80252748   81483804   84556267   87832253\n               2022-08-01 2022-09-01 2022-10-01 2022-11-01 2022-12-01\nChina             2265424    2510703    2762150    2959481    3764783\nItaly            21059545   21888255   22500346   23531023   24260660\nJapan            12935010   19116684   21329519   22389872   24933509\nKorea, South     19932439   23417425   24819611   25670407   27208800\nSpain            13226579   13342530   13422984   13511768   13595504\nUnited Kingdom   23515928   23738035   23893496   24122922   24251636\nUS               91515236   94659072   96369625   97540736   98903928\n               2023-01-01 2023-02-01 2023-03-01\nChina             4612203    4903498    4903524\nItaly            25143705   25453789   25576852\nJapan            29321601   32610584   33241180\nKorea, South     29139535   30213928   30533573\nSpain            13684258   13731478   13763336\nUnited Kingdom   24365688   24507372   24603450\nUS              100769628  102479379  103533872\n\n\n\nm.death.case\n\n               2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\nChina                 259       2872       3332       4698       4708\nItaly                   0         34      13155      28236      33475\nJapan                   0          6         72        510        900\nKorea, South            0         17        165        250        272\nSpain                   0          0       9387      24543      27127\nUnited Kingdom          1          3       6070      39849      52768\nUS                      0          1       6996      68518     108624\n               2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01\nChina                4713       4737       4797       4813       4814\nItaly               34788      35146      35491      35918      38826\nJapan                 976       1013       1314       1583       1776\nKorea, South          282        301        326        416        468\nSpain               28364      28445      29152      31973      35878\nUnited Kingdom      56338      57454      57995      58946      64667\nUS                 128134     155059     183855     206852     231054\n               2020-12-01 2021-01-01 2021-02-01 2021-03-01 2021-04-01\nChina                4830       4884       4966       5011       5023\nItaly               56361      74621      88845      97945     109847\nJapan                2193       3541       5833       7948       9194\nKorea, South          526        942       1435       1606       1737\nSpain               45511      50837      59081      69609      75541\nUnited Kingdom      78184      95917     132799     148935     153012\nUS                 273099     352844     448381     513045     549448\n               2021-05-01 2021-06-01 2021-07-01 2021-08-01 2021-09-01\nChina                5031       5031       5033       5047       5056\nItaly              121033     126221     127587     128068     129290\nJapan               10326      13160      14808      15198      16138\nKorea, South         1833       1965       2024       2099       2303\nSpain               78216      79983      80883      81486      84472\nUnited Kingdom     154085     154509     155010     156941     160317\nUS                 572904     590904     600972     609715     639812\n               2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01\nChina                5069       5081       5089       5103       5119\nItaly              130973     132120     133931     137513     146925\nJapan               17685      18274      18361      18392      18885\nKorea, South         2504       2874       3705       5694       6787\nSpain               86463      87368      88080      89405      93633\nUnited Kingdom     164780     169438     173903     178046     184840\nUS                 699021     746135     781422     825870     892252\n               2022-03-01 2022-04-01 2022-05-01 2022-06-01 2022-07-01\nChina                5901      12869      14697      14899      14928\nItaly              155000     159537     163612     166756     168425\nJapan               23908      28202      29605      30659      31302\nKorea, South         8266      16929      22958      24212      24562\nSpain               99883     102541     104456     106493     108111\nUnited Kingdom     188681     193232     198276     200347     201869\nUS                 952086     983972     996109    1007741    1017872\n               2022-08-01 2022-09-01 2022-10-01 2022-11-01 2022-12-01\nChina               15052      15251      15719      15965      16001\nItaly              172207     175663     177130     179101     181098\nJapan               32707      40245      45023      46817      49834\nKorea, South        25084      26940      28489      29239      30621\nSpain              110719     112600     114179     115078     115901\nUnited Kingdom     205574     207875     209346     212435     214234\nUS                1030654    1046956    1059542    1070821    1081153\n               2023-01-01 2023-02-01 2023-03-01\nChina               17167      97668     101051\nItaly              184642     186833     188094\nJapan               57521      68407      72494\nKorea, South        32272      33522      34003\nSpain              117095     118434     119380\nUnited Kingdom     217175     220129     220721\nUS                1092779    1109996    1120897\n\n\n\nAccess to the matrix\n\nUK’s total confirmed cases on 2021-10-01\nSouth Korea’s total confirmed cases on 2021-10-01\nChina’s total confirmed cases on 2021-10-01\nSouth Korea’s increasing confirmed cases on 2021-10-01 compared to the previous month\nJapan’s increasing confirmed cases on 2021-10-01 compared to the previous month\n\n\n\n\nCreate three vectors for the next step:\n\n\n\n\nPopulation vector\nCountry names vector. Created to give a name to the population vector\n\ncountry.name\n\n        country1         country2         country3         country4 \n         \"China\"          \"Italy\"          \"Japan\"   \"Korea, South\" \n        country5         country6         country7 \n         \"Spain\" \"United Kingdom\"             \"US\" \n\n\nVector inputting the population numbers of the selected countries in order\n\npop&lt;-c(1439323776, 60461826, 126476461, 51269185, 46754778, 67886011, 331002651)\npop\n\n[1] 1439323776   60461826  126476461   51269185   46754778   67886011  331002651\n\n\n\nnames(pop)\n\nNULL\n\n\nIn the pop vector, specify which country’s population has each population using the names() function.\n\npop&lt;-c(1439323776, 60461826, 126476461, 51269185, 46754778, 67886011, 331002651)\nnames(pop)&lt;-country.name\npop\n\n         China          Italy          Japan   Korea, South          Spain \n    1439323776       60461826      126476461       51269185       46754778 \nUnited Kingdom             US \n      67886011      331002651 \n\n\n\n\nGDP vector\nLikewise, the names() function provides information about which country the GDP corresponds to.\n\n# round(m.conf.case/pop*1000, 2)\ncountry.name&lt;-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \nGDP&lt;-c(12237700479375,\n1943835376342,\n4872415104315,\n1530750923149,\n1314314164402,\n2637866340434,\n19485394000000)\nnames(GDP)&lt;-country.name\n\nGDP\n\n       China        Italy        Japan        Korea        Spain           UK \n1.223770e+13 1.943835e+12 4.872415e+12 1.530751e+12 1.314314e+12 2.637866e+12 \n          US \n1.948539e+13 \n\n\n\n\nPop density\nLet’s create a population density vector for the selected countries.\n\ncountry.name&lt;-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \npop.density&lt;-c(148, 205, 347, 530, 94, 275, 36)\nnames(pop.density)&lt;-country.name\n\nCheck the created vectors\n\npop\n\n         China          Italy          Japan   Korea, South          Spain \n    1439323776       60461826      126476461       51269185       46754778 \nUnited Kingdom             US \n      67886011      331002651 \n\nGDP\n\n       China        Italy        Japan        Korea        Spain           UK \n1.223770e+13 1.943835e+12 4.872415e+12 1.530751e+12 1.314314e+12 2.637866e+12 \n          US \n1.948539e+13 \n\npop.density\n\nChina Italy Japan Korea Spain    UK    US \n  148   205   347   530    94   275    36 \n\n\n\n\nLet’s visualize the GDP of each country.\n\nbarplot(GDP)\n\n\n\n\nSort by largest GDP\n\nbarplot(sort(GDP))\n\n\n\n\n\nbarplot(sort(GDP, decreasing = T))\n\n\n\n\nLet’s think..\n\n\n\n\n\n\nHow about Bar graph of GDP per capita?\nWe have\n\nGDP vector\nPopulation vector\n\nWe know\n\nbarplot()\nVector calculation\nSort()\nDecreasing=T option\n\n\n\n\n\nmatplot\n\n\nmatplot(m.conf.case)\n\n\n\n\n\n\nmatplot(t(m.conf.case))\n\n\n\n\n\n\n\nmatplot(t(m.conf.case))\n\n\n\n\n\nmatplot(t(m.conf.case), type='b')\n\n\n\n\n\nmatplot(t(m.conf.case), type='b', pch=15:20)\n\n\n\n\n\nmatplot(t(m.conf.case), type='b', pch=15:20, col=c(1:6, 8), \n        ylab=\"Confirmed cases\")\n\n\n\n\n\nmatplot(t(m.conf.case), type='b', pch=15:20, col=c(1:6, 8), \n        ylab=\"Confirmed cases\")\nlegend(\"topleft\", inset=0.01, legend=country.name, pch=15:20, col=c(1:6, 8), horiz=F)\n\n\n\n\n\n\n\nTry the same graph but now use the death rate\n\n\n\n\n\n\n\nCountry’s wealth and COVID19\nI’m now curious about the relationship between countries’ GDP per capita and the death rate at some points\n\nplot(GDP.pc, m.death.rate[,10])\n\n\n\n\n\n\nplot(GDP.pc, m.death.rate[,10],\n     ylab = \"Death rate\")\ntext(GDP.pc, m.death.rate[,16], row.names(m.death.rate),\n     cex = 1, pos = 4, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncreasing rate of confirmed cases\nVisualize like an example below (in 5 mins)\n\n\n\n\n\nLet’s omit US for the clear vision\n\n\n\n\n\nLet’s also visualize the first three periods and the last (recent) four periods\n\n\n\n\n\n\n\n\n\n\n\nCan you also do the same visualization for the specific country like Korea, China, and Japan?"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html",
    "href": "teaching/ds101/weekly/posts/02_week.html",
    "title": "Basic Syntax (1)",
    "section": "",
    "text": "Weekly design\nBefore attending class for Week 2, please complete the following tasks:"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/04_week.html",
    "href": "teaching/ds101/weekly/posts/04_week.html",
    "title": "Basic Syntax (3)",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nData.frame\n\n\n\n# Data Frame #\nname = c(\"Cheolsu\", \"Chunhyang\", \"Gildong\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients = data.frame(name, age, gender, blood.type)\npatients\n\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25      M          B\n\n# Can also be written in one line like this:\npatients1 = data.frame(name = c(\"Cheolsu\", \"Chunhyang\", \"Gildong\"), \n                       age = c(22, 20, 25), \n                       gender = factor(c(\"M\", \"F\", \"M \")), \n                       blood.type = factor(c(\"A\", \"O\", \"B\")))\n\npatients1\n\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25     M           B\n\npatients$name # Print name attribute value\n\n[1] \"Cheolsu\"   \"Chunhyang\" \"Gildong\"  \n\npatients[1, ] # Print row 1 value\n\n     name age gender blood.type\n1 Cheolsu  22      M          A\n\npatients[, 2] # Print 2nd column values\n\n[1] 22 20 25\n\npatients[3, 1] # Prints 3 rows and 1 column values\n\n[1] \"Gildong\"\n\npatients[patients$name==\"Withdrawal\", ] # Extract information about withdrawal among patients\n\n[1] name       age        gender     blood.type\n&lt;0 rows&gt; (or 0-length row.names)\n\npatients[patients$name==\"Cheolsu\", c(\"name\", \"age\")] # Extract only Cheolsu's name and age information\n\n     name age\n1 Cheolsu  22\n\nhead(cars) # Check the cars data set. The basic function of the head function is to extract the first 6 data.\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\nattach(cars) # Use the attach function to use each property of cars as a variable\nspeed # The variable name speed can be used directly.\n\n [1]  4  4  7  7  8  9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15\n[26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25\n\ndetach(cars) # Deactivates the use of each property of cars as a variable through the detach function\n# speed # Try to access the variable called speed, but there is no such variable.\n\n# Apply functions using data properties\nmean(cars$speed)\n\n[1] 15.4\n\nmax(cars$speed)\n\n[1] 25\n\n# Apply a function using the with function\nwith(cars, mean(speed))\n\n[1] 15.4\n\nwith(cars, max(speed))\n\n[1] 25\n\n# Extract only data with speed greater than 20\nsubset(cars, speed &gt; 20)\n\n   speed dist\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\n# Extract only dist data with speed over 20, select multiple columns, separate c() with ,\nsubset(cars, speed &gt; 20, select = c(dist))\n\n   dist\n44   66\n45   54\n46   70\n47   92\n48   93\n49  120\n50   85\n\n# Extract only data excluding dist from data with a speed exceeding 20\nsubset(cars, speed &gt; 20, select = -c(dist))\n\n   speed\n44    22\n45    23\n46    24\n47    24\n48    24\n49    24\n50    25\n\nhead(airquality) # airquality data contains NA\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nhead(na.omit(airquality)) # Extract by excluding values containing NA\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n7    23     299  8.6   65     5   7\n8    19      99 13.8   59     5   8\n\n# merge(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by, all = FALSE, all.x = all, all.y = all, sort = TRUE, suffixes = c(\".x\",\".y\"), incomparables = NULL, ...)\n\nname = c(\"Cheolsu\", \"Chunhyang\", \"Gildong\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients1 = data.frame(name, age, gender)\npatients1\n\n       name age gender\n1   Cheolsu  22      M\n2 Chunhyang  20      F\n3   Gildong  25      M\n\npatients2 = data.frame(name, blood.type)\npatients2\n\n       name blood.type\n1   Cheolsu          A\n2 Chunhyang          O\n3   Gildong          B\n\npatients = merge(patients1, patients2, by = \"name\")\npatients\n\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25      M          B\n\n# If there are no column variables with the same name, when merging them into by.x and by.y of the merge function\n# You must enter the attribute name of each column to be used.\nname1 = c(\"Cheolsu\", \"Chunhyang\", \"Gildong\")\nname2 = c(\"Minsu\", \"Chunhyang\", \"Gildong\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients1 = data.frame(name1, age, gender)\npatients1\n\n      name1 age gender\n1   Cheolsu  22      M\n2 Chunhyang  20      F\n3   Gildong  25      M\n\npatients2 = data.frame(name2, blood.type)\npatients2\n\n      name2 blood.type\n1     Minsu          A\n2 Chunhyang          O\n3   Gildong          B\n\npatients = merge(patients1, patients2, by.x = \"name1\", by.y = \"name2\")\npatients\n\n      name1 age gender blood.type\n1 Chunhyang  20      F          O\n2   Gildong  25      M          B\n\npatients = merge(patients1, patients2, by.x = \"name1\", by.y = \"name2\", all = TRUE)\npatients\n\n      name1 age gender blood.type\n1   Cheolsu  22      M       &lt;NA&gt;\n2 Chunhyang  20      F          O\n3   Gildong  25      M          B\n4     Minsu  NA   &lt;NA&gt;          A\n\nx = array(1:12, c(3, 4))\n\n# Currently x is not a data frame\nis.data.frame(x) \n\n[1] FALSE\n\nas.data.frame(x)\n\n  V1 V2 V3 V4\n1  1  4  7 10\n2  2  5  8 11\n3  3  6  9 12\n\n# Just calling the is.data.frame function does not turn x into a data frame\nis.data.frame(x)\n\n[1] FALSE\n\n# Convert x to data frame format with the as.data.frame function\nx = as.data.frame(x)\nx\n\n  V1 V2 V3 V4\n1  1  4  7 10\n2  2  5  8 11\n3  3  6  9 12\n\n# Verify that x has been converted to data frame format\nis.data.frame(x)\n\n[1] TRUE\n\n# When converting to a data frame, automatically assigned column names are reassigned to the names function.\nnames(x) = c(\"1st\", \"2nd\", \"3rd\", \"4th\")\nx\n\n  1st 2nd 3rd 4th\n1   1   4   7  10\n2   2   5   8  11\n3   3   6   9  12\n\n\n\n\n\nList\n\n\n\n\n# List #\npatients = data.frame(name = c(\"Cheolsu\", \"Chunhyang\", \"Gildong\"), \n                      age = c(22, 20, 25), \n                      gender = factor(c(\"M\", \"F\", \"M \")), \n                      blood.type = factor(c(\"A\", \"O\", \"B\")))\n\nno.patients = data.frame(day = c(1:6), no = c(50, 60, 55, 52, 65, 58))\n\n\n# Simple addition of data\nlistPatients = list(patients, no.patients)\nlistPatients\n\n[[1]]\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25     M           B\n\n[[2]]\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# Add names to each data\nlistPatients = list(patients=patients, no.patients = no.patients)\nlistPatients\n\n$patients\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25     M           B\n\n$no.patients\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# Enter element name\nlistPatients$patients \n\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25     M           B\n\n# Enter index\nlistPatients[[1]] \n\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25     M           B\n\n# Enter the element name in \"\"\nlistPatients[[\"patients\"]] \n\n       name age gender blood.type\n1   Cheolsu  22      M          A\n2 Chunhyang  20      F          O\n3   Gildong  25     M           B\n\n# Enter the element name in \"\"\nlistPatients[[\"no.patients\"]] \n\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# Calculate the average of no.patients elements\nlapply(listPatients$no.patients, mean)\n\n$day\n[1] 3.5\n\n$no\n[1] 56.66667\n\n# Calculate the average of the patients elements. Anything that is not in numeric form is not averaged.\nlapply(listPatients$patients, mean)\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\n\n$name\n[1] NA\n\n$age\n[1] 22.33333\n\n$gender\n[1] NA\n\n$blood.type\n[1] NA\n\nsapply(listPatients$no.patients, mean)\n\n     day       no \n 3.50000 56.66667 \n\n# If the simplify option of sapply() is set to F, the same result as lapply() is returned.\nsapply(listPatients$no.patients, mean, simplify = F)\n\n$day\n[1] 3.5\n\n$no\n[1] 56.66667\n\n\n\n\n\n\nClass\n\n\nPop-up Qz\n\n# Let a vector\nV1 = c(1,2,3,4,5,NA,9,10)\n\nHow can we get a vector of integers less than 4 from V1?\n1) V1[V1 &lt; 4 & !is.na(V1)]\n2) V1[V1 &lt; 4 | !is.na(V1)]\n3) V1(V1 &lt; 4 | !is.na(V1))\n4) V1[V1 &lt; 4] & V1[!is.na(V1)]\n\n\n# Let a data frame as below\ndf &lt;- data.frame(name = c(\"John\", \"Mary\", \"Mark\"),\n                 age  = c(30,16,21),\n                 gender = c(\"M\", \"F\", \"M\"))\ndf\n\n  name age gender\n1 John  30      M\n2 Mary  16      F\n3 Mark  21      M\n\n\nMake an R code to filter if gender is ‘male’ and age is 19 or above.\n\n\n\nPractice: Data Frame & List\n\nKorea Media Panel Data\nhttps://stat.kisdi.re.kr/kor/contents/ContentsList.html\n\nSince 2010 KISDI has surveyed the same people annually about people’s media behavior (Smartphone brand, telecom company, spending related to media, SNS usage, and so on)\n\n\n\n\n\n\nKorea Media Panel data is used in news articles based on media statistics, such as the article below.\n\n\n\n\nConsists of\n\n\n\nI made a toy data set with the KMP like below.\n\nPlease download R data set here: List_KMP.RData\nPlace it in your working directory\n\n\n\n\n\nload(\"data/List_KMP.RData\")\n\nstr(List.KMP)\n\nList of 4\n $ :'data.frame':   10 obs. of  11 variables:\n  ..$ pid              : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ age              : num [1:10] 9 26 12 55 70 58 50 68 39 37\n  ..$ gender           : Factor w/ 2 levels \"Male\",\"Female\": 1 1 2 1 1 1 1 1 1 2\n  ..$ Mobile.lv        : Factor w/ 5 levels \"3G\",\"LTE\",\"LTE-A\",..: 5 2 2 2 5 2 2 2 2 3\n  ..$ Telecom          : Factor w/ 5 levels \"SKT\",\"KT\",\"LG U+\",..: 5 2 3 1 5 1 1 1 2 2\n  ..$ Smartphone.brand : Factor w/ 8 levels \"Samsung\",\"Apple\",..: 8 1 1 1 8 1 1 1 1 3\n  ..$ sp.mobile        : num [1:10] 0 42 19 38 18 65 32 58 63 54\n  ..$ sp.device        : num [1:10] 0 0 0 0 0 0 0 10 0 10\n  ..$ sp.online.content: num [1:10] 0 53 19 38 18 65 32 68 63 114\n  ..$ sp.offline.contet: num [1:10] 10 22 0 0 0 50 36 0 25 40\n  ..$ year             : num [1:10] 2017 2017 2017 2017 2017 ...\n $ :'data.frame':   10 obs. of  11 variables:\n  ..$ pid              : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ age              : num [1:10] 10 27 13 56 71 59 51 69 40 38\n  ..$ gender           : Factor w/ 2 levels \"Male\",\"Female\": 1 1 2 1 1 1 1 1 1 2\n  ..$ Mobile.lv        : Factor w/ 5 levels \"3G\",\"LTE\",\"LTE-A\",..: 5 3 3 2 5 2 2 2 3 3\n  ..$ Telecom          : Factor w/ 5 levels \"SKT\",\"KT\",\"LG U+\",..: 5 3 3 2 5 3 1 2 1 2\n  ..$ Smartphone.brand : Factor w/ 8 levels \"Samsung\",\"Apple\",..: 8 1 3 1 8 3 1 3 1 3\n  ..$ sp.mobile        : num [1:10] 0 90 20 39 30 80 33 36 40 59\n  ..$ sp.device        : num [1:10] 0 60 35 0 0 15 0 10 10 12\n  ..$ sp.online.content: num [1:10] 0 359 55 39 30 95 33 46 50 71\n  ..$ sp.offline.contet: num [1:10] 8 120 0 0 0 12 20 0 0 100\n  ..$ year             : num [1:10] 2018 2018 2018 2018 2018 ...\n $ :'data.frame':   10 obs. of  11 variables:\n  ..$ pid              : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ age              : num [1:10] 11 28 14 57 72 60 52 70 41 39\n  ..$ gender           : Factor w/ 2 levels \"Male\",\"Female\": 1 1 2 1 1 1 1 1 1 2\n  ..$ Mobile.lv        : Factor w/ 5 levels \"3G\",\"LTE\",\"LTE-A\",..: 5 2 3 2 2 2 2 2 3 3\n  ..$ Telecom          : Factor w/ 5 levels \"SKT\",\"KT\",\"LG U+\",..: 5 3 3 1 2 1 1 2 3 3\n  ..$ Smartphone.brand : Factor w/ 8 levels \"Samsung\",\"Apple\",..: 8 2 1 1 1 1 1 3 3 3\n  ..$ sp.mobile        : num [1:10] 0 60 21 30 32 78 35 54 73 45\n  ..$ sp.device        : num [1:10] 0 30 0 0 0 0 0 0 20 15\n  ..$ sp.online.content: num [1:10] 0 90 21 30 32 78 35 54 93 60\n  ..$ sp.offline.contet: num [1:10] 21 0 0 20 0 24 90 0 20 60\n  ..$ year             : num [1:10] 2019 2019 2019 2019 2019 ...\n $ :'data.frame':   10 obs. of  4 variables:\n  ..$ pid           : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ Smarphone.time: num [1:10] 0 60 345 90 40 65 170 95 85 75\n  ..$ SNS.time      : num [1:10] 0 0 10 0 0 0 0 0 0 0\n  ..$ year          : num [1:10] 2019 2019 2019 2019 2019 ...\n\n\n\n\nPersonal Data 2017\n\nList.KMP[[1]]\n\n        pid age gender     Mobile.lv       Telecom Smartphone.brand sp.mobile\n1   9920004   9   Male No Smartphone No Smartphone    No Smartphone         0\n2  12500003  26   Male           LTE            KT          Samsung        42\n3  17350004  12 Female           LTE         LG U+          Samsung        19\n4  17670001  55   Male           LTE           SKT          Samsung        38\n5  23860001  70   Male No Smartphone No Smartphone    No Smartphone        18\n6  24450001  58   Male           LTE           SKT          Samsung        65\n7  27570001  50   Male           LTE           SKT          Samsung        32\n8  53620001  68   Male           LTE           SKT          Samsung        58\n9  59570001  39   Male           LTE            KT          Samsung        63\n10 65840001  37 Female         LTE-A            KT               LG        54\n   sp.device sp.online.content sp.offline.contet year\n1          0                 0                10 2017\n2          0                53                22 2017\n3          0                19                 0 2017\n4          0                38                 0 2017\n5          0                18                 0 2017\n6          0                65                50 2017\n7          0                32                36 2017\n8         10                68                 0 2017\n9          0                63                25 2017\n10        10               114                40 2017\n\n\nPersonal Data 2018\n\nList.KMP[[2]]\n\n        pid age gender     Mobile.lv       Telecom Smartphone.brand sp.mobile\n1   9920004  10   Male No Smartphone No Smartphone    No Smartphone         0\n2  12500003  27   Male         LTE-A         LG U+          Samsung        90\n3  17350004  13 Female         LTE-A         LG U+               LG        20\n4  17670001  56   Male           LTE            KT          Samsung        39\n5  23860001  71   Male No Smartphone No Smartphone    No Smartphone        30\n6  24450001  59   Male           LTE         LG U+               LG        80\n7  27570001  51   Male           LTE           SKT          Samsung        33\n8  53620001  69   Male           LTE            KT               LG        36\n9  59570001  40   Male         LTE-A           SKT          Samsung        40\n10 65840001  38 Female         LTE-A            KT               LG        59\n   sp.device sp.online.content sp.offline.contet year\n1          0                 0                 8 2018\n2         60               359               120 2018\n3         35                55                 0 2018\n4          0                39                 0 2018\n5          0                30                 0 2018\n6         15                95                12 2018\n7          0                33                20 2018\n8         10                46                 0 2018\n9         10                50                 0 2018\n10        12                71               100 2018\n\n\nPersonal Data 2019\n\nList.KMP[[3]]\n\n        pid age gender     Mobile.lv       Telecom Smartphone.brand sp.mobile\n1   9920004  11   Male No Smartphone No Smartphone    No Smartphone         0\n2  12500003  28   Male           LTE         LG U+            Apple        60\n3  17350004  14 Female         LTE-A         LG U+          Samsung        21\n4  17670001  57   Male           LTE           SKT          Samsung        30\n5  23860001  72   Male           LTE            KT          Samsung        32\n6  24450001  60   Male           LTE           SKT          Samsung        78\n7  27570001  52   Male           LTE           SKT          Samsung        35\n8  53620001  70   Male           LTE            KT               LG        54\n9  59570001  41   Male         LTE-A         LG U+               LG        73\n10 65840001  39 Female         LTE-A         LG U+               LG        45\n   sp.device sp.online.content sp.offline.contet year\n1          0                 0                21 2019\n2         30                90                 0 2019\n3          0                21                 0 2019\n4          0                30                20 2019\n5          0                32                 0 2019\n6          0                78                24 2019\n7          0                35                90 2019\n8          0                54                 0 2019\n9         20                93                20 2019\n10        15                60                60 2019\n\n\nPersonal Media Diary 2019\n\nList.KMP[[4]]\n\n        pid Smarphone.time SNS.time year\n1   9920004              0        0 2019\n2  12500003             60        0 2019\n3  17350004            345       10 2019\n4  17670001             90        0 2019\n5  23860001             40        0 2019\n6  24450001             65        0 2019\n7  27570001            170        0 2019\n8  53620001             95        0 2019\n9  59570001             85        0 2019\n10 65840001             75        0 2019\n\n\n\nLet’s name the list elements\n\n# Check the existing names\nnames(List.KMP)\n\nNULL\n\n\n\n# Give names to each element of the list\nnames(List.KMP) &lt;- c(\"p17\", \"p18\", \"p19\", \"d19\")\n\n\nHow can we extract the first element of the list?\n\n# one way\nList.KMP[[1]]\n\n        pid age gender     Mobile.lv       Telecom Smartphone.brand sp.mobile\n1   9920004   9   Male No Smartphone No Smartphone    No Smartphone         0\n2  12500003  26   Male           LTE            KT          Samsung        42\n3  17350004  12 Female           LTE         LG U+          Samsung        19\n4  17670001  55   Male           LTE           SKT          Samsung        38\n5  23860001  70   Male No Smartphone No Smartphone    No Smartphone        18\n6  24450001  58   Male           LTE           SKT          Samsung        65\n7  27570001  50   Male           LTE           SKT          Samsung        32\n8  53620001  68   Male           LTE           SKT          Samsung        58\n9  59570001  39   Male           LTE            KT          Samsung        63\n10 65840001  37 Female         LTE-A            KT               LG        54\n   sp.device sp.online.content sp.offline.contet year\n1          0                 0                10 2017\n2          0                53                22 2017\n3          0                19                 0 2017\n4          0                38                 0 2017\n5          0                18                 0 2017\n6          0                65                50 2017\n7          0                32                36 2017\n8         10                68                 0 2017\n9          0                63                25 2017\n10        10               114                40 2017\n\n\n\n# the other way\nList.KMP[['p17']]\n\n        pid age gender     Mobile.lv       Telecom Smartphone.brand sp.mobile\n1   9920004   9   Male No Smartphone No Smartphone    No Smartphone         0\n2  12500003  26   Male           LTE            KT          Samsung        42\n3  17350004  12 Female           LTE         LG U+          Samsung        19\n4  17670001  55   Male           LTE           SKT          Samsung        38\n5  23860001  70   Male No Smartphone No Smartphone    No Smartphone        18\n6  24450001  58   Male           LTE           SKT          Samsung        65\n7  27570001  50   Male           LTE           SKT          Samsung        32\n8  53620001  68   Male           LTE           SKT          Samsung        58\n9  59570001  39   Male           LTE            KT          Samsung        63\n10 65840001  37 Female         LTE-A            KT               LG        54\n   sp.device sp.online.content sp.offline.contet year\n1          0                 0                10 2017\n2          0                53                22 2017\n3          0                19                 0 2017\n4          0                38                 0 2017\n5          0                18                 0 2017\n6          0                65                50 2017\n7          0                32                36 2017\n8         10                68                 0 2017\n9          0                63                25 2017\n10        10               114                40 2017\n\n\n\n\nLists of a list\n\n\n\n# Create an empty list \nList.KMP[[5]] &lt;- list(0)\n\n\n# See the structure\nstr(List.KMP)\n\nList of 5\n $ p17:'data.frame':    10 obs. of  11 variables:\n  ..$ pid              : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ age              : num [1:10] 9 26 12 55 70 58 50 68 39 37\n  ..$ gender           : Factor w/ 2 levels \"Male\",\"Female\": 1 1 2 1 1 1 1 1 1 2\n  ..$ Mobile.lv        : Factor w/ 5 levels \"3G\",\"LTE\",\"LTE-A\",..: 5 2 2 2 5 2 2 2 2 3\n  ..$ Telecom          : Factor w/ 5 levels \"SKT\",\"KT\",\"LG U+\",..: 5 2 3 1 5 1 1 1 2 2\n  ..$ Smartphone.brand : Factor w/ 8 levels \"Samsung\",\"Apple\",..: 8 1 1 1 8 1 1 1 1 3\n  ..$ sp.mobile        : num [1:10] 0 42 19 38 18 65 32 58 63 54\n  ..$ sp.device        : num [1:10] 0 0 0 0 0 0 0 10 0 10\n  ..$ sp.online.content: num [1:10] 0 53 19 38 18 65 32 68 63 114\n  ..$ sp.offline.contet: num [1:10] 10 22 0 0 0 50 36 0 25 40\n  ..$ year             : num [1:10] 2017 2017 2017 2017 2017 ...\n $ p18:'data.frame':    10 obs. of  11 variables:\n  ..$ pid              : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ age              : num [1:10] 10 27 13 56 71 59 51 69 40 38\n  ..$ gender           : Factor w/ 2 levels \"Male\",\"Female\": 1 1 2 1 1 1 1 1 1 2\n  ..$ Mobile.lv        : Factor w/ 5 levels \"3G\",\"LTE\",\"LTE-A\",..: 5 3 3 2 5 2 2 2 3 3\n  ..$ Telecom          : Factor w/ 5 levels \"SKT\",\"KT\",\"LG U+\",..: 5 3 3 2 5 3 1 2 1 2\n  ..$ Smartphone.brand : Factor w/ 8 levels \"Samsung\",\"Apple\",..: 8 1 3 1 8 3 1 3 1 3\n  ..$ sp.mobile        : num [1:10] 0 90 20 39 30 80 33 36 40 59\n  ..$ sp.device        : num [1:10] 0 60 35 0 0 15 0 10 10 12\n  ..$ sp.online.content: num [1:10] 0 359 55 39 30 95 33 46 50 71\n  ..$ sp.offline.contet: num [1:10] 8 120 0 0 0 12 20 0 0 100\n  ..$ year             : num [1:10] 2018 2018 2018 2018 2018 ...\n $ p19:'data.frame':    10 obs. of  11 variables:\n  ..$ pid              : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ age              : num [1:10] 11 28 14 57 72 60 52 70 41 39\n  ..$ gender           : Factor w/ 2 levels \"Male\",\"Female\": 1 1 2 1 1 1 1 1 1 2\n  ..$ Mobile.lv        : Factor w/ 5 levels \"3G\",\"LTE\",\"LTE-A\",..: 5 2 3 2 2 2 2 2 3 3\n  ..$ Telecom          : Factor w/ 5 levels \"SKT\",\"KT\",\"LG U+\",..: 5 3 3 1 2 1 1 2 3 3\n  ..$ Smartphone.brand : Factor w/ 8 levels \"Samsung\",\"Apple\",..: 8 2 1 1 1 1 1 3 3 3\n  ..$ sp.mobile        : num [1:10] 0 60 21 30 32 78 35 54 73 45\n  ..$ sp.device        : num [1:10] 0 30 0 0 0 0 0 0 20 15\n  ..$ sp.online.content: num [1:10] 0 90 21 30 32 78 35 54 93 60\n  ..$ sp.offline.contet: num [1:10] 21 0 0 20 0 24 90 0 20 60\n  ..$ year             : num [1:10] 2019 2019 2019 2019 2019 ...\n $ d19:'data.frame':    10 obs. of  4 variables:\n  ..$ pid           : num [1:10] 9920004 12500003 17350004 17670001 23860001 ...\n  ..$ Smarphone.time: num [1:10] 0 60 345 90 40 65 170 95 85 75\n  ..$ SNS.time      : num [1:10] 0 0 10 0 0 0 0 0 0 0\n  ..$ year          : num [1:10] 2019 2019 2019 2019 2019 ...\n $    :List of 1\n  ..$ : num 0\n\n\n\n# The first element of the fifth element\nList.KMP[[5]][[1]]&lt;-c(1:10)\n\n# The second element of the fifth element\nList.KMP[[5]][[2]]&lt;-matrix(c(1:12), nrow=4)\n\n\nList.KMP[[5]]\n\n[[1]]\n [1]  1  2  3  4  5  6  7  8  9 10\n\n[[2]]\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\n\n\n\n\n\nExtract the ‘p17’ element\n\np17_df &lt;- List.KMP[[\"p17\"]]\np17_df\n\n        pid age gender     Mobile.lv       Telecom Smartphone.brand sp.mobile\n1   9920004   9   Male No Smartphone No Smartphone    No Smartphone         0\n2  12500003  26   Male           LTE            KT          Samsung        42\n3  17350004  12 Female           LTE         LG U+          Samsung        19\n4  17670001  55   Male           LTE           SKT          Samsung        38\n5  23860001  70   Male No Smartphone No Smartphone    No Smartphone        18\n6  24450001  58   Male           LTE           SKT          Samsung        65\n7  27570001  50   Male           LTE           SKT          Samsung        32\n8  53620001  68   Male           LTE           SKT          Samsung        58\n9  59570001  39   Male           LTE            KT          Samsung        63\n10 65840001  37 Female         LTE-A            KT               LG        54\n   sp.device sp.online.content sp.offline.contet year\n1          0                 0                10 2017\n2          0                53                22 2017\n3          0                19                 0 2017\n4          0                38                 0 2017\n5          0                18                 0 2017\n6          0                65                50 2017\n7          0                32                36 2017\n8         10                68                 0 2017\n9          0                63                25 2017\n10        10               114                40 2017\n\n\n\n# Summary Statistics\nsummary(p17_df)\n\n      pid                age           gender          Mobile.lv\n Min.   : 9920004   Min.   : 9.00   Male  :8   3G           :0  \n 1st Qu.:17430003   1st Qu.:28.75   Female:2   LTE          :7  \n Median :24155001   Median :44.50              LTE-A        :1  \n Mean   :31235002   Mean   :42.40              5G           :0  \n 3rd Qu.:47107501   3rd Qu.:57.25              No Smartphone:2  \n Max.   :65840001   Max.   :70.00                               \n                                                                \n          Telecom       Smartphone.brand   sp.mobile       sp.device \n SKT          :4   Samsung      :7       Min.   : 0.00   Min.   : 0  \n KT           :3   No Smartphone:2       1st Qu.:22.25   1st Qu.: 0  \n LG U+        :1   LG           :1       Median :40.00   Median : 0  \n MVNO         :0   Apple        :0       Mean   :38.90   Mean   : 2  \n No Smartphone:2   Pantech      :0       3rd Qu.:57.00   3rd Qu.: 0  \n                   Xiaomi       :0       Max.   :65.00   Max.   :10  \n                   (Other)      :0                                   \n sp.online.content sp.offline.contet      year     \n Min.   :  0.00    Min.   : 0.00     Min.   :2017  \n 1st Qu.: 22.25    1st Qu.: 0.00     1st Qu.:2017  \n Median : 45.50    Median :16.00     Median :2017  \n Mean   : 47.00    Mean   :18.30     Mean   :2017  \n 3rd Qu.: 64.50    3rd Qu.:33.25     3rd Qu.:2017  \n Max.   :114.00    Max.   :50.00     Max.   :2017  \n                                                   \n\n\n\n\nAmong 10 people, How many people did use Samsung phone at 2017?\nHow much did people spend for the mobile communication on average?\n\n\nDo the same thing to 2019 data set and answer the questions below.\n\nIn 2019, how many people did use Samsung phone?\nDraw boxplot of people’s spending on the mobile communication\n*Hint: use boxplot()\n\n\n\n\nNotice\n\nAbout team project"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/08_week.html",
    "href": "teaching/ds101/weekly/posts/08_week.html",
    "title": "Data Visualization (1)",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nData visualization (1)\n\n\n\n\n# Data Visualization\n\n# average\napply(anscombe, 1, mean)\n\n [1]  8.65250  7.45250 10.47125  8.56625  9.35875 10.49250  6.33750  7.03125\n [9]  9.71000  6.92625  5.75500\n\napply(anscombe, 2, mean)\n\n      x1       x2       x3       x4       y1       y2       y3       y4 \n9.000000 9.000000 9.000000 9.000000 7.500909 7.500909 7.500000 7.500909 \n\n# Dispersion\napply(anscombe, 2, var)\n\n       x1        x2        x3        x4        y1        y2        y3        y4 \n11.000000 11.000000 11.000000 11.000000  4.127269  4.127629  4.122620  4.123249 \n\n# Correlation (correlation coefficient)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\nlibrary(gapminder)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ny &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(c_pop = sum(pop))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nhead(y, 20)\n\n# A tibble: 20 × 3\n# Groups:   year [4]\n    year continent      c_pop\n   &lt;int&gt; &lt;fct&gt;          &lt;dbl&gt;\n 1  1952 Africa     237640501\n 2  1952 Americas   345152446\n 3  1952 Asia      1395357351\n 4  1952 Europe     418120846\n 5  1952 Oceania     10686006\n 6  1957 Africa     264837738\n 7  1957 Americas   386953916\n 8  1957 Asia      1562780599\n 9  1957 Europe     437890351\n10  1957 Oceania     11941976\n11  1962 Africa     296516865\n12  1962 Americas   433270254\n13  1962 Asia      1696357182\n14  1962 Europe     460355155\n15  1962 Oceania     13283518\n16  1967 Africa     335289489\n17  1967 Americas   480746623\n18  1967 Asia      1905662900\n19  1967 Europe     481178958\n20  1967 Oceania     14600414\n\nplot(y$year, y$c_pop)\n\n\n\nplot(y$year, y$c_pop, col = y$continent)\n\n\n\nplot(y$year, y$c_pop, col = y$continent, pch = c(1:5))\nplot(y$year, y$c_pop, col = y$continent, pch = c(1:length(levels(y$continent))))\n\n# Specify the number of legends as a number\nlegend(\"topright\", legend = levels((y$continent)), pch = c(1:5), col = c(1:5))\n\n# Specify the number of legends to match the number of data\nlegend(\"bottomleft\", legend = levels((y$continent)), pch = c(1:length(levels(y$continent))), col = c(1:length(levels(y$continent))) )\n\n\n\n# 02 Basic features of visualization #\nplot(gapminder$gdpPercap, gapminder$lifeExp, col = gapminder$continent)\nlegend(\"bottomright\", \n       legend = levels((gapminder$continent)),\n        pch = c(1:length(levels(gapminder$continent))),\n        col = c(1:length(levels(y$continent))))\n\n\n\nplot(log10(gapminder$gdpPercap), gapminder$lifeExp, col = gapminder$continent)\nlegend(\"bottomright\", legend = levels((gapminder$continent)), pch = c(1:length(levels(gapminder$continent))), col = c(1:length(levels(y$continent))) )\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n\ngapminder %&gt;% ggplot(,aes())\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) +\n   geom_point() +\n   scale_x_log10()\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent, size = pop)) +\n   geom_point() +\n   scale_x_log10()\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent, size = pop)) +\n   geom_point(alpha = 0.5) +\n   scale_x_log10()\n\n\n\ntable(gapminder$year)\n\n\n1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 \n 142  142  142  142  142  142  142  142  142  142  142  142 \n\ngapminder %&gt;% filter(year==1977) %&gt;%\n   ggplot(., aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) +\n   geom_point(alpha=0.5) +\n   scale_x_log10()\n\n\n\ngapminder %&gt;% filter(year==2007) %&gt;%\n   ggplot(., aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) +\n   geom_point(alpha=0.5) +\n   scale_x_log10()\n\n\n\nggplot(gapminder, aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) +\n   geom_point(alpha=0.5) +\n   scale_x_log10() +\n   facet_wrap(~year)\n\n\n\ngapminder %&gt;%\n   filter(year == 1952 & continent ==\"Asia\") %&gt;%\n   ggplot(aes(reorder(country, pop), pop)) +\n   geom_bar(stat = \"identity\") +\n   coord_flip()\n\n\n\ngapminder %&gt;% filter(year==1952 & continent== \"Asia\") %&gt;% ggplot(aes(reorder(country, pop), pop)) + geom_bar(stat = \"identity\") + scale_y_log10() + coord_flip ()\n\n\n\ngapminder %&gt;%\n   filter(country == \"Korea, Rep.\") %&gt;%\n   ggplot(aes(year, lifeExp, col = country)) +\n   geom_point() +\n   geom_line()\n\n\n\ngapminder %&gt;%\n   filter(country == \"Korea, Rep.\") %&gt;%\n   ggplot(aes(year, lifeExp, col = country)) +\n   # geom_point() +\n   geom_line()\n\n\n\ngapminder %&gt;%\n   ggplot(aes(x = year, y = lifeExp, col = continent)) +\n   geom_point(alpha = 0.2) +\n   geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nx = filter(gapminder, year == 1952)\nhist(x$lifeExp, main = \"Histogram of lifeExp in 1952\")\n\n\n\nx %&gt;% ggplot(aes(lifeExp)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nx %&gt;% ggplot(aes(continent, lifeExp)) + geom_boxplot()\n\n\n\nplot(log10(gapminder$gdpPercap), gapminder$lifeExp)\n\n\n\n\n\n\n\nClass\n\nData visualization is an essential skill in data science, helping to turn complex results into comprehensible insights. In R, one of the most powerful tools for creating professional and visually appealing graphs is ggplot2. This package, built on the principles of the Grammar of Graphics by Leland Wilkinson, allows users to create graphs that are both informative and attractive. Let’s delve into the concepts and practical applications of ggplot2 to enhance your data visualization skills.\n\n\n\n\nGrammar of Graphics\n\nggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\nSee the official home of ggplot2: https://ggplot2.tidyverse.org/\n\n\n\n\nUnderstanding ggplot2’s Grammar of Graphics\n\nComponents of the Grammar\nAt its core, ggplot2 operates on a coherent set of principles known as the “Grammar of Graphics.” This framework allows you to specify graphs in terms of their underlying components:\n\n\nAesthetics (aes): These define how data is mapped to visual properties like size, shape, and color.\nGeoms (geometric objects): These are the actual visual elements that represent data—points, lines, bars, etc.\nStats (statistical transformations): Some plots require transformations, such as calculating means or fitting a regression line, which are handled by stats.\nScales: These control how data values are mapped to visual properties.\nCoordinate systems: These define how plots are oriented, with Cartesian coordinates being the most common, but others like polar coordinates are available for specific needs.\nFacets: Faceting allows you to generate multiple plots based on a grouping variable, creating a matrix of panels.\n\n\nLet me explain with the official introduction of ggplot2: https://ggplot2.tidyverse.org/articles/ggplot2.html\n\n\n\n\nSetting Up Your Environment\nBefore diving into creating plots, you need to install and load ggplot2 in your R environment:\n\n# ggplot2 is a package belongs to tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nTest if it works.\n\n ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point()\n\n\n\n\n\n\n\nPractical Examples\n\nBasic Plots\nLet’s start with a basic scatter plot to examine the relationship between two variables in the mtcars dataset:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) + geom_point()\n\n\n\n\nThis code plots the miles per gallon (mpg) against the weight (wt) of various cars. The aes function maps the aesthetics to the respective variables.\n\n\nEnhancing Visualizations\nTo enhance this plot, we might want to add a linear regression line to summarize the relationship between weight and fuel efficiency:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_minimal() +\n  labs(title = \"Fuel Efficiency vs. Weight\", x = \"Weight (1000 lbs)\", y = \"Miles per Gallon\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis code not only adds the regression line but also improves the aesthetics with a minimal theme and labels that clarify what each axis represents.\nPractice once more with palmer penguins dataset.\n\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nDrop missing variables\n\npenguins %&gt;% \n  drop_na()\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nggplot(penguins) +\n  aes(x = bill_length_mm,\n      y = bill_depth_mm,\n      colour = species) +\n  geom_point(shape = \"circle\", size = 1.5) +\n  scale_color_manual(\n    values = c(Adelie = \"#F8766D\",\n    Chinstrap = \"#00C19F\",\n    Gentoo = \"#FF61C3\")\n  ) +\n  ggthemes::theme_fivethirtyeight() +\n  theme(legend.position = \"bottom\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nLayers in use above\n\nLayer connecting the X-axis and Y-axis\n\naes(x = bill_length_mm, y = bill_depth_mm, colour = species)\n\nA layer that sets the elements of the graph\n\ngeom_point(shape = \"circle\", size = 1.5)\n\nA layer that sets the color of the graph\n\nscale_color_manual( values = c(Adelie = \"#F8766D\", Chinstrap = \"#00C19F\", Gentoo = \"#FF61C3\") )\n\nA layer that sets the theme of the graph\n\nggthemes::theme_fivethirtyeight()\n\nLayer to set the position of the legend\n\ntheme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAdvanced ggplot2 Features\n\nFaceting for Comparative Analysis\nTo compare how the relationship between weight and fuel efficiency varies by the number of cylinders in the engine, we can use faceting:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~cyl)\n\n\n\n\nThis will create a separate plot for each number of cylinders, making it easy to see differences across categories.\n\nfacet: a particular aspect of feature of something\n\n\nggplot(penguins) +\n  aes(x = bill_length_mm,\n      y = bill_depth_mm,\n      colour = species) +\n  geom_point(shape = \"circle\", size = 1.5) +\n  scale_color_manual(\n    values = c(Adelie = \"#F8766D\",\n    Chinstrap = \"#00C19F\",\n    Gentoo = \"#FF61C3\")\n  ) +\n  ggthemes::theme_fivethirtyeight() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~island)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\npenguins %&gt;% drop_na %&gt;% \nggplot() +\n  aes(x = bill_length_mm,\n      y = bill_depth_mm,\n      colour = species) +\n  geom_point(shape = \"circle\", size = 1.5) +\n  scale_color_manual(\n    values = c(Adelie = \"#F8766D\",\n    Chinstrap = \"#00C19F\",\n    Gentoo = \"#FF61C3\")\n  ) +\n  ggthemes::theme_fivethirtyeight() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(sex ~ island)\n\n\n\n\n\n\nCustomization and Extensions\nCheck out extentions of ggplot2: https://exts.ggplot2.tidyverse.org/gallery/\nggplot2 is highly customizable, allowing extensive control over nearly every visual aspect of a plot. For users interested in making interactive plots, ggplot2 can be integrated with the plotly library, transforming static charts into interactive visualizations.\nThe power and flexibility of ggplot2 make it an indispensable tool for data visualization in R. Whether you are a beginner or an experienced user, there is always more to explore and learn with ggplot2. Practice regularly, and don’t hesitate to experiment with different components to discover the best ways to convey your insights visually.\n\nTo master ggplot2, see the videos below:\nggplot2 workshop part 1 by Thomas Lin Pedersen\nhttps://www.youtube.com/watch?v=h29g21z0a68\nggplot2 workshop part 2 by Thomas Lin Pedersen\nhttps://www.youtube.com/watch?v=0m4yywqNPVY"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/08_week.html#date-and-location",
    "href": "teaching/ds101/weekly/posts/08_week.html#date-and-location",
    "title": "Data Visualization (1)",
    "section": "Date and Location",
    "text": "Date and Location\n\nDate: 19 April (Wed) 13:00 - 15:00\nLocation: Room 312 (School of Communication, the same building)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/08_week.html#notice",
    "href": "teaching/ds101/weekly/posts/08_week.html#notice",
    "title": "Data Visualization (1)",
    "section": "Notice",
    "text": "Notice\n\nQuiz will be administered through Google Forms.\nPlease bring your laptop for the quiz.\nYou are allowed to access any information through the Internet\nHowever, communication with others is strictly prohibited.\nDo not use any messaging apps (e.g., KakaoTalk, TikTok, Line, WeChat, etc.) during the quiz.\nUpon completion of the quiz, you are required to submit your code."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/08_week.html#data",
    "href": "teaching/ds101/weekly/posts/08_week.html#data",
    "title": "Data Visualization (1)",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html",
    "href": "teaching/ds101/weekly/posts/10_week.html",
    "title": "QZ",
    "section": "",
    "text": "Weekly design"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/13_week.html",
    "href": "teaching/ds101/weekly/posts/13_week.html",
    "title": "Version Control and Collaboration",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\n\nFirst Steps in Learning the Use of Git & GitHub in RStudio\n\n\n\n\n\nCourse materials and books\n\nHow to Use Git/GitHub with R\nhttps://rfortherestofus.com/2021/02/how-to-use-git-github-with-r\nBook: Happy Git and GitHub for the useR\nhttps://happygitwithr.com/\nOther courses and materials for further studies\n\nhttps://stat545.com/index.html\nhttps://ubc-mds.github.io/descriptions/\n\n\n\n\n\n\nClass: Version Control and Collaboration in R\n\nWe cover this topic with this link:\nhttps://biostats-r.github.io/biostats/github/\n\n\n\nIntroduction\nVersion control is an essential skill for any data scientist or programmer, especially when working in teams. It helps manage changes to your codebase and facilitates collaboration. Git is a popular version control system, and GitHub is a web-based platform that uses Git to host and manage code repositories. R Studio, a powerful integrated development environment (IDE) for R, integrates seamlessly with Git and GitHub, making version control and collaboration straightforward.\n\n\n\nObjectives\nBy the end of this tutorial, you will be able to:\n\nUnderstand the basics of Git and GitHub.\nSet up Git and GitHub with R Studio.\nPerform essential version control operations within R Studio.\nCollaborate with others using GitHub.\n\n\n\n\n1. Understanding Git and GitHub\n\nWhat is Git?\n\nGit is a distributed version control system that tracks changes in source code during software development. It allows multiple developers to work on a project simultaneously without overwriting each other’s work.\n\nKey Concepts:\n\nRepository: A directory that contains your project files and the entire history of changes made to them.\nCommit: A snapshot of your repository at a specific point in time.\nBranch: A parallel version of your repository. The main branch is often called master or main.\nMerge: Combining changes from different branches.\n\n\n\n\nWhat is GitHub?\nGitHub is a web-based platform that hosts Git repositories. It provides a collaborative environment with features like issue tracking, project management, and more.\n\n\n\n\n2. Setting Up Git and GitHub with R Studio\n\nPrerequisites\n\nInstall R\nInstall R Studio\nInstall Git\nCreate a GitHub account\n\n\n\nConfiguring Git in R Studio\n\nInstall Git: Download and install Git from Git’s official website. Follow the installation instructions for your operating system.\nConfigure Git: Open a terminal or command prompt and set your Git username and email:\nLinking Git with R Studio: Open R Studio, go to Tools &gt; Global Options &gt; Git/SVN. Ensure the Git executable is correctly detected. If not, manually browse to the location of the Git executable.\nCloning a Repository: In R Studio, go to File &gt; New Project &gt; Version Control &gt; Git. Enter the URL of a GitHub repository you wish to clone. This will create a local copy of the repository on your machine.\n\n\n\n\n\n3. Performing Essential Version Control Operations\n\nCreating a Repository\n\nCreate a new project: Go to File &gt; New Project &gt; New Directory &gt; New Project.\nInitialize Git: Check the box that says Create a git repository.\nCommit Changes:\n\nMake some changes to your files.\nOpen the Git pane in R Studio.\nSelect the files you want to commit, write a commit message, and click Commit.\n\nPush to GitHub:\n\nOpen the terminal in R Studio.\nAdd the remote repository URL:\n\n\n\nPush your changes:\n\n\n\n\nBranching and Merging\n\nCreate a New Branch:\n\n\n\nMerge Changes:\n\nSwitch back to the master branch:\nMerge the new branch:\n\nResolving Conflicts\n\nConflicts occur when changes are made to the same part of a file on different branches. Git will mark the conflict in the file, and you need to manually resolve it before committing the changes.\n\n\n\n\n4. Collaborating with Others\n\nForking and Pull Requests\n\nFork a Repository: On GitHub, navigate to the repository you want to contribute to and click Fork.\nClone Your Fork: Clone the forked repository to your local machine:\nCreate a Branch and Make Changes:\n\nCreate a new branch:\nMake your changes and commit them.\n\nPush Your Changes:\nCreate a Pull Request: Go to the original repository on GitHub and click New pull request. Select your branch and submit the pull request for review.\n\n\n\nCollaborating within a Team\n\nPull Changes: Regularly pull changes from the main repository to keep your local repository up to date:\nReviewing Code: Use GitHub’s review feature to comment on and approve changes made by your collaborators."
  },
  {
    "objectID": "teaching/ds101/pbl/index.html",
    "href": "teaching/ds101/pbl/index.html",
    "title": "PBL",
    "section": "",
    "text": "Students organize teams that meet several conditions.\n\n4~5 members in a team\nBackground diversity: no homogeneous majors in a team\nException: Allowed if persuasion is possible for sufficient reasons\n\nData: Panel Big Data given EMBRAIN corp.\n\nIf persuasion is possible with sufficient reasons, a project using other data can be used\n\n\n\n\nAbout EMBRAIN’s panel bigdata: [pdf]\n\nDue to the limitation of survey, we need a bigdata being collected from consumer\nPanel bigdata\n\nPanel: Panelists are survey respondents who have expressed their intention to participate in the survey in advance and provided personal information under a contract with the survey company.\nPanel Big Data is an integrated platform that allows you to analyze surveys and big data together.\n\nIncluding..\n\nApp using information\nVisited locations (using WiFi & GPS detection technology)\nPayment information\nBasic demographics (gender, age, and so on)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput\n\nData analysis report\n\nAny format is possible (PPT, word, notion web page link, pdf, and so on).\nReport example (but not limited)\n\nData Preparing (or Collecting)\nExplore data (Descriptive stats)\nSet your hypothesis (or research questions)\nVisualize data to confirm your hypo or RQs\nExplain your findings\nExpanding your findings to implications\n\n\nPresentation Video\n\nMaking videos for 10 mins presentation (in any language),\nSubmit a Youtube link\n\n\n\nThe best teams may lead to URP in the summer semester.\n\n\nTeams\n\n윤승현(경영), 최경석(스포츠과학), 박재원(DS), 임서영(미술), 정하경(국국)\n안제민(DS), 이채빈(DS), 김지희(DS), 이윤서(DS), 유선아(AI)\n민범기(CT), 이시연(영상), 김경서(영상), 김지연(소비자), 이지우(CT)\n손채리(한교), 이수아(글경), 최재원(통계), 윤지우(독독), 김서영(CT)\n문정은(경영), 박우혁(DS), 이상훈(DS), 양제니(CT)\n정은채(AI), 강윤경(AI), 이혜연(DS), 우정현(CT), 김민기(유동)\n송준석(CT), 이자의(CT), 최은서(CT), 홍은지(소비자), 이지원(통계)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html#date-and-location",
    "href": "teaching/ds101/weekly/posts/10_week.html#date-and-location",
    "title": "QZ",
    "section": "Date and Location",
    "text": "Date and Location\n\nDate: 8 May (Wed) 09:00 - 11:00\nLocation: Class Room"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html#notice",
    "href": "teaching/ds101/weekly/posts/10_week.html#notice",
    "title": "QZ",
    "section": "Notice",
    "text": "Notice\n\nQuiz will be administered through Google Forms.\nPlease bring your laptop for the quiz.\nYou are allowed to access any information through the Internet\nHowever, communication with others is strictly prohibited.\nDo not use any messaging apps (e.g., KakaoTalk, TikTok, Line, WeChat, etc.) during the quiz.\nUpon completion of the quiz, you are required to submit your code."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html#data",
    "href": "teaching/ds101/weekly/posts/10_week.html#data",
    "title": "QZ",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#introduction",
    "href": "teaching/ds101/weekly/posts/11_week.html#introduction",
    "title": "No Class",
    "section": "",
    "text": "It contains the analysis process of ‘Titanic’, one of the representative ‘Competition’ of ‘Kaggle’."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#prepare-to-work",
    "href": "teaching/ds101/weekly/posts/11_week.html#prepare-to-work",
    "title": "No Class",
    "section": "2. Prepare to work",
    "text": "2. Prepare to work\n\n\n2.1 Packages\nsee “What is a package in R”\n\nThis is the process of loading (loading) the Packages I used for analysis, in addition to the representative Packages of R, such as tidyverse (including ggplot2 and dplyr).\n\n\n# Data input, assesment \nlibrary(titanic)\nlibrary(readr)           # Data input with readr::read_csv()\nlibrary(descr)           # descr::CrossTable() - Frequency by category, check with ratio figures\n\n# Visualization\nlibrary(VIM)             # Missing values assesment used by VIM::aggr()\nlibrary(RColorBrewer)    # Plot color setting\nlibrary(scales)          # plot setting - x, y axis\n\n# Feature engineering, Data Pre-processing\nlibrary(tidyverse)     # dplyr, ggplot2, purrr, etc..      # Feature Engineering & Data Pre-processing\nlibrary(ggpubr)\n\nlibrary(randomForest)\n# Model validation \nlibrary(caret)           # caret::confusionMatrix()\nlibrary(ROCR)            # Plotting ROC Curve\n\n\n\n\n2.2 Raw data import\n\nIn titanic competition, train data used to create Model and test data used for actual prediction (estimation) are separated.\nHere, we will load those two data and combine them into one. The reason for tying the separate data together is to work the same when feature engineering and pre-processing the input variables used in modeling.\nPlease see this link if you want to know about the story of Titanic.\n\n\ntitanic_train %&gt;% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\ntitanic_test %&gt;% glimpse\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;int&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"B45\", \"\",…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\ntrain &lt;- titanic_train\ntest  &lt;- titanic_test\n\nfull &lt;- dplyr::bind_rows(train, test)\nfull %&gt;% glimpse\n\nRows: 1,309\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nThe reason why rbind() was not used even when combining the two data into full is that Survived, the dependent variable (target variable, Y) of Titanic competition, does not exist in test. Therefore, the dimensions (dimension) of the two data do not match, so they are not merged with rbind(). However, if you use dplyr::bind_rows(), Survived in test is treated as NA and merged into one.\n\n\n2.3 variable meaning explanation\n\n\n\n\n\n\n\n\nvariable name\nInterpretation (meaning)\nType\n\n\n\n\nPassengerID\nUnique ID number that identifies the passenger\nInt\n\n\nSurvived\nIndicates whether or not the passenger survived. Survival is 1 and death is 0.\nFactor\n\n\nPclass\nThe class of the cabin, with 3 categories from 1st class (1) to 3rd class (3).\nOrd.Factor\n\n\nName\nPassenger’s name\nFactor\n\n\nSex\nPassenger’s gender\nFactor\n\n\nAge\nAge of passenger\nNumeric\n\n\nSibSp\nVariable describing the number of siblings or spouses accompanying each passenger. It can range from 0 to 8.\nInteger\n\n\nParch\nVariable describing the number of parents or children accompanying each passenger, from 0 to 9.\nInteger\n\n\nTicket\nString variable for the ticket the passenger boarded\nFactor\n\n\nFare\nVariable for how much the passenger has paid for the trip so far\nNumeric\n\n\nCabin\nVariable that distinguishes each passenger’s cabin, with too many categories and missing values.\nFactor\n\n\nEmbarked\nIndicates the boarding port and departure port, and consists of three categories: C, Q, and S.\nFactor\n\n\n\n\n\n\n2.4 Change the variables type\n\nBefore the full-scale EDA and feature engineering, let’s transform some variable properties. For example, Pclass is treated as numeric, but actually 1, 2, 3 are factors representing 1st, 2nd, and 3rd grades.\n\n\nfull &lt;- full %&gt;%\n  dplyr::mutate(Survived = factor(Survived),\n                Pclass   = factor(Pclass, ordered = T),\n                Name     = factor(Name),\n                Sex      = factor(Sex),\n                Ticket   = factor(Ticket),\n                Cabin    = factor(Cabin),\n                Embarked = factor(Embarked))"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#eda-exploratory-data-analysis",
    "href": "teaching/ds101/weekly/posts/11_week.html#eda-exploratory-data-analysis",
    "title": "No Class",
    "section": "3. EDA : Exploratory data analysis",
    "text": "3. EDA : Exploratory data analysis\n\nIt is the process of exploring and understanding raw data, such as how data is structured and whether there are missing values or outliers in it.\nWe will use various functions and visualizations here.\n\n\n3.1 Data confirmation using numerical values\nFirst of all, let’s check the data through the output of various functions such as head() and summary().\n\n\n3.1.1 head()\n\n\nhead(full, 10)\n\n   PassengerId Survived Pclass\n1            1        0      3\n2            2        1      1\n3            3        1      3\n4            4        1      1\n5            5        0      3\n6            6        0      3\n7            7        0      1\n8            8        0      3\n9            9        1      3\n10          10        1      2\n                                                  Name    Sex Age SibSp Parch\n1                              Braund, Mr. Owen Harris   male  22     1     0\n2  Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                               Heikkinen, Miss. Laina female  26     0     0\n4         Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                             Allen, Mr. William Henry   male  35     0     0\n6                                     Moran, Mr. James   male  NA     0     0\n7                              McCarthy, Mr. Timothy J   male  54     0     0\n8                       Palsson, Master. Gosta Leonard   male   2     3     1\n9    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female  27     0     2\n10                 Nasser, Mrs. Nicholas (Adele Achem) female  14     1     0\n             Ticket    Fare Cabin Embarked\n1         A/5 21171  7.2500              S\n2          PC 17599 71.2833   C85        C\n3  STON/O2. 3101282  7.9250              S\n4            113803 53.1000  C123        S\n5            373450  8.0500              S\n6            330877  8.4583              Q\n7             17463 51.8625   E46        S\n8            349909 21.0750              S\n9            347742 11.1333              S\n10           237736 30.0708              C\n\n\n\nLooking at the result of head(), we can see that there is a missing value (NA) in Age.\nIf so, is there only Age missing in the entire data?\nFor the answer, please refer to 3.2 Missing values.\n\n\n\n3.1.2 str()\n\n\nstr(full)\n\n'data.frame':   1309 obs. of  12 variables:\n $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass     : Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : Factor w/ 1307 levels \"Abbing, Mr. Anthony\",..: 156 287 531 430 23 826 775 922 613 855 ...\n $ Sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : Factor w/ 929 levels \"110152\",\"110413\",..: 721 817 915 66 650 374 110 542 478 175 ...\n $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 108 1 72 1 1 165 1 1 1 ...\n $ Embarked   : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 2 4 4 4 3 4 4 4 2 ...\n\n\n\nBy combining the train and test data, the total number of observations (record, row, row) is 1309 (train: 891, test: 418), and the number of variables (column, feature, variable, column) is 12.\nIn addition, you can find out what the attributes of each variable are and how many categories there are for variables that are factor attributes.\nIn addition, in head(), it can be seen that the missing value (NA), which was thought to exist only in Age, also exists in other variables including Cabin.\n\n\n\n3.1.3 summary()\n\n\nsummary(full)\n\n  PassengerId   Survived   Pclass                                Name     \n Min.   :   1   0   :549   1:323   Connolly, Miss. Kate            :   2  \n 1st Qu.: 328   1   :342   2:277   Kelly, Mr. James                :   2  \n Median : 655   NA's:418   3:709   Abbing, Mr. Anthony             :   1  \n Mean   : 655                      Abbott, Master. Eugene Joseph   :   1  \n 3rd Qu.: 982                      Abbott, Mr. Rossmore Edward     :   1  \n Max.   :1309                      Abbott, Mrs. Stanton (Rosa Hunt):   1  \n                                   (Other)                         :1301  \n     Sex           Age            SibSp            Parch            Ticket    \n female:466   Min.   : 0.17   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n male  :843   1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n              Median :28.00   Median :0.0000   Median :0.000   CA 2144 :   8  \n              Mean   :29.88   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n              3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n              Max.   :80.00   Max.   :8.0000   Max.   :9.000   347082  :   7  \n              NA's   :263                                      (Other) :1261  \n      Fare                     Cabin      Embarked\n Min.   :  0.000                  :1014    :  2   \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270   \n Median : 14.454   B57 B59 B63 B66:   5   Q:123   \n Mean   : 33.295   G6             :   5   S:914   \n 3rd Qu.: 31.275   B96 B98        :   4           \n Max.   :512.329   C22 C26        :   4           \n NA's   :1         (Other)        : 271           \n\n\n\nsummary() provides a lot of information about the data.\nThe representative values of quantitative variables (Integer, Numeric), the number of categories of categorical (Factor) variables, and the number of observations belonging to each category are all shown as numerical values.\nHere are the things to check and move on:\n\nSurvived: This is the target variable for this competition, and 418 missing values are due to the test data.\nPclass: There are three categories of 1st class, 2nd class, and 3rd class, and 3rd class passengers are the most.\nName: There are people with similar names. So you can see that some passengers are traveling alone, while others are traveling with their families.\nSex: There are almost twice as many males as females.\nAge: It ranges from 0.17 to 80 years old, but it seems necessary to check whether it is an outlier that incorrectly entered 17, and there are 263 missing values.\nSibSp: From 0 to 8, and the 3rd quartile is 1, so it can be seen that you boarded the Titanic with a couple or siblings.\nParch: It ranges from 0 to 9, but the fact that the 3rd quartile is 0 indicates that there are very few passengers with parents and children.\nBoth SibSp and Parch are variables representing family relationships. Through this, we will find out the total number of people in the family, although we do not know who was on board, and based on that, we will create a categorical derived variable called FamilySized that represents the size of the family.\nTicket: Looking at the result of 3.1.2 str(), you can see that some passengers have exactly the same ticket, some passengers have tickets overlapping only a certain part, and some passengers have completely different tickets. We plan to use this to create a derived variable called ticket.size.\nFare: 0 to 512, with 1 missing value. I care that the 3rd quartile is 31.275 and the max is 512.\nCabin: It has the most (1014) missing values among a total of 12 features. It’s a variable that represents the ship’s area, but if there’s no way to use it, I think it should be discarded.\nEmbarked: It consists of a total of 3 categories, S is the most, and there are 2 missing values.\n\nWhen performing a basic exploration of the data, please look at the outputs of various functions besides summary() and str() while comparing them.\n\n\n\n\n3.2 Missing values\n\nThis is the process of checking which variables have missing values mentioned above and how many of them exist.\nI’m going to check it numerically and visually at the same time using the dplyr, ggplot2, and VIM packages.\nYou don’t have to use all the code I’ve run, you can use only the parts you think you need or like as you read.\n\n\n3.2.1 VIM packages\n\n\nVIM::aggr(full, prop = FALSE, combined = TRUE, numbers = TRUE,\n          sortVars = TRUE, sortCombs = TRUE)\n\n\n\n\n\n Variables sorted by number of missings: \n    Variable Count\n    Survived   418\n         Age   263\n        Fare     1\n PassengerId     0\n      Pclass     0\n        Name     0\n         Sex     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n       Cabin     0\n    Embarked     0\n\n\n\n\n\n3.2.2 tidyverse packages\n\nIn addition to checking missing values at once using the VIM package, these are methods for checking missing values using various packages that exist in the tidyverse.\nFirst, find the proportion of missing values for each variable with dplyr.\n\n\nfull %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n\n\nThere is a way to check the proportion of missing values that exist in variables, but it can also be checked using visual data.\nPlease see the two bar plots below.\n\n\n# Calculate the missing value ratio of each feature -&gt; Data Frame property but has a structure of 1 row and 12 columns.\nmissing_values &lt;- full %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nmissing_values %&gt;% head\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n# Generate the missing_values obtained above as a 12X2 data frame\nmissing_values &lt;- tidyr::gather(missing_values,\n                                key = \"feature\", value = \"missing_pct\")\n\nmissing_values %&gt;% head(12)\n\n       feature  missing_pct\n1  PassengerId 0.0000000000\n2     Survived 0.3193277311\n3       Pclass 0.0000000000\n4         Name 0.0000000000\n5          Sex 0.0000000000\n6          Age 0.2009167303\n7        SibSp 0.0000000000\n8        Parch 0.0000000000\n9       Ticket 0.0000000000\n10        Fare 0.0007639419\n11       Cabin 0.0000000000\n12    Embarked 0.0000000000\n\n# Visualization with missing_values\nmissing_values %&gt;% \n  # Aesthetic setting : missing_pct 내림차순으로 정렬  \n  ggplot(aes(x = reorder(feature, missing_pct), y = missing_pct)) +\n  # Bar plot \n  geom_bar(stat = \"identity\", fill = \"red\") +\n  # Title generation \n  ggtitle(\"Rate of missing values in each features\") +\n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = \"Feature names\", y = \"Rate\") + \n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nIf you look at the bar graph above, you can check the percentage of missing values for all features.\nHowever, what we are actually curious about is which variables have missing values and how many missing values exist in them.\nTherefore, after calculating the proportion of missing values using the purrr package, I extracted only the variables that had at least one and visualized them.\n\n\n# 변수별 결측치 비율 계산\nmiss_pct &lt;- purrr::map_dbl(full, function(x){round((sum(is.na(x))/length(x)) * 100, 1) })\n\n# 결측치 비율이 0%보다 큰 변수들만 선택\nmiss_pct &lt;- miss_pct[miss_pct &gt; 0]\n\n# Data Frame 생성 \ndata.frame(miss = miss_pct, var = names(miss_pct), row.names = NULL) %&gt;%\n  # Aesthetic setting : miss 내림차순으로 정렬 \n  ggplot(aes(x = reorder(var, miss), y = miss)) + \n  # Bar plot \n  geom_bar(stat = 'identity', fill = 'red') +\n  # Plot title setting \n  ggtitle(\"Rate of missing values\") + \n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = 'Feature names', y = 'Rate of missing values') +\n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nThrough this, only 4 variables out of a total of 12 variables have missing values (except Survived because it is due to test data), and there are many missing values in the order of Cabin, Age, Embarked, and Fare.\n\n\n\nNow, it is the process of analyzing and exploring feature through visualization.\n\n\n\n3.3 Age\n\n\nage.p1 &lt;- full %&gt;% \n  ggplot(aes(Age)) + \n  geom_histogram(breaks = seq(0, 80, by = 1), # interval setting \n                 col    = \"red\",              # bar border color\n                 fill   = \"green\",            # bar inner color\n                 alpha  = .5) +               # Bar Transparency = 50%\n  \n  # Plot title\n  ggtitle(\"All Titanic passengers age hitogram\") +\n  theme(plot.title = element_text(face = \"bold\",     \n                                  hjust = 0.5,      # Horizon (horizontal ratio) = 0.5\n                                  size = 15, color = \"darkblue\"))\n\nage.p2 &lt;- full %&gt;% \n# Exclude values where Survived == NA in the test dataset  \n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Age, fill = Survived)) + \n  geom_density(alpha = .5) +\n  ggtitle(\"Titanic passengers age density plot\") + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5,\n                                  size = 15, color = \"darkblue\"))\n\n# Display the two graphs created above on one screen\nggarrange(age.p1, age.p2, ncol=2)\n\n\n\n\n\n\n\n3.4 Pclass\n\nLet’s visualize the frequency of passengers for each Pclass.\nAfter grouping (grouping) by Pclass using dplyr package, Data Frame representing frequency by category was created and visualized with ggplot.\n\n\nfull %&gt;% \n  # Get Pclass frequencies using dplyr::group_by(), summarize()\n  group_by(Pclass) %&gt;% \n  summarize(N = n()) %&gt;% \n  # Aesthetic setting \n  ggplot(aes(Pclass, N)) +\n  geom_col() +\n  geom_text(aes(label = N),       \n            size = 5,             \n            vjust = 1.2,           \n            color = \"#FFFFFF\") +  \n  # Plot title \n  ggtitle(\"Number of each Pclass's passengers\") + \n  # Title setting \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15)) +\n  # x, y axis name change  \n  labs(x = \"Pclass\", y = \"Count\")\n\n\n\n\n\nIt can be seen that the largest number of passengers boarded in the 3-class cabin.\n\n\n\n3.5 Fare\n\nThis is a visualization of the ‘Fare’ variable, which represents the amount paid by the passenger.\nTwo histograms and boxplots were used.\n\n# Histogram \nFare.p1 &lt;- full %&gt;%\n  ggplot(aes(Fare)) + \n  geom_histogram(col    = \"yellow\",\n                 fill   = \"blue\", \n                 alpha  = .5) +\n  ggtitle(\"Histogram of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\n# Boxplot \nFare.p2 &lt;- full %&gt;%\n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Survived, Fare)) + \n  # Observations are drawn as gray dots, but overlapping areas are spread out.  \n  geom_jitter(col = \"gray\") + \n  # Boxplot: 50% transparency\n  geom_boxplot(alpha = .5) + \n  ggtitle(\"Boxplot of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\nggarrange(Fare.p1, Fare.p2, ncol=2)\n\n\n\n\n\nYou can see that the survivors have a higher ‘Fare’ than the deceased passengers, but not by much.\n\n\n\n3.6 Sex\nAre there differences in survival rates between men and women? See the plot below.\n\n\nsex.p1 &lt;- full %&gt;% \n  dplyr::group_by(Sex) %&gt;% \n  summarize(N = n()) %&gt;% \n  ggplot(aes(Sex, N)) +\n  geom_col() +\n  geom_text(aes(label = N), size = 5, vjust = 1.2, color = \"#FFFFFF\") + \n  ggtitle(\"Bar plot of Sex\") +\n  labs(x = \"Sex\", y = \"Count\")\n  \nsex.p2 &lt;- full[1:891, ] %&gt;% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  ggtitle(\"Survival Rate by Sex\") + \n  labs(x = \"Sex\", y = \"Rate\")\n\nggarrange(sex.p1, sex.p2, ncol = 2)\n\n\n\nmosaicplot(Survived ~ Sex,\n           data = full[1:891, ], col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\nIf you interpret the graph, you can see that the survival rate is higher for female passengers, while there are far more males than females."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#feature-engineering-data-pre-processing",
    "href": "teaching/ds101/weekly/posts/11_week.html#feature-engineering-data-pre-processing",
    "title": "No Class",
    "section": "4. Feature engineering & Data Pre-processing",
    "text": "4. Feature engineering & Data Pre-processing\n\nThis is the process of filling in missing values (‘NA’) based on the contents of ‘Chapter 3 EDA’ and creating derived variables at the same time.\n\n\n4.1 Age -&gt; Age.Group\n\n\nfull &lt;- full %&gt;%\n# The missing value (NA) is filled in first, and the average of the values excluding the missing value is filled.\n  mutate(Age = ifelse(is.na(Age), mean(full$Age, na.rm = TRUE), Age),\n# Create a categorical derived variable Age.Group based on Age values\n        Age.Group = case_when(Age &lt; 13             ~ \"Age.0012\",\n                               Age &gt;= 13 & Age &lt; 18 ~ \"Age.1317\",\n                               Age &gt;= 18 & Age &lt; 60 ~ \"Age.1859\",\n                               Age &gt;= 60            ~ \"Age.60inf\"),\n# Convert Chr attribute to Factor\n        Age.Group = factor(Age.Group))\n\n\n\n\n4.3 SibSp & Parch -&gt; FamilySized\n\n\nfull &lt;- full %&gt;% \n # First create a derived variable called FamilySize by adding SibSp, Parch and 1 (self)\n  mutate(FamilySize = .$SibSp + .$Parch + 1,\n        # Create a categorical derived variable FamilySized according to the value of FamilySize\n         FamilySized = dplyr::case_when(FamilySize == 1 ~ \"Single\",\n                                        FamilySize &gt;= 2 & FamilySize &lt; 5 ~ \"Small\",\n                                        FamilySize &gt;= 5 ~ \"Big\"),\n        # Convert the Chr property FamilySized to a factor\n        # Assign new levels according to the size of the group size\n         FamilySized = factor(FamilySized, levels = c(\"Single\", \"Small\", \"Big\")))\n\n\nCeated FamilySized using SibSp and Parch.\nReducing these two variables to one has the advantage of simplifying the model.\nA similar use case is to combine height and weight into a BMI index.\n\n\n\n4.4 Name & Sex -&gt; title\n\nWhen looking at the results of ‘Chapter 3.6 Sex’, it was confirmed that the survival rate of women was higher than that of men.\nTherefore, in Name, “Wouldn’t it be useful to extract only names related to gender and categorize them?” I think it is.\nFirst, extract only the column vector named Name from full data and save it as title.\n\n\n# First, extract only the Name column vector and store it in the title vector\ntitle &lt;- full$Name\ntitle %&gt;% head(20)\n\n [1] Braund, Mr. Owen Harris                                \n [2] Cumings, Mrs. John Bradley (Florence Briggs Thayer)    \n [3] Heikkinen, Miss. Laina                                 \n [4] Futrelle, Mrs. Jacques Heath (Lily May Peel)           \n [5] Allen, Mr. William Henry                               \n [6] Moran, Mr. James                                       \n [7] McCarthy, Mr. Timothy J                                \n [8] Palsson, Master. Gosta Leonard                         \n [9] Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      \n[10] Nasser, Mrs. Nicholas (Adele Achem)                    \n[11] Sandstrom, Miss. Marguerite Rut                        \n[12] Bonnell, Miss. Elizabeth                               \n[13] Saundercock, Mr. William Henry                         \n[14] Andersson, Mr. Anders Johan                            \n[15] Vestrom, Miss. Hulda Amanda Adolfina                   \n[16] Hewlett, Mrs. (Mary D Kingcome)                        \n[17] Rice, Master. Eugene                                   \n[18] Williams, Mr. Charles Eugene                           \n[19] Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\n[20] Masselmani, Mrs. Fatima                                \n1307 Levels: Abbing, Mr. Anthony ... Zimmerman, Mr. Leo\n\n# Using regular expression and gsub(), extract only names that are highly related to gender and save them as title vectors\ntitle &lt;- gsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title)\ntitle %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n# Save the title vector saved above to full again, but save it as a title derived variable\nfull$title &lt;- title\n\nfull$title %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n\n\ngsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title) In short, this code uses gsub to search for and replace a pattern in the title string, where the pattern is defined by the regular expression \"^.*, (.*?)\\\\..*$\" and the replacement is defined by the string \"\\\\1\". If you want to understand more about regular expression. Please see my blog post: What are Regular Expressions and How to Use Them in R\nThen check what are the Unique titles.\n\n\nunique(full$title)\n\n [1] \"Mr\"           \"Mrs\"          \"Miss\"         \"Master\"       \"Don\"         \n [6] \"Rev\"          \"Dr\"           \"Mme\"          \"Ms\"           \"Major\"       \n[11] \"Lady\"         \"Sir\"          \"Mlle\"         \"Col\"          \"Capt\"        \n[16] \"the Countess\" \"Jonkheer\"     \"Dona\"        \n\n\n\nYou can see that there are 18 categories in total.\nIf you use this derived variable called ‘title’ as it is, the complexity of the model (especially the tree based model) increases considerably, so you need to reduce the category.\nBefore that, let’s check the frequency and rate for each category using the descr package.\n\n\n# Check frequency, ratio by category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|         Capt |          Col |          Don |         Dona |           Dr |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            4 |            1 |            1 |            8 |\n|        0.001 |        0.003 |        0.001 |        0.001 |        0.006 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|     Jonkheer |         Lady |        Major |       Master |         Miss |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            1 |            2 |           61 |          260 |\n|        0.001 |        0.001 |        0.002 |        0.047 |        0.199 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|         Mlle |          Mme |           Mr |          Mrs |           Ms |\n|--------------|--------------|--------------|--------------|--------------|\n|            2 |            1 |          757 |          197 |            2 |\n|        0.002 |        0.001 |        0.578 |        0.150 |        0.002 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|          Rev |          Sir | the Countess |\n|--------------|--------------|--------------|\n|            8 |            1 |            1 |\n|        0.006 |        0.001 |        0.001 |\n|--------------|--------------|--------------|\n\n\n\nThe frequencies and proportions of the 18 categories are very different.\nSo let’s narrow these down to a total of five categories.\n\n\n# Simplify into 5 categories\nfull &lt;- full %&gt;%\n# If you use \"==\" instead of \"%in%\", it won't work as you want because of Recyling Rule.\n  mutate(title = ifelse(title %in% c(\"Mlle\", \"Ms\", \"Lady\", \"Dona\"), \"Miss\", title),\n         title = ifelse(title == \"Mme\", \"Mrs\", title),\n         title = ifelse(title %in% c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\", \"Don\",\n                                     \"Sir\", \"the Countess\", \"Jonkheer\"), \"Officer\", title),\n         title = factor(title))\n\n# After creating the derived variable, check the frequency and ratio for each category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|  Master |    Miss |      Mr |     Mrs | Officer |\n|---------|---------|---------|---------|---------|\n|      61 |     266 |     757 |     198 |      27 |\n|   0.047 |   0.203 |   0.578 |   0.151 |   0.021 |\n|---------|---------|---------|---------|---------|\n\n\n\n\n\n4.5 Ticket -&gt; ticket.size\n\nAs we saw in Chapter 3.1.3 Summary(), the number of passengers (train and test together) is 1309. However, all passengers’ tickets are not different.\nSee the results of summary() and unique() below.\n\n\n# We used length() to get only the number of unique categories.\nlength(unique(full$Ticket))\n\n[1] 929\n\n# Printing all of them was too messy, so only 10 were printed.\nhead(summary(full$Ticket), 10)\n\n    CA. 2343         1601      CA 2144      3101295       347077       347082 \n          11            8            8            7            7            7 \n    PC 17608 S.O.C. 14879       113781        19950 \n           7            7            6            6 \n\n\n\nWhy are there 929 unique tickets when there are no missing values in feature?\nEven the ticket is CA. There are 11 exactly the same number of people as 2343.\nLet’s see who the passengers are.\n\n\nfull %&gt;% \n# Filter only 11 passengers with matching tickets\n  filter(Ticket == \"CA. 2343\") %&gt;% \n  # We don't need to check for all variables, so we only want to look at the variables below.\n  select(Pclass, Name, Age, FamilySized)\n\n   Pclass                              Name      Age FamilySized\n1       3        Sage, Master. Thomas Henry 29.88114         Big\n2       3      Sage, Miss. Constance Gladys 29.88114         Big\n3       3               Sage, Mr. Frederick 29.88114         Big\n4       3          Sage, Mr. George John Jr 29.88114         Big\n5       3           Sage, Miss. Stella Anna 29.88114         Big\n6       3          Sage, Mr. Douglas Bullen 29.88114         Big\n7       3 Sage, Miss. Dorothy Edith \"Dolly\" 29.88114         Big\n8       3                   Sage, Miss. Ada 29.88114         Big\n9       3             Sage, Mr. John George 29.88114         Big\n10      3       Sage, Master. William Henry 14.50000         Big\n11      3    Sage, Mrs. John (Annie Bullen) 29.88114         Big\n\n\n\nYou can see that the 11 passengers above are all from the same family, brothers.\nWhile there are passengers whose tickets are exactly the same, there are also passengers whose tickets are partially matched.\nCreate a ticket.unique derived variable that represents the number of unique numbers (number of characters) of such a ticket.\nLet’s create a derived variable ticket.size with 3 categories based on ticket.unique.\n\n\n# First of all, ticket.unique is saved as all 0\nticket.unique &lt;- rep(0, nrow(full))\n\n# Extract only the unique ones from ticket features and store them in the tickets vector\ntickets &lt;- unique(full$Ticket)\n\n# After extracting only passengers with the same ticket by using overlapping loops, extract and store the length (number of characters) of each ticket.\n\nfor (i in 1:length(tickets)) {\n  current.ticket &lt;- tickets[i]\n  party.indexes &lt;- which(full$Ticket == current.ticket)\n    # For loop 중첩 \n    for (k in 1:length(party.indexes)) {\n    ticket.unique[party.indexes[k]] &lt;- length(party.indexes)\n    }\n  }\n\n# Save ticket.unique calculated above as a derived variable\nfull$ticket.unique &lt;- ticket.unique\n\n# Create ticket.size variable by dividing it into three categories according to ticket.unique\n\nfull &lt;- full %&gt;% \n  mutate(ticket.size = case_when(ticket.unique == 1 ~ 'Single',\n                                 ticket.unique &lt; 5 & ticket.unique &gt;= 2 ~ \"Small\",\n                                 ticket.unique &gt;= 5 ~ \"Big\"),\n         ticket.size = factor(ticket.size,\n                              levels = c(\"Single\", \"Small\", \"Big\")))\n\n\n\n\n4.6 Embarked\n\nThis is feature with two missing values (NA). In the case of Embarked, replace it with S, which is the most frequent value among the three categories.\n\n\nfull$Embarked &lt;- replace(full$Embarked,               # Specify Data$feature to replace\n                         which(is.na(full$Embarked)), # Find only missing values\n                         'S')                        # specify the value to replace\n\n\n\n\n4.7 Fare\n\nFor Fare, there was only one missing value.\nBased on the histogram seen above (Chapter 3.5 Fare), missing values are replaced with 0.\n\n\nfull$Fare &lt;- replace(full$Fare, which(is.na(full$Fare)), 0)\n\n\nAt this point, data preprocessing is complete.\nThe following is the process of selecting the variables to be used for model creation while exploring the derived variables created so far.\nIn other words, Feature selection."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#relationship-to-target-feature-survived-feature-selection",
    "href": "teaching/ds101/weekly/posts/11_week.html#relationship-to-target-feature-survived-feature-selection",
    "title": "No Class",
    "section": "5. Relationship to target feature Survived & Feature selection",
    "text": "5. Relationship to target feature Survived & Feature selection\n\nPrior to full-scale visualization, since the purpose here is to see how well each variable correlates with the survival rate, we did not use the entire full data, but only the train data set that can determine survival and death.\nAlso, please note that the plot used above may be duplicated as it is.\n\n\n5.0 Data set split\nFirst, use the code below to split preprocessed full data into train and test.\n\n# Before feature selection, select all variables first.\n\ntrain &lt;- full[1:891, ]\n\ntest &lt;- full[892:1309, ]\n\n\n\n\n5.1 Pclass\n\n\ntrain %&gt;% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(position = \"fill\") +\n# Set plot theme: Converts to a more vivid color.\n  scale_fill_brewer(palette = \"Set1\") +\n  # Y axis setting \n  scale_y_continuous(labels = percent) +\n# Set x, y axis names and plot main title, sub title\n  labs(x = \"Pclass\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Pclass?\")\n\n\n\n\n\n\n\n5.2 Sex\n\nSame as Chapter 3.6 Sex.\n\n\nmosaicplot(Survived ~ Sex,\n           data = train, col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\n\n\n5.3 Embarked\n\n\ntrain %&gt;% \n  ggplot(aes(Embarked, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Embarked\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Embarked?\")\n\n\n\n\n\n\n\n5.4 FamilySized\n\n\ntrain %&gt;% \n  ggplot(aes(FamilySized, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"FamilySized\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by FamilySized\")\n\n\n\n\n\nIt can be seen that there is a difference in survival rate depending on the number of people on board, and that ‘FamilySized’ and ‘Survived’ have a non-linear relationship.\n\n\n5.5 Age.Group\n\n\ntrain %&gt;% \n  ggplot(aes(Age.Group, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"Age group\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by Age group\")\n\n\n\n\n\n\n\n5.6 title\n\n\ntrain %&gt;% \n  ggplot(aes(title, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"title\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by passengers title\")\n\n\n\n\n\n\n\n5.7 ticket.size\n\n\ntrain %&gt;% \n  ggplot(aes(ticket.size, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"ticket.size\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by ticket.size\")\n\n\n\n\n\n\n\n5.8 Description of actual used features\n\nNow that all the derived variables created so far have been found to be useful, select and save only the variables you will actually use.\nThe table below is a brief description of the actual selected variables.\n\n\n\n\n\n\n\n\nvariable name\nType\nDescription\n\n\n\n\nSurvived\nfactor\nTarget feature, survival == 1, death == 0\n\n\nSex\nfactor\ngender, male or female\n\n\nPclass\nfactor\nCabin Class, First Class (1), Second Class (2), Third Class (3)\n\n\nEmbarked\nfactor\nPort of embarkation, Southampton (S), Cherbourg (C), Queenstown (Q)\n\n\nFamilySized\nfactor\nFamily size, a derived variable created using SibSp and Parch, with 3 categories\n\n\nAge.Group\nfactor\nAge group, a derived variable created using Age, with 4 categories\n\n\ntitle\nfactor\nA part of the name, a derived variable made using Name, and 5 categories\n\n\nticket.size\nfactor\nThe length of the unique part of the ticket, a derived variable created using ticket, with 3 categories\n\n\n\n\n\n# Excluding ID number, select and save 7 input variables and 1 target variable to actually use\n\ntrain &lt;- train %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\", \"Survived\")\n\n# For Submit, extract the Id column vector and store it in ID\n\nID &lt;- test$PassengerId\n\n# Select and save the remaining 6 variables except for Id and Survived\n\ntest &lt;- test %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\")"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#machine-learning-model-generation",
    "href": "teaching/ds101/weekly/posts/11_week.html#machine-learning-model-generation",
    "title": "No Class",
    "section": "6. Machine learning model generation",
    "text": "6. Machine learning model generation\n\nNow is the time to create a machine learning model using the train data set.\nOriginally, it is correct to create train, validation, test data sets first, create various models, and then select the final model through cross validation (CV, Cross Validation), but these processes are omitted here and RandomForest After creating only, we will predict (estimate) the test data and even create data to Submit to competition.\n\n\n6.1 Random Forest model generation\n\n\n# Set the seed number for reproducibility.\nset.seed(1901)\n\ntitanic.rf &lt;- randomForest(Survived ~ ., data = train, importance = T, ntree = 2000)\n\n\n\n\n6.2 Feature importance check\n\n\nimportance(titanic.rf)\n\n                    0        1 MeanDecreaseAccuracy MeanDecreaseGini\nPclass      47.442449 53.94070             64.73724        36.807804\nSex         54.250630 37.30378             58.66109        57.223102\nEmbarked    -6.328112 38.10930             27.32587         9.632958\nFamilySized 32.430898 31.24383             50.13349        18.086894\nAge.Group   15.203313 26.72696             29.36321        10.201187\ntitle       48.228450 41.60124             57.02653        73.146999\nticket.size 39.544367 37.80849             59.59915        22.570142\n\nvarImpPlot(titanic.rf)\n\n\n\n\n\n\n6.3 Predict test data and create submit data\n\n\n# Prediction \n\npred.rf &lt;- predict(object = titanic.rf, newdata = test, type = \"class\")\n\n# Data frame generation\nsubmit &lt;- data.frame(PassengerID = ID, Survived = pred.rf)\n\n# Write the submit data frame to file : csv is created in the folder designated by setwd().\n\n# write.csv(submit, file = './titanic_submit.csv', row.names = F)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#introduction-1",
    "href": "teaching/ds101/weekly/posts/11_week.html#introduction-1",
    "title": "No Class",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nIt contains the analysis process of ‘Titanic’, one of the representative ‘Competition’ of ‘Kaggle’."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#prepare-to-work-1",
    "href": "teaching/ds101/weekly/posts/11_week.html#prepare-to-work-1",
    "title": "No Class",
    "section": "2. Prepare to work",
    "text": "2. Prepare to work\n\n\n2.1 Packages\nsee “What is a package in R”\n\nThis is the process of loading (loading) the Packages I used for analysis, in addition to the representative Packages of R, such as tidyverse (including ggplot2 and dplyr).\n\n\n# Data input, assesment \nlibrary(titanic)\nlibrary(readr)           # Data input with readr::read_csv()\nlibrary(descr)           # descr::CrossTable() - Frequency by category, check with ratio figures\n\n# Visualization\nlibrary(VIM)             # Missing values assesment used by VIM::aggr()\nlibrary(RColorBrewer)    # Plot color setting\nlibrary(scales)          # plot setting - x, y axis\n\n# Feature engineering, Data Pre-processing\nlibrary(tidyverse)     # dplyr, ggplot2, purrr, etc..      # Feature Engineering & Data Pre-processing\nlibrary(ggpubr)\n\nlibrary(randomForest)\n# Model validation \nlibrary(caret)           # caret::confusionMatrix()\nlibrary(ROCR)            # Plotting ROC Curve\n\n\n\n\n2.2 Raw data import\n\nIn titanic competition, train data used to create Model and test data used for actual prediction (estimation) are separated.\nHere, we will load those two data and combine them into one. The reason for tying the separate data together is to work the same when feature engineering and pre-processing the input variables used in modeling.\nPlease see this link if you want to know about the story of Titanic.\n\n\ntitanic_train %&gt;% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\ntitanic_test %&gt;% glimpse\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;int&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"B45\", \"\",…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\ntrain &lt;- titanic_train\ntest  &lt;- titanic_test\n\nfull &lt;- dplyr::bind_rows(train, test)\nfull %&gt;% glimpse\n\nRows: 1,309\nColumns: 12\n$ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nThe reason why rbind() was not used even when combining the two data into full is that Survived, the dependent variable (target variable, Y) of Titanic competition, does not exist in test. Therefore, the dimensions (dimension) of the two data do not match, so they are not merged with rbind(). However, if you use dplyr::bind_rows(), Survived in test is treated as NA and merged into one.\n\n\n2.3 variable meaning explanation\n\n\n\n\n\n\n\n\nvariable name\nInterpretation (meaning)\nType\n\n\n\n\nPassengerID\nUnique ID number that identifies the passenger\nInt\n\n\nSurvived\nIndicates whether or not the passenger survived. Survival is 1 and death is 0.\nFactor\n\n\nPclass\nThe class of the cabin, with 3 categories from 1st class (1) to 3rd class (3).\nOrd.Factor\n\n\nName\nPassenger’s name\nFactor\n\n\nSex\nPassenger’s gender\nFactor\n\n\nAge\nAge of passenger\nNumeric\n\n\nSibSp\nVariable describing the number of siblings or spouses accompanying each passenger. It can range from 0 to 8.\nInteger\n\n\nParch\nVariable describing the number of parents or children accompanying each passenger, from 0 to 9.\nInteger\n\n\nTicket\nString variable for the ticket the passenger boarded\nFactor\n\n\nFare\nVariable for how much the passenger has paid for the trip so far\nNumeric\n\n\nCabin\nVariable that distinguishes each passenger’s cabin, with too many categories and missing values.\nFactor\n\n\nEmbarked\nIndicates the boarding port and departure port, and consists of three categories: C, Q, and S.\nFactor\n\n\n\n\n\n\n2.4 Change the variables type\n\nBefore the full-scale EDA and feature engineering, let’s transform some variable properties. For example, Pclass is treated as numeric, but actually 1, 2, 3 are factors representing 1st, 2nd, and 3rd grades.\n\n\nfull &lt;- full %&gt;%\n  dplyr::mutate(Survived = factor(Survived),\n                Pclass   = factor(Pclass, ordered = T),\n                Name     = factor(Name),\n                Sex      = factor(Sex),\n                Ticket   = factor(Ticket),\n                Cabin    = factor(Cabin),\n                Embarked = factor(Embarked))"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#eda-exploratory-data-analysis-1",
    "href": "teaching/ds101/weekly/posts/11_week.html#eda-exploratory-data-analysis-1",
    "title": "No Class",
    "section": "3. EDA : Exploratory data analysis",
    "text": "3. EDA : Exploratory data analysis\n\nIt is the process of exploring and understanding raw data, such as how data is structured and whether there are missing values or outliers in it.\nWe will use various functions and visualizations here.\n\n\n3.1 Data confirmation using numerical values\nFirst of all, let’s check the data through the output of various functions such as head() and summary().\n\n\n3.1.1 head()\n\n\nhead(full, 10)\n\n   PassengerId Survived Pclass\n1            1        0      3\n2            2        1      1\n3            3        1      3\n4            4        1      1\n5            5        0      3\n6            6        0      3\n7            7        0      1\n8            8        0      3\n9            9        1      3\n10          10        1      2\n                                                  Name    Sex Age SibSp Parch\n1                              Braund, Mr. Owen Harris   male  22     1     0\n2  Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                               Heikkinen, Miss. Laina female  26     0     0\n4         Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                             Allen, Mr. William Henry   male  35     0     0\n6                                     Moran, Mr. James   male  NA     0     0\n7                              McCarthy, Mr. Timothy J   male  54     0     0\n8                       Palsson, Master. Gosta Leonard   male   2     3     1\n9    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female  27     0     2\n10                 Nasser, Mrs. Nicholas (Adele Achem) female  14     1     0\n             Ticket    Fare Cabin Embarked\n1         A/5 21171  7.2500              S\n2          PC 17599 71.2833   C85        C\n3  STON/O2. 3101282  7.9250              S\n4            113803 53.1000  C123        S\n5            373450  8.0500              S\n6            330877  8.4583              Q\n7             17463 51.8625   E46        S\n8            349909 21.0750              S\n9            347742 11.1333              S\n10           237736 30.0708              C\n\n\n\nLooking at the result of head(), we can see that there is a missing value (NA) in Age.\nIf so, is there only Age missing in the entire data?\nFor the answer, please refer to 3.2 Missing values.\n\n\n\n3.1.2 str()\n\n\nstr(full)\n\n'data.frame':   1309 obs. of  12 variables:\n $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass     : Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : Factor w/ 1307 levels \"Abbing, Mr. Anthony\",..: 156 287 531 430 23 826 775 922 613 855 ...\n $ Sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : Factor w/ 929 levels \"110152\",\"110413\",..: 721 817 915 66 650 374 110 542 478 175 ...\n $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 108 1 72 1 1 165 1 1 1 ...\n $ Embarked   : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 2 4 4 4 3 4 4 4 2 ...\n\n\n\nBy combining the train and test data, the total number of observations (record, row, row) is 1309 (train: 891, test: 418), and the number of variables (column, feature, variable, column) is 12.\nIn addition, you can find out what the attributes of each variable are and how many categories there are for variables that are factor attributes.\nIn addition, in head(), it can be seen that the missing value (NA), which was thought to exist only in Age, also exists in other variables including Cabin.\n\n\n\n3.1.3 summary()\n\n\nsummary(full)\n\n  PassengerId   Survived   Pclass                                Name     \n Min.   :   1   0   :549   1:323   Connolly, Miss. Kate            :   2  \n 1st Qu.: 328   1   :342   2:277   Kelly, Mr. James                :   2  \n Median : 655   NA's:418   3:709   Abbing, Mr. Anthony             :   1  \n Mean   : 655                      Abbott, Master. Eugene Joseph   :   1  \n 3rd Qu.: 982                      Abbott, Mr. Rossmore Edward     :   1  \n Max.   :1309                      Abbott, Mrs. Stanton (Rosa Hunt):   1  \n                                   (Other)                         :1301  \n     Sex           Age            SibSp            Parch            Ticket    \n female:466   Min.   : 0.17   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n male  :843   1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n              Median :28.00   Median :0.0000   Median :0.000   CA 2144 :   8  \n              Mean   :29.88   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n              3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n              Max.   :80.00   Max.   :8.0000   Max.   :9.000   347082  :   7  \n              NA's   :263                                      (Other) :1261  \n      Fare                     Cabin      Embarked\n Min.   :  0.000                  :1014    :  2   \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270   \n Median : 14.454   B57 B59 B63 B66:   5   Q:123   \n Mean   : 33.295   G6             :   5   S:914   \n 3rd Qu.: 31.275   B96 B98        :   4           \n Max.   :512.329   C22 C26        :   4           \n NA's   :1         (Other)        : 271           \n\n\n\nsummary() provides a lot of information about the data.\nThe representative values of quantitative variables (Integer, Numeric), the number of categories of categorical (Factor) variables, and the number of observations belonging to each category are all shown as numerical values.\nHere are the things to check and move on:\n\nSurvived: This is the target variable for this competition, and 418 missing values are due to the test data.\nPclass: There are three categories of 1st class, 2nd class, and 3rd class, and 3rd class passengers are the most.\nName: There are people with similar names. So you can see that some passengers are traveling alone, while others are traveling with their families.\nSex: There are almost twice as many males as females.\nAge: It ranges from 0.17 to 80 years old, but it seems necessary to check whether it is an outlier that incorrectly entered 17, and there are 263 missing values.\nSibSp: From 0 to 8, and the 3rd quartile is 1, so it can be seen that you boarded the Titanic with a couple or siblings.\nParch: It ranges from 0 to 9, but the fact that the 3rd quartile is 0 indicates that there are very few passengers with parents and children.\nBoth SibSp and Parch are variables representing family relationships. Through this, we will find out the total number of people in the family, although we do not know who was on board, and based on that, we will create a categorical derived variable called FamilySized that represents the size of the family.\nTicket: Looking at the result of 3.1.2 str(), you can see that some passengers have exactly the same ticket, some passengers have tickets overlapping only a certain part, and some passengers have completely different tickets. We plan to use this to create a derived variable called ticket.size.\nFare: 0 to 512, with 1 missing value. I care that the 3rd quartile is 31.275 and the max is 512.\nCabin: It has the most (1014) missing values among a total of 12 features. It’s a variable that represents the ship’s area, but if there’s no way to use it, I think it should be discarded.\nEmbarked: It consists of a total of 3 categories, S is the most, and there are 2 missing values.\n\nWhen performing a basic exploration of the data, please look at the outputs of various functions besides summary() and str() while comparing them.\n\n\n\n\n3.2 Missing values\n\nThis is the process of checking which variables have missing values mentioned above and how many of them exist.\nI’m going to check it numerically and visually at the same time using the dplyr, ggplot2, and VIM packages.\nYou don’t have to use all the code I’ve run, you can use only the parts you think you need or like as you read.\n\n\n3.2.1 VIM packages\n\n\nVIM::aggr(full, prop = FALSE, combined = TRUE, numbers = TRUE,\n          sortVars = TRUE, sortCombs = TRUE)\n\n\n\n\n\n Variables sorted by number of missings: \n    Variable Count\n    Survived   418\n         Age   263\n        Fare     1\n PassengerId     0\n      Pclass     0\n        Name     0\n         Sex     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n       Cabin     0\n    Embarked     0\n\n\n\n\n\n3.2.2 tidyverse packages\n\nIn addition to checking missing values at once using the VIM package, these are methods for checking missing values using various packages that exist in the tidyverse.\nFirst, find the proportion of missing values for each variable with dplyr.\n\n\nfull %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n\n\nThere is a way to check the proportion of missing values that exist in variables, but it can also be checked using visual data.\nPlease see the two bar plots below.\n\n\n# Calculate the missing value ratio of each feature -&gt; Data Frame property but has a structure of 1 row and 12 columns.\nmissing_values &lt;- full %&gt;%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\nmissing_values %&gt;% head\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n# Generate the missing_values obtained above as a 12X2 data frame\nmissing_values &lt;- tidyr::gather(missing_values,\n                                key = \"feature\", value = \"missing_pct\")\n\nmissing_values %&gt;% head(12)\n\n       feature  missing_pct\n1  PassengerId 0.0000000000\n2     Survived 0.3193277311\n3       Pclass 0.0000000000\n4         Name 0.0000000000\n5          Sex 0.0000000000\n6          Age 0.2009167303\n7        SibSp 0.0000000000\n8        Parch 0.0000000000\n9       Ticket 0.0000000000\n10        Fare 0.0007639419\n11       Cabin 0.0000000000\n12    Embarked 0.0000000000\n\n# Visualization with missing_values\nmissing_values %&gt;% \n  # Aesthetic setting : missing_pct 내림차순으로 정렬  \n  ggplot(aes(x = reorder(feature, missing_pct), y = missing_pct)) +\n  # Bar plot \n  geom_bar(stat = \"identity\", fill = \"red\") +\n  # Title generation \n  ggtitle(\"Rate of missing values in each features\") +\n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = \"Feature names\", y = \"Rate\") + \n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nIf you look at the bar graph above, you can check the percentage of missing values for all features.\nHowever, what we are actually curious about is which variables have missing values and how many missing values exist in them.\nTherefore, after calculating the proportion of missing values using the purrr package, I extracted only the variables that had at least one and visualized them.\n\n\n# 변수별 결측치 비율 계산\nmiss_pct &lt;- purrr::map_dbl(full, function(x){round((sum(is.na(x))/length(x)) * 100, 1) })\n\n# 결측치 비율이 0%보다 큰 변수들만 선택\nmiss_pct &lt;- miss_pct[miss_pct &gt; 0]\n\n# Data Frame 생성 \ndata.frame(miss = miss_pct, var = names(miss_pct), row.names = NULL) %&gt;%\n  # Aesthetic setting : miss 내림차순으로 정렬 \n  ggplot(aes(x = reorder(var, miss), y = miss)) + \n  # Bar plot \n  geom_bar(stat = 'identity', fill = 'red') +\n  # Plot title setting \n  ggtitle(\"Rate of missing values\") + \n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = 'Feature names', y = 'Rate of missing values') +\n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nThrough this, only 4 variables out of a total of 12 variables have missing values (except Survived because it is due to test data), and there are many missing values in the order of Cabin, Age, Embarked, and Fare.\n\n\n\nNow, it is the process of analyzing and exploring feature through visualization.\n\n\n\n3.3 Age\n\n\nage.p1 &lt;- full %&gt;% \n  ggplot(aes(Age)) + \n  geom_histogram(breaks = seq(0, 80, by = 1), # interval setting \n                 col    = \"red\",              # bar border color\n                 fill   = \"green\",            # bar inner color\n                 alpha  = .5) +               # Bar Transparency = 50%\n  \n  # Plot title\n  ggtitle(\"All Titanic passengers age hitogram\") +\n  theme(plot.title = element_text(face = \"bold\",     \n                                  hjust = 0.5,      # Horizon (horizontal ratio) = 0.5\n                                  size = 15, color = \"darkblue\"))\n\nage.p2 &lt;- full %&gt;% \n# Exclude values where Survived == NA in the test dataset  \n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Age, fill = Survived)) + \n  geom_density(alpha = .5) +\n  ggtitle(\"Titanic passengers age density plot\") + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5,\n                                  size = 15, color = \"darkblue\"))\n\n# Display the two graphs created above on one screen\nggarrange(age.p1, age.p2, ncol=2)\n\n\n\n\n\n\n\n3.4 Pclass\n\nLet’s visualize the frequency of passengers for each Pclass.\nAfter grouping (grouping) by Pclass using dplyr package, Data Frame representing frequency by category was created and visualized with ggplot.\n\n\nfull %&gt;% \n  # Get Pclass frequencies using dplyr::group_by(), summarize()\n  group_by(Pclass) %&gt;% \n  summarize(N = n()) %&gt;% \n  # Aesthetic setting \n  ggplot(aes(Pclass, N)) +\n  geom_col() +\n  geom_text(aes(label = N),       \n            size = 5,             \n            vjust = 1.2,           \n            color = \"#FFFFFF\") +  \n  # Plot title \n  ggtitle(\"Number of each Pclass's passengers\") + \n  # Title setting \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15)) +\n  # x, y axis name change  \n  labs(x = \"Pclass\", y = \"Count\")\n\n\n\n\n\nIt can be seen that the largest number of passengers boarded in the 3-class cabin.\n\n\n\n3.5 Fare\n\nThis is a visualization of the ‘Fare’ variable, which represents the amount paid by the passenger.\nTwo histograms and boxplots were used.\n\n# Histogram \nFare.p1 &lt;- full %&gt;%\n  ggplot(aes(Fare)) + \n  geom_histogram(col    = \"yellow\",\n                 fill   = \"blue\", \n                 alpha  = .5) +\n  ggtitle(\"Histogram of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\n# Boxplot \nFare.p2 &lt;- full %&gt;%\n  filter(!is.na(Survived)) %&gt;% \n  ggplot(aes(Survived, Fare)) + \n  # Observations are drawn as gray dots, but overlapping areas are spread out.  \n  geom_jitter(col = \"gray\") + \n  # Boxplot: 50% transparency\n  geom_boxplot(alpha = .5) + \n  ggtitle(\"Boxplot of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\nggarrange(Fare.p1, Fare.p2, ncol=2)\n\n\n\n\n\nYou can see that the survivors have a higher ‘Fare’ than the deceased passengers, but not by much.\n\n\n\n3.6 Sex\nAre there differences in survival rates between men and women? See the plot below.\n\n\nsex.p1 &lt;- full %&gt;% \n  dplyr::group_by(Sex) %&gt;% \n  summarize(N = n()) %&gt;% \n  ggplot(aes(Sex, N)) +\n  geom_col() +\n  geom_text(aes(label = N), size = 5, vjust = 1.2, color = \"#FFFFFF\") + \n  ggtitle(\"Bar plot of Sex\") +\n  labs(x = \"Sex\", y = \"Count\")\n  \nsex.p2 &lt;- full[1:891, ] %&gt;% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  ggtitle(\"Survival Rate by Sex\") + \n  labs(x = \"Sex\", y = \"Rate\")\n\nggarrange(sex.p1, sex.p2, ncol = 2)\n\n\n\nmosaicplot(Survived ~ Sex,\n           data = full[1:891, ], col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\nIf you interpret the graph, you can see that the survival rate is higher for female passengers, while there are far more males than females."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#feature-engineering-data-pre-processing-1",
    "href": "teaching/ds101/weekly/posts/11_week.html#feature-engineering-data-pre-processing-1",
    "title": "No Class",
    "section": "4. Feature engineering & Data Pre-processing",
    "text": "4. Feature engineering & Data Pre-processing\n\nThis is the process of filling in missing values (‘NA’) based on the contents of ‘Chapter 3 EDA’ and creating derived variables at the same time.\n\n\n4.1 Age -&gt; Age.Group\n\n\nfull &lt;- full %&gt;%\n# The missing value (NA) is filled in first, and the average of the values excluding the missing value is filled.\n  mutate(Age = ifelse(is.na(Age), mean(full$Age, na.rm = TRUE), Age),\n# Create a categorical derived variable Age.Group based on Age values\n        Age.Group = case_when(Age &lt; 13             ~ \"Age.0012\",\n                               Age &gt;= 13 & Age &lt; 18 ~ \"Age.1317\",\n                               Age &gt;= 18 & Age &lt; 60 ~ \"Age.1859\",\n                               Age &gt;= 60            ~ \"Age.60inf\"),\n# Convert Chr attribute to Factor\n        Age.Group = factor(Age.Group))\n\n\n\n\n4.3 SibSp & Parch -&gt; FamilySized\n\n\nfull &lt;- full %&gt;% \n # First create a derived variable called FamilySize by adding SibSp, Parch and 1 (self)\n  mutate(FamilySize = .$SibSp + .$Parch + 1,\n        # Create a categorical derived variable FamilySized according to the value of FamilySize\n         FamilySized = dplyr::case_when(FamilySize == 1 ~ \"Single\",\n                                        FamilySize &gt;= 2 & FamilySize &lt; 5 ~ \"Small\",\n                                        FamilySize &gt;= 5 ~ \"Big\"),\n        # Convert the Chr property FamilySized to a factor\n        # Assign new levels according to the size of the group size\n         FamilySized = factor(FamilySized, levels = c(\"Single\", \"Small\", \"Big\")))\n\n\nCeated FamilySized using SibSp and Parch.\nReducing these two variables to one has the advantage of simplifying the model.\nA similar use case is to combine height and weight into a BMI index.\n\n\n\n4.4 Name & Sex -&gt; title\n\nWhen looking at the results of ‘Chapter 3.6 Sex’, it was confirmed that the survival rate of women was higher than that of men.\nTherefore, in Name, “Wouldn’t it be useful to extract only names related to gender and categorize them?” I think it is.\nFirst, extract only the column vector named Name from full data and save it as title.\n\n\n# First, extract only the Name column vector and store it in the title vector\ntitle &lt;- full$Name\ntitle %&gt;% head(20)\n\n [1] Braund, Mr. Owen Harris                                \n [2] Cumings, Mrs. John Bradley (Florence Briggs Thayer)    \n [3] Heikkinen, Miss. Laina                                 \n [4] Futrelle, Mrs. Jacques Heath (Lily May Peel)           \n [5] Allen, Mr. William Henry                               \n [6] Moran, Mr. James                                       \n [7] McCarthy, Mr. Timothy J                                \n [8] Palsson, Master. Gosta Leonard                         \n [9] Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      \n[10] Nasser, Mrs. Nicholas (Adele Achem)                    \n[11] Sandstrom, Miss. Marguerite Rut                        \n[12] Bonnell, Miss. Elizabeth                               \n[13] Saundercock, Mr. William Henry                         \n[14] Andersson, Mr. Anders Johan                            \n[15] Vestrom, Miss. Hulda Amanda Adolfina                   \n[16] Hewlett, Mrs. (Mary D Kingcome)                        \n[17] Rice, Master. Eugene                                   \n[18] Williams, Mr. Charles Eugene                           \n[19] Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\n[20] Masselmani, Mrs. Fatima                                \n1307 Levels: Abbing, Mr. Anthony ... Zimmerman, Mr. Leo\n\n# Using regular expression and gsub(), extract only names that are highly related to gender and save them as title vectors\ntitle &lt;- gsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title)\ntitle %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n# Save the title vector saved above to full again, but save it as a title derived variable\nfull$title &lt;- title\n\nfull$title %&gt;% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n\n\ngsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title) In short, this code uses gsub to search for and replace a pattern in the title string, where the pattern is defined by the regular expression \"^.*, (.*?)\\\\..*$\" and the replacement is defined by the string \"\\\\1\". If you want to understand more about regular expression. Please see my blog post: What are Regular Expressions and How to Use Them in R\nThen check what are the Unique titles.\n\n\nunique(full$title)\n\n [1] \"Mr\"           \"Mrs\"          \"Miss\"         \"Master\"       \"Don\"         \n [6] \"Rev\"          \"Dr\"           \"Mme\"          \"Ms\"           \"Major\"       \n[11] \"Lady\"         \"Sir\"          \"Mlle\"         \"Col\"          \"Capt\"        \n[16] \"the Countess\" \"Jonkheer\"     \"Dona\"        \n\n\n\nYou can see that there are 18 categories in total.\nIf you use this derived variable called ‘title’ as it is, the complexity of the model (especially the tree based model) increases considerably, so you need to reduce the category.\nBefore that, let’s check the frequency and rate for each category using the descr package.\n\n\n# Check frequency, ratio by category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|         Capt |          Col |          Don |         Dona |           Dr |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            4 |            1 |            1 |            8 |\n|        0.001 |        0.003 |        0.001 |        0.001 |        0.006 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|     Jonkheer |         Lady |        Major |       Master |         Miss |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            1 |            2 |           61 |          260 |\n|        0.001 |        0.001 |        0.002 |        0.047 |        0.199 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|         Mlle |          Mme |           Mr |          Mrs |           Ms |\n|--------------|--------------|--------------|--------------|--------------|\n|            2 |            1 |          757 |          197 |            2 |\n|        0.002 |        0.001 |        0.578 |        0.150 |        0.002 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|          Rev |          Sir | the Countess |\n|--------------|--------------|--------------|\n|            8 |            1 |            1 |\n|        0.006 |        0.001 |        0.001 |\n|--------------|--------------|--------------|\n\n\n\nThe frequencies and proportions of the 18 categories are very different.\nSo let’s narrow these down to a total of five categories.\n\n\n# Simplify into 5 categories\nfull &lt;- full %&gt;%\n# If you use \"==\" instead of \"%in%\", it won't work as you want because of Recyling Rule.\n  mutate(title = ifelse(title %in% c(\"Mlle\", \"Ms\", \"Lady\", \"Dona\"), \"Miss\", title),\n         title = ifelse(title == \"Mme\", \"Mrs\", title),\n         title = ifelse(title %in% c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\", \"Don\",\n                                     \"Sir\", \"the Countess\", \"Jonkheer\"), \"Officer\", title),\n         title = factor(title))\n\n# After creating the derived variable, check the frequency and ratio for each category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|  Master |    Miss |      Mr |     Mrs | Officer |\n|---------|---------|---------|---------|---------|\n|      61 |     266 |     757 |     198 |      27 |\n|   0.047 |   0.203 |   0.578 |   0.151 |   0.021 |\n|---------|---------|---------|---------|---------|\n\n\n\n\n\n4.5 Ticket -&gt; ticket.size\n\nAs we saw in Chapter 3.1.3 Summary(), the number of passengers (train and test together) is 1309. However, all passengers’ tickets are not different.\nSee the results of summary() and unique() below.\n\n\n# We used length() to get only the number of unique categories.\nlength(unique(full$Ticket))\n\n[1] 929\n\n# Printing all of them was too messy, so only 10 were printed.\nhead(summary(full$Ticket), 10)\n\n    CA. 2343         1601      CA 2144      3101295       347077       347082 \n          11            8            8            7            7            7 \n    PC 17608 S.O.C. 14879       113781        19950 \n           7            7            6            6 \n\n\n\nWhy are there 929 unique tickets when there are no missing values in feature?\nEven the ticket is CA. There are 11 exactly the same number of people as 2343.\nLet’s see who the passengers are.\n\n\nfull %&gt;% \n# Filter only 11 passengers with matching tickets\n  filter(Ticket == \"CA. 2343\") %&gt;% \n  # We don't need to check for all variables, so we only want to look at the variables below.\n  select(Pclass, Name, Age, FamilySized)\n\n   Pclass                              Name      Age FamilySized\n1       3        Sage, Master. Thomas Henry 29.88114         Big\n2       3      Sage, Miss. Constance Gladys 29.88114         Big\n3       3               Sage, Mr. Frederick 29.88114         Big\n4       3          Sage, Mr. George John Jr 29.88114         Big\n5       3           Sage, Miss. Stella Anna 29.88114         Big\n6       3          Sage, Mr. Douglas Bullen 29.88114         Big\n7       3 Sage, Miss. Dorothy Edith \"Dolly\" 29.88114         Big\n8       3                   Sage, Miss. Ada 29.88114         Big\n9       3             Sage, Mr. John George 29.88114         Big\n10      3       Sage, Master. William Henry 14.50000         Big\n11      3    Sage, Mrs. John (Annie Bullen) 29.88114         Big\n\n\n\nYou can see that the 11 passengers above are all from the same family, brothers.\nWhile there are passengers whose tickets are exactly the same, there are also passengers whose tickets are partially matched.\nCreate a ticket.unique derived variable that represents the number of unique numbers (number of characters) of such a ticket.\nLet’s create a derived variable ticket.size with 3 categories based on ticket.unique.\n\n\n# First of all, ticket.unique is saved as all 0\nticket.unique &lt;- rep(0, nrow(full))\n\n# Extract only the unique ones from ticket features and store them in the tickets vector\ntickets &lt;- unique(full$Ticket)\n\n# After extracting only passengers with the same ticket by using overlapping loops, extract and store the length (number of characters) of each ticket.\n\nfor (i in 1:length(tickets)) {\n  current.ticket &lt;- tickets[i]\n  party.indexes &lt;- which(full$Ticket == current.ticket)\n    # For loop 중첩 \n    for (k in 1:length(party.indexes)) {\n    ticket.unique[party.indexes[k]] &lt;- length(party.indexes)\n    }\n  }\n\n# Save ticket.unique calculated above as a derived variable\nfull$ticket.unique &lt;- ticket.unique\n\n# Create ticket.size variable by dividing it into three categories according to ticket.unique\n\nfull &lt;- full %&gt;% \n  mutate(ticket.size = case_when(ticket.unique == 1 ~ 'Single',\n                                 ticket.unique &lt; 5 & ticket.unique &gt;= 2 ~ \"Small\",\n                                 ticket.unique &gt;= 5 ~ \"Big\"),\n         ticket.size = factor(ticket.size,\n                              levels = c(\"Single\", \"Small\", \"Big\")))\n\n\n\n\n4.6 Embarked\n\nThis is feature with two missing values (NA). In the case of Embarked, replace it with S, which is the most frequent value among the three categories.\n\n\nfull$Embarked &lt;- replace(full$Embarked,               # Specify Data$feature to replace\n                         which(is.na(full$Embarked)), # Find only missing values\n                         'S')                        # specify the value to replace\n\n\n\n\n4.7 Fare\n\nFor Fare, there was only one missing value.\nBased on the histogram seen above (Chapter 3.5 Fare), missing values are replaced with 0.\n\n\nfull$Fare &lt;- replace(full$Fare, which(is.na(full$Fare)), 0)\n\n\nAt this point, data preprocessing is complete.\nThe following is the process of selecting the variables to be used for model creation while exploring the derived variables created so far.\nIn other words, Feature selection."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#relationship-to-target-feature-survived-feature-selection-1",
    "href": "teaching/ds101/weekly/posts/11_week.html#relationship-to-target-feature-survived-feature-selection-1",
    "title": "No Class",
    "section": "5. Relationship to target feature Survived & Feature selection",
    "text": "5. Relationship to target feature Survived & Feature selection\n\nPrior to full-scale visualization, since the purpose here is to see how well each variable correlates with the survival rate, we did not use the entire full data, but only the train data set that can determine survival and death.\nAlso, please note that the plot used above may be duplicated as it is.\n\n\n5.0 Data set split\nFirst, use the code below to split preprocessed full data into train and test.\n\n# Before feature selection, select all variables first.\n\ntrain &lt;- full[1:891, ]\n\ntest &lt;- full[892:1309, ]\n\n\n\n\n5.1 Pclass\n\n\ntrain %&gt;% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(position = \"fill\") +\n# Set plot theme: Converts to a more vivid color.\n  scale_fill_brewer(palette = \"Set1\") +\n  # Y axis setting \n  scale_y_continuous(labels = percent) +\n# Set x, y axis names and plot main title, sub title\n  labs(x = \"Pclass\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Pclass?\")\n\n\n\n\n\n\n\n5.2 Sex\n\nSame as Chapter 3.6 Sex.\n\n\nmosaicplot(Survived ~ Sex,\n           data = train, col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\n\n\n5.3 Embarked\n\n\ntrain %&gt;% \n  ggplot(aes(Embarked, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Embarked\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Embarked?\")\n\n\n\n\n\n\n\n5.4 FamilySized\n\n\ntrain %&gt;% \n  ggplot(aes(FamilySized, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"FamilySized\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by FamilySized\")\n\n\n\n\n\nIt can be seen that there is a difference in survival rate depending on the number of people on board, and that ‘FamilySized’ and ‘Survived’ have a non-linear relationship.\n\n\n5.5 Age.Group\n\n\ntrain %&gt;% \n  ggplot(aes(Age.Group, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"Age group\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by Age group\")\n\n\n\n\n\n\n\n5.6 title\n\n\ntrain %&gt;% \n  ggplot(aes(title, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"title\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by passengers title\")\n\n\n\n\n\n\n\n5.7 ticket.size\n\n\ntrain %&gt;% \n  ggplot(aes(ticket.size, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"ticket.size\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by ticket.size\")\n\n\n\n\n\n\n\n5.8 Description of actual used features\n\nNow that all the derived variables created so far have been found to be useful, select and save only the variables you will actually use.\nThe table below is a brief description of the actual selected variables.\n\n\n\n\n\n\n\n\nvariable name\nType\nDescription\n\n\n\n\nSurvived\nfactor\nTarget feature, survival == 1, death == 0\n\n\nSex\nfactor\ngender, male or female\n\n\nPclass\nfactor\nCabin Class, First Class (1), Second Class (2), Third Class (3)\n\n\nEmbarked\nfactor\nPort of embarkation, Southampton (S), Cherbourg (C), Queenstown (Q)\n\n\nFamilySized\nfactor\nFamily size, a derived variable created using SibSp and Parch, with 3 categories\n\n\nAge.Group\nfactor\nAge group, a derived variable created using Age, with 4 categories\n\n\ntitle\nfactor\nA part of the name, a derived variable made using Name, and 5 categories\n\n\nticket.size\nfactor\nThe length of the unique part of the ticket, a derived variable created using ticket, with 3 categories\n\n\n\n\n\n# Excluding ID number, select and save 7 input variables and 1 target variable to actually use\n\ntrain &lt;- train %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\", \"Survived\")\n\n# For Submit, extract the Id column vector and store it in ID\n\nID &lt;- test$PassengerId\n\n# Select and save the remaining 6 variables except for Id and Survived\n\ntest &lt;- test %&gt;% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\")"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/11_week.html#machine-learning-model-generation-1",
    "href": "teaching/ds101/weekly/posts/11_week.html#machine-learning-model-generation-1",
    "title": "No Class",
    "section": "6. Machine learning model generation",
    "text": "6. Machine learning model generation\n\nNow is the time to create a machine learning model using the train data set.\nOriginally, it is correct to create train, validation, test data sets first, create various models, and then select the final model through cross validation (CV, Cross Validation), but these processes are omitted here and RandomForest After creating only, we will predict (estimate) the test data and even create data to Submit to competition.\n\n\n6.1 Random Forest model generation\n\n\n# Set the seed number for reproducibility.\nset.seed(1901)\n\ntitanic.rf &lt;- randomForest(Survived ~ ., data = train, importance = T, ntree = 2000)\n\n\n\n\n6.2 Feature importance check\n\n\nimportance(titanic.rf)\n\n                    0        1 MeanDecreaseAccuracy MeanDecreaseGini\nPclass      47.442449 53.94070             64.73724        36.807804\nSex         54.250630 37.30378             58.66109        57.223102\nEmbarked    -6.328112 38.10930             27.32587         9.632958\nFamilySized 32.430898 31.24383             50.13349        18.086894\nAge.Group   15.203313 26.72696             29.36321        10.201187\ntitle       48.228450 41.60124             57.02653        73.146999\nticket.size 39.544367 37.80849             59.59915        22.570142\n\nvarImpPlot(titanic.rf)\n\n\n\n\n\n\n6.3 Predict test data and create submit data\n\n\n# Prediction \n\npred.rf &lt;- predict(object = titanic.rf, newdata = test, type = \"class\")\n\n# Data frame generation\nsubmit &lt;- data.frame(PassengerID = ID, Survived = pred.rf)\n\n# Write the submit data frame to file : csv is created in the folder designated by setwd().\n\n# write.csv(submit, file = './titanic_submit.csv', row.names = F)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#course-description",
    "href": "teaching/ds101/weekly/posts/01_week.html#course-description",
    "title": "Course Intro",
    "section": "Course description",
    "text": "Course description\n\nCourse description"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#about-lecturer",
    "href": "teaching/ds101/weekly/posts/01_week.html#about-lecturer",
    "title": "Course Intro",
    "section": "About Lecturer",
    "text": "About Lecturer\nChangjun LEE\n\nHome: https://changjunlee.com/\n\nAssociate Professor (Head of Culture & Tech)\nSchool of Convergence. SKKU.\n\nAs a computational social scientist, I bring a unique interdisciplinary perspective to the fields of economics, innovation studies, and convergence technologies. With a background in natural sciences, including a bachelor’s degree in biology and chemistry, I went on to earn a Ph.D. in technology management, economics, and policy. My research focuses on utilizing computational methods to tackle a wide range of social phenomena, including technology evolution & regional growth, knowledge management, and technology & convergence innovation. I am passionate about using technology and data to drive innovation and solve real-world problems.\n\n\nResearch interest: Media & Innovation, Immersive media and users’ behavior, Technology management, Data Science\nTeaching: Culture & Technology | Data Science in CNT\nThings I love\n\nResearch\nChat & Coffee & MBTI\nTravel\nPlay (Things)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#weekly-design",
    "href": "teaching/ds101/weekly/posts/01_week.html#weekly-design",
    "title": "Course Intro",
    "section": "Weekly Design",
    "text": "Weekly Design\n\nWeekly design"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#data-science-r",
    "href": "teaching/ds101/weekly/posts/01_week.html#data-science-r",
    "title": "Course Intro",
    "section": "Data Science & R",
    "text": "Data Science & R\n\nData Science is a modern academic area helping people to draw useful information and intuition so that making them to make reasonable decisions\n\n\nWhy Data Science is So Important to Learn?\nIn the age of information, data science has emerged as a pivotal skill set across industries, transforming the way decisions are made and offering insights that were previously inaccessible. Its importance stems from the ability to analyze and derive meaningful insights from data, which can drive strategic business decisions, lead to technological innovations, and solve complex problems. Here’s why learning data science is crucial, even for those not pursuing a career in computer science.\n\nData-Driven Decision Making\nOne of the foremost reasons for learning data science is its role in data-driven decision making. With an increasing amount of data generated every day, the ability to sift through, analyze, and interpret this data can be the difference between staying ahead or falling behind in any industry. Data science skills enable individuals and organizations to make informed decisions based on empirical evidence rather than intuition or speculation.\n\n\nCross-Disciplinary Application\nData science is not confined to the tech industry. It finds applications in healthcare, finance, retail, education, and more. For instance, in healthcare, data science can predict disease outbreaks, in finance, it can assess risk and in retail, it can help understand consumer behavior. This cross-disciplinary nature of data science means that learning it can open up opportunities in virtually any field, making it a valuable skill set for all majors.\n\n\nEnhancing Problem-Solving Skills\nLearning data science enhances critical thinking and problem-solving skills. It involves identifying patterns, making predictions, and solving complex problems using a data-driven approach. These skills are transferable and valuable in any career path, making data science learning beneficial beyond the technical aspects.\n\n\nFuture Job Market\nThe demand for data science skills is growing rapidly. According to the U.S. Bureau of Labor Statistics, the job market for data science and analytical roles is projected to grow much faster than the average for all occupations. Learning data science can thus provide a competitive edge in the job market, even for those in non-computer science fields.\n\n\nEmpowering Innovation\nData science is at the heart of innovation today. From developing new products and services to improving existing processes, the insights derived from data science can lead to significant advancements and efficiencies. By understanding data science, individuals in non-computer science fields can contribute to innovation within their industries.\n\n\nFor non-computer science majors: You are the one who has to learn this.\nThe importance of data science transcends traditional boundaries of computer science and technology. It is a critical skill for the future, enabling individuals to navigate and excel in a data-driven world. For non-computer science majors, learning data science not only enhances employability and problem-solving skills but also opens up new avenues for innovation and impact in their respective fields. In essence, data science is a universal language of the future, and learning it is key to unlocking potential across the spectrum of professional endeavors.\n\n\n\n\nWhy is it better for a non-major (#muggle) to become a data scientist (#wizard)?\n\n\n\n\n\nNon-CS major being a data-scientist has an advantage\n\n\n\nActually, non-major groups can do even better!\n\n\n\nAgain, Data Never Sleeps\n\n\nhttps://www.domo.com/data-never-sleeps\n\n\n\nSo Data Scientist is..\n\n\nRequirements\n\n\nDomain Expertise is getting more important as the world is getting more complicated\nThis course will cover coding part and a little bit of statistical skills\nDon’t worry! Practice makes perfect.\n\n\n\n\n\nWhy R is the Best Language for Non-Computer Majors to Learn Data Science\nIn the realm of data science, the choice of programming language is pivotal. For non-computer majors venturing into this field, R stands out as a particularly accessible and powerful tool. Here’s why R is often considered the best language for those new to data science.\n\n1. Designed for Statistics and Data Analysis\nR was specifically created for statistical analysis and data visualization. Unlike general-purpose programming languages that can be used for data science, R’s syntax and functions are tailored to statistical analysis, making it more intuitive for analyzing data. For students and professionals without a computer science background, this focus makes R an excellent entry point into data science.\n\n\n2. Comprehensive Libraries and Packages\nR boasts a vast ecosystem of packages designed for data science tasks, including dplyr for data manipulation, ggplot2 for data visualization, and caret for machine learning. These packages simplify complex tasks into more manageable functions, allowing users to perform sophisticated data analysis with relatively simple commands. This extensive library support means that non-computer majors can accomplish more with less coding expertise.\n\n\n3. Strong Community and Support\nThe R community is known for its inclusivity and support, especially for beginners. Numerous online forums, such as R-bloggers and Stack Overflow, provide a platform for learners to seek help, exchange ideas, and stay updated on the latest developments in R. This community support is invaluable for non-computer majors who may require guidance as they navigate their data science journey.\n\n\n4. Free and Open Source\nR is a free, open-source software, making it accessible to everyone without the need for expensive licenses or subscriptions. This democratizes the learning process, allowing individuals from diverse backgrounds to explore data science without financial barriers. Moreover, being open-source encourages users to contribute to the development of R, further enriching its capabilities and resources.\n\n\n5. Versatile Data Visualization Capabilities\nVisual data representation is crucial in data science for understanding complex datasets and communicating findings effectively. R’s superior data visualization capabilities, particularly through the ggplot2 package, allow users to create high-quality, publication-ready graphs and plots. This is particularly beneficial for non-computer majors, who might rely more heavily on visual representations to understand and present data insights.\n\n\n6. Industry Adoption and Academic Support\nR is widely adopted in both academia and industry for research and data analysis. This widespread use means that learning R can open up opportunities in various fields, including biostatistics, epidemiology, economics, and social sciences. For non-computer majors, the ability to apply their data science skills directly to their field of study or industry is a significant advantage.\n\nFor non-computer majors looking to delve into data science, R offers a unique blend of accessibility, specialized functionality, and community support. Its design for statistical analysis, along with a rich ecosystem of packages and resources, makes R an ideal starting point for those new to programming and data analysis. By learning R, non-computer majors can not only gain valuable data science skills but also apply these skills directly to their fields of interest, enhancing their research capabilities and career prospects.\n\n\n\n\n\nHow About Python?\nA good quesiton! R and Python are both powerful and popular programming languages in the data science community, each with its unique strengths. When comparing R to Python, especially from the perspective of those with non-computer science backgrounds or specific analytical needs, several aspects of R stand out, highlighting its strengths:\n\n\n1. Specialization in Statistical Analysis\n\nR’s Foundation: R is specifically designed for statistical analysis and graphical models. It was developed by statisticians for statisticians. This specialization gives it an edge in handling complex statistical data analyses out of the box, without the need for extensive programming knowledge or additional libraries.\nComprehensive Statistical Packages: R has a comprehensive collection of packages for various statistical analyses, including linear and nonlinear modeling, classical statistical tests, time-series analysis, classification, clustering, and more. These packages are often written by statisticians and subject matter experts, ensuring they are tailored for rigorous statistical analysis.\n\n\n\n2. Superior Data Visualization\n\nggplot2 and Beyond: R’s ggplot2 package is renowned for its capability to create advanced and beautiful data visualizations with ease. The grammar of graphics implemented by ggplot2 allows for the layering of data, aesthetic mappings, and statistical transformations, facilitating the creation of complex plots intuitively.\nRich Ecosystem for Visualization: Beyond ggplot2, R offers a rich ecosystem of visualization packages like lattice, plotly (for interactive plots), and shiny (for interactive web apps), providing a wide range of options for presenting data insights effectively.\n\n\n\n3. Integrated Development Environment (IDE) Support\n\nRStudio: RStudio is a powerful and popular IDE specifically designed for R. It provides an integrated environment that makes data analysis, visualization, and programming more accessible. RStudio enhances the R programming experience with features like code completion, easy package management, and markdown (quarto, nowadays) support for reproducible research.\n\n\n\n4. Community and Support\n\nDedicated Community: R has a vibrant and welcoming community, particularly attractive to those in academia, research, and various scientific disciplines. The community offers extensive resources, forums, and groups for support, making it easier for newcomers to get started and for experts to dive deep into specific statistical challenges.\nCRAN Repository: The Comprehensive R Archive Network (CRAN) is a repository of over 16,000 R packages (as of my last update in April 2023). This vast and well-organized repository ensures that R users have access to tools and libraries for nearly every statistical and graphical method imaginable.\n\n\n\n5. Data Handling and Analysis\n\nData Wrangling: While Python’s pandas library is powerful for data manipulation, R’s dplyr and data.table packages offer syntax that is arguably more intuitive for data transformation and aggregation, especially for those with a background in SQL or those new to programming.\nAnalysis Workflow: R is designed with an analysis-first approach, emphasizing data exploration and analysis workflows. This makes R particularly suited for iterative data exploration, hypothesis testing, and modeling in a way that is highly accessible to researchers and analysts.\nR is intuitive for analysis: R may not work with a wide variety of projects, but it is the best choice for analysis and inference work. If you plan to work in a specialized field, you’ll want a specialized programming language. R also offers a powerful environment ideally suited to the types of data visualizations data scientists employ.\n\n\n\nComparative Consideration\nWhile Python is a general-purpose programming language with broad applications ranging from web development to software engineering and has significant strengths in machine learning and deep learning with libraries like TensorFlow and PyTorch, R’s focused design for statistical analysis and data visualization makes it exceptionally well-suited for data scientists, statisticians, and researchers. This focus, combined with its comprehensive statistical packages, superior data visualization capabilities, and supportive community, positions R as a strong choice for those particularly interested in the statistical and analytical aspects of data science.\nSource: https://www.edx.org/resources/r-vs-python-for-data-science-explainer-learning-tips\n\n\n\nInstall gadgets\n\nInstall R, R Studio, & Rtools\n\nR\n\nFor window https://cran.r-project.org/bin/windows/base/\nFor mac https://cran.r-project.org/bin/macosx/\n\nR Studio\n\nhttps://posit.co/download/rstudio-desktop/\n\nRtools (only for window user)\n\nhttps://cran.r-project.org/bin/windows/Rtools/\nTo install R packages containing C/C++ language for window users (no need for Linux and Mac users)\n\n\nThings you need to know\n\nDon’t Use OneDrive.\n\nUse Github instead\nMany people get an error when installing because of OneDrive\n\nSet Windows user name to English\n\nIf Korean characters are mixed in the installation path, there is a high probability of error occurrence\n\n\nInstallation Order\n\nStep 1 ‑ Download the file\n\nDownload R, Rtools, Rstudio installation files\n\nStep 2 - Install R\n\nUnified installation path: All will be installed in the C:/R folder\nRun in administrator mode when running the R installation file\nAfter installing R, grant write permission to the R folder, Right-click and turn off read only\n\nStep 3 ‑ Install Rtools\n\nRtools? RTools is a collection of software tools and libraries that are necessary for building and compiling packages in the R programming language, especially on Windows.\nAdministrator mode execution installation and folder setting as C:\\R\\rtools40\nCreate environment variable RTOOLS40_HOME after installation: Value - C:\\R\\rtools40\\\nAdd %RTOOLS40_HOME%\\usr\\bin\\ to the Path variable.\n\nStep 4 - Install Rstudio\n\nRight-click and run as administrator - installation path C:\\R\\Rstudio\nCheck rtools connection with Sys.which(“make”) command after installation"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#course-management",
    "href": "teaching/ds101/weekly/posts/01_week.html#course-management",
    "title": "Course Intro",
    "section": "Course management",
    "text": "Course management\n\nLecturer: Changjun Lee (Associate Professor in SKKU School of Convergence)\n\nchangjunlee@skku.edu\n\nTA: Ye Seo Lim (Master Student, SKKU Immersive Media Engineering)\n\nivisy6952@g.skku.edu\n\nTime\n\n(1h) Flipped learning content\n(2h) Wed 09:00 ~ 10:50\n\nLocation: International Hall High-Tech e+ Lecture Room (9B312)\n\n\n\nClass consists of Pre-class, Class, and PBL project\n\n\nPre-class\n\nStudents will be required to watch the lecturer’s recorded lecture (or other given videos) before the off-line (or online streaming ZOOM) class and learn themselves\nVideo is about the concept of the data science and the programming language\n(Sometimes) Students are required to submit Discussions to check the level of their understanding\n\nClass\n\nLecturer summarize the pre-class lecture and explain more details\n\nAsk students about the pre-class content to check whether they learned themselves\nOK to answer incorrectly, but if you cannot answer at all, it will be reflected in your pre-class discussion score.\n\nStudents will practice with the advanced code\nA Quiz will be in the class to check the level of understanding\n\nPBL project\n\nStudents organize teams that meet several conditions.\n\n4~5 members in a team\nBackground diversity: no homogeneous majors in a team\nException: Allowed if persuasion is possible for sufficient reasons\n\nData will be given. Teams are going to choose the data they want to explore considering their interest\nTeams can offer a zoom meeting with lecturer if they need\n\n\n\n\nFinal outputs (An example not limited)\n\nData Preparing (or Collecting)\nExplore data (Descriptive stats)\nSet your hypothesis (or research questions)\nVisualize data to confirm your hypo or RQs\nExplain your findings\nExpanding your findings to implications\n\n\n\n\nTextbooks for the course\n\nR4DS: R for Data Science (written by Hadley Wickham and Garrett Grolemund)\n\nis an excellent resource for learning data science using R, covering data manipulation, visualization, and modeling with R. The book is available as a free online resource.\n\nRC2E: R Cookbook (written by JD Long and Paul Teetor)\n\nis a comprehensive resource for data scientists, statisticians, and programmers who want to explore the capabilities of R programming for data analysis and visualization.\n\nRGC: R Graphic Cookbook (written by Winston Chang)\n\nis a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems\n\nMDR: Statistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\nis a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language.\n\nISR: Introductory Statistics with R (written by Peter Dalgaard)\n\nis a great resource for learning basic statistics with a focus on R programming. This book covers a wide range of statistical concepts, from descriptive statistic"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#score",
    "href": "teaching/ds101/weekly/posts/01_week.html#score",
    "title": "Course Intro",
    "section": "Score",
    "text": "Score\n\nAttendance & Participation (10 %)\nPreclass Discussion Submission (10 %)\nQZ (40 %)\nProject (40 %)\n\nRubric Grade Table for the Final Output (40)\n\nPeer Review Weight (0.6 ~ 1)\n\nIndividual Score(40) = Team Score(40) * Peer Review Weight(0.6-1)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#communication",
    "href": "teaching/ds101/weekly/posts/01_week.html#communication",
    "title": "Course Intro",
    "section": "Communication",
    "text": "Communication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gNpAFOcg\nWhen you enter, please make sure to enter your name as it is on the attendance sheet. (입장하셔서 이름을 꼭 출석부에 있는 이름으로 설정해주세요.)\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/01_week.html#notice",
    "href": "teaching/ds101/weekly/posts/01_week.html#notice",
    "title": "Course Intro",
    "section": "Notice",
    "text": "Notice\n\nWatch and learn yourself with the pre-class content (Week 2)\nEnsure that your laptop has R and RStudio installed and ready to use."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#preview",
    "href": "teaching/ds101/weekly/posts/02_week.html#preview",
    "title": "Basic Syntax (1)",
    "section": "Preview",
    "text": "Preview\n\n\nThe best way to learn data science: practice with examples\n\nSwim: Jump into the water and play around…\nCarpentry: Cut woods, drill, and practice\nData science: with data, visualize, analyze..\n\nData has a thousand faces\n\nNumeric like distance or weight\nFactors like blood type\nCharacters like address or name"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#check-if-you-install-r-r-studio",
    "href": "teaching/ds101/weekly/posts/02_week.html#check-if-you-install-r-r-studio",
    "title": "Basic Syntax (1)",
    "section": "Check if you install R & R Studio",
    "text": "Check if you install R & R Studio\n\n\nBase R\n\nThe Data given in base R\n\nCan be checked by data() command\nex) ChickWeight data “Weight versus age of chicks on different diets”, women data “Average heights and weights for American women aged 30-39”\n\n\n\n\n\nwomen\n\n   height weight\n1      58    115\n2      59    117\n3      60    120\n4      61    123\n5      62    126\n6      63    129\n7      64    132\n8      65    135\n9      66    139\n10     67    142\n11     68    146\n12     69    150\n13     70    154\n14     71    159\n15     72    164\n\n\n\nCar datset\n\n\nstr(cars)\n\n'data.frame':   50 obs. of  2 variables:\n $ speed: num  4 4 7 7 8 9 10 10 10 11 ...\n $ dist : num  2 10 4 22 16 10 18 26 34 17 ...\n\ncars\n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\n\nstr function: Function summarizing the contents of the data\n\nVarious visualization functions\n\nMost widely used function in base R: plot\n\nplot(women)\n\n\n\n\n\nApply different visualization options\n\nColor options (parameters) col,\nxlab and ylab to name the axis,\npch to specify the symbol shape\n\nplot(cars)\n\n\n\n\n\nplot(cars, col = 'blue')\n\n\n\n\n\nplot(cars, col = 'blue', xlab = \"speed\")\n\n\n\n\n\nplot(cars, col = 'blue', xlab = \"speed\", ylab = 'distance')\n\n\n\n\n\nplot(cars, col = 'blue', xlab = \"speed\", ylab = 'distance', pch = 18)\n\n\n\n\n\n\n\n\n\n\nGood habits in learning data science\n\n\n# ?plot\n# help(plot)\n\n\nThink incrementally (Step by Step)\n\nAfter creating the most basic features, check the behavior, add a new feature to it, and add another feature to verify it.\nOnce you’ve created everything and checked it, it’s hard to find out where the cause is later\nSee Figure above: Check the most basic plot function, add the col option to check, add the xlab and ylab options, and add the pch option to check\n\nSpecify working directory\n\nThe way to Save Data Files in a Specified Directory (Folder)\ngetwd() function displays the current working directory (the red part is the computer name)\n\ngetwd()\n\n[1] \"C:/R/Rproj/[2]web_pages/changjunlee_com_2/teaching/ds101/weekly/posts\"\n\n\nsetwd() to set the new working directory\n\nUse of library (package)\n\nLibraries are software that collects R functions developed for specific fields.\n\nE.g.) ggplot2 is a collection of functions that visualize your data neatly and consistently\nE.g.) gapminder is a collection of functions needed to utilize gapminder data, which gathers population, GDP per capita, and life expectancy in five years from 1952 to 2007.\n\nR is so powerful and popular because of its huge library\nIf you access the CRAN site, you will see that it is still being added.\n\n[Packages] menu: see all libraries provided by R [Task Views] menu: Introduce libraries field by field\n\n\nWhen using it, attach it using the library function\n\nLibrary installation saves library files to your hard disk\nLibrary Attachment loads it from Hard Disk to Main Memory\n\n\n\n\nData for example..\n\nLovely iris data\n\nIn 1936, Edger Anderson collected irises in the Gaspe Peninsula in eastern Canada.\nCollect 50 from each three species(setosa, versicolor, verginica) on the same day\nThe same person measures the width and length of the petals and sepals with the same ruler\nHas been famous since Statistician Professor Ronald Fisher published a paper with this data and is still widely used.\n\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nhead(iris, 10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\n\nplot(iris)\n\n\n\n\nSee the correlation of two properties\n\ncol = iris$Species is an option to draw colors differently by species\n\nplot(iris$Petal.Width, \n     iris$Petal.Length,\n     col = iris$Species)\n\n\n\n\n\n\n\n\nData Science Process with example data\n\n\n\n\nflowchart LR\n  A[Collecting Data] --&gt; B(EDA)\n  B --&gt; C{Modeling}\n\n\n\n\n\n\nTips data\n\nTips earning at tables in a restaurant\nCan we get more tips using data science?\n\nStep 1: Data collecting\n\nCollect values in seven variables\n\ntotal_bill\ntip\ngender\nsmoker\nday\ntime\nsize: number of people in a table\n\nAfter weeks of hard work, collected 244 and saved it to the tips.csv file\n\n\n\ntips = read.csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\nstr(tips)\n\n'data.frame':   244 obs. of  7 variables:\n $ total_bill: num  17 10.3 21 23.7 24.6 ...\n $ tip       : num  1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ...\n $ sex       : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ smoker    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ day       : chr  \"Sun\" \"Sun\" \"Sun\" \"Sun\" ...\n $ time      : chr  \"Dinner\" \"Dinner\" \"Dinner\" \"Dinner\" ...\n $ size      : int  2 3 3 2 4 4 2 4 2 2 ...\n\nhead(tips, 10)\n\n   total_bill  tip    sex smoker day   time size\n1       16.99 1.01 Female     No Sun Dinner    2\n2       10.34 1.66   Male     No Sun Dinner    3\n3       21.01 3.50   Male     No Sun Dinner    3\n4       23.68 3.31   Male     No Sun Dinner    2\n5       24.59 3.61 Female     No Sun Dinner    4\n6       25.29 4.71   Male     No Sun Dinner    4\n7        8.77 2.00   Male     No Sun Dinner    2\n8       26.88 3.12   Male     No Sun Dinner    4\n9       15.04 1.96   Male     No Sun Dinner    2\n10      14.78 3.23   Male     No Sun Dinner    2\n\n\nInterpreting the first sample, it was shown that two people had dinner on Sunday, no smokers, and a $1.01 tip at the table where a woman paid the total $16.99.\n\n\nStep 2: Exploratory Data Analysis (EDA)\n\nsummary function to check the summary statistics\nHow to explain the summary statistics below?\n\n\n\nsummary(tips)              \n\n   total_bill         tip             sex               smoker         \n Min.   : 3.07   Min.   : 1.000   Length:244         Length:244        \n 1st Qu.:13.35   1st Qu.: 2.000   Class :character   Class :character  \n Median :17.80   Median : 2.900   Mode  :character   Mode  :character  \n Mean   :19.79   Mean   : 2.998                                        \n 3rd Qu.:24.13   3rd Qu.: 3.562                                        \n Max.   :50.81   Max.   :10.000                                        \n     day                time                size     \n Length:244         Length:244         Min.   :1.00  \n Class :character   Class :character   1st Qu.:2.00  \n Mode  :character   Mode  :character   Median :2.00  \n                                       Mean   :2.57  \n                                       3rd Qu.:3.00  \n                                       Max.   :6.00  \n\n\nThis statistic summary doesn’t reveal the effect of day or gender on the tip, so let’s explore it further with visualization.\n\nAttach dplyr and ggplot2 libraries (for now just run it and study the meaning)\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n\nWhat do you see in the figures below?\n\nDistribution of fellow persons in a table\n\n\n\ntips %&gt;% ggplot(aes(size)) + geom_histogram()                                            \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nTip amount according to bill amount (total_bill)\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + geom_point()                                     \n\n\n\n\nAdded day information using color\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + \n  geom_point(aes(col = day))                       \n\n\n\n\nWomen and men separated by different symbols\n\ntips %&gt;% ggplot(aes(total_bill, tip)) + \n  geom_point(aes(col = day, pch = sex), size = 3) \n\n\n\n\n\nStep 3: Modeling\n\nLimitations of Exploratory Data Analysis: You can design a strategy to make more money, but you can’t predict exactly how much more income will come from the new strategy.\nModeling allows predictions\nCreate future financial portfolios\n\nE.g.) Know how much your income will increase as fellows in a table grow, and how much your income will change when paying people’s gender changes"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#qz",
    "href": "teaching/ds101/weekly/posts/02_week.html#qz",
    "title": "Basic Syntax (1)",
    "section": "QZ",
    "text": "QZ\n\nWhat will the following code return?\n\nMyVector &lt;- c(12, 456, 34.5, 23, 55, “34hello”)\ntypeof(MyVector)\n\n\ninteger\ndouble\ncharacter\nFALSE\n\nWhich of these functions is NOT used to create vectors?\n\nc()\ntypeof()\nseq()\nrep()\n\nCreate the vector below by using ‘seq’ function\n\n2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nCreate the vector below by using ‘rep’ function\n\n3 3 3 3 3 3 3 3 3\n\nCreate the vector below by using ‘rep’ function\n\n80 20 80 20 80 20 80 20\n\nAre these vectors possible forms in R?\n\n\n\nmountain &lt;- c(\"tree\", \"rock\", \"dirt\", \"dolphin\", \"waterfall\")\n\n\nHow would you access the word “dolphin” in this vector?\n\nc[4]\nmountain[-2]\nmountain[4]\nmountain(4)\n\nHow to extract 3rd and 5th values from the vector below?\n\nFrom x vector, How to extract vectors like below?"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/index.html",
    "href": "teaching/cul_tech_101/weekly_2/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n2\n\n\n융합 콘텐츠 기획과 제작\n\n\n강혜원\n\n\n\n\n3\n\n\n음악, K-pop, 엔터테인먼트\n\n\n정헌섭\n\n\n\n\n4\n\n\n패션과 뉴테크\n\n\n류현석\n\n\n\n\n5\n\n\n게임 & 인터랙티브 디자인\n\n\n김수완\n\n\n\n\n6\n\n\n한류와 팬덤\n\n\n이종명\n\n\n\n\n7\n\n\n데이터 시각화의 예술\n\n\n전서연\n\n\n\n\n8\n\n\n문화콘텐츠와 자연어 처리\n\n\n구영은\n\n\n\n\n9\n\n\n메타버스와 메타휴먼\n\n\n원종서\n\n\n\n\n10\n\n\n인터랙션 사이언스, UX\n\n\n이대호\n\n\n\n\n11\n\n\n서비스 디자인, 데이터 드리븐 마케팅\n\n\n설상훈\n\n\n\n\n12\n\n\n가상/증강현실 콘텐츠 제작\n\n\n김태원\n\n\n\n\n13\n\n\n융합콘텐츠와 창업\n\n\n윤영훈\n\n\n\n\n14\n\n\n엔터테인먼트 경영\n\n\n이동찬\n\n\n\n\n15\n\n\n기말시험\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/02_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/02_week.html",
    "title": "융합 콘텐츠 기획과 제작",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 월요일 20:00까지\n\n\n\n\nWeek 2: 융합 콘텐츠 기획과 제작\n\nDate: 20240312\n수업자료\nPre-class video\n\n직업탐구- 별일입니다 - 버추얼 휴먼 매니저 | EBS\n\n\n\n덕후에서 콘텐츠 크리에이터가 되는 방법 | 대도서관\n\n\n\n좋은 컨텐츠를 만드는 법 | 에이틴 제작사\n\n\n\nWhy AI Will Spark Exponential Economic Growth | Cathie Wood | TED\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/05_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/05_week.html",
    "title": "게임 & 인터랙티브 디자인",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 5: 게임 & 인터랙티브 디자인\n\nDate: 20240402\n수업자료\nPre-class video\n\n게임 과몰입, 게임 잘못일까요? | 한덕현 중앙대학교 병원 정신건강의학과 교수 | | 세바시\n\n\n\nGoogles New STUNNING AGI Breakthrough “Genie 1.0” (Bigger Than You Think)\n\n\n\nDiscussion\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/14_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/14_week.html",
    "title": "엔터테인먼트 경영",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 14: 엔터테인먼트 경영\n\nDate: 20240604\nPre-class video\n\nHow Netflix changed entertainment – and where it’s headed | Reed Hastings\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n넷플릭스가 현재의 넷플릭스가 되는 과정에서 얼마나 과감하고 혁신적인 결정들을 내려야 했는지 알게 되었다. 초등학생 시절에 잠시 미국에 살았을 때 넷플릭스에서 DVD를 빌려서 보곤 했었는데, 몇년후에 넷플릭스가 스트리밍 서비스로 변신하여 한국을 더불어 전세계에 진출해 있었다. 서비스 사용자 입장에선 단순히 영상을 보는 매체가 DVD에서 인터넷 스트리밍으로 바꼈으니 넷플릭스도 그렇게 바뀐 것이라 생각했는데, 전세계적인 비즈니스가 되기 위해선 미리 시대의 흐름을 읽고, 그 새로운 것을 시도하는 용기가 필요하다는 것을 인터뷰를 통해 알게 되었다. 현재에서 되돌아보면 당연해보이는 것들이 당시에는 전혀 없던 것들(예를 들어 Binge-watching, streaming)이었고, 누군가는 그것을 최초로 사업화했다는 것이다. 넷플릭스의 문화는 많은 자유와 책임이라고 했다. 모두가 정보를 공유하고, 일부 상사들이 결정하는 것이 아닌 각자가 자유롭게 결정하는 방식이다. 구글이나 넷플릭스와 같은 혁신적이고 전세계적인 기업들의 공통점인 것 같다. 자유에는 책임이 따른다는 말이 있듯, 자유로우면 오히려 많은 지적인 사람들의 책임감 있는 아이디어들을 더 많이 모을 수 있는 것 같다.\n\n\n\nOTT 서비스에서 오리지널 콘텐츠에 투자하는 이유와 수익 모델에 대해 더 깊게 이해할 수 있었습니다. 다른 회사에 저작권이 있는 작품을 들여와 제공하는 것은 회원을 만족시키는 데에는 도움이 되지만, 넷플릭스라는 브랜드에 대한 인상을 강력하게 만드는 데에는 큰 영향을 주지 않는다고 한 점이 인상깊었고, 이것이 OTT 브랜드에서 오리지널 콘텐츠를 늘려 가고 있는 이유가 될 것 같습니다. 회원제이기 때문에 많은 회원을 유치하는 것이 목표이고, 그렇기 때문에 어떤 작품이 얼마나 더 큰 이익을 주는가를 세세하게 분석하는 것을 많이 중요하지 않다고 여기며 다양한 분위기를 제공하는 것에 초점을 맞추는 점도 뜻밖이었습니다. 그런데 제가 OTT 서비스를 사용해온 방식들을 생각해 보니 강의 내용에 들어맞는 것 같습니다. 저는 넷플릭스를 사용하고 있는데, 넷플릭스가 특정 콘텐츠를 가지고 있기 때문에 이 서비스를 사용하기로 결정한 것이 아니라, 오히려 넷플릭스를 결제했기 때문에 알지도 못했던 다양한 콘텐츠를 새롭게 시청하게 되고, 시청 경험이 계속되니까 회원을 유지하고 있는 것 같습니다. 티빙이나 디즈니 플러스 같은 다른 서비스는 보고 싶은 콘텐츠가 있을 때만 결제하는데, 이 경우 OTT 브랜드 오리지널 시리지였던 경우가 많았습니다…\n\n\n\nranked value와 reavealed value로 사용자의 별점 평가과 실제 시청 경험의 차이를 짚어낸 것이 인상적이었다. 결국 사용자가 ‘의식’해서 콘텐츠를 평가하는 것과, ’무의식’으로 콘텐츠를 시청하는 것은 다른 층위의 가치가 작용한다는 것이다. 놀라운 건 이것을 짚어내고, 바로 서비스에 반영했다는 것이다. 넷플릭스는 기존에 있던 별점 평가 시스템을 제외하고, ’좋아요-별로예요’ 정도로 평가 시스템을 축소했다. 별점이 사용자의 실제 시청 경험에 유익하지 않다는 것을, 사용자 스스로도 인식하기 전에 먼저 인식하고 적용해서 사용자 경험에 편의를 더한 것이다. 이렇게 인식하고, 그 인식을 바로 반영해내는 결단력이 지금의 넷플릭스를 있게 한 원동력이구나 싶었다.\n\n\n\n\nBest Questions\n\n2011년 넷플릭스는 ’하우스 오브 카드’에 회사 수익의 상당 부분을 차지하는 1억 달러 가량을 투자했는데, 이는 가히 큰 위험이라고 말할 수 있다. 수익의 상당 부분이면 그 결과가 좋지 않을 때 거의 부도가 날 수 있는데, 결과적으로는 성공했지만 이러한 결정을 내리기 위해서는 어떤 것이 필요할까? 하이 리스크를 겁내 큰 도전을 하지 못할 사람들이 더 많다고 생각되는데, 큰 결심과 올바른 선택을 하기 위해서 필요한 자질과 근거에는 무엇이 있을지에 대해 궁금하다.\n\n\n\n“최근 OTT 업계는 과거와 달리 점차 정체되고 있다고 생각합니다. 넷플릭스에서는 이를 해결하기 위해서 계정 공유 금지 정책을 실시하고 있습니다. 교수님께서는 OTT 업계의 이러한 공유 금지가 더욱 확산할 것으로 전망하실 지 여쭤보고 싶습니다. 또한, 이러한 정책이 OTT를 구독하고 있는 사용자들이 감소하는 효과를 불러와, 전반적으로 산업을 축소시킬 지, 아니면 더욱 소비를 증가시켜 시장을 더욱 크게 만들 것으로 전망하시는 지에 대해서도 알고 싶습니다.\n\n\n\n저번학기 (2023-2)에 문화테크놀로지2 수업을 수강하면서 테오의 국내 프로그램 특징을 분석하고 테오의 프로그램을 분류해 어떤 플랫폼에 공급하면 좋을지에 대한 조별 발표를 진행했었습니다. 당시 국내 OTT 서비스의 미래와 테오의 프로그램을 엮어서 발표했었었는데요. 국내의 웨이브와 티빙의 합병이 진행되냐 마냐 논의가 진행되는 상태였어서 자세한 국내 엔터의 방향성이 결정된지 않았다는 피드백을 받았던 기억이 납니다. 최근 뉴스에서 웨이브와 티빙의 합병이 가시화되고 있다는 뉴스를 보았는데요. 이러한 국내 오티티 시장의 변화가 엔터 사업 내의 큰 영향을 끼칠 수 있을 것인지에 대한 의견이 궁금합니다!"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/12_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/12_week.html",
    "title": "가상/증강현실 콘텐츠 제작",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 12: 가상/증강현실 콘텐츠 제작\n\nDate: 20240521\n수업자료\n수업 녹음 자료 (For 예비군 수강생)\n\n\n\n\n\nPre-class video\n\n상상이 현실화가 되는 세계, 그 중심에 서서 외치다 | 서동일 오큘러스VR코리아 지사장\n\n\n\nWhat is extended reality? | The Gadget Show\n\n\n\n소아암, 장기치료 어린이를 위한 디지털 테마파크 | RGB MAKERS\n\n\n\n\n\n아래 영상들은 RGB MAKERS 사에서 제작한 작품들입니다.\n\n2021 CSR 필름 페스티벌-희망 나눔 부문(보건복지부장관상) 수상작\n\n\n\nTabernacle; Realistic Bible Experience Contents\n\n\n\n김태원 대표님 수업을 통해 학생들이 제작한 영상: healing planet metaverse space makers\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n\n이 영상은 12주간 내가 봤던 모든 사전 영상 중 가장 흥미로운 영상이었다. 나는 이제껏 인간이 가상의 공간에 침투하는 것은 봐왔지만, 가상의 것이 현실로 구현되는 것은 처음 봤기에 더욱 신선하였다. 가상/증강 현실로 없는 공간을 창작하고, LED가 설치되지 않은 곳까지 확장하여 가상 현실을 확장하고, 가상 현실에 멀리있는 사람이나 존재하지 않은 사물을 불러올 수 있었다. 어릴 적 본 &lt;레디플레이어 원&gt;이라는 영화가 생각날 정도로 세상은 생각보다 더욱 고도화된 기술력을 가지고 있었다. 하지만 가장 충격이었던 점은 이 영상이 무려 3년 전 영상이라는 것이다. 3년이 지난 지금은 어느 정도로 기술이 발전하였을까? XR기술을 직접 체험해보고 싶다고 생각하게 되는 영상이었다.\n이 영상은 12주간 내가 봤던 모든 사전 영상 중 가장 흥미로운 영상이었다. 나는 이제껏 인간이 가상의 공간에 침투하는 것은 봐왔지만, 가상의 것이 현실로 구현되는 것은 처음 봤기에 더욱 신선하였다. 가상/증강 현실로 없는 공간을 창작하고, LED가 설치되지 않은 곳까지 확장하여 가상 현실을 확장하고, 가상 현실에 멀리있는 사람이나 존재하지 않은 사물을 불러올 수 있었다. 어릴 적 본 &lt;레디플레이어 원&gt;이라는 영화가 생각날 정도로 세상은 생각보다 더욱 고도화된 기술력을 가지고 있었다. 하지만 가장 충격이었던 점은 이 영상이 무려 3년 전 영상이라는 것이다. 3년이 지난 지금은 어느 정도로 기술이 발전하였을까? XR기술을 직접 체험해보고 싶다고 생각하게 되는 영상이었다. 첫 번째 영상에서, 서동일 사장님은 VR이 우리에게 어릴 적 이루지 못한 꿈을 이루게 해줄 것이라고 말하였다. 어릴 때부터 나는 런닝맨과 1박2일과 같은 방송을 보면서, 방송용 레크레이션 게임을 창작해보고 싶다는 생각을 하였지만, 방송 업계가 많이 저물면서 나는 자연스럽게 나의 꿈을 접게 되었다. 사실 IT기술이 점점 우리 사회에 스며들면서, 나라는 사람이 무엇을 할 수 있는지에 대해 고민하는 과정을 계속해서 반복해왔다. 내가 할 수 있는 일은 인공지능이 훨씬 더 빠르고 정확하게 처리해주기에, 나라는 사람이 쓸모가 있는 분야가 있을까라는 회의감도 들었었다.\n이 영상은 이러한 나의 고민을 타파하였다. 방송용 게임에서 VR 게임으로 시각을 전환하니, 소비자들이 흥미를 느낄만한 콘텐츠를 새롭게 기획하는 것은 결국 인간의 영역이겠구나 생각하였다.영상에서 소아암 환자를 타겟으로 어린아이의 시각에 맞게 테마파크, 총게임, 롤러코스터 등의 콘텐츠를 제작하는 것을 보면서, 나라면 어떤 식의 게임을 기획하였을지 생각하게 되었다. VR 콘텐츠 크리에이터라는 직업에 대해 희망을 가지게 되면서 어릴 적 꿈을 실현해보고 싶다는 희망을 가지게 되었다.\n\n\n\n\n\n가상 환경을 구축하는 것은 단순히 초록 배경에서 디자인 그래픽을 입힌 것이라고 생각했는데 LED를 이용해 입체적으로 가상 환경을 구축하는 것을 보며 신기했습니다.언젠가 모든 TV쇼가 저 기술을 활용하여 배경을 구축할 수 있지 않을까 생각했습니다. 조금 다른 이야기일 수 있지만 최근에 종방한 눈물의 여왕이라는 드리마에서도 AI를 활용하여 눈이 오는 배경을 만들어냈다고 했습니다. 이제는 촬영지를 찾으러 다니지 않아도 원하는 배경을 보다 정확하게 구현할 수 있으니 효율적이라고 생각합니다.\n아이들이 좋아하는 모습을 보면서 저도 모르게 흐뭇해졌습니다. 문득 이 영상을 보다가 어르신분들이 떠올랐습니다. 매장에 거의 대부분이 키오스크로 주문을 하는데, 어르신분들께 키오스크를 하는 상황을 VR로 제작하여 연습해보도록 만드는 상품은 어떨까 생각해보았습니다. 그렇다면 직접 가서 키오스크를 이용하는 데에 조금이라도 어려움을 덜어드릴 수 있지 않을까 생각했습니다. 사회의 어려움, 문제를 해결하기 위해 재밌는 방법으로 해결할 수 있는 것도 VR의 기능이구나 느꼈습니다.\n\n\n\n\n상상이 현실화되는 세계는 매우 독특하고 환상적인 개념입니다. 이러한 세계에서는 개인의 생각과 꿈이 현실의 형태로 나타나므로, 이는 인간의 창의력과 내면의 욕망을 물리적인 현실로 전환시키는 능력을 부여합니다. 상상력이 현실이 되는 이 세계에서는 무한한 가능성과 동시에 예측 불가능한 위험들이 존재합니다. 두번째 영상을 통해서 가상 세계 기술은 교육 분야에서도 엄청난 잠재력을 보여주습니다. 가상 현실 기술을 통해 학생들은 전 세계의 역사 유적지를 탐방하거나 우주의 신비를 탐험하거나 심지어 인체 내부를 공부하는 등 다양한 학습을 할 수 있습니다. 이러한 몰입형 학습 방식은 학생들의 학습 흥미를 높일 뿐만 아니라, 지식에 대한 이해도를 깊게 합니다. 생병에 걸린 아이들이 AR 등 기술을 통해 즐겁게 노는 모습을 보니, 나도 진심으로 기쁩니다. 기술의 발전은 단순히 오락을 넘어서, 병마와 싸우는 아이들에게 희망과 웃음을 선사합니다. 병원이라는 제한된 공간에서 아이들은 가상 현실을 통해 다양한 세상을 탐험하고, 그들의 상상력을 자유롭게 펼칠 수 있습니다. 이 과정에서 아이들은 잠시나마 고통을 잊고, 순수한 기쁨을 느낄 수 있습니다. 이러한 모습을 보면서, 기술이 사람들에게 줄 수 있는 긍정적인 영향력에 대해 다시금 깨닫게 됩니다. 아이들이 건강하게 자라날 수 있도록, 그리고 그들이 더 많은 행복을 느낄 수 있도록 도와주는 기술의 힘에 감사함을 느낍니다.\n\n\n\nMake people to experience impossible!라는 목적 자체가 사람들에게 다양한 경험을 제공할 수 있고 여러가지 중요한 의미를 내포하고 있는 것 같습니다. 우선 이 목적은 사용자들에게 일상적인 한계를 뛰어넘는 새로운 경험을 제공합니다. 또한 소비자들이 현실에서 경험할 수 없는 상황, 장소, 또는 감정을 가상현실을 통해 체험하게 한다는 것은 소비자들의 만족도를 높이고 더 많은 관심과 참여를 유도할 수 있을 것 같습니다. 앞으로의 기술 발전을 통해 가능해질 새로운 경험에는 어떤 것들이 새롭게 생길지에 대한 궁금증을 유발하는 영상이었던 것 같습니다. XR기술은 실제와 가상세계를 융합한 것이다. 화면을 보며 바닥에 있는 타이어를 가리키거나 보는 것이 가능한 것과 같이 가상환경과 실제환경이 함께 상호작용하다. 또한, 소형 스튜디오 공간만 있어도 몰입할 수 있는 가상환경을 화면 상에도 더 크게 만들 수 있다는 이점이 있다. 그만큼, XR 기술의 발전은 가상환경과 실제환경의 연결을 도울 수 있다는 점을 알 수 있었다. 이러한 디지털 테마파크는 고통을 참고 있는 아이들의 힘든 시간을 조금이나마 즐겁게, 잠시나마 아이들의 아픔을 잊게 해줄 수 있는 좋은 아이디어인 것 같다고 느껴졌습니다. 영상에서 보이는 게임에 집중하는 아이들의 모습은 너무나도 사랑스럽습니다.\n\n\n\n이 영상을 통해 가상현실의 필요성과 가능성에 대해 생각하게 되었다. 인간 대신 기계에게 일을 시킬 수 있도록하는 에너지 발전기나 공장 등 일상 속 필수적인 기술과 달리 가상 현실은 버추얼휴먼처럼 우리에게 꼭 필요한 것인지 이것이 어디에 쓰일 수 있을지 감이 잘 안 잡힌다. 그런데 돈, 시간, 두려움, 건강 등으로 인한 현실 속 제약에서 벗어나 꿈을 간접적으로나마 이룰 수 있게 해준다는 점에 있어 다양한 가능성이 존재하는 분야임을 알게 되었다. 가상현실을 구현하는 기술뿐만 아니라 이 기술을 어떤 식으로 활용할 수 있을지 상상하는 것이 중요한 것 같다. XR(확장 현실)이 구현되는 방식을 간단하게나마 시각적으로 볼 수 있었다. 또한 가상현실 관련 기술을 생각했을 때, 단순 가상현실이나 현실 세계에 가상의 요소를 덧입힌 증강현실 정도만 생각했는데, 이 모든 기술을 결합한 XR에 대해 좀 더 배울 수 있었다. 마커를 통해 가상의 물체도 실시간으로 움직이는 기술이 정말 인상깊었고, 좀 더 구체적인 기술의 메커니즘을 배워보고 싶다는 열망이 생겼다. 첫 번째 영상에서 언급한 메시지 중 하나가 구현된 한 가지 예시가 이 영상에 나왔다. 건강이라는 제약 조건에 의해 놀이공원이나 신체 놀이 등을 하기 힘든 소아암 어린이들이 가상현실 기술을 통해 간접적이지만 정말 실감 나게 디지털 테마파크를 체험할 수 있는 것이 정말 의미가 있다고 느꼈다. 실제 놀이공원을 마음 껏 갈 수 있는 사람이라면 디지털 테마파크에 큰 흥미를 못 느낄 수도 있지만, 병원을 벗어나기 힘든 장기 치료 어린이들에게 디지털 테마파크는 훨씬 큰 의미를 가질 수 있겠다는 생각을 했다. 이렇게 제약을 갖고 있는 사람들과 상황들을 관찰해서 해당 기술의 순기능을 발휘하는 것이 중요하다고 본다.\n\n\n\n\nBest Questions\n\n어린이 소아암 관련 영상을 보면서 단순히 가상 증강 현실 콘텐츠가 사람들의 오락과 여가 시간을 보내기 위한 수단이 아닌 그 이상의 더움이 될 수 있댜는 것을 몸소 깨달을 수 있었습니다. 이러한 가상/ 증강 현실 콘텐츠로 많은 병들을 고칠 수 있을 것 같은데 교수님께서 생각하실 때 어떤 분야의 의료와 접목이 가능하실거라고 생각하시나요? 개인적으로는 정신 질환을 치료하는데고 꽤나 좋은 치료방법으로 사용할 수 있을 것 같다는 생각이 들었습니다.\n\n\n\n코로나19 창궐 당시 가상현실이나 메타버스와 관련한 붐이 잠시 일었다가 소강 상태에 접어든 데에는 감염병 사태가 종국의 단계에 들어선 것도 있겠지만 미진한 기기 보급과도 관련이 있다고 생각합니다. 일반 디스플레이로도 가상현실 콘텐츠의 감상이 불가능한 것은 아니지만 창작자의 의도대로 콘텐츠가 감상되기 위해선 적어도 구글 카드보드라도 쓰는 것이 맞을 터이나 아직까지 많은 사람들이 HMD 장비를 머리에 장착하고 손에 무엇인가 쥐는 행위를 번거롭고 거추장스럽게 여기는 것이 아닌가 생각합니다. 결국 사용자의 경험 향상을 위한 기술 발전의 모멘텀이 필요하다고 보는데 업계에서는 이러한 문제점 해결을 위해 어떤 기술적 노력을 하고 있는지 궁금합니다.\n\n\n\n가상/증강현실 관련 콘텐츠는 게임이나 여행 등 유흥적인 요소가 현재 우선적으로 활성화되고 산업에 높은 비중을 차지하고 있는 것 같습니다. 이 때, 일반 게임도 과몰입하고 현실 세계와 구분이 안되는 중독 문제가 이슈가 됐었는데, 가상/증강현실 관련 게임은 그 몰입도가 일반 게임에 비해 엄청날 것 같습니다. 그래서 현실 세계와 구분이 안되는 정도는 아니더라도, 더 쉽게 중독되고 현실을 살기보다는 가상현실 속 게임을 계속해서 즐기는 문제도 발생할 수 있을 것 같다는 생각이 들었습니다. 그래서 정부 차원에서는 만약 가상/증강현실이 대중화된다면 게임 관련 규제를 내릴 수 밖에 없을 것 같은데 이에 대한 산업의 대처방안, 중독 문제에 관련된 교수님의 의견이 궁금합니다."
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/10_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/10_week.html",
    "title": "인터랙션 사이언스, UX",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 10: 인터랙션 사이언스, UX\n\nDate: 20240507\n\n\n\n수업자료\n\n\n\nPre-class video\n\nHuman-Computer Interaction and User Interface Design | Tony Tang\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nBest Discussion\n\nAI 등 인공지능, 기계 등의 기술이 점점 발전되면서 사회가 많이 변했다고 느꼈습니다. 이제 어느 가게를 가도 키오스크 없는 곳을 찾기 어렵고, 모바일 앱으로 주문이 안되는 곳을 찾기 어려운 것도 하나의 예시라고 생각합니다. 어느 순간부터 우리 생활에 기술이 가장 깊숙히 들어와있다고 느껴졌습니다. 그냥 오랜 시간 기술과 함께 사회를 이루어갔기 때문이라고 생각했는데, 이 영상을 보고 인간이 편리함을 느끼게끔 기술이 변했다고 생각했습니다. 인간이 느끼는 사회의 불편함을 기술이 해결하고 있다고 생각했습니다. 그리하여 이제는 이런 기술 없이는 살 수 없는 분위기, 사회가 형성된 것이라 예상했습니다. 수요가 많아져 발전되는 속도가 빨라지면 공급이 더뎌져 빈 공간이 생기기 마련입니다. 카페 아르바이트를 하며 키오스크를 어려워하는 고령층분들이 눈에 많이 밟힙니다. 그럴 때마다 저희 부모님도 생각나면서 이런 기술이 발전하면 이 기술을 모든 사람이 잘 활용할 수 있도록 교육, 안내를 잘 해야 하지 않을까 생각합니다.\n\n\n\n시스템을 만드는 디자이너들이 실제로 시스템을 사용하는 사람들이 아닌 경우가 많다는 이야기가 가장 공감이 갔습니다. UX research로 인터뷰를 시행한 경험이 있는데, 실제로 해당 제품을 사용하는 사용자가 아니다보니, 어떤 점이 불편할지, 어떤 점을 조사해야 할 지 인터뷰 질문을 구성하는 과정 자체가 막막하기도 했습니다. 영상에서 나온, 디자이너들이 시스템이 어떻게 사용될 것인지 가정한다는 구절이, 제 상황을 정확히 지적하고 있었습니다. 사용자 경험 연구와 사용자 경험 디자인에서 해야 할 일은 사용자가 누구인지를 이해하기 위해 공감 도구를 개발하는 것이라고 하는데, 어떻게 하면 사용자 입장에서 생각할 수 있을지 여전히 아리송한 상황입니다. 특히 요즘에는 사용자들의 유형이 개별화되다보니, 보편적인 사용자 디자인을 만들기는 더욱 어렵게 느껴집니다.\n\n\n\n외국인이 음료수 “갈아만든 배”의 “배”를 IdH 로 읽는다던지, 너구리를 뒤집어 읽어 “RTA”로 본다던지의 사례를 보면서 웃은적이 있다. 하지만 이번 영상을 보며 위 사례를 미루어 좋은 커뮤니케이션, 디자인이 무엇인지 생각해볼 수 있었다. 위의 경우에는 그저 문자의 차이에서 생긴 단순한 해프닝으로 볼수 있지만, 적어도 좋은 커뮤니케이션이 일어났다고는 할 수 없을 것이다. 어떤 제품, 서비스 등을 기획 및 제작함에 그 의도에 맞는 공통된 행동 양식을 보일 수 없다면 그것은 좋은 디자인이 아닌 것이다. 만약 우리가 “밥 뭐 먹었니?”라고 물어보면 “도시락 먹었어”로 대답하지, “응/아니”라고 대답하지 않듯, 좋은 UX디자인은 자신이 의도한 대로 사용자의 행동을 유도하는 “의도에 따른 행동 유도”로 정리할 수 있을 것 같다.\n\n\n\n인터랙션 사이언스와 UX 분야가 이론부터 기술을 총망라하는 분야임을 알 수 있었다. 사용자에 대한 관심을 기술로 풀어낸다는 점에서, 굉장히 매력적인 분야인 것 같다. 인터랙션 사이언스에서 인터랙션은 어떤 의미일 수 있을지 생각해보게 되었다. 처음에는 단순 기술을 고안하는 인간과, 기술을 실현하는 도구인 컴퓨터(혹은 그외 도구) 간의 교류라고 생각했는데 영상을 보니 사용자와, 사용자를 생각하는 기획자와의 교류 사이에 도구가 연결되어 있는 소통으로도 뜻을 이해하게 되었다.\n\n\n\n영상 처음 부분에 실패한 UX 디자인의 예시를 보며, 과연 나도 살면서 저러한 제품들, 혹은 상황들은 마주한 적이 있는가에 대해 생각해보게 되었습니다. 생각해보면 저러한 상황들은 다수 있었습니다. 간단한 예시를 들자면, 요즘에는 어떠한 사이트에 들어가려면 간편하게 로그인을 구글로 로그인하기, 카카오톡으로 로그인하기 버튼을 누르면 빠르게 로그인을 할 수 있습니다. 하지만 가끔씩 그렇지 않은 사이트들도 다수 있는데요, 로그인을 하려고 다른 페이지로 넘어가는 순간 보이는 복잡한 로그인 단계를 보면 한숨부터 나오게 됩니다. 과연 이러한 정보까지 입력을 굳이 해야되나 싶을 정도입니다. 영상에서 나오는 예시들을 보며 많이 답답했고, 공감을 얻은 것 같아 흥미로웠습니다. 또한 실패한 UX 디자인을 보면서, 어떻게 저러한 제품이 상용화가 되었는지 많은 의문이 들었습니다.\n\n\n\n기술이라는 것이 인간의 의도를 실현해주는 역할을 한다고 생각하는데, 이 인터렉션 사이언스는 ’소통’이라는 의도를 실현해주는 역할을 하며, 그 의도를 망각해서는 안된다고 느꼈다. 소통을 원활히 하기 위해서 영상 속의 사람들은 무의식을 연구하고, 공감 도구를 활용하고 프로토 타입을 제작하는 등의 노력을 기울인다고 이해했다. 결국 우리 컬처앤테크놀로지전공생의 역할은 이 ’소통’이라는 것을 개발자와 협업해 원활하게 해주는 사람이 아닐까 싶었다.\n\n\n\n\nBest Questions\n\n(인터랙션 사이언스와 AI) “인터랙션 사이언스는 인간과 기술이 상호작용하는 방법과 현상에 대한 이론을 다양한 학제간 접근을 통해 연구하는 학문이라고 정의를 내리고 있고, 기술을 통해 중요한 정보 전달을 강조하는 중간 시스템의 역할은 해당 정보를 가장 적절하고 맥락에 맞게 제공하는 것, 즉 기술은 맥락을 이해하고 정보를 적절하게 제공해야 한다는 것이 위의 영상의 핵심이라고 생각했습니다. 몇주 전 수업시간에 음성인식 기술 관련해서 의견을 작성했었는데요. 기술은 맥락을 이해하고 정보 전달에 탁월해야 한다는 것이 인터랙션 사이언스가 존재하는 이유들 중 하나라면, 음성인식 비서 즉 AI 기술들이 인간의 언어의 맥락을 파악하고 이를 전달하거나 완벽하게 이해할 수 있을 것이라고 생각하시는 지 질문 드리고 싶습니다. 예를 들어 중의적인 표현이나, 사람의 억양, 단어 선택에 따라 말의 의미가 정말 많이 바뀌는데 이러한, 음성적인 정보도 파악이 가능하다고 생각하시는 지 궁금합니다.\n\n\n\n(인터랙션 사이언스와 심리학) “인터랙션 사이언스, UX 디자인도 점점 더 개인화된 경험을 개선하기 위해 집중할 것이라 추측된다. 이를 위해서는 사용자의 경험을 이해하기 위한 심리적 측면의 연구도 선행되어야 할 것 같은데, 실제로 인터랙션 사이언스와 UX 디자인을 구상할 때 이론적인 연구가 얼만큼 수행되고 적용되는지 궁금하다. 또한 인터랙션 사이언스와 UX를 기획하고 구체화할 때, 이론연구와 기술구현 그리고 이후 영향분석의 과정까지 정말 다양한 분야의 지식이 필요할 것 같다. 지금은 기술구현 측면이 더 두드러지고 있는 것 같은데, 이러한 현상에 대한 전문가의 견해가 궁금하다. 앞으로 인터랙션 사이언스와 UX가 발전하기 위해서는 어떤 점에 더 집중해야 하는지도 의견을 묻고 싶다.”\n\n\n\n(특이성 vs. 대중성) 사용자와 기술 간 상호작용을 이해하고 설계하는 과학이라고 불리는 인터랙션 사이언스와, 사용자 경험을 나타내는 UX는 서로 밀접한 관련이 있다는 것은 누구나 다 알고 있는 사실입니다. 학문적 영역과 디자인 접근 방식의 결합을 통해 실생활의 문제점들을 해결할 수 있다고 생각하는데, 결국 사람들 다수의 공통의 사고를 얼마나 잘 분석해 내는지가 중요한 요소로 작용할 것이라고 생각합니다. 이 과정에서 사람들의 특이성에 초점을 두고 특성에 기반한 해결책을 마련하는 것이 좋은지, 모두가 알고 있는 대중성에 초점을 두고 노말한 요소를 만들어내는 것이 좋은지 궁금합니다."
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/08_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/08_week.html",
    "title": "문화콘텐츠와 자연어 처리",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 8: 문화콘텐츠와 자연어 처리\n\nDate: 20240423\n수업자료\nPre-class video\n\nText 데이터를 활용한 개인화 음악 추천서비스 웹 데모 구축 (혁신성장청년인재양성사업 프로젝트 발표)\n\n\n\n[TED] Why can’t AI ‘think’ like us?\n\n\n\n[TED] The language of computational linguistics.\n\n\n\n[엔씨소프트] 장애인의 목소리가 되어주는 AI 기술\n\n\n\n[노마드코더] 민트 초코 논란! 자연어 처리(NLP)로 종결해드림.\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nBest Discussion\n\n넷플릭스에서 collaborative filtering을 통해 영화 추천을 한다는 것을 알고 있었습니다. 이 기술을 음악 스트리밍 플랫폼에서 진행한다면 좀 더 다양한 장르를 접할 수 있다고 생각했습니다. 개인적으로 저는 영화보다 노래를 다양하게 즐기기 어렵다고 생각합니다. 그 이유는 영화는 상영관에서 상영하는 영화를 보고 고를 수 있고 또 스토리가 있기 때문에 대부분 SF가 아닌 이상 이해하는 데에만 집중할 수 있기 때문입니다. 그러나 음악은 락, 인디밴드, 발라드 등 정말 많은 장르가 있고 이는 취향이 확고하게 드러난다고 생각합니다. 그래서 collaborative filtering이 적용된다면 개인 취향이 아닌 것 같아 도전하지 못했던 노래들도 쉽게 접할 수 있지 않을까 생각했습니다.\n제가 FLO 앱을 사용하는데, 그 앱은 ‘3월 4일에 들은 노래와 비슷한 플레이리스트’ 이렇게 추천을 해주곤 합니다. 저는 이 기능이 정말 좋았던 게, 이런 노래 장르를 좀 더 들어보고 싶다고 생각한 찰나에 비슷한 느낌의 노래를 찾지 않아도 접할 수 있었기 때문입니다. 개인화된 맞춤 서비스가 더 강해질 수록 확증편향이 강해진다고 생각했는데 그 안에서 다른 길로 뚫릴 수도 있겠다고 생각했습니다.\n\n\n\n1번 - 제한된 데이터로 유저의 의도를 파악했다고 했는데 이 과정에서 정보의 왜곡이 일어날 수 있지 않을까 라는 생각이 들었습니다. 자료가 적다보니 조금의 분석 방향이 달라질 경우, 전체 분석의 의도, 흐름이 달라질 수 있을 것 같은데, 이런 경우 어떤 식으로 보안이 가능할까요? 인기상품, 컨텐츠 필터링, 유저를 통한 필터링 총 세가지를 영상에서 이야기 해줬는데 이러한 알고리즘이 꽤 도움이 될 것이라는 생각이 들었습니다. 다만 이미 에플뮤직, 멜론, 유튜브 뮤직 사이트에서 비슷한 알고리즘을 사용하고 있다고 체감했고, 따라서 현행 알고리즘과 차이점을 느끼지 못했던 것 같습니다.\n4번 - AI 음성을 탑재한 기술을 통해 단일한 톤이 아닌, 사람의 말투, 억양 등을 담고 있는 목소리 기술이 상당히 도움이 많이 될 것이라는 것을 깨달았습니다. 평소 사람과 사람 사이의 말에서 중요한 요소 중 하나가 목소리의 억양, 말투, 톤이라고 생각했는데 이런 것을 장애인의 목소리가 되어주는 그러한 목소리에서 고려하지 못했다는 점에 스스로의 생각이 짧았던 것 같습니다. AI의 기술의 발달도 모든 사람들이 향유할 수 있는, 그런 베리어 프리적인 기술이 되었으면 좋겠다는 생각이 들었습니다. 기술의 발달이 어떤 사람은 포함되고 포함되지 않는, 불공평한 분야갸 되지 않았으면 좋겠다는 생각을 하게 되었습니다.\n\n\n\n(두 번째 영상) Generalization is not the reality. 영상 속 작가의 이 한 마디는 아무리 AI가 발전하더라도 철학 인문 계열의 전문가는 사라지지 않을 수 있겠다는 희망을 주었습니다. AI는 세상에 공개되어있는 대부분의 사람들이 바라보는 세상을 학습합니다. 하지만 과연 다수가 바라보는 시각이 항상 옳을까요? 우리는 데이터와 알고리즘을 창조하는 동시에 영향을 받기도 합니다. 바쁘게 변해가는 사회 속에서 우리가 구축한 인공지능에게 압도당하거나 휩쓸리지 않기 위해서는, 서로 올바른 영향을 주고 받기 위해서는 인문 계열의 전문가가 끊임없이 세상의 정의에 대해 철학적으로 탐구해야 겠다고 생각했습니다.\n\n\n\n2번영상과 3번 영상에서 일관되게 AI는 인간과 사고하는 방식이 다르다고 말하고 있다. 이부분이 인상적이고 너무 공감됐다. 물론 기술이 발전하면서 AI가 학습한 데이터의 양은 기하급수적으로 늘고 있지만 여전히 부족한 것이 사실이며, 과연 인간의 사고방식을 온전히 구현할 수 있을지에 대한 현실적 의문이 든다.\n이와 관련해 개인적으로 최근에 겪었던 재미있는 이야기를 해보자면, 수업을 같이 듣는 친구들끼리 개설된 단톡방이있는데, 평소에 서로의 이름에 ~튜브(에: 홍길동 - 길동튜브/홍튜브)를 붙여 부른다. 카카오톡에서도 그렇게 부르면서 이야기를 떨고 있는데 어느 한 친구가 카카오톡 AI 대화요약 기능을 사용한 결과가 재미있다며 캡쳐해서 보내줬다. 그 결과는 “튜브에 관련된 무의미한 대화”였다.\n데이터가 많이 학습된건 사실이지만, 특정 집단에서 맥락에 맞춰 쓰이는 단어들을 AI는가이해하지 못하고 있는 것이다. 인간의 언어는 결국 약속에 의한 것인데 AI는 그 약속을 지키는 것처럼 모방하는 것일 뿐이지, 약속을 이해하지는 못한다고 생각한다.\n\n\n\nAI를 어디까지나 인간을 보조하기 위한 수단, 있으면 좋고 없으면 마는, 그런 존재라고 생각해왔다. 하지만 장애인을 위핸 “나의 ACC”앱을 보면서 누군가에게는 AI가 반드시 필요할 수도, 새로운 기회를 주는 것일 수도 있다는 생각이 들었다.\n또한 기술을 가진 이들과 그렇지 못한 이들의 격차가 마치 자연의 순리처럼 당연하게 여겨지는 사회에서 이런 사례들은 그래도 우리 사회가 아직은 어둡지만은 않다는 생각이 들게한다. “배워서 남주랴?” 이제는 다 옛 말이다. 이제는 배워서 남을 주는 것이 남을 위한 길이기도 하고 새로운 영역을 개척하는 나의 길이기도 한 것 같다.\n\n\n\n[엔씨소프트] 장애인의 목소리가 되어주는 AI 기술\n영상을 통해 AAC라는 의사소통 툴을 처음 알게되었습니다. 2015년에 서비스가 제작되었지만, 이제 알았다는 점이 아주 조금은 부끄럽기도 합니다. 의사소통은 사람과 사람간의 소통인데 의사소통에 어려움을 겪는 분들께 이러한 서비스는 정말 많은 도움이 될 것 같다는 생각이 듭니다. 또한 리뉴얼된 버전이 소통을 하며 감정을 전달하는 데에 있어 훨씬 더 잘 될 것 같고 어려움을 겪는 이들의 삶의 질을 향상시켜준다는 생각이 듭니다. 본 영상을 시청한 모두가, 그리고 ’나의 AAC’라는 어플을 알고 있는 사람들이 어플을 설치한다면, 앞으로 세상을 살면서 마주할 의사소통에 어려움을 겪는 이들과 조금 더 원활한 소통을 할 수 있을 것이라 예상한다.\n\n\n\n\nBest Questions\n\n다섯 개의 영상을 보면 아직은 “일반화”된 기준에 맞춰진 AI가 구축되고 있는 것 같습니다. 다만, 인간의 감정은 지극히 개인적이며 일반화하기에는 복잡하다고 생각합니다. 네번째 영상에서처럼 AI에 의존하여 대화를 하는 사람들의 경우, 자신이 전달하려는 감정과는 다르게 일반화된 기준에 따라 정제된 잘못된 감정이 전달될 수도 있다고 생각했습니다. 교수님께서는 훗날에는 이런 개인적인 해석과 기준에 의거한 AI도 만들어질 수 있다고 생각하시나요? 왜 일반화된 기준의 AI만 구축이 되고 있는지, 아니면 개인적인 감정까지 AI로 만들려는 노력이 일어나고 있는지 알고싶습니다!\n이는 자연어 처리와는 별개의 질문입니다만… 영상 속에서 자연어 분석의 데이터를 얻는 과정에 API라는 단어가 자주 나와 검색을 해보았으나, 인터페이스와의 차이점을 이해하기 어려웠습니다.. 둘의 차이점이 무엇인가요?\n제가 느끼기에는 자연어 처리는 분류, 탐색, 추천 등 분석적인 툴처럼 느껴지고, 문화 콘텐츠는 분석도 물론 필요하지만 그 이외에 크리에이티브함이 필수불가결하다는 생각이 들었습니다.\n다시 말해, 문화 콘텐츠를 기획할 때 분석 과정에서는 자연어 처리가 쓰일 수 있지만 소비자 마음에 소구되기 위해서는 사람만이 느끼고 생각할 수 있는 창의성이 빠질 수는 없다는 생각이 들었는데요.\n혹시 나중에는 자연어 처리가 창의성의 영역도 뛰어넘는 시대가 올지, 이에 대해서 어떻게 생각하시는지 궁금합니다.\n자연어처리는 머신러닝 기법을 활용하여 텍스트의 의미를 파악하는 것으로 알고있습니다. 그렇다면, 표준어가 아닌, 한국에서 사용하는 사투리나 방언을 이해하고 처리하는 데에 문제나 어려움은 없나요?"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/06_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/06_week.html",
    "title": "한류와 팬덤",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 6: 한류와 팬덤\n\nDate: 20240409\n수업자료\nPre-class video\n\nAll I Really Need To Know, I Learned From KPOP | Donald Lim | TEDxYouth@SJCS\n\n\n\nHas K-pop prepared us for the metaverse? | Alex Karlsson | TEDxBrunkebergstorg\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nQ&A Session\n\n한류와 K-Pop의 글로벌 확산 및 팬덤 문화\n\n(스타가 팬을 대하는 태도가 성공에 미치는 영향) 한류가 전세계적으로 유행하는 데에는 팬덤도 큰 영향을 미치고 있는지에 대해 여쭙고 싶습니다. 또한, 미국의 유명 가수인 테일러 스위프트를 보면 자신의 팬을 대하는 태도에서 한국의 K-Pop 아이돌(연예인)들과 유사한 점이 있다고 생각하는데, 이러한 특징이 테일러 스위프트의 성공에 기여하였다고 보시는 지에 대해 여쭤보고 싶습니다. 만약 그렇다면 이러한 K-Pop의 특징은 점차 세계화될 것이라 전망하시는 지에 대해서도 궁금합니다.\n\n\n팬덤이 K-Pop 세계화에 큰 영향을 미친것은 사실입니다. 하지만 주목해야할 점은, 테일러 스위프트처럼 스타성, 대중성이 큰 가수와 달리 K-Pop은 보다 섬세함이 특징적입니다. 여기에는 소셜 미디어 활용이라는 기술적 차원이 접목됩니다. 미디어 기술이 관여되고 케이팝 산업과 가수들이 이를 적극 활용한 것입니다. 예컨대 스타들과 위버스 등 소셜 미디어 플랫폼에서의 라이브채팅(유료회원)을 할 수 있도록, 더 가까이 내적 친밀감을 유도합니다. 이를 통해 케이팝 팬덤은 능동적 수용자로 자리매김하고, 미디어 기술과 엔터테인먼트 산업계가 팬덤을 상업적으로 활용하는 것이 복합적으로 작용한 형태가 작금의 K-Pop 시장인 것입니다. 이중 하나만 없어도 향후 케이팝 세계적 인기는 어렵다고 해석할 수 있습니다.\n\n\n\n(팬덤 갈등) KPOP의 흥행 덕분에 여러 아이돌 가수들이 많이 생겨났고, 다양한 팬덤들 또한 많이 생겼다. 하지만 이런 팬덤들 간의 갈등과 구분짓기 같은 문제들이 없지 않아 있는데, 이와 같은 것은 어떻게 해결하면 좋을지 궁금합니다.\n\n\n팬덤 갈등은 기실 유구한 역사입니다. 여러분들이 아마 보셨을지 모르겠습니다만, &lt;응답하라 1997&gt;과 같은 드라마에서도 묘사되는 바, “HOT” 팬덤과 “잭스키스” 팬덤의 갈등이 실제 사회적 문제로 대두되기도 했습니다. 이는 비단 한국만의 문제가 아니기도 합니다. 외국에서도 팬덤들 간의 충돌이 빈번하게 일어납니다. 아이돌 등과 같은 팝 아티스트 뿐만 아니라, 스포츠 계에서도 일어나는 일이지요. 따라서 쉽게 해결될 수 있는 문제라고 생각되지는 않습니다. 어찌보면 팬심이라는 것이 해결되어서도 안될 일일 것 같기도 하고요. 다만, 그것이 사회적 문제로 확산되지 않도록 경각심을 갖고 모두가 이해하고 관리하는 노력이 필요한 차원으로 접근해야겠습니다.\n\n\n(팬과 스타의 관계)팬덤의 문화는 사실 플라토닉 러브를 뛰어넘어서 유사연애와 어떻게 보면 위험한 심리 현상을 일으킨다고 봅니다. 메타버스를 활용하여 팬들이 본인의 스타에게 접근성이 높아져 더 가까워졌다는 인식을 가지게 되면 스토킹이나 유사연애 같은 안 좋은 심리적 현상이 늘어나지 않을까 걱정이 됩니다. 교수님께서는 메타버스를 팬덤에 적용하는 것에 어떤 부작용이 있을 것이라고 생각하시며, 어느 정도로 메타버스를 활용하는 것이 옳다고 생각하시나요?\n“1. 팬과 연예인의 관계는 복잡하고 다양한 관계인데, 이 관계도 열정적인 지지와 함께 이성과 존중을 유지해야 하며, 팬덤에서 발생하는 극단적인 행동에 대해 회사에서 관리나 도움을 줄 수 있을까요?\n“이번 카리나 이재욱 열애설에 아이돌들의 연애를 격하게 지탄하는 팬들의 모습이 외신으로 보도되면서 국제적으로 한국의 팬덤 문화에 대한 얘기가 나눠졌습니다. ‘돈은 덕후가 쓰고 용서는 머글이 한다.’ 파벌과 ‘진짜 현생을 살아라’ 파로 나눠서 갑론을박이 펼쳐지고 있는데 아이돌의 연애에 대한 팬덤의 반응과 앞으로 팬덤 문화는 어떻게 발전해야 한다고 생각하시는지 궁금합니다.”\n(과격한 팬덤 활동?) 앨범 판매량 부풀리기 등 k팝 팬들의 과격한 행동을 어떻게 보십니까.\n팬덤의 건전성과 수익성은 양립할 수 있나요?\n(팬밍아웃, 부끄러움?) 국내 케이팝 팬들은 팬덤에 속해있는걸 부끄러워 한다. 한국 팬덤 사이에는 일코라는 용어가 있을 정도이다. 하지만 해외에서는 kpop을 좋아하는 것을 부끄러하지 않는다. 이 차이가 있는 이유가 궁금하다\n케이팝 아이돌을 보며 그들이 전하고자 하는 메세지를 알기보다 그들의 멋진 겉면만 보고 좋아하는 사람들도 상당하다. 이는 과연 바람직하지 않은 것이라고 말할 수 있을까?\n\n\nK-Pop을 주도하는 보이그룹과 걸그룹의 과거 형태로 볼 수 있는 비틀즈, 백스트릿 보이즈, 원 디렉션, 스파이스 걸즈 등의 그룹들의 소통 방식을 보면, 과거 적절한 소통의 도구가 부재했습니다. 즉 소셜 미디어와 같은 기술이 없었습니다. 이로 인해 스타와 대중의 거리감을 상당히 했습니다. 거기에서 오는 신비주의 등이 새로운 아티스트의 색깔을 만드는 장점도 있었습니다. 그러나 지금의 기술 문화 시대에는 그런 것 없이, 질문자께서 말씀하신대로 일상에 대한 접근을 넘어 유사 연애, 스토킹과 같은 접촉, 스며듦이 심각한 문제가 되고 있습니다.\nK-Pop은 미디어 기술의 발달에 편승하여 거리감을 좁히며 소통하며 전 지구적인 인기를 누려 왔습니다. 그러나 반대급부로, 오히려 이 때문에 스타들로 하여금 또 다른 노동을 초래하기도 했습니다. 즉 노래하고 춤추는 것 이외에 팬과 소통이라는 것을 강요당하는 것이지요.\n팬덤 문화를 하나로 설명하기란 불가능에 가까우며, 또 과격한 반발심 역시 어제오늘의 일만은 아닙니다. 90년대 후반, 연예 산업 발달과 함께 소위 ’빠순이’라고 불리는 열혈 팬층의 활동이 사회적 문제로 급격히 대두되어 문제시되었습니다. 그들은 저마다의 아이돌에게 열혈 팬심을을 자청하며 파벌을 형성하고, (심지어는 그룹 내 멤버 간 팬 갈등도 있었습니다 - 문희준 간미연 열애설 당시 간미연은 면도칼 등 위협도구를 받기도 했습니다-) 팬덤은 하나의 시대적 현상으로 이해될 필요가 있습니다. 즉 인간이 특정 대상을 신격화고, 심리학적으로 광적으로 따르는 것으로 보아야지, 그것을 K-Pop이라고, 카리나라고, 누구라고 특정할 필요는 없다고 봅니다.\n\n\n케이팝 산업의 현황 및 도전\n\n(앨범 구매의 본질적인 변화) 최근 앨범 구매의 목적이 바뀌었다. 더 이상 자신의 아이돌과 연결하기 위해 애정을 쏟는 것이 아니라 이제는 하나의 유통 화폐가 된 포토카드 구매에 치중하고 있다. 앨범 구매의 본질적인 변화에 대해 교수님은 어떻게 생각하십니까?\n외국어 가사와 외국인 멤버가 해외에서 K-POP이 통하기 위한 필요조건일지 아닐지 궁금합니다.\n최근 에스파 카리나의 열애설로 sm 주가가 하락하는 모습까지 보이는 큰 파장을 일으켰습니다. 이를 해외에서는 한국의 팬덤 문화는 연예인의 사생활까지 관여한다라고 저격하기도 했습니다. 우리나라의 이런 팬덤 문화가 발달한 이유는 무엇일까요? 그리고 앞으로도 이런 자유롭지 못한 팬덤 문화가 지속될까요? 혹여나 해외 특히 미국처럼 자유롭게 변할 가능성은 없을까요?\n\n\n저는 그 본질은 크게 달라지지 않았다고 생각합니다. 그 옛날, 소위 ’덕질’의 선조들이라 불리는 서구 스타워즈, 스타트렉 덕질 하는 사람들도 소장용, 재판매용 앨범, 디스크 등을 하나씩 더 구매하기도 하는 등 다양한 형태로 팬 소비를 했습니다. 낯설지 않기에, 본질적인 변화는 아니라고 생각합니다.\n최근 한국인 없는 K-Pop 그룹이 등장하고, 영어, 중국어, 일본어 등으로 된 노래들을 내놓는 일이 빈번합니다. 이른바 융합(Convergence)가 더욱 가속화되는 모양새입니다. 강의 중에서 말씀드렸듯이, 문화와 기술의 융합뿐만 아니라 문화와 문화 간의 융합도 더욱 강화될 것으로 보입니다. 한국적 문화의 맥락을 지키는 것도 K-Pop의 힘을 오롯이 가꾸는 일이겠지만, 한편으로는 BTS가 다이너마이트, 버터, 퍼미션투댄스 와 같이 다양한 영어 노래로 국제적 성공을 거둔 것처럼 글로벌 시장에서의 성공을 위해 영어 노래 전략을 융화했듯이 융합적 전략을 구사할 필요는 있을 것입니다.\nK-Pop 현상을 BTS를 중심으로 풀어낸 미국의 한 사설(https://www.dw.com/en/whats-behind-the-bts-phenomenon/a-62113315 )에서는, 한류 신화의 몇 가지 특징 중 하나로 ’개인적 문제에 대한 터부시’를 꼽고 있습니다. 이는 비단 K-Pop 만의 문제가 아닙니다. 고 이선균씨의 문제도 마찬가지입니다. 이는 한국의 문화적 배경과 맞닿아 있는 것으로 이해됩니다. 아마 미국처럼 자유롭게 될 가능성은 지극히 낮아 보입니다. 이는 역사적 배경과 민족성에서 분리될 수 없는 배경 때문일 것입니다.\n\n\n\n(NEXT K?) k-pop, k-beauty가 현 한류라고 생각하는데 혹시 다음 k가 붙을 문화는 무엇이라고 예측하시나요?\nK-POP이 세계적으로 확산되면서 한국의 콘텐츠 산업이 겪게 된 도전과 기회는 무엇이 있나요?\nK-pop의 시장 가치가 국내외로 높게 인정받고 있는데, 왜 엔터사의 평균 보수는 (대기업에 준하는 정도로) 높은 편이 아닌지 궁금합니다.\n\n\n한류의 새로운 미래는 FOOD라 생각합니다. 이제 먹거리에 대한 관심이 높아지고 있습니다. 진정으로 음식 문화에 대한 세계인의 관심이 전지구적으로 높아지고 있으며, 냉동 김밥, 만두, 김치 등에 대한 수요가 확산되고 있는 것이 시장에서도 관찰됩니다.\n엔터사의 평균 보수…. 는 계약직의 한계로서 방송사 계약 구조…. 의 불공정성 때문입니다. 이 모든 것들은 선배들의 잘못된 관행과 시장의 불균형성 때문입니다. 여러분들이 조금씩 바꿔나갈 수 있기를 고대해봅니다. 미안합니다. ^^;;\n\n\n디지털 시대의 콘텐츠 소비와 팬덤의 변화\n\n메타버스는 어떤 모양으로 발전하게 될까? 메타버스가 발전을 거듭나면, 언젠가 메타버스가 우리의 일상이 될 수 있을까?\n메타버스를 통한 K-pop의 가능성은 아이돌 그룹의 온라인 플랫폼 활성화 등을 통해 입증되고 있는 것 같습니다. 앞으로 이런 플랫폼을 비롯한 산업 전반이 글로벌적 관점에서 어떻게 변화할지 궁금합니다.\n점점 기술이 접목되고 있는 한류 시장에서 팬들이 이를 진정으로 선호할 수 있을까? 이에 따른 매출 변화에 어떻게 대응할 수 있을까? (ex. 에스파의 컨셉에 불호가 있었음)\n플레이브의 성공요인, 그리고 플레이브와 같은 가상 아이돌이 지속적으로 성공할 수 있을지에 대한 교수님의 인사이트가 궁금합니다.\n\n\n메타버스는 아직 놀이문화 중심으로 크게 자리잡지 못한 느낌입니다. VR/AR 시장이 정착되지 못한 것은 크게 두 가지 이유라고 생각되는데, 하나는 일상에서의 불편함(착용, 사용의 불편함 – 배터리 문제, 무게감, 이질감 등), 다른 하나는 충분한 앱의 부재 두 가지 이유라 사료됩니다. 반면 마이너한 한류 팬덤 시장에서 이를 접목할 수 있을 것인가에서, 반은 회의적이고, 반은 희망적일 수 있습니다. 국내 OTT 시장에서 유일하게 흑자를 내는 것이 바로 애니매이션 OTT 플랫폼입니다. 마이너한 플랫폼 소구층에서 독자적으로 소비층을 확보해낸다면 이를 시장으로 만들어내는 힘이 생기듯, 메타버스 시장도 충분히 확보해낼 수 있을 것입니다.\n플레이브, 저는 그 선조격인 아담… 부터 알던 세대로서, 다소간에 회의적이긴 합니다. 그럼에도 일본에서는 버츄얼 아이돌이 성공하고(최근 도쿄 출장을 다녀왔는데, 거리에서도, 또 모바일로 소비하는 사람들의 모습에서도 충분히 그 시장과 수요를 짐작할 수 있을 정도였습니다) 확장하는 모습을 볼 때 한국에서도 가능성을 점칠 수 있었습니다. 다만 한국에서는 보다 즉물적이고 현실적인 것들을 쫓는 인상이 강합니다. 한국의 감각적인 것들, 강렬한 것들을 찾는 민족성과 맞닿은 것이 아닐까 개인적으로 생각해 봅니다. 따라서 플레이브의 성공 그 다음을 짐작하기란 대단히 힘듭니다만, 그럼에도 보다 감각적이면서 더 즉물적인 무언가를 소구할 수 있는 가상의 아이돌이라면 소비할 수용자가 있지 않을까 짐작해 봅니다.\n\n\nOTT 와 진로\n\nott 플랫폼으로 한국 작품도 많이 국제적인 사랑을 받고 있다고 생각하는데, ott 관련해서 진로를 결정한다면 어떤 준비를 하면 좋을까요?”\n\n\n’한국 OTT 시청자의 OK를 받으면, 전세계에서 OK다’라는 우스갯소리가 있다고 합니다. 그만큼 한국 시장의 시청자 눈이 높습니다. 콘텐츠 큐레이터, 기획자, 플랫폼 중개업, 심지어는 카피라이터와 OTT 포스터 디자이너, OTT 플랫폼 유튜브 콘텐츠 관리자 까지 다양한 2차, 3차 생산자 영역까지 크리에이터 영역이 펼집니다. 컬쳐 앤 테크놀로지 분야의 전공자들이 뛰어들 수 있는 영토가 더욱 넓어지고 있습니다."
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/03_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/03_week.html",
    "title": "음악, K-pop, 엔터테인먼트",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\n수업자료\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n\n\n\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n\n\n\n소속사 없이 음원 유통하는 법\n\n\n\nAI가 만든 앨범 자켓\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/04_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/04_week.html",
    "title": "패션과 뉴테크",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 4: 패션과 뉴테크\n\nDate: 20240326\n수업자료\nPre-class video\n\nHow data is driving the future of fashion | Steve Brown | TED Institute\n\n\n\nThe New Reality of Fashion is Digital | Gala Marija Vrbanic | TED\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/07_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/07_week.html",
    "title": "데이터 시각화의 예술",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 7: 데이터 시각화의 예술\n\nDate: 20240416\n수업자료_1\n수업자료_2\n수업자료_3\n수업자료_4\nPre-class video\n\nThe beauty of data visualization - David McCandless\n\n\n\nData Visualization Best Practices - Stephanie Evergreen\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nBest Discussion\n\n#1\n\n고등학교 때부터 정보라는 과목이 정식으로 생기면서 배운 경험이 있습니다. 그 때 엑셀 활용 방법을 배우면서 데이터 자료를 정리하는 방법을 배웠습니다. 네이터 검색 데이터를 활용한 그래프 만들기를 했었는데, 그 때 네이터 검색 키워드로 데이터가 정렬되는 모습을 보며 현대 사회에서는 사람들을 군집으로 분류하기 편리해졌다고 생각했습니다. 저는 그 때, 우산의 검색량이 가장 높아지는 달은? 이라는 주제로 과제를 진행했고 7, 8월이 가장 압도적일 것이라 예상했는데 결과는 의외로 3월 혹은 6월이 많았습니다. 함께 검색된 키워드 데이터를 통해 봄비 혹은 장마를 대비하기 위해서라는 이유를 얻었습니다. 이처럼 현대 사회에서는 데이터의 시각화를 통해 사회 현상을 읽을 수 있으며 이제는 이를 상업적으로 이용해야지만 살아남을 수 있겠다는 생각을 했습니다.\n또한 영상을 통해 증거의 등급을 매길 수 있다는 점을 새롭게 알게 되었습니다. 사실 저는 데이터 시각화는 사람들의 주관적인 생각을 시각화했다고 생각했습니다. 그래서 많이 나올 수록 무조건 좋은 게 아니라고 생각했습니다. 물론 증거의 등급을 나눈다고 해서 많이 나온 데이터가 무조건 옳고, 좋은 것은 아니지만 좀 더 신뢰를 가질 수 있을 것이라 생각합니다. 앞으로도 데이터 시각화에 대해 좀 더 편리하고 보다 정확한 기술이 나온다면 자료 수집 과정에서 시간을 상당히 줄일 수 있을 것이라 예상합니다.\n\n\n\n데이터 시각화는 영상에서 설명하는 것과 같이 복잡한 데이터를 이해하기 쉽고, 효과적으로 사람들에게 전달하는 수단인 것 같습니다. 데이터를 시각화하는 가장 기초적인 방법은 차트나 그래프 등이 있습니다. 이는 우리가 접하기 가장 쉬운 방법 중 하나인데요, 영상에서 나온 것과 같이 다양한 디자인과 패턴으로도 시각화가 가능하다는 것을 새롭게 알게되었습니다. 작년 2학기 때 구글 코랩으로 데이터를 시각화하는 법을 배웠었는데요, 그 당시에도 그냥 숫자로만 데이터를 보는 것보다 시각화하는 것이 데이터를 습득하는 데 더 많은 도움이 된다고 생각했었습니다. 오늘 영상 시청 후, 아트에 관심이 있는 저는, 앞으로 데이터를 시각화하는 데에 있어 디자인과 패턴 등이 얼마나 더 다양해질 지 궁금해졌습니다.\n\n\n\n저는 데이터 시각화에 대해 더 깊이 이해하게 되었습니다. 데이터 시각화는 단순히 데이터를 제시하는 것이 아니라 예술과 과학의 결합이기도 합니다. 복잡한 데이터를 단순화하고 추세와 패턴을 강조하며 데이터 스토리를 설명하고 탐색을 지원하고 비교를 촉진하여 복잡한 정보를 효과적으로 전달할 수 있습니다. 데이터 시각화의 목적은 데이터를 표시하는 것뿐만 아니라 시각적 효과를 통해 일관된 이야기를 전달하고 청중이 논리적 순서로 주요 통찰력을 이해하도록 안내하는 것입니다. 대화형 시각화는 청중에게 자신의 관심사와 필요에 따라 관련 통찰력을 찾을 수 있는 독립적인 데이터 탐색 기회를 제공합니다. 이 정보 폭발의 시대에 데이터 시각화는 중요한 의사소통 도구가 되었습니다.간결하고 명확하며 적절한 시각화 유형 선택과 데이터 스토리텔링.. 탐색 지원 및 비교 촉진과 같은 모범 사례를 준수함으로써 데이터의 아름다움을 더 잘 발견하고 데이터를 더 깊이 이해하고 활용하며 의사 결정과 행동에 대한 보다 강력한 지원을 제공할 수 있습니다.\n\n\n\n#2\n\n이 영상에서는 데이터 전문가가 사용하는 데이터 시각화 툴에 대해 많이 배우게 되었던 것 같습니다. QuickBooks Online부터 Excel과 Tableau까지 정말 많은 시각화 툴이 발전되고 있으며, 더불어 많은 회사들도 해당 툴을 활용하여 경영을 발전하는 트렌드를 보이고 있는 점이 신선했습니다(Salesforce, Survey Monkey 등). 해외 회사 말고 국내 회사들은 어떤 시각화 툴을 사용하고 있는지 궁금증이 들게되었습니다! 더불어 영상을 보면서 데이터 시각화에 대해 조금 친근함을 느끼게 되었습니다. 어릴 때 자주했던 마인드 맵이나 to do list 작성 등등, 이 모든 것들도 나의 머릿 속 정보들을 시각화하여 정리하는 것이라고 말해주는 작가의 말 한마디 덕에, 멀게만 느껴졌던 데이터 시각화가 생각보다 우리의 삶 가까이에 활용되고 있었음을 새삼 깨닫게 되었습니다!\n\n\n\n데이터 시각화에 인간의 개입이 중요하다고 밝힌 점이 인상깊었다. 결국 시각화 결과물을 받아들이는 자는 인간이기에, 인간이 보기 용이하려면 당연히 인간의 개입이 중요할 수밖에 없으리라는 생각이 든다. 그러나 이렇게 당연한 점이 중요하게 작용하는 이유는, 다양한 기술이 활성화된 이 시대에, 그저 자동화된 프로그램이나 생성형 AI 등 시각화 툴이 그려주는 데이터 형상들을 정제 없이 그대로 받아들인다면 데이터에 대한 주도적인 인사이트를 얻지 못할 가능성이 매우 크기 때문이다. 인간이 스스로 시각화를 할 만한 가치가 있는 데이터의 특성들을 선택할 수 있는 능력을 가지고 있어야 사회에 필요한 요소들을 시각적으로 바로 받아들일 수 있기 때문이다.\n\n\n\n\n\nBest Questions\n1. 데이터 시각화의 한계와 가능성에 대한 깊은 고민\n데이터 시각화를 통해 일어난 일들을 분석하고, 앞으로 일어나는 일에 대해 추측하는부분에 있어 도움을 주는 것은 사실입니다. 하지만 역사를 보면 알 수 있듯이 생각지도 못한 일들이 우리 가운데에 일어나고 변화와 혁신을 주기도 합니다. 이러한 부분에서 데이터 시각화가 오히려 혁신을 불러일으키는 데에 방해를 줄 수도 있을 것이라 생각이 드는데, 어떻게 하면 혁신을 가로막지 않으면서도 데이터 시각화를 통해 영감을 줄 수 있을지 궁금합니다.\n\n2. 데이터 시각화의 목적에 대한 의문 제기로 출발점과 목적을 다시 생각하게 만드는 질문\n사회에서 데이터를 활용하는 것이 일반화되기까지 얼마 되지 않았다고 생각하는데, 데이터 시각화가 시작된 계기가 궁금합니다. 어떤 것에 활용하기 위해 시작되었나요?\n데이터는 사람들에 의해 형성이 된다고 하잖아요. 근데 반대로 데이터에 의해 만들어지는 사회 흐름, 상황으로 인해 사람들이 따라가는 것은 아닌가요? 예를 들어, 어떤 신규 카페의 검색량이 높아져 이 카페가 인기 카페로 분류되고, 사람들이 많이 방문하는 것처럼 말입니다. 만약 아니라고 해도 앞으로도 그러지 않을 것이라고 생각하시나요?\n\n3. 데이터 시각화의 신뢰성에 대한 중요한 이슈\n1. 사전 영상을 보고 나서 데이터 시각화에 대한 회의감이 좀 많이 들었던 것 같습니다. 두 영상의 작가는 모두 우리가 데이터를 통해 얻고자 하는 답과 통찰력만 가지고 있다면, 그것에 맞춰서 데이터를 시각화할 수 있다고 합니다.\n의도적으로 일부 데이터를 시각화 단계에서 제외를 하거나, 왜곡하는 방향으로 시각화 차트를 만들 수도 있어 오히려 의사결정에 혼란을 줄 수 있다면, 데이터는 일부 중요한 의사결정자에게만 공개하는게 낫지 않을까요?\n데이터를 시각화를 하는 것이 진실을 알려주기보다는 많은 사람에게 나의 의견을 뒷받침받기 위한 전략으로 보입니다.모든 사람들에게 시각화된 데이터를 공개를 해야하는 이유는 무엇인가요?\n2. 영상은 해외 데이터 전문가가 강연을 하여서 해외에서 사용하는 시각화 툴을 많이 알려주었다. 국내에서는 어떤 시각화 툴이 유행을 하고 있는지, 시각화를 전문적으로 하는 기업이 있는지, 교수님께서 눈여겨 보는 시각화 트렌드가 있는지 궁금증이 들게되었습니다!"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/09_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/09_week.html",
    "title": "메타버스와 메타휴먼",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 9: 메타버스와 메타휴먼\n\nDate: 20240430\n수업자료\nPre-class video\n\n버추얼 아이돌 그룹 플레이브의 탄생 과정과 기술 이야기 (언리얼 페스트 2023 서울)\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n기술 개발과 적용 당시 선풍적인 인기를 끌었던 실제 사람과 비슷한 모델을 만들고자 하는 것이 아닌 만화 형식을 발전시키는 선택을 했다는 것이 인상 깊었다. 메타버스 기술과 가상인간에 대한 접근이 늘어나면서, 이러한 기술을 만들고 일반 대중에게 공개할 때, 우리가 처음 느끼는 감정은 신기함보다는 실사화에 대한 어색함이라고 생각했다. 평소 이러한 부분에 대해 생각을 갖고 있었는데, 만화 형식을 차용한 부분에서 어색함에 대한 역치를 낮출 수 있는 좋은 시도라는 생각을 할 수 있었던 것 같다. 항상 우리가 새로운 것을 만들어 가는데 있어 앞으로 가져야 할 자세는 기존의 것과 동떨어진 신기한 것을 만들어 내는 것이 아니라, 기존의 것과 잘 융화될 수 있는 자세를 갖추는 것임을 다시금 알게 되었던 것 같다.\n\n\n\n왜 실제 사람도 아닌 모습으로 캐릭터를 구축했을까 의문이 들었습니다. 너무 캐릭터 같아서 애니메이션에 나오는 2D 캐릭터 같았는데, 너무 실사화 된다면 타겟층의 소비자들이 오히려 거부감이 들 수 있다는 것을 고려했다는 점을 새로 알게 되었습니다. 플레이브가 음악방송에 출현하는 모습을 보며 항상 생각했습니다. 촬영 감독 등의 스태프들은 그들의 원래 모습, 본 모습을 볼 수 있는 것 아닌가? 근데 왜 소문이 나지 않을까, 만약 본 모습을 들키게 된다면 어떤 파장을 일으켜올까? 등 너무 궁금했습니다. 이건 아직도 궁금한 점입니다. 최근 런닝맨 프로그램에서도 버추얼 아이돌 경험해보기로 직접 멤버들이 캐릭터를 만들고 실시간 방송을 한 적이 있었습니다. 그걸 보면서 버추얼 캐릭터를 이용해 어떻게 실시간으로 방송하는지 알게 되었다. 이전에는 모델의 모션을 따서 어색한 느낌을 주는 캐릭터만 구현 가능했다면 이제는 실시간으로 어떤 모션을 취하는지에 따라 완벽한 싱크를 보여주며 구현할 수 있다는 점이 AI 기술의 발전이 엄청나게 되었다는 점을 몸소 느끼게 해주었습니다.\n\n\n\nVFX 또는 SFX 등 CG 기술력의 발전에 놀라운 동시에 아이러니하게도 CG 기술력이 비전공자들에게 과대평가되고 있다는 생각도 하게 되었다. 영상에서 언급되었듯, 영화계의 비주얼 이펙트 기술은 대단하지만 후가공의 과정이 불가피하다는 것이다. 또한 플레이브와 같은 버추얼의 영역은 기존 영역과는 다르고 기존의 모션 캡쳐 기술력으로는 표현하기 부족하다는 사실을 알게 되었다. 이러한 사실을 비전공자들은 자세히 알기 어렵기 때문에 과대평가되고 있다고 생각하게 되었다. 그럼에도 블래스트는 언리얼 엔진을 이용한 시네마틱+라이브의 영역에 발을 들였고, 자체 기술을 개발해 성공적인 결과를 얻었다. 특히 높은 기술력으로 기존 버튜버 산업과 차별을 두고 대중성 강한 케이팝 장르와 접목한 점, 모션 캡쳐 과정에서 신체간섭문제나 리타게팅의 한계를 해결하고자 매 프레임마다 백터 계산을 적용한 것이 놀라웠다.\n\n\n\n최근 민희진 프로듀서의 기자회견을 통해 드러난 아이돌 업계의 현실을 알고 나서 다시 사전 영상을 보니, 버추얼 아이돌을 프로듀싱의 측면에서 바라보지 않을 수 없었다. 영상 전반에 걸쳐, 블래스트 관계자는 실시간 모션 캡처의 어려움과 기술적 극복과정을 설명한다. 놀라웠던 점은, 이 프로젝트가 수많은 장애물을 극복할 투자가치가 있다고 생각해 포기하지 않고 개발한 프로듀서의 선구안이었다. 안되는 이유에 비해 해야하는 이유는 너무나도 적었지만, 포기하지 않고 독자적인 기술을 발전한 블래스트팀의 노고가 대단하게 느껴지며, 이런 프로듀서가 성공하는구나 생각하였다!\n하지만 걱정되는 점은, 플래이브의 소속사인 블래스트는 영상에도 나왔듯이 기존에는 시네마틱 라이브 영상을 만들기 위해 설립된 VFX회사이다. 하지만 기술을 발전하고 수요를 파악하다 연예 기획사의 업무까지 겸업을 하게 되었다. 버추얼 아이돌의 경우, 100프로 AI가 활동하는 것이 아닌, AI의 필터를 쓴 인물들이 아이돌로 활동하는 것이다. 아무래도 ’얼굴없는가수’와 비슷한 계열로 활동을 하다보면, 팬층이 두꺼워져도 본인 그 자체를 좋아해주기보단 AI의 모습을 좋아해주는 것은 아닌지, 멘탈 관리가 힘들어질 것 같다. 하지만 소속사는 기술회사의 업무를 중점적으로 하고 있다보니 과연 소속 아이돌들의 멘탈 관리에 신경을 쓸 수 있는지, 아티스트 케어보다는 기술 발전에만 노력을 쏟고 있는 것은 아닌지 프로듀싱의 상황이 많이 궁금해지는 영상이었다.\n\n\n\n플레이브 대한 거부감의 원인을 알아보고, 플레이브가 사람들에게 친근하고 거부감 없게 다가가려면 어떤 식으로 기획하고 마케팅해야할 지 연구해본 적이 있는데, 이런 영상은 컬텍 학도로서는 관련 지식을 얻을 수 있어 정말 도움 되지만, 플레이브 자체를 마케팅하는 데 있어서는 역효과가 날 것이라고 생각했다. 플레이브의 차별점, 셀링 포인트는 ’춤을 추는 인형탈 알바, 서로 싸우는 야구 마스코트’처럼 자기가 쓴 탈에 몰입하고, 자기가 탈을 쓰고 있음을 알고 있음에도 나오는 뻔뻔함이라고 생각했기 때문이다. 멤버들이 나오는 영상을 보면, 상황에 몰입하거나, 그래픽으로 나오는 벌칙 등을 진짜 맞는 것처럼 오버액션 하는 등의 모습을 보이는데 이런 모습이 보는 이로 하여금 친근함을 느낄 수 있게 할 것이라고 생각했기 때문이다.\n\n\n\n디지털 전환이 가속화됨에 따라 메타버스가 미래 생활과 학습의 중요한 부분이 되고 있습니다. 가상 세계와 현실 세계의 융합은 엔터테인먼트 분야뿐만 아니라 교육, 업무 등 다양한 분야에서 새로운 가능성을 열어주고 있습니다. 또한 메타버스 환경에서의 메타휴먼이라는 가상 인물은 독특한 특징을 가지고 있습니다. 메타휴먼은 360도 전방위 모델링과 실시간 상호작용 능력을 갖추고 있으며, 이를 위해서는 3D 모델링, 모션 캡처, 실시간 렌더링 등 첨단 기술의 융합이 필요합니다. 더불어 메타휴먼이 엔터테인먼트 산업에 미치는 영향에 대해서도 생각해 보았습니다. 메타휴먼은 가상 환경에서 실시간 공연을 할 수 있어, 엔터테인먼트 소비 패턴에 새로운 가능성을 열어줄 것입니다. 학생으로서 저 또한 이 분야에서 미래의 발전 가능성을 고려해 보고 있습니다. 마지막으로 메타버스 환경에서는 문화적 차이에 대한 이해가 중요하다는 점을 깨달았습니다. 다양한 문화적 배경을 가진 사용자들의 요구를 존중하고 이해하는 것이 메타버스 애플리케이션의 광범위한 수용과 포용성을 위해 필수적입니다. 전반적으로 메타버스와 메타휴먼에 대한 학습을 통해 미래 발전 동향에 대한 깊이 있는 인식을 갖게 되었습니다. 또한 기술 학습, 산업 적용, 문화 적응 등 다양한 측면에서 새로운 아이디어를 얻게 되었습니다. 이를 바탕으로 학생으로서 메타버스 시대에 보다 잘 준비할 수 있을 것 같습니다.\n\n\n\n\nBest Questions\n\n30분이라는 긴 영상동안 블래스트 관계자가 설명하는 실시간 모션 캡처의 수많은 변수와 어려움, 그리고 이를 기술적 극복 과정을 과정이 정말 인상 깊었습니다. 불필요한 아트 에셋 누적 관리의 어려움, 시네마틱과 라이브 작업의 충돌, FK와 IK의 밸런스 조절 어려움, 후가공 단계의 소멸 등 이 프로젝트가 망할 이유는 수만가지가 있었습니다. 그럼에도 불구하고 블래스트 팀이 이 메타버스와 메타휴먼의 가능성을 믿고 프로젝트를 진행할 수 있었던 마음가짐은 어디서부터 나왔던 것인지, 교수님이 바라보셨을때 메타버스와 메타휴먼이 나아갈 수 있는 가능성은 어디까지인지 궁금합니다! 더 나아가 블래스트 외에 눈여겨 보시는 비슷한 계열의 회사가 있으신지 궁금합니다~\n\n\n\n앞으로 메타버스, 그리고 메타휴먼에 대한 개발에 있어 계속해서 이슈화 될 수 있는 내용은 필연적으로 ’현실성’과 관련이 있다고 생각한다. 1학년 교양 수업으로, ’창의적융합디자인’이라는 수업을 수강했던 경험이 있다. 이 때 당시, 제페토를 구현하며 학교 주변 건물과, 사회 문제의 해결을 위한 아이디어 제시에 활용했던 경험이 있다. 이 당시에 들었던 생각은, ’아직까지 많이 부족하다’는 생각이었다. 나의 기술적 숙련도가 부족한 것이 가장 큰 원인이었겠지만, 기술적으로 제대로 구현되었다 하더라도 인식 체계가 이를 쉽게 받아들이지 못할 것 같은 느낌이었다. 사람들의 인식 체계와 기술 사이의 괴리를 줄이기 위한 방안으로 어색함의 역치를 감소시키는 것과 같은 개념이 아닌, 새로운 것의 창조를 통한 해결 방안은 없을까?\n\n\n\n저는 오랫동안 즐겨한 RPG 게임이 있습니다… (조금 마이너하다고 생각해서… 이름을 써도 잘 모르실 것 같지만 &lt;파이널판타지14&gt;라는 게임입니다…) 게임을 하면서 제 캐릭터의 외관을 보기 좋게 만들고, 재화로 예쁜 옷을 사서 입히고, 전투적인 콘텐츠를 즐기지 않더라도 광장 같은 곳에 캐릭터를 함께 세워 두고 그냥 유저들과 대화를 하기도 합니다. 어떤 사람들은 공간을 꾸미는 ’하우징’이라는 기능을 이용해서 과제를 위한 스터디카페를 열고, 극장을 만들고, 게임 내의 기능으로 합을 맞춰 연주를 선보이기도 하고요… 그냥 예쁘게 만들어진 공간을 걸어다니며 사진 찍는 것을 좋아하는 유저도 있습니다. 그래서 몇년 전, 한창 ’메타버스’라는 개념이 뜨기 시작할 때, 유저들과 우스갯소리로 ’여기가 메타버스다. 여기로 와라.’라는 말을 주고받기도 했습니다. 그래서 하고자 하는 질문은… 메타버스와 RPG게임의 차이가 무엇인가요? 실제로 게임적 요소의 유무 외에는 메타버스와 RPG 게임 사이에 어떤 차이점이 있는지 잘 모르겠습니다. 게다가 메타버스를 표방하는 플랫폼에서는 콘텐츠로서 게임적인 요소를 추가하고 있기도 하고요. 기존에 존재하던 서비스와 효용이 크게 다르지 않다면, 왜 갑자기 메타버스라는 기술이 뜨고 관심을 받게 된 건지도 궁금합니다…"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/11_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/11_week.html",
    "title": "서비스 디자인, 데이터 드리븐 마케팅",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 11: 서비스 디자인, 데이터 드리븐 마케팅\n\nDate: 20240514\n수업자료 1\n수업자료 2\nPre-class video\n\nWhy Data Marketing So Important | 세바시\n\n\n\nAge of Experience, Data+Service | 설상훈 교수님\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n글부터 미디어까지, 형태에 따라 의사소통의 해상도가 높아지며 경험의 전달률이 높아진다는 내용이 흥미로웠다. 동시에 모든 의사소통의 행위자가, 자신의 경험의 전달률을 높이고 싶은 목적으로 의사소통에 임하는 것은 아니라는 것 역시 느꼈다. 실제로 나는 영상 미디어보다 텍스트를 선호하는데, 영상 미디어에서는 그 영상의 기획/제작/공유자의 의도가 너무 직관적으로 다가와서 오히려 수용자로 하여금 거리감을 마련해주지 않다고 느껴지기 때문이다. 의사소통 과정에서 틈을, 이를테면 노이즈를 만들어주는 텍스트가 나에게는 해상도가 낮을 수는 있어도 의사소통을 더 편하게 해주는 형태인 것이다. 그래서 앞으로 데이터를 통한 경험 전달률을 무조건 극대화하는 게 아닌, 더 높은 해상도를 원하면서 동시에 수용할 수 있는 상대와 분야에 한정해 이뤄져야 하지 않을까 생각했다.\n\n\n\n현재 ‘우리는 인공지능에 의존하게 될 것이다.’ 혹은 ’인공지능이 인간을 대체할 것이다’라는 우려와 두려움에 조금씩 잠식되어 가는 중인 것 같습니다. 하지만, 영상에서도 언급하고 있는 내용처럼 인간이 할 수 있는 일, 인간이 더 잘할 수 있는 일인 “해석”과 그 결과로 새로운 통찰을 찾는 것이 있습니다. 역사적으로 인간과 비인간을 구분함에 인간은 항상 자신들만이 할 수 있는 것만을 찾아왔습니다. 인공지능이 발달한다고 해서 인간이 설 수 있는 자리가 없는 것은 아닐 것입니다. 인공지능을 활용해서 다만 우리가 현재로서는 느끼기 어려운 또 다른 인간의 장점들이 드러나고, 그것을 발전시키는 방향으로 변화할 것이라고 생각합니다. 양극단에 있는 데이터 역시도 간과해서는 안된다는 내용이 가장 기억에 남습니다. 특히, 경험에는 좋고 나쁜 것이 없으며 그것을 나눌 수도 없다는 부분이 인상깊었습니다. 비슷한 맥락에서 몇 주전 “자연어처리”를 주제로 수업을 진행했을 때, 장애인의 목소리가 되어주는 AI 기술과 관련된 영상이 제시되었는데, 이 역시도 일반적으로 우리가 지나칠 수 있는 이들의 경험, 그리고 이들의 경험을 더 나은 방향으로 발전시킬 수 있도록 하는 것이 또 하나의 기회로 작용할 수 있으며, 데이터는 이런 측면에서 우리가 쉽게 지나칠 수 있는 부분을 알려주는 수단이라고 생각합니다.\n\n\n\n데이터 기반 의사결정이 중요하다고 들었는데, 늘 ’어떤 데이터’를 가지고 ’어떤 가설’을 만들어야하는지 궁금했었다. 영상에서 예시로 나온 ’예측 정교화를 위한 여러 가설들’을 보니, 어쩌면 당연한 이야기들이었다. (ex. 긍정적인 바이럴과 판매량 비례하여 증가) 해당 가설을 검증하기 위해 필요한 데이터, 이 데이터를 수집하기 위한 키워드를 차례로 붙여보니 ’어떤 데이터’들이 필요한지 쉽게 와닿았다. 가설과 검증이라고 해서 처음부터 새롭고 신박하며 복잡하지만, 논리적으로 타당해야한다고 생각했는데, 창의력은 검증 과정이 아니라 검증 결과를 통해 마케팅 아이디어를 도출하는데에 필요하다는 것을 깨달았다. 경험데이터 정규분포표에 나타냈을 때, 양쪽 끝에 해당되는 이상치를 배제하지 않음으로서 아직 오지 않은 트렌드를 예측할 수 있다는 것을 알게 되었다. 이 데이터들을 입체적으로 분석해보면 지금 유행하고 있는 데이터와 아닌 것으로 구분할 수 있고, 메인 데이터를 제거하면 가치있는 데이터를 사용할 수 있게 된다. 결국 낮은 신뢰도의 데이터 구간 중에서도 신뢰도가 더 낮은 데이터들이 서비스 디자인에서는 유효하게 작용할 수 있다는 것을 알았다.\n\n\n\n강연에서 제시된 구체적인 사례를 보고 마케팅에서 데이터를 어떻게 활용할 수 있는지, 데이터가 어떤 효과를 가져올 수 있는지를 이해할 수 있었습니다. 그리고 사례를 보니 좋은 마케팅은 어느 산업 분야에서나 중요 요소라는 것이 새삼 느껴지는데, 추상적으로 존재했던 소비자들의 니즈나 평가를 데이터 수집을 통해 정량적으로 확인할 경우, 마케팅 계획이 더 효율적으로 이루어질 수 있을 것 같습니다. 그리고 모든 사람들이 데이터 마케팅의 중요함을 알고 채택할 경우, 더욱 중요해지는 것은 데이터 수집의 영역보다 해석 방식이 될 것이라는 점도 깨달았습니다. 경험을 전달하는 데에 있어서 어떤 방식이 의사소통의 해상도가 높은지 순서대로 나타낸 부분이 인상 깊었습니다. 가장 해상도가 높은 경험을 실현하는 방법으로는 지금까지 프로토 타입과 메타버스가 있다는 것을 알게 되었고, ’앞으로 그 이상 어떤 방법이 있을 것인가’에 대한 질문에 대해 깊게 생각해보게 되었습니다.\n\n\n\n\nBest Questions\n\n(그래서 어떤 데이터가 필요할까요?) 데이터 기반 접근이 마케팅 전략에 큰 영향을 주고 있다는 사실은 인지하고 있는데, 질문하고 싶은 것은 서비스 디자인과 데이터 드리븐 마케팅을 효과적으로 통합하기 위해서는 어떤 종류의 데이터가 필요한지 궁금합니다. 그리고 이러한 데이터를 수집하고 분석하여 서비스 디자인에 어떻게 반영할 수 있는지도 궁금합니다.\n\n\n\n(비주류 데이터를 바라보는 관점) 데이터 베이스의 마케팅에 초점을 두게 되면 두번째 영상에서 언급된 양 끝의 데이터는 점점 더 소외될 것이다. 데이터에 의해 주된 정보, 유행중인 문화는 더욱 부각되고 마케팅 됨으로써 더더욱 주류가 될 것이다. 하지만 이러한 현상이 지속된다면 문화가 단면적이게 되지 않을까라는 의문을 갖게 되었다. 실제로 세계적으로 문화 획일화가 기하급수적인 속도로 진행되고 있다.\n\n\n\n(크롤링의 어려움) 최근 구글을 비롯한 주요 웹사이트에서 서드 파티 쿠키를 제한함으로서 사용자들의 정보를 모으기가 점점 더 어려워진다고 생각합니다. 이러한 상황에 대해 어떻게 보시고, 데이터 드리븐 마케팅의 방식이 어떠한 방식으로 발전할 것이라거 보시는 지 궁금합니다."
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/13_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/13_week.html",
    "title": "융합콘텐츠와 창업",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 13: 융합콘텐츠와 창업\n\nDate: 20240528\nPre-class video\n\nGPT 있는데 왜 또 만들어요? 손쉽게 이해하는 AI 개발 트렌드와 비하인드\n\n\n\n세상을 바꾸는 뉴콘텐츠\n\n\n\n콘텐츠 창업에서 가장 중요한 것 | CEO 노정석\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n\n나는 이 영상을 보기 전까지, AI를 창업에 가져온다는 생각은 너무나도 큰 자본과 기술력이 필요할 것이라는 편견에 휩싸여 생각조차 안했었다. 하지만 수직적 AI라는 개념을 알게 되면서, AI시장에서도 틈새시장이 존재한다는 점을 알게 되었다. 특정 산업 분야에서 문제를 해결하기 위해 AI를 활용하거나 창작하는 것은 생각보다 도전할만하며, 특히 여러 수직적 회사 중 런웨이(runway)라는 회사가 가장 인상깊었다.\n\n\n\n\n\nVR과 AR과 MR의 명확한 개념과 실제 적용 사례를 비교하며 의미를 이해할 수 있었다. 그중에서도 흥미로운 것은 VR툰이었다. 드라마 기획자를 꿈꾸는 입장에서, 드라마라는 콘텐츠 역시 VR, AR, MR과 같은 기술요소를 접목시키는 시기가 올 수 있다고 생각하는데, 웹툰 시장에서는 이미 그런 시도가 있다는 점에서 큰 인사이트가 되었다. 웹툰은 스토리를 이미지 단위로 전달하고, VR은 이미지를 실감나게 구현하는 기술이라는 점에서 콘텐츠 특성과 기술 특성이 잘 접목된 것 같다. 다만 드라마는 여러 이미지들이 연결된 장면이 스토리 단위라서, 동일한 접근으로 기술 접목을 하면 부자연스러울 수 있을 것 같다. 드라마 콘텐츠 기획자로서, 드라마의 특성과 기술의 특성을 모두 잘 이해해야 앞으로 드라마 시장에서의 새로운 콘텐츠를 상상하고 실제로 구현해낼 수 있을 것이라 생각한다. 때문에 기술 분야에 대한 심도 깊은 이해가 결국 앞으로의 콘텐츠 기획과 창작에 인사이트가 될 것임을 다시 인지할 수 있었던 영상이었다.\n“꾸준히 하나를 하고 싶어도, 큰 거 하나를 이루지 못했기에 끊임없이 도전하는 것이다.” 연쇄 창업가를 하는 이유에 대해 솔직하게 발언하신 게 인상적이다. 창업은 돈을 벌어야 하는 것과는 별개로, 계속 상상해내고 상상해낸 것을 구현해내고 구현해낸 것의 한계를 부딪혀야 하고, 그 한계를 계속 버텨내야 하고, 버텨내면서 극복해내야 하는 과정의 연속인 것 같다. 설령 극복이 안 되어도 사명감과 진정성으로 계속 견뎌내야 한다. 나 역시 이전에 청년협동조합 법인을 세우고, 사업자로서 2년 가량 활동하면서 나에 대한 발견을 할 수 있었다. 나는 스스로 창의적이며 창조하는 것을 좋아하는 사람인 줄 알았지만, 그것을 업으로 지속할 만큼의 집념과 진정성이 부족한 사람이었다. 이처럼 창업은 즐거움보다는 고통을 더 자주 느끼게 되는 과정임은 분명하지만, 그만큼 무엇이든 ‘보게 하는’ 기회임도 분명하다. 나 자신에 대해, 또 내가 관심 있던 주제에 대해, 나의 가치관에 대해, 폭넓게는 정말 사회에 대해서도. 뭐든 그전에는 보지 못했던 것을 보게 한다는 점에서, 영상의 발언자와는 다르게 대학생 시절 창업을 도전하는 것을 권장해보고 싶다.\n\n\n\n\n\n거대언어모델(LLM)이 중요한 것을 알고 있었지만, 이 영상을 통해 왜 중요한지를 좀 더 구체적으로 알게 되었다. 어떤 분야의 인공지능이던 결국 인간과 소통하려면 인간의 언어를 이해해서 아웃풋을 만들어내거나, 인공지능이 알아낸 정보를 자연어로 표현해야 하기 때문에 인공지능의 코어에 해당하는 것을 이해할 수 있었다.\nAI의 발전으로 인해 일자리가 줄어든다는 부정적인 예측들도 있고, 그것이 일정 부분 맞는 예측이긴 하지만 한 편으로는 개인들이 쉽고 저렴하게 더 퀄리티 있는 콘텐츠나 작업을 할 수 있도록 하는 것도 사실인 것 같다. NC의 LLM을 개발하신 분께서 마지막에 말씀하신 것처럼 인공지능을 활용해서 더 고차원적인 활동을 어떤 식으로 할 수 있을지 상상하고 고민하는 것이 중요한 것 같다. 구글과 같이 큰 회사에 자신의 회사를 매각하며 큰 성공을 거두었으면서도 끊임없이 창업을 하시는 노정석 CEO님의 열정이 가장 인상깊었다. 사람들은 사업을 단순히 돈벌이 수단으로 여기곤 하지만 하지만 노대표님의 경우 수익 창출 그 이상을 것을 위해 창업을 하시는다는 것을 그 짧은 영상을 통해서도 알 수 있었다. “이건 확실히 올 미래이고, 내가 하지 않으면 아무도 하지 않을 것 같아”라는 정신이 있어야 한다는 말씀을 하셨다. 이러한 생각을 해야 창업을 할 수 있을 것 같고, 이런 생각을 하려면 전문적인 지식과 더불어 시장을 읽는 능력, 보통 사람들이 알아차리지 못하는 문제를 인식하는 통찰력 등이 필요할 것 같다.\n\n\n\n\n\nBest Questions\n\nAI 기술이 콘텐츠 창작을 어느정도 자동화하고 창의력을 보조하는 도구로 활용될 수 있다는 점이 인상적이었습니다. 이러한 AI 기술을 이용한 콘텐츠 창작이 창업 초기 단계에서 어떤 이점을 제공할 수 있을까요? 또한, 창업을 준비하는 사람들이 AI 기술을 효과적으로 활용하기 위해 고려해야 할 중요한 요소는 무엇인가요?\n\n\n\n창업 시 자신이 하는 일이 성공할 것이라고 확신할 수 있는 방법은 무엇인가요?창업을 준비하는 과정에서 가장 큰 불확실성은 바로 성공 가능성입니다. 많은 시간과 자원을 투자하기 전에, 그 사업 아이디어가 성공할 가능성이 높은지 평가할 수 있는 방법을 알고 싶습니다. 이는 창업자들이 더 나은 의사결정을 하고 리스크를 최소화하는 데 도움이 될 것입니다. 성공 여부를 미리 판단할 수 있는 지표나 방법이 있다면, 창업 과정에서 자신감을 가질 수 있을 것입니다."
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html#intro",
    "href": "teaching/cul_tech_101/weekly/index.html#intro",
    "title": "Weekly Content",
    "section": "Intro",
    "text": "Intro\n\n\nWeek 1: Course Intro\n\nDate: 20240305\nClass: Course Introduction\n\nAn overview of the course"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html#part-i-콘텐츠-기획과-제작",
    "href": "teaching/cul_tech_101/weekly/index.html#part-i-콘텐츠-기획과-제작",
    "title": "Weekly Content",
    "section": "Part I: 콘텐츠 기획과 제작",
    "text": "Part I: 콘텐츠 기획과 제작\n\n\nWeek 2: 융합 콘텐츠 기획과 제작\n\nDate: 20240312\nPre-class video\n\n덕후에서 콘텐츠 크리에이터가 되는 방법 | 대도서관\n좋은 컨텐츠를 만드는 법 | 에이틴 제작사\nWhy AI Will Spark Exponential Economic Growth | Cathie Wood | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 강혜원 교수(SKKU)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html#part-ii-문화와-문화-산업",
    "href": "teaching/cul_tech_101/weekly/index.html#part-ii-문화와-문화-산업",
    "title": "Weekly Content",
    "section": "Part II: 문화와 문화 산업",
    "text": "Part II: 문화와 문화 산업\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n소속사 없이 음원 유통하는 법\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 정헌섭 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 4: 패션과 뉴테크\n\nDate: 20240326\nPre-class video\n\nHow data is driving the future of fashion | Steve Brown | TED Institute\nThe New Reality of Fashion is Digital | Gala Marija Vrbanic | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 류현석 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 5: 게임 & 인터랙티브 디자인\n\nDate: 20240402\nPre-class video\n\n게임 과몰입, 게임 잘못일까요? | 한덕현 중앙대학교 병원 정신건강의학과 교수 | | 세바시\nDiscussion\n\nClass\n\nGuest Lecturer: 김수완 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 6: 한류와 팬덤\n\nDate: 20240409\nPre-class video\n\nAll I Really Need To Know, I Learned From KPOP | Donald Lim | TEDxYouth@SJCS\nHas K-pop prepared us for the metaverse? | Alex Karlsson | TEDxBrunkebergstorg\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이종명 교수(SKKU)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html#part-iii-문화-기술",
    "href": "teaching/cul_tech_101/weekly/index.html#part-iii-문화-기술",
    "title": "Weekly Content",
    "section": "Part III: 문화 기술",
    "text": "Part III: 문화 기술\n\n\nWeek 7: 데이터 시각화의 예술\n\nDate: 20240416\nPre-class video\n\nThe beauty of data visualization - David McCandless\nData Visualization Best Practices - Stephanie Evergreen\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 전서연 강사\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 8: 문화콘텐츠와 자연어 처리\n\nDate: 20240423\nPre-class video\n\nText 데이터를 활용한 개인화 음악 추천서비스 웹 데모 구축 (혁신성장청년인재양성사업 프로젝트 발표)\nNLP Project - Emotion In Text Classifier App with Streamlit and Python\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 구영은 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 9: 메타버스와 메타휴먼\n\nDate: 20240430\nPre-class video\n\n버추얼 아이돌 그룹 플레이브의 탄생 과정과 기술 이야기 (언리얼 페스트 2023 서울)\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 원종서 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 10: 인터랙션 사이언스, UX\n\nDate: 20240507\nPre-class video\n\nHuman-Computer Interaction and User Interface Design | Tony Tang\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이대호 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 11: 서비스 디자인, 데이터 드리븐 마케팅\n\nDate: 20240514\nPre-class video\n\nWhy Data Marketing So Important | 세바시\nAge of Experience, Data+Service | 설상훈 교수님\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 설상훈 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 12: 가상/증강현실 콘텐츠 제작\n\nDate: 20240521\nPre-class video\n\n상상이 현실화가 되는 세계, 그 중심에 서서 외치다 | 서동일 오큘러스VR코리아 지사장\nWhat is extended reality? | The Gadget Show\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 김태원 대표(RGB Makers)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html#part-iv-문화-콘텐츠-경영",
    "href": "teaching/cul_tech_101/weekly/index.html#part-iv-문화-콘텐츠-경영",
    "title": "Weekly Content",
    "section": "Part IV: 문화 콘텐츠 경영",
    "text": "Part IV: 문화 콘텐츠 경영\n\n\nWeek 13: 융합콘텐츠와 창업\n\nDate: 20240528\nPre-class video\n\nGPT 있는데 왜 또 만들어요? 손쉽게 이해하는 AI 개발 트렌드와 비하인드\n세상을 바꾸는 뉴콘텐츠\n콘텐츠 창업에서 가장 중요한 것 | CEO 노정석\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 윤영훈 대표 (주)ASSI\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 14: 엔터테인먼트 경영\n\nDate: 20240604\nPre-class video\n\nHow Netflix changed entertainment – and where it’s headed | Reed Hastings\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이동찬 경영총괄 (TEO Universe)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly/index.html#conclusion",
    "href": "teaching/cul_tech_101/weekly/index.html#conclusion",
    "title": "Weekly Content",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "teaching/cul_tech_101/weekly_2/posts/15_week.html",
    "href": "teaching/cul_tech_101/weekly_2/posts/15_week.html",
    "title": "기말시험",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\n\n\n\nConclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "blogs/posts/15_skku_bp.html",
    "href": "blogs/posts/15_skku_bp.html",
    "title": "학생들이 주는 상의 가치",
    "section": "",
    "text": "세상에서 가장 큰 상을 받았다.\n바로 학생들이 주는 상인 Best Professor.\n영상의 인터뷰에서 했던 말들이 다 진심이었던만큼,\n초심을 잃지 않도록 기록."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#kaggle-competitions-the-heartbeat-of-innovation",
    "href": "teaching/ds101/weekly/posts/02_week.html#kaggle-competitions-the-heartbeat-of-innovation",
    "title": "Basic Syntax (1)",
    "section": "Kaggle Competitions: The Heartbeat of Innovation",
    "text": "Kaggle Competitions: The Heartbeat of Innovation\nAt its core, Kaggle is synonymous with its competitions. These challenges, sponsored by organizations ranging from small startups to tech giants, present real-world problems that require innovative data science solutions. Competitors from around the globe vie for prestige, cash prizes, and sometimes even job offers by developing the most accurate models. These competitions cover a broad spectrum of topics, from predictive modeling and analytics to deep learning and computer vision.\nThe competitive environment not only spurs innovation but also provides participants with a tangible way to benchmark their skills against a global talent pool. For beginners, Kaggle competitions offer a structured learning path. They can start with “Getting Started” competitions, which are designed for educational purposes and ease learners into the world of data science competitions.\nCheckout Kaggle"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#datasets-a-treasure-trove-for-exploration-and-learning",
    "href": "teaching/ds101/weekly/posts/02_week.html#datasets-a-treasure-trove-for-exploration-and-learning",
    "title": "Basic Syntax (1)",
    "section": "Datasets: A Treasure Trove for Exploration and Learning",
    "text": "Datasets: A Treasure Trove for Exploration and Learning\nKaggle’s dataset repository is a goldmine for data scientists seeking to experiment with different types of data or to undertake new projects. With thousands of datasets available, covering everything from economics and health to video games and sports, Kaggle makes it easy for users to find data that matches their interests or research needs.\nThese datasets are not only free to access but also come with community insights, kernels (code notebooks), and discussions that help users understand the data better and how to apply various analysis techniques effectively."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#kaggle-notebooks-collaborate-learn-and-share",
    "href": "teaching/ds101/weekly/posts/02_week.html#kaggle-notebooks-collaborate-learn-and-share",
    "title": "Basic Syntax (1)",
    "section": "Kaggle Notebooks: Collaborate, Learn, and Share",
    "text": "Kaggle Notebooks: Collaborate, Learn, and Share\nKaggle Notebooks (formerly known as Kernels) provide an integrated cloud computational environment where users can write, run, and share Python and R code. These notebooks support seamless data analysis without the need for local setup, making it easier for learners and practitioners to jump straight into coding.\nThe collaborative aspect of Kaggle Notebooks fosters learning and knowledge sharing. Users can build upon others’ work, which accelerates the learning process and encourages innovation. This feature is particularly useful for beginners who can learn from real-world examples and for experienced data scientists looking to showcase their expertise."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#community-the-pulse-of-kaggle",
    "href": "teaching/ds101/weekly/posts/02_week.html#community-the-pulse-of-kaggle",
    "title": "Basic Syntax (1)",
    "section": "Community: The Pulse of Kaggle",
    "text": "Community: The Pulse of Kaggle\nWhat truly sets Kaggle apart is its community. With millions of users, Kaggle’s forums and discussions are a hub for knowledge exchange, networking, and support. Whether you’re looking for advice on how to improve your model, seeking partners for a competition, or curious about the latest trends in data science, the Kaggle community is there to support you."
  },
  {
    "objectID": "teaching/ds101/weekly/posts/02_week.html#korean-version-of-kaggle-dacon",
    "href": "teaching/ds101/weekly/posts/02_week.html#korean-version-of-kaggle-dacon",
    "title": "Basic Syntax (1)",
    "section": "Korean version of Kaggle: DACON",
    "text": "Korean version of Kaggle: DACON\nCheckout DACON"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/05_week.html#format",
    "href": "teaching/ds101/weekly/posts/05_week.html#format",
    "title": "Data Manipulation",
    "section": "Format",
    "text": "Format\nA data frame with 153 observations on 6 variables.\n\n\n\n[,1]\nOzone\nnumeric\nOzone (ppb)\n\n\n[,2]\nSolar.R\nnumeric\nSolar R (lang)\n\n\n[,3]\nWind\nnumeric\nWind (mph)\n\n\n[,4]\nTemp\nnumeric\nTemperature (degrees F)\n\n\n[,5]\nMonth\nnumeric\nMonth (1–12)"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/05_week.html#details",
    "href": "teaching/ds101/weekly/posts/05_week.html#details",
    "title": "Data Manipulation",
    "section": "Details",
    "text": "Details\nDaily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.\n\nOzone: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island\nSolar.R: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park\nWind: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport\nTemp: Maximum daily temperature in degrees Fahrenheit at La Guardia Airport.\n\n\npairs(airquality, panel = panel.smooth, main = \"airquality data\")\n\n\n\n\n\nWhat are the column names of the data frame?\n*Hint: names()\n\nWhat are the row names of the data frame?\n*Hint: rownames()\n\nExtract the first 10 rows in ‘airquality’\n*Hint: One way: by using [] / Alternative way: by using head()\n\nCheck the number of rows (observations) in the dataset\n*Hint: length() or nrow()\n\nHow many missing values are in the ‘Ozone’ column ?\n*Hint: is.na() and sum()\n\nWhat is the mean value of the ‘Ozone’ column? (Exclude missing values (coded as NA) from this calculation)\n*Hint: Use mean() with an option na.rm = T\n\nExtract the subset where Ozone values are above 31 and Temp values are above 90.\n*Hint: Use subset()\n\nUse the apply function to calculate the standard deviation of each column in the data frame\n*Hint: Use apply() with an option na.rm = T / Use sd() function\n\nCalculate the mean value of ‘Ozone’ for each Month in the data frame and create a vector containing the monthly means (exclude all missing values)\n*Hint: Use tapply() with an option na.rm = T\n\nDraw a random sample of 5 rows from the data frame\n*Hint: Use sample(), nrow(), airquality[]"
  },
  {
    "objectID": "blogs/posts/2_about_a_package.html",
    "href": "blogs/posts/2_about_a_package.html",
    "title": "What is a Package in R",
    "section": "",
    "text": "R is a popular programming language for data analysis and statistical computing. One of the most powerful features of R is the availability of packages, which are collections of functions, data sets, and documentation that extend the functionality of the base R system. In this blog post, we will discuss what packages are in R, why they are so important, and provide some examples of how to use them in your code.\nA package in R is essentially a bundle of R code, data, and documentation that is designed to perform a specific task or set of tasks. Packages are created by individuals, organizations, and communities of developers who want to share their work with others. Some packages are developed for general use, while others are more specialized, designed to address specific problems or perform specific analyses.\nThe R community has created thousands of packages that cover a wide range of topics and applications. Some of the most popular packages in R include ggplot2 for data visualization, dplyr for data manipulation, and tidyr for data cleaning. These packages provide users with a wealth of tools and resources that they can use to perform complex analyses and visualizations with ease.\nOne of the most important benefits of packages in R is that they are easy to install and use. With just a few simple commands, users can download and install the packages they need, and then access the functions, data sets, and documentation that come with them. Here’s an example of how to install and load the ggplot2 package in R:\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nOnce you have installed and loaded the ggplot2 package, you can start using its functions to create beautiful data visualizations. Here’s a simple example of how to use the ggplot function to create a scatterplot:\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\nAnother benefit of packages in R is that they are highly customizable. Users can modify the code in packages to suit their specific needs, or even create their own packages from scratch. This allows users to tailor their work to their specific requirements and work more efficiently.\nIn conclusion, packages in R are essential tools for data analysis and statistical computing. They provide users with a wealth of resources that they can use to perform complex analyses and visualizations with ease, and they are easy to install and use. Whether you are a beginner or an experienced R programmer, packages are a must-have resource for your work. With the right packages, you can accomplish more in less time and get better results from your data."
  },
  {
    "objectID": "blogs/posts/13_kku_special.html",
    "href": "blogs/posts/13_kku_special.html",
    "title": "데이터 사이언스, 인공지능, 그리고 진로",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Courses\"&gt;Courses&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:All Courses\"&gt;All Courses&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/index.html\"&gt;/teaching/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:CNT101\"&gt;CNT101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/cul_tech_101/index.html\"&gt;/teaching/cul_tech_101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:CNT102\"&gt;CNT102&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/cul_tech/index.html\"&gt;/teaching/cul_tech/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:DS101\"&gt;DS101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/ds101/index.html\"&gt;/teaching/ds101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:ML101\"&gt;ML101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/ml101/index.html\"&gt;/teaching/ml101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Research\"&gt;Research&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Publications\"&gt;Publications&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/research/index.html\"&gt;/research/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Work in progress\"&gt;Work in progress&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/research/working.html\"&gt;/research/working.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Proj\"&gt;Proj&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/proj/index.html\"&gt;/proj/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Blogs\"&gt;Blogs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/blogs/index.html\"&gt;/blogs/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:LAB\"&gt;LAB&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://ctcl.netlify.app\"&gt;https://ctcl.netlify.app&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;CJL &amp; Lab - 데이터 사이언스, 인공지능, 그리고 진로&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;CJL &amp; Lab - 데이터 사이언스, 인공지능, 그리고 진로&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;CJL &amp; Lab - 데이터 사이언스, 인공지능, 그리고 진로&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;2023 건국대학교 데이터사이언스 특강&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;2023 건국대학교 데이터사이언스 특강&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &lt;ul class=\"footer-items list-unstyled\"&gt;\n    &lt;li class=\"nav-item compact\"&gt;\n    &lt;a class=\"nav-link\" href=\"https://github.com/ChangjunChrisLee/\"&gt;\n      &lt;i \n  class=\"bi bi-github\" \n  role=\"img\" \n&gt;\n&lt;/i&gt; \n    &lt;/a&gt;\n  &lt;/li&gt;  \n    &lt;li class=\"nav-item compact\"&gt;\n    &lt;a class=\"nav-link\" href=\"https://twitter.com/Dr_CJLee\"&gt;\n      &lt;i \n  class=\"bi bi-twitter\" \n  role=\"img\" \n&gt;\n&lt;/i&gt; \n    &lt;/a&gt;\n  &lt;/li&gt;  \n&lt;/ul&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "blogs/posts/11_dt_ecosystem.html",
    "href": "blogs/posts/11_dt_ecosystem.html",
    "title": "디지털전환 스타트업 생태계",
    "section": "",
    "text": "Slide"
  },
  {
    "objectID": "blogs/posts/10_topic_model.html",
    "href": "blogs/posts/10_topic_model.html",
    "title": "Topic Modeling in R",
    "section": "",
    "text": "Introduction:\nTopic modeling is an unsupervised machine learning technique used to discover latent structures within a large collection of documents or text data. This technique is particularly useful for exploring, organizing, and understanding vast text corpora. One popular method for topic modeling is Latent Dirichlet Allocation (LDA), which is a generative probabilistic model that assumes a mixture of topics over documents and words within topics. In this blog post, we will delve into the details of LDA and demonstrate how to perform topic modeling in R using the ‘topicmodels’ package.\nLatent Dirichlet Allocation (LDA):\nLDA is based on the idea that documents are mixtures of topics, where each topic is a probability distribution over a fixed vocabulary. The generative process for LDA can be summarized as follows:\n\nFor each topic k, sample a word distribution \\(φ_k\\) ~ Dir(β).\nFor each document d, sample a topic distribution \\(θ_d\\) ~ Dir(α).\nFor each word w in document d, sample a topic \\(z_d\\), w ~ Multinomial(\\(θ_d\\)), then sample the word \\(w_d\\), n ~ Multinomial(\\(φ_z\\)).\n\nHere, Dir(α) and Dir(β) denote Dirichlet distributions with parameters α and β, respectively. α and β are hyperparameters controlling the shape of the distributions. The main goal of LDA is to infer the latent topic structures θ and φ by observing the documents.\n\n\nPerforming LDA in R:\nWe will use the ‘topicmodels’ package in R to perform LDA. First, let’s install and load the necessary packages:\n\n# install.packages(\"topicmodels\")\n# install.packages(\"tm\")\nlibrary(topicmodels)\nlibrary(tm)\n\nLoading required package: NLP\n\n\nNow, let’s preprocess our text data using the ‘tm’ package. For this example, we will use the ‘AssociatedPress’ dataset, which is available within the ‘topicmodels’ package:\n\ndata(\"AssociatedPress\")\ndtm &lt;- AssociatedPress\nhead(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 6, terms: 10473)&gt;&gt;\nNon-/sparse entries: 1045/61793\nSparsity           : 98%\nMaximal term length: 18\nWeighting          : term frequency (tf)\n\n\nNow, we are ready to fit the LDA model using the ‘LDA’ function from the ‘topicmodels’ package. We will specify the number of topics (K) and the hyperparameters α and β:\n\nK &lt;- 10 # Number of topics\nalpha &lt;- 50/K\nbeta &lt;- 0.1\nlda_model &lt;- LDA(dtm, K, method = \"Gibbs\", control = list(alpha = alpha, delta = beta, iter = 1000, verbose = 50))\n\nK = 10; V = 10473; M = 2246\nSampling 1000 iterations!\nIteration 50 ...\nIteration 100 ...\nIteration 150 ...\nIteration 200 ...\nIteration 250 ...\nIteration 300 ...\nIteration 350 ...\nIteration 400 ...\nIteration 450 ...\nIteration 500 ...\nIteration 550 ...\nIteration 600 ...\nIteration 650 ...\nIteration 700 ...\nIteration 750 ...\nIteration 800 ...\nIteration 850 ...\nIteration 900 ...\nIteration 950 ...\nIteration 1000 ...\nGibbs sampling completed!\n\n\nHere, we use the Gibbs sampling method to estimate the LDA parameters with 1000 iterations. The ‘verbose’ option is set to 50, which means that the progress will be displayed every 50 iterations.\nOnce the model is fitted, we can extract the topic-word and document-topic distributions using the ‘posterior’ function:\n\nposterior_lda &lt;- posterior(lda_model)\nsummary(posterior_lda)\n\n       Length Class  Mode   \nterms  104730 -none- numeric\ntopics  22460 -none- numeric\n\n\nTo visualize the results, we can display the top words for each topic:\n\ntop_words &lt;- 10\ntop_terms &lt;- terms(lda_model, top_words)\n\nfor (k in 1:K) {\n  cat(\"Topic\", k, \":\", paste(top_terms[k,], collapse = \" \"), \"\\n\")\n}\n\nTopic 1 : new program court bush police i united million government percent \nTopic 2 : school report case house two people states company party market \nTopic 3 : city health attorney president people dont soviet billion south year \nTopic 4 : york state office dukakis killed like west year political prices \nTopic 5 : students system judge campaign miles just east new president million \nTopic 6 : show medical law committee officials think american workers soviet new \nTopic 7 : year officials state congress three get world business people higher \nTopic 8 : high department charges bill fire day foreign corp national rose \nTopic 9 : john study federal senate four time war money country oil \nTopic 10 : last problems trial new spokesman going military inc communist dollar \n\n\nThis will output the top 10 words for each of the 10 topics in our LDA model.\n\n\nConclusion:\nIn this blog post, we introduced the Latent Dirichlet Allocation (LDA) method for topic modeling and demonstrated"
  },
  {
    "objectID": "blogs/posts/12_media_panel_2023.html",
    "href": "blogs/posts/12_media_panel_2023.html",
    "title": "메타버스, 게임화, 선택 모형",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Courses\"&gt;Courses&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:All Courses\"&gt;All Courses&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/index.html\"&gt;/teaching/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:CNT101\"&gt;CNT101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/cul_tech_101/index.html\"&gt;/teaching/cul_tech_101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:CNT102\"&gt;CNT102&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/cul_tech/index.html\"&gt;/teaching/cul_tech/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:DS101\"&gt;DS101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/ds101/index.html\"&gt;/teaching/ds101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:ML101\"&gt;ML101&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/teaching/ml101/index.html\"&gt;/teaching/ml101/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Research\"&gt;Research&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Publications\"&gt;Publications&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/research/index.html\"&gt;/research/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Work in progress\"&gt;Work in progress&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/research/working.html\"&gt;/research/working.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Proj\"&gt;Proj&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/proj/index.html\"&gt;/proj/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Blogs\"&gt;Blogs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/blogs/index.html\"&gt;/blogs/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:LAB\"&gt;LAB&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://ctcl.netlify.app\"&gt;https://ctcl.netlify.app&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;CJL &amp; Lab - 메타버스, 게임화, 선택 모형&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;CJL &amp; Lab - 메타버스, 게임화, 선택 모형&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;CJL &amp; Lab - 메타버스, 게임화, 선택 모형&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;CJL &amp; Lab&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;2023 한국미디어패널 학술대회&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;2023 한국미디어패널 학술대회&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &lt;ul class=\"footer-items list-unstyled\"&gt;\n    &lt;li class=\"nav-item compact\"&gt;\n    &lt;a class=\"nav-link\" href=\"https://github.com/ChangjunChrisLee/\"&gt;\n      &lt;i \n  class=\"bi bi-github\" \n  role=\"img\" \n&gt;\n&lt;/i&gt; \n    &lt;/a&gt;\n  &lt;/li&gt;  \n    &lt;li class=\"nav-item compact\"&gt;\n    &lt;a class=\"nav-link\" href=\"https://twitter.com/Dr_CJLee\"&gt;\n      &lt;i \n  class=\"bi bi-twitter\" \n  role=\"img\" \n&gt;\n&lt;/i&gt; \n    &lt;/a&gt;\n  &lt;/li&gt;  \n&lt;/ul&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "blogs/posts/1_first_post.html",
    "href": "blogs/posts/1_first_post.html",
    "title": "Quarto, the game changer",
    "section": "",
    "text": "R Markdown을 처음 접하고 “와 멋지다..!” 했었는데 이제는 Quarto 가 나와 Data communication 영역을 평정해버렸다. Quarto의 등장은 기술 블로그, 특히 데이터 사이언스를 업으로 삼고 있는 사람들에게는 큰 축복이 아닐 수 없다. 기록이 중요한 세상에서 손쉽게 데이터 작업의 흔적들을 남기고 포스팅 할 수 있으니 말이다.\n포스팅의 의미는 두 가지이다.\n\n하나는 내가 한 작업들을 잊지 않기 위한 포스팅,\n두 번째는 다른 사람들에게 보여주기 위한 포스팅\n\n둘 다 중요하지만 사실 첫 번째가 조금 더 중요하다. 기록용이라면 옵시디언이 더 좋지 않나? 라고 생각할 수 있는데 반은 맞고 반은 틀리다. 생각을 정리하고 엮어 나가는데 옵시디언이 가장 좋은 노트앱이이지만 결국엔 내 생각을 누구에게 보여준다는 생각으로 정리하지 않으면 잘 쌓이지 않는다는 단점이 있기 때문이다.\nQuarto의 가장 큰 장점은 아래와 같이 R이나 파이썬 또는 그 어떤 코드도 자유롭게 임베딩 할 수 있다는 것.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmtcars %&gt;% head\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n임베딩할 수 있을 뿐만 아니라 결과도 같이 레포팅해준다.\n\nmtcars %&gt;% \n  mutate(cyl = as.factor(cyl)) %&gt;% \n  ggplot(aes(x = wt, y = mpg)) + \n  geom_point(aes(color = cyl, size = qsec), \n             alpha = 0.5) +\n  scale_color_manual(values = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n  scale_size(range = c(0.5, 12))  # Adjust the range of points size\n\n\n\n\n정말 멋진 세상이 아닐 수 없다.\n이 홈페이지도 Quarto webpage 를 참고해서 아주 쉽게 만들 수 있었다. Shiny가 data communication 에 interactive 한 새로운 차원을 열었다면 Quarto 는 디지털 퍼블리싱의 높은 문턱을 한참 아래로 낮춘 혁신이라고 말할 수 있다.\n몸 담고 있는 한양대학교 정보사회미디어학과의 슬로건은 “데이터로 세상을 읽고 콘텐츠로 주장하는 지능정보사회의 저널리스트” 이다. 이 슬로건을 실행하기 위해서 앞으로는 R 과 Python과 같은 데이터 사이언스 언어와 분석 역량 뿐만 아니라 Shiny, Quarto와 같은 디지털 문해력을 기를 수 있는 교육이 보완되어야 하겠다."
  },
  {
    "objectID": "blogs/posts/3_regular_expression_part_1.html",
    "href": "blogs/posts/3_regular_expression_part_1.html",
    "title": "What are Regular Expressions and How to Use Them in R",
    "section": "",
    "text": "Regular expressions are a powerful tool for searching, matching, and manipulating text. They are a type of pattern language that can be used to describe sets of character strings, such as words or sentences, in a concise and flexible way. In this blog post, we will introduce what regular expressions are, why they are useful, and how to use them in R.\nA regular expression is a sequence of characters that define a search pattern. The pattern can be used to match (and sometimes replace) strings, or to perform some other manipulation of strings. Regular expressions are commonly used for searching for patterns in text data, such as extracting specific information from a document, validating input data, or matching and replacing specific characters.\nOne of the most powerful features of regular expressions is their ability to specify patterns using a compact and flexible syntax. For example, the regular expression a.b will match any string that contains an a followed by any single character followed by a b. The dot (.) is a special character that matches any single character. Similarly, the regular expression [abc] will match any string that contains one of the characters a, b, or c.\nIn R, there are several functions for working with regular expressions, including grep, grepl, sub, and gsub. These functions allow you to search for, match, and replace patterns in strings. For example, the grep function can be used to search for patterns in a vector of strings:\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^b\", text)\n\n[1] 2\n\n\nThis code uses the grep function to search for strings in the text vector that start with the letter b. The ^ symbol is a special character that matches the start of a string. The result of this code will be a vector of the indices of the strings in the text vector that match the pattern:\nThe grepl function is similar to grep, but returns a logical vector indicating which elements of the input vector match the pattern, rather than the indices:\n\ngrepl(\"^b\", text)\n\n[1] FALSE  TRUE FALSE\n\n\nThe sub and gsub functions can be used to replace matches in a string:\n\nsub(\"b\", \"B\", text)\n\n[1] \"apple\"  \"Banana\" \"cherry\"\n\ngsub(\"b\", \"B\", text)\n\n[1] \"apple\"  \"Banana\" \"cherry\"\n\n\nRegular expressions are a powerful tool for searching, matching, and manipulating text data. They are used to describe patterns in text data in a concise and flexible way, and can be used for a variety of purposes, such as extracting specific information from a document, validating input data, or matching and replacing specific characters. With the grep, grepl, sub, and gsub functions in R, you can easily search for and manipulate patterns in text data.\nOne of the key features of regular expressions is the ability to use special characters to define patterns. Here are some of the most commonly used special characters in regular expressions:\n\n[] (brackets): Matches any single character that is contained within the brackets. For example, [abc] matches any string that contains one of the characters a, b, or c.\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^[abc]\", text)\n\n[1] 1 2 3\n\n\nThis code uses the grep function to search for strings in the text vector that start with the characters a, b, or c. The result of this code will be a vector of the indices of the strings in the text vector that match the pattern:\n\n[^] (negated brackets): Matches any single character that is not contained within the brackets. For example, [^abc] matches any string that contains any character other than a, b, or c.\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^[^abc]\", text)\n\ninteger(0)\n\n\n\n^ (caret): Matches the start of a string. For example, ^b matches any string that starts with the letter b.\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^b\", text)\n\n[1] 2\n\n\n\n$ (dollar sign ): Matches the end of a string. For example, b$ matches any string that ends with the letter b.\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"b$\", text)\n\ninteger(0)\n\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"e$\", text)\n\n[1] 1\n\n\n\n+(plus sign): Matches one or more occurrences of the preceding character or group. For example, a+ matches any string that contains one or more occurrences of the letter a.\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"a+\", text)\n\n[1] 1 2\n\n\n\n? (question mark): Matches zero or one occurrence of the preceding character or group. For example, a? matches any string that contains zero or one occurrences of the letter a.\n\n\ntext &lt;- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"a?\", text)\n\n[1] 1 2 3\n\n\nIf you want to learn more about regular expressions in R, check out this video tutorial: https://www.youtube.com/watch?v=q8SzNKib5-4. The video provides a comprehensive explanation of regular expressions and how to use them in R, with practical examples and hands-on exercises. Whether you’re a beginner or an advanced user, this video is a great resource to deepen your understanding of regular expressions and their applications in R.\n한글로 된 설명은 안도현 교수님의 eBook 참고: https://bookdown.org/ahn_media/bookdown-demo/cleantool.html#%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9Dregular-expressions"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html#submit-your-answers",
    "href": "teaching/ds101/weekly/posts/10_week.html#submit-your-answers",
    "title": "QZ",
    "section": "Submit your answers",
    "text": "Submit your answers\nSolve the problem and submit your answer by entering it in the Google form at the link below.\nhttps://forms.gle/aiEqgtjGqCuY5Emx6"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html#qz-content",
    "href": "teaching/ds101/weekly/posts/10_week.html#qz-content",
    "title": "QZ",
    "section": "QZ content",
    "text": "QZ content\nhttps://changjunchrislee.github.io/ds101_qz/"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/10_week.html#qz",
    "href": "teaching/ds101/weekly/posts/10_week.html#qz",
    "title": "QZ",
    "section": "QZ",
    "text": "QZ\n\n\nPART I. Basic Syntax in R\nPlease download the data below\nqz_a.RData\nLocate “qz_a.RData” at your working directory and load the file\n\nload(\"data/qz_a.RData\")\n\nThen you will see the object “KMP_list_qz_a” in the Environment window of RStudio.\nOK, then let’s begin the qz.\n\n\nList\n\nHow many elements are in the list?\n\n\n\n5\n6\n7\n8\n\n\n\nWhat is the name of the second element of the list?\n\n\n\nvector\nmatrix\np20\nNone\n\n\n\nWhat is the data type of the first element in the list?\n\n\n\nvector\nmatrix\ndata.frame (tibble)\nlist\n\n\n\nWhich of the following statements correctly creates the sixth element of the list as an empty list?\n\n\n\nKMP_list_qz_a[[6]] &lt;- list(0)\nKMP_list_qz_a[6] &lt;- list()\nKMP_list_qz_a[[6]] &lt;- empty_list(0)\nKMP_list_qz_a[6] &lt;- list(0)\n\n\n\nWhich of the following correctly describes the creation of a vector {2,4,6,8,10} using the seq function and placing it as the first element of the sixth element of a list?\n\n\n\nKMP_list_qz_a[[6]][[1]] &lt;- seq(2,10,2)\nKMP_list_qz_a[[6]][1] &lt;- seq(2,10,2)\nKMP_list_qz_a[[6]][1] &lt;- c(2,4,6,8,10)\nKMP_list_qz_a[6][[1]] &lt;- seq(2,10,2)\n\n\n\nWhich of the following options correctly interprets the meaning of the code KMP_list_qz_a[[6]][[1]][4]?\n\n\n\nAccess the fourth element of the first element of the sixth element of the list KMP_list_qz_a.\nAssign the value 4 to the fourth element of the first element of the sixth element of the list KMP_list_qz_a.\nRetrieve the fourth element of the first sub-list within the sixth element of the list KMP_list_qz_a.\nRemove the fourth element from the first sub-list within the sixth element of the list KMP_list_qz_a.\n\n\n\n\nVector\nLet a vector X be like below\n\nX &lt;- KMP_list_qz_a[[3]]\n\n\nHow many missing valued in vector X?\n\n\n\n2\n3\n4\n5\n\n\n\nWhat is the position of value 41 in X? (Note: it’s not about the value of X[41])\n\n\n\n33\n43\n53\n63\n\n\n\nWhich of the following expressions correctly represents the number of elements in vector X where the values are not missing and are less than 27?\n\n\n\nlength(X[X &lt; 27 & !is.na(X)])\nlength(X[X &lt; 27 | !is.na(X)])\nlength(X[X &gt;= 27 & !is.na(X)])\nlength(X[X &gt;= 27 | !is.na(X)])\n\n\n\n\nMatrix\nLet a matrix M be like below\n\nM &lt;- KMP_list_qz_a[[4]]\n\n\n\nWhich of the following statements is true regarding the expression M[3,3], t(M)[3,3], and M[9]?\n\n\n\nM[3,3] is equivalent to t(M)[3,3] but different from M[9].\nM[3,3] is equivalent to M[9] but different from t(M)[3,3].\nM[3,3] is different from both t(M)[3,3] and M[9].\nM[3,3], t(M)[3,3], and M[9] are all equivalent.\n\n\n\nWhich of the following correctly describes the output of the given code snippets?\n\n\n# 1\napply(M, 2, mean)\n\n[1]  2  5  8 11 14\n\n# 2\napply(M, 1, mean)\n\n[1] 7 8 9\n\n\n\nThe first snippet calculates the average value of each column in matrix M, while the second snippet calculates the average value of each row.\nThe first snippet calculates the average value of each row in matrix M, while the second snippet calculates the average value of each column.\nBoth snippets calculate the average value of each element in matrix\n\n\n\nThe snippets produce errors because the apply function does not support averaging operations\n\n\n\n\nData.frame\n\nLet DF1 and DF2 as like below\n\nDF1 &lt;- KMP_list_qz_a[[1]]\nDF2 &lt;- KMP_list_qz_a[[2]]\n\n\nHow many variables in DF1?\n\n\n\n8\n10\n12\n14\n\n\n\nHow many people (observations) in DF2?\n\n\n\n40\n50\n60\n70\n\n\n\nIn DF1, how many people are in the category of 3000-4000K of the income?\n\n\n\n5\n6\n7\n8\n\n\n\nIn DF1, how many “Male” & “Never married” people, and how many SKT (Telecom) users among them? (Use the variable: telecom)\n\n\n\n5, 1\n5, 3\n3, 1\n3, 3\n\n\n\nHow many people who used an LG smartphone in 2019 (in DF1) switched to a Samsung smartphone in 2020 (in DF2)?\n\n\n\n3\n5\n7\n9\n\n\n\nOf those who used an LG smartphone in 2019, how many people still use LG in 2020?\n\n\n\nNone of them\n2\n4\n6\n\n\nThe code processes a text containing lyrics of “Let It Be” by The Beatles, removing newline characters and commas using the gsub function, then creates a word cloud visualization to display the frequency of words in the song.\n\nletitbe &lt;- c(\"When I find myself in times of trouble, Mother Mary comes to me\n  Speaking words of wisdom, let it be\n  And in my hour of darkness she is standing right in front of me\n  Speaking words of wisdom, let it be\n  Let it be, let it be, let it be, let it be\n  Whisper words of wisdom, let it be\n  And when the broken hearted people living in the world agree\n  There will be an answer, let it be\n  For though they may be parted, there is still a chance that they will see\n  There will be an answer, let it be\n  Let it be, let it be, let it be, let it be\n  There will be an answer, let it be\n  Let it be, let it be, let it be, let it be\n  Whisper words of wisdom, let it be\n  Let it be, let it be, let it be, let it be\n  Whisper words of wisdom, let it be, be\n  And when the night is cloudy there is still a light that shines on me\n  Shinin' until tomorrow, let it be\n  I wake up to the sound of music, Mother Mary comes to me\n  Speaking words of wisdom, let it be\n  And let it be, let it be, let it be, let it be\n  Whisper words of wisdom, let it be\n  And let it be, let it be, let it be, let it be\n  Whisper words of wisdom, let it be\")\n\nletitbe_rm &lt;- gsub(\"\\n\", \"\",letitbe)\nletitbe_rm &lt;- gsub(\",\", \"\",letitbe_rm)\n\nword_list &lt;- strsplit(letitbe_rm, split = \" \")\nword_vec &lt;- unlist(word_list)\n\nword_count &lt;- sort(table(word_vec), decreasing = T)\ndf_word_count &lt;- data.frame(word_count)\n\nlibrary(wordcloud2)\nwordcloud2(df_word_count)\n\n\n\n\n\n\n\nWhat is the purpose of using the gsub function in the provided code snippet?\n\n\n\nIt converts the text into a word vector for further analysis.\nIt prepares the text data by removing unwanted characters like newline characters and commas.\nIt calculates the frequency of each word in the text.\nIt generates a word cloud visualization based on the frequency of words in the text.\n\n\n\n\n\nPART II. Data Wrangling with tidyverse\n\nDownload the gapminder package for using gapminder dataset.\n\n# install.packages(\"gapminder\")\n\nImport required libraries\n\nlibrary(gapminder)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe code works like below:\n\nfilters only 1962 data from gapminder data,\nselects only country and gdpPercap variables,\nrenames gdpPercap variables to gdp_1962,\nand assigns it to object ‘gap_1962’.\n\n\n\nChoose the one that fits the blank (a), (b), (c)\n\nA) (a) filter (b) select (c) rename\nB) (a) subset (b) keep (c) relabel\nC) (a) arrange (b) pick (c) change_name\nD) (a) extract (b) choose (c) modify\n\nThe code below did the same procedure with above but for the year 2007. (a), (b), (c) are the same with the answers above.\n\nThe code below finds the difference in gdpPercap from 1962 to 2007 and sorts them in descending order of growth (gdp_gap).\n\n\nChoose the one that fits the blank (a), (b), (c)\n\nA) (a) inner_join (b) transform (c) setNames\nB) (a) left_join (b) mutate (c) desc\nC) (a) with (b) modify_if (c) -\nD) (a) left_join (b) apply (c) recast\n\n\nIn the result above, what is the rank of “Korea, Rep.” in terms of gdp_gap?\n\nA) 9\nB) 11\nC) 13\nD) 16\n\n\nUsing the codes above, create a table for population growth from 1962 to 2007, and choose the country with the highest population growth during the period.\n\nA) China\nB) India\nC) Indonesia\nD) Brazil\n\n\n\nPART III. Data Visualization\n\nThe following is the code to draw the graph below using the gdp_gap_1962_2007 table created above.\n\n\n\nFill in the blanks.\n\nA) (a) gdp_gap (b) country (c) identity\nB) (a) country (b) year (c) “dodge”\nC) (a) country (b) gdp_gap (c) “identity”\nD) (a) gdp_gap (b) country (c) “identity”\n\nLet’s use ‘mtcars’ dataset\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nThe value obtained by dividing horsepower (hp) by weight (wt) is called marginal horsepower (hp_by_wt). Choose the car with the largest hp_by_wt among the cars in the mtcars data.\n\nA) Lotus Europa\nB) Ford Pantera\nC) Camaro Z28\nD) Maserati Bora\n\n\nExecute the code below to check the graph, and choose the relationship between the car’s weight (wt) and fuel efficiency (mpg).\n\n\nmtcars %&gt;% \n  ggplot(aes(x=wt, y=mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\nA) positive relationship\nB) negative relationship\nC) no correlation\nD) We cannot tell with this graph\n\n\nExecute the code below to check the correlation between variables, and choose a variable relationship that is not a positive relationship.\n\n\nmtcars %&gt;% \n  select(mpg, disp, hp, drat, wt) %&gt;% \n  plot\n\n\n\n\n\n\n\n\nA) disp ~ hp\nB) wt ~ hp\nC) drat ~ mpg\nD) mpg ~ disp\n\n\n\nPART IV. Advanced\n\nThe code below expresses the correlation between variables in another way.\n\nWhich of the following is not suitable as an advantage compared to the previous graph?\n\n\n# install.packages(\"corrplot\")\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nmtcars %&gt;% \n  select(mpg, disp, hp, drat, wt) %&gt;% \n  cor %&gt;% \n  corrplot(method=\"circle\")\n\n\n\n\n\n\n\n\nA) Easy to find whether the relationship of two variables is positive or negative\nB) It can be understood at a glance\nC) More colorful\nD) It can check each observation in the graph\n\nThe code below shows the distribution of petal length variables in iris data using geom_density. Run the code to check the graph and answer the questions.\n\niris %&gt;% \n  ggplot(aes(x=Petal.Length, colour=Species, fill=Species)) +\n  geom_density(alpha=.3) +\n  geom_vline(aes(xintercept=mean(Petal.Length),  \n                 colour=Species),\n             linetype=\"dashed\", color=\"grey\", size=1)+\n  xlab(\"Petal Length (cm)\") +  \n  ylab(\"Density\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nIf by chance you found an iris and the petal length was 1.5 cm, what is the species of this iris?\n\nA) Setosa\nB) Versicolor\nC) Virginica\n\nThe graph below is an interactive graph with color classification for each species in the scatter plot for the relationship between Petal Length and Petal Width.\n\nHover your mouse cursor over a dot on the graph and Choose the answer which is not an advantage of this type of graph.\n\n\n# install.packages(\"plotly\")\nlibrary(plotly)\n\niris %&gt;% \n  plot_ly(\n    x = ~Petal.Length, \n    y = ~Petal.Width,\n    color = ~Species,  \n    type = \"scatter\", \n    mode = \"markers\") %&gt;%  \n  layout(scene = list(xaxis = list(title = 'Petal length'),\n                      yaxis = list(title = 'Petal width')))   \n\nA) Information on the specific observation can be easily obtained\nB) You can zoom in by specifying the range\nC) You can expand the scale with the mouse\nD) You can freely change the X-axis’s and Y-axis’s variables\n\n\nModify the graph code on the left as the graph on the right.\n\n\n\n\n\n\n\n\n\n\n\n# Graph in the left side\niris %&gt;% \n  ggplot(aes(x=Species, y=Petal.Length)) +\n  geom_boxplot() + \n  geom_jitter()+\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\nModify the graph code on the left as the graph on the right.\n\n\n\n\n\n\n\n\n\n\n\n# Graph in the left side\nadd_text &lt;- data.frame(Species=\"setosa\", \n                       Petal.Length=6,\n                       lab=\"Wonderful!\")\n\niris %&gt;% \n  ggplot(aes(x=Species, y=Petal.Length)) +\n  geom_boxplot() +\n  geom_text(data=add_text, aes(label=lab))+\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\nModify the graph code on the top as the graph on the bottom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Graph on the top\ngapminder %&gt;% \n  filter(country %in% c(\"Korea, Rep.\", \"Korea, Dem. Rep.\")) %&gt;% \n  ggplot(aes(x=gdpPercap/1000, y=lifeExp, col=country)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/ds101/weekly/posts/12_week.html#class",
    "href": "teaching/ds101/weekly/posts/12_week.html#class",
    "title": "Interactive Web: Shiny",
    "section": "Class",
    "text": "Class\n\nIntroduction to R Shiny\n\nWhat is R Shiny?\nUse cases and applications.\nBasic structure of a Shiny app.\nBuilding Your First Shiny App\n\n\n\nSetting up the environment.\n\nCreating the user interface (UI).\nWriting the server function.\nRunning the app.\n\n\n\nAdding Reactivity\n\nUnderstanding reactivity in Shiny.\nUsing reactive expressions.\nCreating reactive outputs.\n\n\n\nHands-On Practice\n\n\n1. Introduction to R Shiny\n\nWhat is R Shiny?\n\nR Shiny is an R package that makes it easy to build interactive web applications (apps) straight from R.\nA web application framework for R.\nAllows building interactive web apps directly from R.\n\nUse cases and applications:\n\nData visualization dashboards.\nInteractive data analysis tools.\nPrototyping and demonstrating models.\n\nSee an example!\n\nhttps://shiny.posit.co/r/gallery/start-simple/kmeans-example/\n\nBasic structure of a Shiny app:\n\nui.R: Defines the user interface.\nserver.R: Contains the server logic.\napp.R: Combines UI and server into a single file.\n\n\n\n\n\n\n2. Building Your First Shiny App\n\nSetting up the environment:\n\nInstall Shiny: install.packages(\"shiny\")\nLoad Shiny: library(shiny)\n\nData in use\n\nThe faithful Dataset\n\nDescription: The faithful dataset is a built-in dataset in R that contains observations from the Old Faithful geyser in Yellowstone National Park, USA. It is commonly used for teaching and demonstrating data analysis techniques because it has a straightforward structure and interesting patterns.\nDataset Structure:\n\nThe dataset consists of 272 observations on 2 variables.\nThe variables are:\n\neruptions: The duration of the geyser eruptions in minutes.\nwaiting: The waiting time in minutes between eruptions.\n\n\nWe will display the histogram of Old Faithful’s waiting time varying the number of bins\n\n\nCreating the user interface (UI):\n\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\",\n                  \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n\nElements\n\nfluidPage(): Creates a fluid page layout that adjusts its elements based on the size of the browser window. This makes the app responsive and ensures it looks good on different screen sizes.\ntitlePanel(): Adds a title to the app, typically displayed at the top of the page. It gives users an idea of what the app is about.\nsidebarLayout(): Creates a layout with a sidebar and a main panel. The sidebar is usually used for inputs, and the main panel is used for outputs like plots or tables.\nsidebarPanel(): Defines the content of the sidebar. This is where you place input controls like sliders, dropdowns, and buttons.\nsliderInput(): Creates a slider input control that allows users to select a numeric value by dragging a handle along a track.\nmainPanel(): Defines the main content area where outputs are displayed. This is where you place elements like plots, tables, and text outputs.\nplotOutput(): Creates an area for rendering a plot output. The actual plot is generated by the server function and displayed in this area.\n\nWriting the server function:\n\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  })\n}\n\n\nElements\n\nserver: Defines the server logic.\ninput: Contains input values from the UI.\noutput: Contains output objects to be sent to the UI.\nrenderPlot(): Renders a plot based on the inputs.\n\n\n\n\nRunning the app:\n\n\n# shinyApp(ui = ui, server = server)\n\n\nElements\n\nshinyApp(): Combines the UI and server components to create a Shiny app.\nui: The user interface component.\nserver: The server logic component.\n\n\n\n\n3. Adding Reactivity\n\nUnderstanding reactivity in Shiny:\n\nReactive inputs and outputs.\nReactive expressions: reactive()\n\nUsing reactive expressions:\n\n\nserver &lt;- function(input, output) {\n  \n    reactive_hist_data &lt;- reactive({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    list(x = x, bins = bins)\n  })\n  \n  output$distPlot &lt;- renderPlot({\n    hist_data &lt;- reactive_hist_data()\n    hist(hist_data$x, breaks = hist_data$bins, col = 'darkgray', border = 'white')\n  })\n  \n}\n\n\nIn this simple example, the result of the histogram plot might look the same whether you use a reactive expression or not. The benefit of using a reactive expression becomes more apparent in more complex applications where you want to reuse the reactive computation in multiple places or when you have more complex dependencies.\nLet’s clarify the purpose of using a reactive expression and demonstrate a more nuanced example where its use is beneficial.\n\n\n\nWhy Use reactive?\n\nReusability:\n\nIf you need the computed data in multiple places within your server logic, using a reactive expression prevents the need to duplicate code.\n\nEfficiency:\n\nReactive expressions cache their results. If the inputs haven’t changed, Shiny can use the cached result instead of re-evaluating the expression.\n\nComplex Dependencies:\n\nIn more complex apps with multiple reactive inputs and outputs, managing dependencies and ensuring that everything updates correctly can be challenging. Reactive expressions help by encapsulating these dependencies clearly.\n\n\n\n\n\nMore Advanced Example\nLet’s consider a more advanced example where we compute multiple reactive outputs based on the same reactive data.\n\n\nExample: Interactive Histogram with Summary Statistics\nIn this example, we will:\n\nCreate a histogram based on user input.\nDisplay summary statistics (mean, median, and standard deviation) that update based on the user’s bin selection.\n\n\nlibrary(shiny)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Histogram with Summary Statistics\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\"),\n      textOutput(\"summaryStats\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  \n  # Reactive expression to compute histogram data\n  reactive_hist_data &lt;- reactive({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    list(x = x, bins = bins)\n  })\n  \n  # Render histogram plot\n  output$distPlot &lt;- renderPlot({\n    hist_data &lt;- reactive_hist_data()\n    hist(hist_data$x, breaks = hist_data$bins, col = 'darkgray', border = 'white',\n         main = \"Histogram of Waiting Times\",\n         xlab = \"Waiting Time (minutes)\")\n  })\n  \n  # Render summary statistics\n  output$summaryStats &lt;- renderText({\n    hist_data &lt;- reactive_hist_data()\n    x &lt;- hist_data$x\n    mean_val &lt;- mean(x)\n    median_val &lt;- median(x)\n    sd_val &lt;- sd(x)\n    paste(\"Mean: \", round(mean_val, 2), \n          \"Median: \", round(median_val, 2), \n          \"SD: \", round(sd_val, 2))\n  })\n}\n\n# Run the application\n\n# shinyApp(ui = ui, server = server)\n\n\n\n\n4. Hands-On Practice\n\nStep-by-step guided coding of a simple Shiny app:\n\nStart with a basic template:\n\n\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Basic Shiny App\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  })\n}\n\n# shinyApp(ui = ui, server = server)\n\n\nAdding features to the app:\n\nAdding another input and output.\n\n\n\nui &lt;- fluidPage(\n  titlePanel(\"Enhanced Shiny App\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30),\n      selectInput(\"color\", \"Choose color:\", choices = c(\"darkgray\", \"blue\", \"green\"))\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x, breaks = bins, col = input$color, border = 'white')\n  })\n}\n\n\n# shinyApp(ui = ui, server = server)\n\n\nWe use selectInput()here.\nThere are many buttons and widgets you can use in Shiny\n\nSee here: https://shiny.posit.co/r/gallery/widgets/widget-gallery/\n\n\n\n\n\n\n\nAdvanced one (for practice)\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Simple Data Analyzer\"),\n  sidebarLayout(\n    sidebarPanel(\n      helpText(\"Enter numeric data separated by commas (e.g., 1, 2, 3, 4, 5)\"),\n      textInput(\"dataInput\", \"Enter data:\", value = \"\"),\n      actionButton(\"submit\", \"Submit\"),\n      hr(),\n      helpText(\"Output Plot and Statistics\")\n    ),\n    mainPanel(\n      plotOutput(\"dataPlot\"),\n      verbatimTextOutput(\"dataSummary\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  data &lt;- reactive({\n    req(input$submit)  # Wait until the submit button is clicked\n    as.numeric(unlist(strsplit(input$dataInput, \",\")))\n  })\n  \n  output$dataPlot &lt;- renderPlot({\n    req(data())  # Ensure data is available\n    ggplot(data = data.frame(x = data()), aes(x = x)) +\n      geom_histogram(binwidth = 1, fill = \"blue\", color = \"white\") +\n      theme_minimal() +\n      labs(title = \"Histogram of Data\", x = \"Data Values\", y = \"Count\")\n  })\n  \n  output$dataSummary &lt;- renderPrint({\n    req(data())  # Ensure data is available\n    summary &lt;- summary(data())\n    c(\"Data Summary:\", paste(\"Mean =\", mean(data(), na.rm = TRUE)),\n      paste(\"Variance =\", var(data(), na.rm = TRUE)),\n      paste(\"N =\", length(data())))\n  })\n}\n\n# Run the application \n\n# shinyApp(ui = ui, server = server)\n\n\nExplanation:\n\nTextInput: Users can enter their data as comma-separated values.\nActionButton: A button that users click to submit their data. The server logic processes the data only after the button is clicked.\nReactive expression: data is a reactive expression that only updates when the submit button is clicked and the input data changes.\nPlotOutput and TextOutput: These display the plot and data summary, respectively.\n\nWhy Use req()?\n\nUsing req() is important for preventing errors and ensuring that your Shiny app behaves correctly. For example, you might want to ensure that some input data is available before generating a plot or a table. If the required data is not available, req() will prevent the subsequent code from running, which could otherwise lead to errors or unintended behavior.\n\nUse ChatGPT or other genAI to modify this shiny app:\n\nAdd things like below:\n\nA widget to choose the color of the bars in the histogram.\nA boxplot of the selected dataset’s variable.\n\n\n\n\n\n\n\nFurther Study (Now, it’s your turn!)\nTo deepen your understanding and improve your skills with R Shiny, here are some additional resources that will be helpful.\n\nGallery of Shiny Apps\nExplore Examples and Benchmark Your Code:\n\nShiny Gallery: The official Shiny Gallery by RStudio showcases a variety of Shiny apps, ranging from simple to complex. You can view the source code for each app and use it as a benchmark for your own projects.\n\nShiny Gallery\n\nShiny Demos\n\nShiny Demo Gallery\n\n\n\n\nRecommended Textbooks\nBooks for In-Depth Learning:\n\nMastering Shiny by Hadley Wickham\n\nA comprehensive guide to building interactive web applications with R Shiny. The book covers the basics as well as advanced topics, including reactivity, UI layout, and deploying Shiny apps.\nMastering Shiny - O’Reilly\n\nR Shiny by Example by Jonathan Owen and Joe Cheng\n\nThis book presents various Shiny applications and explains the underlying code, making it easier to understand and implement Shiny functionalities in your own projects.\nR Shiny by Example - Manning\n\n\n\n\nOther Educational Materials\nOnline Courses, Tutorials, and Community Resources:\n\nRStudio Shiny Tutorial: A beginner-friendly tutorial from RStudio that introduces the basics of Shiny, including building and deploying Shiny apps.\n\nRStudio Shiny Tutorial\n\nCoursera - Developing Data Products by Johns Hopkins University: This online course covers the development of data products, including Shiny apps. It’s part of the Data Science Specialization.\n\nCoursera - Developing Data Products\n\nDataCamp - Building Web Applications in R with Shiny: An interactive course that teaches you how to build Shiny apps from scratch, with hands-on exercises and projects.\n\nDataCamp - Building Web Applications in R with Shiny"
  },
  {
    "objectID": "blogs/posts/16_.html",
    "href": "blogs/posts/16_.html",
    "title": "헬스커뮤니케이션2024",
    "section": "",
    "text": "Presentation"
  },
  {
    "objectID": "teaching/ds101/pbl/index.html#final-output",
    "href": "teaching/ds101/pbl/index.html#final-output",
    "title": "PBL",
    "section": "Final output",
    "text": "Final output\n\nTeam 1\n\nReport link\n\n\n\nTeam 2\n\nReport link\n\n\n\nTeam 3\n\nReport link\n\n\n\nTeam 4\n\nReport link\n\n\n\nTeam 5\n\nReport link\n\n\n\nTeam 6\n\nReport link\n\n\n\nTeam 7\n\nReport link\n\n\nStudents organize teams that meet several conditions.\n\n4~5 members in a team\nBackground diversity: no homogeneous majors in a team\nException: Allowed if persuasion is possible for sufficient reasons\n\nData: Panel Big Data given EMBRAIN corp.\n\nIf persuasion is possible with sufficient reasons, a project using other data can be used\n\n\n\n\nAbout EMBRAIN’s panel bigdata: [pdf]\n\nDue to the limitation of survey, we need a bigdata being collected from consumer\nPanel bigdata\n\nPanel: Panelists are survey respondents who have expressed their intention to participate in the survey in advance and provided personal information under a contract with the survey company.\nPanel Big Data is an integrated platform that allows you to analyze surveys and big data together.\n\nIncluding..\n\nApp using information\nVisited locations (using WiFi & GPS detection technology)\nPayment information\nBasic demographics (gender, age, and so on)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput\n\nData analysis report\n\nAny format is possible (PPT, word, notion web page link, pdf, and so on).\nReport example (but not limited)\n\nData Preparing (or Collecting)\nExplore data (Descriptive stats)\nSet your hypothesis (or research questions)\nVisualize data to confirm your hypo or RQs\nExplain your findings\nExpanding your findings to implications\n\n\nPresentation Video\n\nMaking videos for 10 mins presentation (in any language),\nSubmit a Youtube link\n\n\n\nThe best teams may lead to URP in the summer semester.\n\n\nTeams\n\n윤승현(경영), 최경석(스포츠과학), 박재원(DS), 임서영(미술), 정하경(국국)\n안제민(DS), 이채빈(DS), 김지희(DS), 이윤서(DS), 유선아(AI)\n민범기(CT), 이시연(영상), 김경서(영상), 김지연(소비자), 이지우(CT)\n손채리(한교), 이수아(글경), 최재원(통계), 윤지우(독독), 김서영(CT)\n문정은(경영), 박우혁(DS), 이상훈(DS), 양제니(CT)\n정은채(AI), 강윤경(AI), 이혜연(DS), 우정현(CT), 김민기(유동)\n송준석(CT), 이자의(CT), 최은서(CT), 홍은지(소비자), 이지원(통계)"
  },
  {
    "objectID": "teaching/index.html#fall",
    "href": "teaching/index.html#fall",
    "title": "Courses",
    "section": "2024-Fall",
    "text": "2024-Fall\n\nMon 15:00 (3h) 실감미디어문화테크놀로지 (Immersive Media & CT)\nTue 09:00 (3h) 문화데이터와머신러닝 (ML101)\nWed 09:00 (3h) HCI기초통계와데이터사이언스 (STAT101)"
  },
  {
    "objectID": "teaching/grad_stat/index.html",
    "href": "teaching/grad_stat/index.html",
    "title": "STAT101",
    "section": "",
    "text": "About Course\n\n\n\nCourse materials"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/15_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/15_week.html",
    "title": "기말시험",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\n\n\n\nConclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/13_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/13_week.html",
    "title": "실제 연구 replicate (1)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/11_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/11_week.html",
    "title": "HCI 데이터를 활용한 데이터 시각화(1)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/09_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/09_week.html",
    "title": "추론 통계 (2)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/07_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/07_week.html",
    "title": "확률 분포",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/05_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/05_week.html",
    "title": "R의 데이터 유형 및 구조(2)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/03_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/03_week.html",
    "title": "음악, K-pop, 엔터테인먼트",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\n수업자료\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n\n\n\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n\n\n\n소속사 없이 음원 유통하는 법\n\n\n\nAI가 만든 앨범 자켓\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/index.html",
    "href": "teaching/grad_stat/weekly_2/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nWeek\n\n\nTitle\n\n\n\n\n\n\n1\n\n\nCourse Intro\n\n\n\n\n2\n\n\nHCI 실험 디자인 및 실제 연구 사례 소개\n\n\n\n\n4\n\n\nR의 데이터 유형 및 구조(1)\n\n\n\n\n5\n\n\nR의 데이터 유형 및 구조(2)\n\n\n\n\n6\n\n\n기술 통계\n\n\n\n\n7\n\n\n확률 분포\n\n\n\n\n8\n\n\n추론 통계 (1)\n\n\n\n\n9\n\n\n추론 통계 (2)\n\n\n\n\n10\n\n\nWrap-up QZ\n\n\n\n\n11\n\n\nHCI 데이터를 활용한 데이터 시각화(1)\n\n\n\n\n12\n\n\nHCI 데이터를 활용한 데이터 시각화(2)\n\n\n\n\n13\n\n\n실제 연구 replicate (1)\n\n\n\n\n14\n\n\n실제 연구 replicate (2)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/grad_stat/about/index.html",
    "href": "teaching/grad_stat/about/index.html",
    "title": "Course description & Communication",
    "section": "",
    "text": "Course description\n\n인간-컴퓨터 상호 작용(HCI)에 적용되는 통계 및 데이터 과학의 기본 개념을 소개하고 연구에 적용할 수 있는 역량을 갖출 수 있도록 수업을 구성하였음. 데이터 분석 도구인 R 프로그래밍을 활용하여 실험(또는 설문) 데이터를 효과적으로 분석, 해석 및 시각화하는 데 필요한 기술을 갖추는 것을 목표로 한다.\n\n\n\n\nTime & Location\n\n(3h) Wed 09:00 ~ 11:50 @국제관 첨단강의실[9B217]\n\n\n\n\nWeekly Design\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nAssignment\nNote\n\n\n\n\n1\n09/04/2024\nCourse Intro\n\nARS 참가\n\n\n2\n09/11/2024\nHCI 실험 디자인 및 실제 연구 사례 소개\n\n\n\n\n3\n09/18/2024\n(휴강)\n\n추석연휴\n\n\n4\n09/25/2024\nR의 데이터 유형 및 구조(1)\n\n건학기념\n\n\n5\n10/02/2024\nR의 데이터 유형 및 구조(2)\n\n\n\n\n6\n10/09/2024\n기술 통계\n\n한글날\n\n\n7\n10/16/2024\n확률 분포\n\n\n\n\n8\n10/23/2024\n추론 통계 (1)\n\n\n\n\n9\n10/30/2024\n추론 통계 (2)\n\n\n\n\n10\n11/06/2024\nWrap-up QZ\n\n\n\n\n11\n11/13/2024\nHCI 데이터를 활용한 데이터 시각화(1)\n\n\n\n\n12\n11/20/2024\nHCI 데이터를 활용한 데이터 시각화(2)\n\n\n\n\n13\n11/27/2024\n실용적인 예 및 실제 연구 replicate (1)\n\n\n\n\n14\n12/04/2024\n실제 연구 replicate (2)\n\n\n\n\n15\n12/11/2024\nHCI 연구 디자인 발제 및 피드백\n\n\n\n\n16\n12/18/2024\nHCI 연구 디자인 수정 발제 및 피드백\n\n\n\n\n\n\n\n첫주차 강의는 녹화 강의입니다 (ARS Electronica 참여)\n건학기념일과 한글날에는 강의실에서 수업과 ZOOM 녹화가 동시에 진행됩니다. 참여하셔도 되고 이후 업로드되는 녹화강의로 들으셔도 됩니다.\n\n\n\n\nAbout Lecturer (Modulator)\n\nChangjun LEE\nAssociate Professor (Head of Culture & Tech)\nSchool of Convergence. SKKU.\nchangjunlee.com\n\n\n\nEvaluation\n\nAttendance & Participation (20 %)\nQZ (30 %)\nFinal Report (50 %)\n\n\n\n\nCommunication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gSXAqhDg\n입장하셔서 이름을 꼭 출석부에 있는 이름으로 설정해주세요.\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/14_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/14_week.html",
    "title": "엔터테인먼트 경영",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 14: 엔터테인먼트 경영\n\nDate: 20240604\nPre-class video\n\nHow Netflix changed entertainment – and where it’s headed | Reed Hastings\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n넷플릭스가 현재의 넷플릭스가 되는 과정에서 얼마나 과감하고 혁신적인 결정들을 내려야 했는지 알게 되었다. 초등학생 시절에 잠시 미국에 살았을 때 넷플릭스에서 DVD를 빌려서 보곤 했었는데, 몇년후에 넷플릭스가 스트리밍 서비스로 변신하여 한국을 더불어 전세계에 진출해 있었다. 서비스 사용자 입장에선 단순히 영상을 보는 매체가 DVD에서 인터넷 스트리밍으로 바꼈으니 넷플릭스도 그렇게 바뀐 것이라 생각했는데, 전세계적인 비즈니스가 되기 위해선 미리 시대의 흐름을 읽고, 그 새로운 것을 시도하는 용기가 필요하다는 것을 인터뷰를 통해 알게 되었다. 현재에서 되돌아보면 당연해보이는 것들이 당시에는 전혀 없던 것들(예를 들어 Binge-watching, streaming)이었고, 누군가는 그것을 최초로 사업화했다는 것이다. 넷플릭스의 문화는 많은 자유와 책임이라고 했다. 모두가 정보를 공유하고, 일부 상사들이 결정하는 것이 아닌 각자가 자유롭게 결정하는 방식이다. 구글이나 넷플릭스와 같은 혁신적이고 전세계적인 기업들의 공통점인 것 같다. 자유에는 책임이 따른다는 말이 있듯, 자유로우면 오히려 많은 지적인 사람들의 책임감 있는 아이디어들을 더 많이 모을 수 있는 것 같다.\n\n\n\nOTT 서비스에서 오리지널 콘텐츠에 투자하는 이유와 수익 모델에 대해 더 깊게 이해할 수 있었습니다. 다른 회사에 저작권이 있는 작품을 들여와 제공하는 것은 회원을 만족시키는 데에는 도움이 되지만, 넷플릭스라는 브랜드에 대한 인상을 강력하게 만드는 데에는 큰 영향을 주지 않는다고 한 점이 인상깊었고, 이것이 OTT 브랜드에서 오리지널 콘텐츠를 늘려 가고 있는 이유가 될 것 같습니다. 회원제이기 때문에 많은 회원을 유치하는 것이 목표이고, 그렇기 때문에 어떤 작품이 얼마나 더 큰 이익을 주는가를 세세하게 분석하는 것을 많이 중요하지 않다고 여기며 다양한 분위기를 제공하는 것에 초점을 맞추는 점도 뜻밖이었습니다. 그런데 제가 OTT 서비스를 사용해온 방식들을 생각해 보니 강의 내용에 들어맞는 것 같습니다. 저는 넷플릭스를 사용하고 있는데, 넷플릭스가 특정 콘텐츠를 가지고 있기 때문에 이 서비스를 사용하기로 결정한 것이 아니라, 오히려 넷플릭스를 결제했기 때문에 알지도 못했던 다양한 콘텐츠를 새롭게 시청하게 되고, 시청 경험이 계속되니까 회원을 유지하고 있는 것 같습니다. 티빙이나 디즈니 플러스 같은 다른 서비스는 보고 싶은 콘텐츠가 있을 때만 결제하는데, 이 경우 OTT 브랜드 오리지널 시리지였던 경우가 많았습니다…\n\n\n\nranked value와 reavealed value로 사용자의 별점 평가과 실제 시청 경험의 차이를 짚어낸 것이 인상적이었다. 결국 사용자가 ‘의식’해서 콘텐츠를 평가하는 것과, ’무의식’으로 콘텐츠를 시청하는 것은 다른 층위의 가치가 작용한다는 것이다. 놀라운 건 이것을 짚어내고, 바로 서비스에 반영했다는 것이다. 넷플릭스는 기존에 있던 별점 평가 시스템을 제외하고, ’좋아요-별로예요’ 정도로 평가 시스템을 축소했다. 별점이 사용자의 실제 시청 경험에 유익하지 않다는 것을, 사용자 스스로도 인식하기 전에 먼저 인식하고 적용해서 사용자 경험에 편의를 더한 것이다. 이렇게 인식하고, 그 인식을 바로 반영해내는 결단력이 지금의 넷플릭스를 있게 한 원동력이구나 싶었다.\n\n\n\n\nBest Questions\n\n2011년 넷플릭스는 ’하우스 오브 카드’에 회사 수익의 상당 부분을 차지하는 1억 달러 가량을 투자했는데, 이는 가히 큰 위험이라고 말할 수 있다. 수익의 상당 부분이면 그 결과가 좋지 않을 때 거의 부도가 날 수 있는데, 결과적으로는 성공했지만 이러한 결정을 내리기 위해서는 어떤 것이 필요할까? 하이 리스크를 겁내 큰 도전을 하지 못할 사람들이 더 많다고 생각되는데, 큰 결심과 올바른 선택을 하기 위해서 필요한 자질과 근거에는 무엇이 있을지에 대해 궁금하다.\n\n\n\n“최근 OTT 업계는 과거와 달리 점차 정체되고 있다고 생각합니다. 넷플릭스에서는 이를 해결하기 위해서 계정 공유 금지 정책을 실시하고 있습니다. 교수님께서는 OTT 업계의 이러한 공유 금지가 더욱 확산할 것으로 전망하실 지 여쭤보고 싶습니다. 또한, 이러한 정책이 OTT를 구독하고 있는 사용자들이 감소하는 효과를 불러와, 전반적으로 산업을 축소시킬 지, 아니면 더욱 소비를 증가시켜 시장을 더욱 크게 만들 것으로 전망하시는 지에 대해서도 알고 싶습니다.\n\n\n\n저번학기 (2023-2)에 문화테크놀로지2 수업을 수강하면서 테오의 국내 프로그램 특징을 분석하고 테오의 프로그램을 분류해 어떤 플랫폼에 공급하면 좋을지에 대한 조별 발표를 진행했었습니다. 당시 국내 OTT 서비스의 미래와 테오의 프로그램을 엮어서 발표했었었는데요. 국내의 웨이브와 티빙의 합병이 진행되냐 마냐 논의가 진행되는 상태였어서 자세한 국내 엔터의 방향성이 결정된지 않았다는 피드백을 받았던 기억이 납니다. 최근 뉴스에서 웨이브와 티빙의 합병이 가시화되고 있다는 뉴스를 보았는데요. 이러한 국내 오티티 시장의 변화가 엔터 사업 내의 큰 영향을 끼칠 수 있을 것인지에 대한 의견이 궁금합니다!"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/12_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/12_week.html",
    "title": "가상/증강현실 콘텐츠 제작",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 12: 가상/증강현실 콘텐츠 제작\n\nDate: 20240521\n수업자료\n수업 녹음 자료 (For 예비군 수강생)\n\n\n\n\n\nPre-class video\n\n상상이 현실화가 되는 세계, 그 중심에 서서 외치다 | 서동일 오큘러스VR코리아 지사장\n\n\n\nWhat is extended reality? | The Gadget Show\n\n\n\n소아암, 장기치료 어린이를 위한 디지털 테마파크 | RGB MAKERS\n\n\n\n\n\n아래 영상들은 RGB MAKERS 사에서 제작한 작품들입니다.\n\n2021 CSR 필름 페스티벌-희망 나눔 부문(보건복지부장관상) 수상작\n\n\n\nTabernacle; Realistic Bible Experience Contents\n\n\n\n김태원 대표님 수업을 통해 학생들이 제작한 영상: healing planet metaverse space makers\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n\n이 영상은 12주간 내가 봤던 모든 사전 영상 중 가장 흥미로운 영상이었다. 나는 이제껏 인간이 가상의 공간에 침투하는 것은 봐왔지만, 가상의 것이 현실로 구현되는 것은 처음 봤기에 더욱 신선하였다. 가상/증강 현실로 없는 공간을 창작하고, LED가 설치되지 않은 곳까지 확장하여 가상 현실을 확장하고, 가상 현실에 멀리있는 사람이나 존재하지 않은 사물을 불러올 수 있었다. 어릴 적 본 &lt;레디플레이어 원&gt;이라는 영화가 생각날 정도로 세상은 생각보다 더욱 고도화된 기술력을 가지고 있었다. 하지만 가장 충격이었던 점은 이 영상이 무려 3년 전 영상이라는 것이다. 3년이 지난 지금은 어느 정도로 기술이 발전하였을까? XR기술을 직접 체험해보고 싶다고 생각하게 되는 영상이었다.\n이 영상은 12주간 내가 봤던 모든 사전 영상 중 가장 흥미로운 영상이었다. 나는 이제껏 인간이 가상의 공간에 침투하는 것은 봐왔지만, 가상의 것이 현실로 구현되는 것은 처음 봤기에 더욱 신선하였다. 가상/증강 현실로 없는 공간을 창작하고, LED가 설치되지 않은 곳까지 확장하여 가상 현실을 확장하고, 가상 현실에 멀리있는 사람이나 존재하지 않은 사물을 불러올 수 있었다. 어릴 적 본 &lt;레디플레이어 원&gt;이라는 영화가 생각날 정도로 세상은 생각보다 더욱 고도화된 기술력을 가지고 있었다. 하지만 가장 충격이었던 점은 이 영상이 무려 3년 전 영상이라는 것이다. 3년이 지난 지금은 어느 정도로 기술이 발전하였을까? XR기술을 직접 체험해보고 싶다고 생각하게 되는 영상이었다. 첫 번째 영상에서, 서동일 사장님은 VR이 우리에게 어릴 적 이루지 못한 꿈을 이루게 해줄 것이라고 말하였다. 어릴 때부터 나는 런닝맨과 1박2일과 같은 방송을 보면서, 방송용 레크레이션 게임을 창작해보고 싶다는 생각을 하였지만, 방송 업계가 많이 저물면서 나는 자연스럽게 나의 꿈을 접게 되었다. 사실 IT기술이 점점 우리 사회에 스며들면서, 나라는 사람이 무엇을 할 수 있는지에 대해 고민하는 과정을 계속해서 반복해왔다. 내가 할 수 있는 일은 인공지능이 훨씬 더 빠르고 정확하게 처리해주기에, 나라는 사람이 쓸모가 있는 분야가 있을까라는 회의감도 들었었다.\n이 영상은 이러한 나의 고민을 타파하였다. 방송용 게임에서 VR 게임으로 시각을 전환하니, 소비자들이 흥미를 느낄만한 콘텐츠를 새롭게 기획하는 것은 결국 인간의 영역이겠구나 생각하였다.영상에서 소아암 환자를 타겟으로 어린아이의 시각에 맞게 테마파크, 총게임, 롤러코스터 등의 콘텐츠를 제작하는 것을 보면서, 나라면 어떤 식의 게임을 기획하였을지 생각하게 되었다. VR 콘텐츠 크리에이터라는 직업에 대해 희망을 가지게 되면서 어릴 적 꿈을 실현해보고 싶다는 희망을 가지게 되었다.\n\n\n\n\n\n가상 환경을 구축하는 것은 단순히 초록 배경에서 디자인 그래픽을 입힌 것이라고 생각했는데 LED를 이용해 입체적으로 가상 환경을 구축하는 것을 보며 신기했습니다.언젠가 모든 TV쇼가 저 기술을 활용하여 배경을 구축할 수 있지 않을까 생각했습니다. 조금 다른 이야기일 수 있지만 최근에 종방한 눈물의 여왕이라는 드리마에서도 AI를 활용하여 눈이 오는 배경을 만들어냈다고 했습니다. 이제는 촬영지를 찾으러 다니지 않아도 원하는 배경을 보다 정확하게 구현할 수 있으니 효율적이라고 생각합니다.\n아이들이 좋아하는 모습을 보면서 저도 모르게 흐뭇해졌습니다. 문득 이 영상을 보다가 어르신분들이 떠올랐습니다. 매장에 거의 대부분이 키오스크로 주문을 하는데, 어르신분들께 키오스크를 하는 상황을 VR로 제작하여 연습해보도록 만드는 상품은 어떨까 생각해보았습니다. 그렇다면 직접 가서 키오스크를 이용하는 데에 조금이라도 어려움을 덜어드릴 수 있지 않을까 생각했습니다. 사회의 어려움, 문제를 해결하기 위해 재밌는 방법으로 해결할 수 있는 것도 VR의 기능이구나 느꼈습니다.\n\n\n\n\n상상이 현실화되는 세계는 매우 독특하고 환상적인 개념입니다. 이러한 세계에서는 개인의 생각과 꿈이 현실의 형태로 나타나므로, 이는 인간의 창의력과 내면의 욕망을 물리적인 현실로 전환시키는 능력을 부여합니다. 상상력이 현실이 되는 이 세계에서는 무한한 가능성과 동시에 예측 불가능한 위험들이 존재합니다. 두번째 영상을 통해서 가상 세계 기술은 교육 분야에서도 엄청난 잠재력을 보여주습니다. 가상 현실 기술을 통해 학생들은 전 세계의 역사 유적지를 탐방하거나 우주의 신비를 탐험하거나 심지어 인체 내부를 공부하는 등 다양한 학습을 할 수 있습니다. 이러한 몰입형 학습 방식은 학생들의 학습 흥미를 높일 뿐만 아니라, 지식에 대한 이해도를 깊게 합니다. 생병에 걸린 아이들이 AR 등 기술을 통해 즐겁게 노는 모습을 보니, 나도 진심으로 기쁩니다. 기술의 발전은 단순히 오락을 넘어서, 병마와 싸우는 아이들에게 희망과 웃음을 선사합니다. 병원이라는 제한된 공간에서 아이들은 가상 현실을 통해 다양한 세상을 탐험하고, 그들의 상상력을 자유롭게 펼칠 수 있습니다. 이 과정에서 아이들은 잠시나마 고통을 잊고, 순수한 기쁨을 느낄 수 있습니다. 이러한 모습을 보면서, 기술이 사람들에게 줄 수 있는 긍정적인 영향력에 대해 다시금 깨닫게 됩니다. 아이들이 건강하게 자라날 수 있도록, 그리고 그들이 더 많은 행복을 느낄 수 있도록 도와주는 기술의 힘에 감사함을 느낍니다.\n\n\n\nMake people to experience impossible!라는 목적 자체가 사람들에게 다양한 경험을 제공할 수 있고 여러가지 중요한 의미를 내포하고 있는 것 같습니다. 우선 이 목적은 사용자들에게 일상적인 한계를 뛰어넘는 새로운 경험을 제공합니다. 또한 소비자들이 현실에서 경험할 수 없는 상황, 장소, 또는 감정을 가상현실을 통해 체험하게 한다는 것은 소비자들의 만족도를 높이고 더 많은 관심과 참여를 유도할 수 있을 것 같습니다. 앞으로의 기술 발전을 통해 가능해질 새로운 경험에는 어떤 것들이 새롭게 생길지에 대한 궁금증을 유발하는 영상이었던 것 같습니다. XR기술은 실제와 가상세계를 융합한 것이다. 화면을 보며 바닥에 있는 타이어를 가리키거나 보는 것이 가능한 것과 같이 가상환경과 실제환경이 함께 상호작용하다. 또한, 소형 스튜디오 공간만 있어도 몰입할 수 있는 가상환경을 화면 상에도 더 크게 만들 수 있다는 이점이 있다. 그만큼, XR 기술의 발전은 가상환경과 실제환경의 연결을 도울 수 있다는 점을 알 수 있었다. 이러한 디지털 테마파크는 고통을 참고 있는 아이들의 힘든 시간을 조금이나마 즐겁게, 잠시나마 아이들의 아픔을 잊게 해줄 수 있는 좋은 아이디어인 것 같다고 느껴졌습니다. 영상에서 보이는 게임에 집중하는 아이들의 모습은 너무나도 사랑스럽습니다.\n\n\n\n이 영상을 통해 가상현실의 필요성과 가능성에 대해 생각하게 되었다. 인간 대신 기계에게 일을 시킬 수 있도록하는 에너지 발전기나 공장 등 일상 속 필수적인 기술과 달리 가상 현실은 버추얼휴먼처럼 우리에게 꼭 필요한 것인지 이것이 어디에 쓰일 수 있을지 감이 잘 안 잡힌다. 그런데 돈, 시간, 두려움, 건강 등으로 인한 현실 속 제약에서 벗어나 꿈을 간접적으로나마 이룰 수 있게 해준다는 점에 있어 다양한 가능성이 존재하는 분야임을 알게 되었다. 가상현실을 구현하는 기술뿐만 아니라 이 기술을 어떤 식으로 활용할 수 있을지 상상하는 것이 중요한 것 같다. XR(확장 현실)이 구현되는 방식을 간단하게나마 시각적으로 볼 수 있었다. 또한 가상현실 관련 기술을 생각했을 때, 단순 가상현실이나 현실 세계에 가상의 요소를 덧입힌 증강현실 정도만 생각했는데, 이 모든 기술을 결합한 XR에 대해 좀 더 배울 수 있었다. 마커를 통해 가상의 물체도 실시간으로 움직이는 기술이 정말 인상깊었고, 좀 더 구체적인 기술의 메커니즘을 배워보고 싶다는 열망이 생겼다. 첫 번째 영상에서 언급한 메시지 중 하나가 구현된 한 가지 예시가 이 영상에 나왔다. 건강이라는 제약 조건에 의해 놀이공원이나 신체 놀이 등을 하기 힘든 소아암 어린이들이 가상현실 기술을 통해 간접적이지만 정말 실감 나게 디지털 테마파크를 체험할 수 있는 것이 정말 의미가 있다고 느꼈다. 실제 놀이공원을 마음 껏 갈 수 있는 사람이라면 디지털 테마파크에 큰 흥미를 못 느낄 수도 있지만, 병원을 벗어나기 힘든 장기 치료 어린이들에게 디지털 테마파크는 훨씬 큰 의미를 가질 수 있겠다는 생각을 했다. 이렇게 제약을 갖고 있는 사람들과 상황들을 관찰해서 해당 기술의 순기능을 발휘하는 것이 중요하다고 본다.\n\n\n\n\nBest Questions\n\n어린이 소아암 관련 영상을 보면서 단순히 가상 증강 현실 콘텐츠가 사람들의 오락과 여가 시간을 보내기 위한 수단이 아닌 그 이상의 더움이 될 수 있댜는 것을 몸소 깨달을 수 있었습니다. 이러한 가상/ 증강 현실 콘텐츠로 많은 병들을 고칠 수 있을 것 같은데 교수님께서 생각하실 때 어떤 분야의 의료와 접목이 가능하실거라고 생각하시나요? 개인적으로는 정신 질환을 치료하는데고 꽤나 좋은 치료방법으로 사용할 수 있을 것 같다는 생각이 들었습니다.\n\n\n\n코로나19 창궐 당시 가상현실이나 메타버스와 관련한 붐이 잠시 일었다가 소강 상태에 접어든 데에는 감염병 사태가 종국의 단계에 들어선 것도 있겠지만 미진한 기기 보급과도 관련이 있다고 생각합니다. 일반 디스플레이로도 가상현실 콘텐츠의 감상이 불가능한 것은 아니지만 창작자의 의도대로 콘텐츠가 감상되기 위해선 적어도 구글 카드보드라도 쓰는 것이 맞을 터이나 아직까지 많은 사람들이 HMD 장비를 머리에 장착하고 손에 무엇인가 쥐는 행위를 번거롭고 거추장스럽게 여기는 것이 아닌가 생각합니다. 결국 사용자의 경험 향상을 위한 기술 발전의 모멘텀이 필요하다고 보는데 업계에서는 이러한 문제점 해결을 위해 어떤 기술적 노력을 하고 있는지 궁금합니다.\n\n\n\n가상/증강현실 관련 콘텐츠는 게임이나 여행 등 유흥적인 요소가 현재 우선적으로 활성화되고 산업에 높은 비중을 차지하고 있는 것 같습니다. 이 때, 일반 게임도 과몰입하고 현실 세계와 구분이 안되는 중독 문제가 이슈가 됐었는데, 가상/증강현실 관련 게임은 그 몰입도가 일반 게임에 비해 엄청날 것 같습니다. 그래서 현실 세계와 구분이 안되는 정도는 아니더라도, 더 쉽게 중독되고 현실을 살기보다는 가상현실 속 게임을 계속해서 즐기는 문제도 발생할 수 있을 것 같다는 생각이 들었습니다. 그래서 정부 차원에서는 만약 가상/증강현실이 대중화된다면 게임 관련 규제를 내릴 수 밖에 없을 것 같은데 이에 대한 산업의 대처방안, 중독 문제에 관련된 교수님의 의견이 궁금합니다."
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/10_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/10_week.html",
    "title": "인터랙션 사이언스, UX",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 10: 인터랙션 사이언스, UX\n\nDate: 20240507\n\n\n\n수업자료\n\n\n\nPre-class video\n\nHuman-Computer Interaction and User Interface Design | Tony Tang\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nBest Discussion\n\nAI 등 인공지능, 기계 등의 기술이 점점 발전되면서 사회가 많이 변했다고 느꼈습니다. 이제 어느 가게를 가도 키오스크 없는 곳을 찾기 어렵고, 모바일 앱으로 주문이 안되는 곳을 찾기 어려운 것도 하나의 예시라고 생각합니다. 어느 순간부터 우리 생활에 기술이 가장 깊숙히 들어와있다고 느껴졌습니다. 그냥 오랜 시간 기술과 함께 사회를 이루어갔기 때문이라고 생각했는데, 이 영상을 보고 인간이 편리함을 느끼게끔 기술이 변했다고 생각했습니다. 인간이 느끼는 사회의 불편함을 기술이 해결하고 있다고 생각했습니다. 그리하여 이제는 이런 기술 없이는 살 수 없는 분위기, 사회가 형성된 것이라 예상했습니다. 수요가 많아져 발전되는 속도가 빨라지면 공급이 더뎌져 빈 공간이 생기기 마련입니다. 카페 아르바이트를 하며 키오스크를 어려워하는 고령층분들이 눈에 많이 밟힙니다. 그럴 때마다 저희 부모님도 생각나면서 이런 기술이 발전하면 이 기술을 모든 사람이 잘 활용할 수 있도록 교육, 안내를 잘 해야 하지 않을까 생각합니다.\n\n\n\n시스템을 만드는 디자이너들이 실제로 시스템을 사용하는 사람들이 아닌 경우가 많다는 이야기가 가장 공감이 갔습니다. UX research로 인터뷰를 시행한 경험이 있는데, 실제로 해당 제품을 사용하는 사용자가 아니다보니, 어떤 점이 불편할지, 어떤 점을 조사해야 할 지 인터뷰 질문을 구성하는 과정 자체가 막막하기도 했습니다. 영상에서 나온, 디자이너들이 시스템이 어떻게 사용될 것인지 가정한다는 구절이, 제 상황을 정확히 지적하고 있었습니다. 사용자 경험 연구와 사용자 경험 디자인에서 해야 할 일은 사용자가 누구인지를 이해하기 위해 공감 도구를 개발하는 것이라고 하는데, 어떻게 하면 사용자 입장에서 생각할 수 있을지 여전히 아리송한 상황입니다. 특히 요즘에는 사용자들의 유형이 개별화되다보니, 보편적인 사용자 디자인을 만들기는 더욱 어렵게 느껴집니다.\n\n\n\n외국인이 음료수 “갈아만든 배”의 “배”를 IdH 로 읽는다던지, 너구리를 뒤집어 읽어 “RTA”로 본다던지의 사례를 보면서 웃은적이 있다. 하지만 이번 영상을 보며 위 사례를 미루어 좋은 커뮤니케이션, 디자인이 무엇인지 생각해볼 수 있었다. 위의 경우에는 그저 문자의 차이에서 생긴 단순한 해프닝으로 볼수 있지만, 적어도 좋은 커뮤니케이션이 일어났다고는 할 수 없을 것이다. 어떤 제품, 서비스 등을 기획 및 제작함에 그 의도에 맞는 공통된 행동 양식을 보일 수 없다면 그것은 좋은 디자인이 아닌 것이다. 만약 우리가 “밥 뭐 먹었니?”라고 물어보면 “도시락 먹었어”로 대답하지, “응/아니”라고 대답하지 않듯, 좋은 UX디자인은 자신이 의도한 대로 사용자의 행동을 유도하는 “의도에 따른 행동 유도”로 정리할 수 있을 것 같다.\n\n\n\n인터랙션 사이언스와 UX 분야가 이론부터 기술을 총망라하는 분야임을 알 수 있었다. 사용자에 대한 관심을 기술로 풀어낸다는 점에서, 굉장히 매력적인 분야인 것 같다. 인터랙션 사이언스에서 인터랙션은 어떤 의미일 수 있을지 생각해보게 되었다. 처음에는 단순 기술을 고안하는 인간과, 기술을 실현하는 도구인 컴퓨터(혹은 그외 도구) 간의 교류라고 생각했는데 영상을 보니 사용자와, 사용자를 생각하는 기획자와의 교류 사이에 도구가 연결되어 있는 소통으로도 뜻을 이해하게 되었다.\n\n\n\n영상 처음 부분에 실패한 UX 디자인의 예시를 보며, 과연 나도 살면서 저러한 제품들, 혹은 상황들은 마주한 적이 있는가에 대해 생각해보게 되었습니다. 생각해보면 저러한 상황들은 다수 있었습니다. 간단한 예시를 들자면, 요즘에는 어떠한 사이트에 들어가려면 간편하게 로그인을 구글로 로그인하기, 카카오톡으로 로그인하기 버튼을 누르면 빠르게 로그인을 할 수 있습니다. 하지만 가끔씩 그렇지 않은 사이트들도 다수 있는데요, 로그인을 하려고 다른 페이지로 넘어가는 순간 보이는 복잡한 로그인 단계를 보면 한숨부터 나오게 됩니다. 과연 이러한 정보까지 입력을 굳이 해야되나 싶을 정도입니다. 영상에서 나오는 예시들을 보며 많이 답답했고, 공감을 얻은 것 같아 흥미로웠습니다. 또한 실패한 UX 디자인을 보면서, 어떻게 저러한 제품이 상용화가 되었는지 많은 의문이 들었습니다.\n\n\n\n기술이라는 것이 인간의 의도를 실현해주는 역할을 한다고 생각하는데, 이 인터렉션 사이언스는 ’소통’이라는 의도를 실현해주는 역할을 하며, 그 의도를 망각해서는 안된다고 느꼈다. 소통을 원활히 하기 위해서 영상 속의 사람들은 무의식을 연구하고, 공감 도구를 활용하고 프로토 타입을 제작하는 등의 노력을 기울인다고 이해했다. 결국 우리 컬처앤테크놀로지전공생의 역할은 이 ’소통’이라는 것을 개발자와 협업해 원활하게 해주는 사람이 아닐까 싶었다.\n\n\n\n\nBest Questions\n\n(인터랙션 사이언스와 AI) “인터랙션 사이언스는 인간과 기술이 상호작용하는 방법과 현상에 대한 이론을 다양한 학제간 접근을 통해 연구하는 학문이라고 정의를 내리고 있고, 기술을 통해 중요한 정보 전달을 강조하는 중간 시스템의 역할은 해당 정보를 가장 적절하고 맥락에 맞게 제공하는 것, 즉 기술은 맥락을 이해하고 정보를 적절하게 제공해야 한다는 것이 위의 영상의 핵심이라고 생각했습니다. 몇주 전 수업시간에 음성인식 기술 관련해서 의견을 작성했었는데요. 기술은 맥락을 이해하고 정보 전달에 탁월해야 한다는 것이 인터랙션 사이언스가 존재하는 이유들 중 하나라면, 음성인식 비서 즉 AI 기술들이 인간의 언어의 맥락을 파악하고 이를 전달하거나 완벽하게 이해할 수 있을 것이라고 생각하시는 지 질문 드리고 싶습니다. 예를 들어 중의적인 표현이나, 사람의 억양, 단어 선택에 따라 말의 의미가 정말 많이 바뀌는데 이러한, 음성적인 정보도 파악이 가능하다고 생각하시는 지 궁금합니다.\n\n\n\n(인터랙션 사이언스와 심리학) “인터랙션 사이언스, UX 디자인도 점점 더 개인화된 경험을 개선하기 위해 집중할 것이라 추측된다. 이를 위해서는 사용자의 경험을 이해하기 위한 심리적 측면의 연구도 선행되어야 할 것 같은데, 실제로 인터랙션 사이언스와 UX 디자인을 구상할 때 이론적인 연구가 얼만큼 수행되고 적용되는지 궁금하다. 또한 인터랙션 사이언스와 UX를 기획하고 구체화할 때, 이론연구와 기술구현 그리고 이후 영향분석의 과정까지 정말 다양한 분야의 지식이 필요할 것 같다. 지금은 기술구현 측면이 더 두드러지고 있는 것 같은데, 이러한 현상에 대한 전문가의 견해가 궁금하다. 앞으로 인터랙션 사이언스와 UX가 발전하기 위해서는 어떤 점에 더 집중해야 하는지도 의견을 묻고 싶다.”\n\n\n\n(특이성 vs. 대중성) 사용자와 기술 간 상호작용을 이해하고 설계하는 과학이라고 불리는 인터랙션 사이언스와, 사용자 경험을 나타내는 UX는 서로 밀접한 관련이 있다는 것은 누구나 다 알고 있는 사실입니다. 학문적 영역과 디자인 접근 방식의 결합을 통해 실생활의 문제점들을 해결할 수 있다고 생각하는데, 결국 사람들 다수의 공통의 사고를 얼마나 잘 분석해 내는지가 중요한 요소로 작용할 것이라고 생각합니다. 이 과정에서 사람들의 특이성에 초점을 두고 특성에 기반한 해결책을 마련하는 것이 좋은지, 모두가 알고 있는 대중성에 초점을 두고 노말한 요소를 만들어내는 것이 좋은지 궁금합니다."
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/08_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/08_week.html",
    "title": "문화콘텐츠와 자연어 처리",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 8: 문화콘텐츠와 자연어 처리\n\nDate: 20240423\n수업자료\nPre-class video\n\nText 데이터를 활용한 개인화 음악 추천서비스 웹 데모 구축 (혁신성장청년인재양성사업 프로젝트 발표)\n\n\n\n[TED] Why can’t AI ‘think’ like us?\n\n\n\n[TED] The language of computational linguistics.\n\n\n\n[엔씨소프트] 장애인의 목소리가 되어주는 AI 기술\n\n\n\n[노마드코더] 민트 초코 논란! 자연어 처리(NLP)로 종결해드림.\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nBest Discussion\n\n넷플릭스에서 collaborative filtering을 통해 영화 추천을 한다는 것을 알고 있었습니다. 이 기술을 음악 스트리밍 플랫폼에서 진행한다면 좀 더 다양한 장르를 접할 수 있다고 생각했습니다. 개인적으로 저는 영화보다 노래를 다양하게 즐기기 어렵다고 생각합니다. 그 이유는 영화는 상영관에서 상영하는 영화를 보고 고를 수 있고 또 스토리가 있기 때문에 대부분 SF가 아닌 이상 이해하는 데에만 집중할 수 있기 때문입니다. 그러나 음악은 락, 인디밴드, 발라드 등 정말 많은 장르가 있고 이는 취향이 확고하게 드러난다고 생각합니다. 그래서 collaborative filtering이 적용된다면 개인 취향이 아닌 것 같아 도전하지 못했던 노래들도 쉽게 접할 수 있지 않을까 생각했습니다.\n제가 FLO 앱을 사용하는데, 그 앱은 ‘3월 4일에 들은 노래와 비슷한 플레이리스트’ 이렇게 추천을 해주곤 합니다. 저는 이 기능이 정말 좋았던 게, 이런 노래 장르를 좀 더 들어보고 싶다고 생각한 찰나에 비슷한 느낌의 노래를 찾지 않아도 접할 수 있었기 때문입니다. 개인화된 맞춤 서비스가 더 강해질 수록 확증편향이 강해진다고 생각했는데 그 안에서 다른 길로 뚫릴 수도 있겠다고 생각했습니다.\n\n\n\n1번 - 제한된 데이터로 유저의 의도를 파악했다고 했는데 이 과정에서 정보의 왜곡이 일어날 수 있지 않을까 라는 생각이 들었습니다. 자료가 적다보니 조금의 분석 방향이 달라질 경우, 전체 분석의 의도, 흐름이 달라질 수 있을 것 같은데, 이런 경우 어떤 식으로 보안이 가능할까요? 인기상품, 컨텐츠 필터링, 유저를 통한 필터링 총 세가지를 영상에서 이야기 해줬는데 이러한 알고리즘이 꽤 도움이 될 것이라는 생각이 들었습니다. 다만 이미 에플뮤직, 멜론, 유튜브 뮤직 사이트에서 비슷한 알고리즘을 사용하고 있다고 체감했고, 따라서 현행 알고리즘과 차이점을 느끼지 못했던 것 같습니다.\n4번 - AI 음성을 탑재한 기술을 통해 단일한 톤이 아닌, 사람의 말투, 억양 등을 담고 있는 목소리 기술이 상당히 도움이 많이 될 것이라는 것을 깨달았습니다. 평소 사람과 사람 사이의 말에서 중요한 요소 중 하나가 목소리의 억양, 말투, 톤이라고 생각했는데 이런 것을 장애인의 목소리가 되어주는 그러한 목소리에서 고려하지 못했다는 점에 스스로의 생각이 짧았던 것 같습니다. AI의 기술의 발달도 모든 사람들이 향유할 수 있는, 그런 베리어 프리적인 기술이 되었으면 좋겠다는 생각이 들었습니다. 기술의 발달이 어떤 사람은 포함되고 포함되지 않는, 불공평한 분야갸 되지 않았으면 좋겠다는 생각을 하게 되었습니다.\n\n\n\n(두 번째 영상) Generalization is not the reality. 영상 속 작가의 이 한 마디는 아무리 AI가 발전하더라도 철학 인문 계열의 전문가는 사라지지 않을 수 있겠다는 희망을 주었습니다. AI는 세상에 공개되어있는 대부분의 사람들이 바라보는 세상을 학습합니다. 하지만 과연 다수가 바라보는 시각이 항상 옳을까요? 우리는 데이터와 알고리즘을 창조하는 동시에 영향을 받기도 합니다. 바쁘게 변해가는 사회 속에서 우리가 구축한 인공지능에게 압도당하거나 휩쓸리지 않기 위해서는, 서로 올바른 영향을 주고 받기 위해서는 인문 계열의 전문가가 끊임없이 세상의 정의에 대해 철학적으로 탐구해야 겠다고 생각했습니다.\n\n\n\n2번영상과 3번 영상에서 일관되게 AI는 인간과 사고하는 방식이 다르다고 말하고 있다. 이부분이 인상적이고 너무 공감됐다. 물론 기술이 발전하면서 AI가 학습한 데이터의 양은 기하급수적으로 늘고 있지만 여전히 부족한 것이 사실이며, 과연 인간의 사고방식을 온전히 구현할 수 있을지에 대한 현실적 의문이 든다.\n이와 관련해 개인적으로 최근에 겪었던 재미있는 이야기를 해보자면, 수업을 같이 듣는 친구들끼리 개설된 단톡방이있는데, 평소에 서로의 이름에 ~튜브(에: 홍길동 - 길동튜브/홍튜브)를 붙여 부른다. 카카오톡에서도 그렇게 부르면서 이야기를 떨고 있는데 어느 한 친구가 카카오톡 AI 대화요약 기능을 사용한 결과가 재미있다며 캡쳐해서 보내줬다. 그 결과는 “튜브에 관련된 무의미한 대화”였다.\n데이터가 많이 학습된건 사실이지만, 특정 집단에서 맥락에 맞춰 쓰이는 단어들을 AI는가이해하지 못하고 있는 것이다. 인간의 언어는 결국 약속에 의한 것인데 AI는 그 약속을 지키는 것처럼 모방하는 것일 뿐이지, 약속을 이해하지는 못한다고 생각한다.\n\n\n\nAI를 어디까지나 인간을 보조하기 위한 수단, 있으면 좋고 없으면 마는, 그런 존재라고 생각해왔다. 하지만 장애인을 위핸 “나의 ACC”앱을 보면서 누군가에게는 AI가 반드시 필요할 수도, 새로운 기회를 주는 것일 수도 있다는 생각이 들었다.\n또한 기술을 가진 이들과 그렇지 못한 이들의 격차가 마치 자연의 순리처럼 당연하게 여겨지는 사회에서 이런 사례들은 그래도 우리 사회가 아직은 어둡지만은 않다는 생각이 들게한다. “배워서 남주랴?” 이제는 다 옛 말이다. 이제는 배워서 남을 주는 것이 남을 위한 길이기도 하고 새로운 영역을 개척하는 나의 길이기도 한 것 같다.\n\n\n\n[엔씨소프트] 장애인의 목소리가 되어주는 AI 기술\n영상을 통해 AAC라는 의사소통 툴을 처음 알게되었습니다. 2015년에 서비스가 제작되었지만, 이제 알았다는 점이 아주 조금은 부끄럽기도 합니다. 의사소통은 사람과 사람간의 소통인데 의사소통에 어려움을 겪는 분들께 이러한 서비스는 정말 많은 도움이 될 것 같다는 생각이 듭니다. 또한 리뉴얼된 버전이 소통을 하며 감정을 전달하는 데에 있어 훨씬 더 잘 될 것 같고 어려움을 겪는 이들의 삶의 질을 향상시켜준다는 생각이 듭니다. 본 영상을 시청한 모두가, 그리고 ’나의 AAC’라는 어플을 알고 있는 사람들이 어플을 설치한다면, 앞으로 세상을 살면서 마주할 의사소통에 어려움을 겪는 이들과 조금 더 원활한 소통을 할 수 있을 것이라 예상한다.\n\n\n\n\nBest Questions\n\n다섯 개의 영상을 보면 아직은 “일반화”된 기준에 맞춰진 AI가 구축되고 있는 것 같습니다. 다만, 인간의 감정은 지극히 개인적이며 일반화하기에는 복잡하다고 생각합니다. 네번째 영상에서처럼 AI에 의존하여 대화를 하는 사람들의 경우, 자신이 전달하려는 감정과는 다르게 일반화된 기준에 따라 정제된 잘못된 감정이 전달될 수도 있다고 생각했습니다. 교수님께서는 훗날에는 이런 개인적인 해석과 기준에 의거한 AI도 만들어질 수 있다고 생각하시나요? 왜 일반화된 기준의 AI만 구축이 되고 있는지, 아니면 개인적인 감정까지 AI로 만들려는 노력이 일어나고 있는지 알고싶습니다!\n이는 자연어 처리와는 별개의 질문입니다만… 영상 속에서 자연어 분석의 데이터를 얻는 과정에 API라는 단어가 자주 나와 검색을 해보았으나, 인터페이스와의 차이점을 이해하기 어려웠습니다.. 둘의 차이점이 무엇인가요?\n제가 느끼기에는 자연어 처리는 분류, 탐색, 추천 등 분석적인 툴처럼 느껴지고, 문화 콘텐츠는 분석도 물론 필요하지만 그 이외에 크리에이티브함이 필수불가결하다는 생각이 들었습니다.\n다시 말해, 문화 콘텐츠를 기획할 때 분석 과정에서는 자연어 처리가 쓰일 수 있지만 소비자 마음에 소구되기 위해서는 사람만이 느끼고 생각할 수 있는 창의성이 빠질 수는 없다는 생각이 들었는데요.\n혹시 나중에는 자연어 처리가 창의성의 영역도 뛰어넘는 시대가 올지, 이에 대해서 어떻게 생각하시는지 궁금합니다.\n자연어처리는 머신러닝 기법을 활용하여 텍스트의 의미를 파악하는 것으로 알고있습니다. 그렇다면, 표준어가 아닌, 한국에서 사용하는 사투리나 방언을 이해하고 처리하는 데에 문제나 어려움은 없나요?"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/06_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/06_week.html",
    "title": "한류와 팬덤",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 6: 한류와 팬덤\n\nDate: 20240409\n수업자료\nPre-class video\n\nAll I Really Need To Know, I Learned From KPOP | Donald Lim | TEDxYouth@SJCS\n\n\n\nHas K-pop prepared us for the metaverse? | Alex Karlsson | TEDxBrunkebergstorg\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nQ&A Session\n\n한류와 K-Pop의 글로벌 확산 및 팬덤 문화\n\n(스타가 팬을 대하는 태도가 성공에 미치는 영향) 한류가 전세계적으로 유행하는 데에는 팬덤도 큰 영향을 미치고 있는지에 대해 여쭙고 싶습니다. 또한, 미국의 유명 가수인 테일러 스위프트를 보면 자신의 팬을 대하는 태도에서 한국의 K-Pop 아이돌(연예인)들과 유사한 점이 있다고 생각하는데, 이러한 특징이 테일러 스위프트의 성공에 기여하였다고 보시는 지에 대해 여쭤보고 싶습니다. 만약 그렇다면 이러한 K-Pop의 특징은 점차 세계화될 것이라 전망하시는 지에 대해서도 궁금합니다.\n\n\n팬덤이 K-Pop 세계화에 큰 영향을 미친것은 사실입니다. 하지만 주목해야할 점은, 테일러 스위프트처럼 스타성, 대중성이 큰 가수와 달리 K-Pop은 보다 섬세함이 특징적입니다. 여기에는 소셜 미디어 활용이라는 기술적 차원이 접목됩니다. 미디어 기술이 관여되고 케이팝 산업과 가수들이 이를 적극 활용한 것입니다. 예컨대 스타들과 위버스 등 소셜 미디어 플랫폼에서의 라이브채팅(유료회원)을 할 수 있도록, 더 가까이 내적 친밀감을 유도합니다. 이를 통해 케이팝 팬덤은 능동적 수용자로 자리매김하고, 미디어 기술과 엔터테인먼트 산업계가 팬덤을 상업적으로 활용하는 것이 복합적으로 작용한 형태가 작금의 K-Pop 시장인 것입니다. 이중 하나만 없어도 향후 케이팝 세계적 인기는 어렵다고 해석할 수 있습니다.\n\n\n\n(팬덤 갈등) KPOP의 흥행 덕분에 여러 아이돌 가수들이 많이 생겨났고, 다양한 팬덤들 또한 많이 생겼다. 하지만 이런 팬덤들 간의 갈등과 구분짓기 같은 문제들이 없지 않아 있는데, 이와 같은 것은 어떻게 해결하면 좋을지 궁금합니다.\n\n\n팬덤 갈등은 기실 유구한 역사입니다. 여러분들이 아마 보셨을지 모르겠습니다만, &lt;응답하라 1997&gt;과 같은 드라마에서도 묘사되는 바, “HOT” 팬덤과 “잭스키스” 팬덤의 갈등이 실제 사회적 문제로 대두되기도 했습니다. 이는 비단 한국만의 문제가 아니기도 합니다. 외국에서도 팬덤들 간의 충돌이 빈번하게 일어납니다. 아이돌 등과 같은 팝 아티스트 뿐만 아니라, 스포츠 계에서도 일어나는 일이지요. 따라서 쉽게 해결될 수 있는 문제라고 생각되지는 않습니다. 어찌보면 팬심이라는 것이 해결되어서도 안될 일일 것 같기도 하고요. 다만, 그것이 사회적 문제로 확산되지 않도록 경각심을 갖고 모두가 이해하고 관리하는 노력이 필요한 차원으로 접근해야겠습니다.\n\n\n(팬과 스타의 관계)팬덤의 문화는 사실 플라토닉 러브를 뛰어넘어서 유사연애와 어떻게 보면 위험한 심리 현상을 일으킨다고 봅니다. 메타버스를 활용하여 팬들이 본인의 스타에게 접근성이 높아져 더 가까워졌다는 인식을 가지게 되면 스토킹이나 유사연애 같은 안 좋은 심리적 현상이 늘어나지 않을까 걱정이 됩니다. 교수님께서는 메타버스를 팬덤에 적용하는 것에 어떤 부작용이 있을 것이라고 생각하시며, 어느 정도로 메타버스를 활용하는 것이 옳다고 생각하시나요?\n“1. 팬과 연예인의 관계는 복잡하고 다양한 관계인데, 이 관계도 열정적인 지지와 함께 이성과 존중을 유지해야 하며, 팬덤에서 발생하는 극단적인 행동에 대해 회사에서 관리나 도움을 줄 수 있을까요?\n“이번 카리나 이재욱 열애설에 아이돌들의 연애를 격하게 지탄하는 팬들의 모습이 외신으로 보도되면서 국제적으로 한국의 팬덤 문화에 대한 얘기가 나눠졌습니다. ‘돈은 덕후가 쓰고 용서는 머글이 한다.’ 파벌과 ‘진짜 현생을 살아라’ 파로 나눠서 갑론을박이 펼쳐지고 있는데 아이돌의 연애에 대한 팬덤의 반응과 앞으로 팬덤 문화는 어떻게 발전해야 한다고 생각하시는지 궁금합니다.”\n(과격한 팬덤 활동?) 앨범 판매량 부풀리기 등 k팝 팬들의 과격한 행동을 어떻게 보십니까.\n팬덤의 건전성과 수익성은 양립할 수 있나요?\n(팬밍아웃, 부끄러움?) 국내 케이팝 팬들은 팬덤에 속해있는걸 부끄러워 한다. 한국 팬덤 사이에는 일코라는 용어가 있을 정도이다. 하지만 해외에서는 kpop을 좋아하는 것을 부끄러하지 않는다. 이 차이가 있는 이유가 궁금하다\n케이팝 아이돌을 보며 그들이 전하고자 하는 메세지를 알기보다 그들의 멋진 겉면만 보고 좋아하는 사람들도 상당하다. 이는 과연 바람직하지 않은 것이라고 말할 수 있을까?\n\n\nK-Pop을 주도하는 보이그룹과 걸그룹의 과거 형태로 볼 수 있는 비틀즈, 백스트릿 보이즈, 원 디렉션, 스파이스 걸즈 등의 그룹들의 소통 방식을 보면, 과거 적절한 소통의 도구가 부재했습니다. 즉 소셜 미디어와 같은 기술이 없었습니다. 이로 인해 스타와 대중의 거리감을 상당히 했습니다. 거기에서 오는 신비주의 등이 새로운 아티스트의 색깔을 만드는 장점도 있었습니다. 그러나 지금의 기술 문화 시대에는 그런 것 없이, 질문자께서 말씀하신대로 일상에 대한 접근을 넘어 유사 연애, 스토킹과 같은 접촉, 스며듦이 심각한 문제가 되고 있습니다.\nK-Pop은 미디어 기술의 발달에 편승하여 거리감을 좁히며 소통하며 전 지구적인 인기를 누려 왔습니다. 그러나 반대급부로, 오히려 이 때문에 스타들로 하여금 또 다른 노동을 초래하기도 했습니다. 즉 노래하고 춤추는 것 이외에 팬과 소통이라는 것을 강요당하는 것이지요.\n팬덤 문화를 하나로 설명하기란 불가능에 가까우며, 또 과격한 반발심 역시 어제오늘의 일만은 아닙니다. 90년대 후반, 연예 산업 발달과 함께 소위 ’빠순이’라고 불리는 열혈 팬층의 활동이 사회적 문제로 급격히 대두되어 문제시되었습니다. 그들은 저마다의 아이돌에게 열혈 팬심을을 자청하며 파벌을 형성하고, (심지어는 그룹 내 멤버 간 팬 갈등도 있었습니다 - 문희준 간미연 열애설 당시 간미연은 면도칼 등 위협도구를 받기도 했습니다-) 팬덤은 하나의 시대적 현상으로 이해될 필요가 있습니다. 즉 인간이 특정 대상을 신격화고, 심리학적으로 광적으로 따르는 것으로 보아야지, 그것을 K-Pop이라고, 카리나라고, 누구라고 특정할 필요는 없다고 봅니다.\n\n\n케이팝 산업의 현황 및 도전\n\n(앨범 구매의 본질적인 변화) 최근 앨범 구매의 목적이 바뀌었다. 더 이상 자신의 아이돌과 연결하기 위해 애정을 쏟는 것이 아니라 이제는 하나의 유통 화폐가 된 포토카드 구매에 치중하고 있다. 앨범 구매의 본질적인 변화에 대해 교수님은 어떻게 생각하십니까?\n외국어 가사와 외국인 멤버가 해외에서 K-POP이 통하기 위한 필요조건일지 아닐지 궁금합니다.\n최근 에스파 카리나의 열애설로 sm 주가가 하락하는 모습까지 보이는 큰 파장을 일으켰습니다. 이를 해외에서는 한국의 팬덤 문화는 연예인의 사생활까지 관여한다라고 저격하기도 했습니다. 우리나라의 이런 팬덤 문화가 발달한 이유는 무엇일까요? 그리고 앞으로도 이런 자유롭지 못한 팬덤 문화가 지속될까요? 혹여나 해외 특히 미국처럼 자유롭게 변할 가능성은 없을까요?\n\n\n저는 그 본질은 크게 달라지지 않았다고 생각합니다. 그 옛날, 소위 ’덕질’의 선조들이라 불리는 서구 스타워즈, 스타트렉 덕질 하는 사람들도 소장용, 재판매용 앨범, 디스크 등을 하나씩 더 구매하기도 하는 등 다양한 형태로 팬 소비를 했습니다. 낯설지 않기에, 본질적인 변화는 아니라고 생각합니다.\n최근 한국인 없는 K-Pop 그룹이 등장하고, 영어, 중국어, 일본어 등으로 된 노래들을 내놓는 일이 빈번합니다. 이른바 융합(Convergence)가 더욱 가속화되는 모양새입니다. 강의 중에서 말씀드렸듯이, 문화와 기술의 융합뿐만 아니라 문화와 문화 간의 융합도 더욱 강화될 것으로 보입니다. 한국적 문화의 맥락을 지키는 것도 K-Pop의 힘을 오롯이 가꾸는 일이겠지만, 한편으로는 BTS가 다이너마이트, 버터, 퍼미션투댄스 와 같이 다양한 영어 노래로 국제적 성공을 거둔 것처럼 글로벌 시장에서의 성공을 위해 영어 노래 전략을 융화했듯이 융합적 전략을 구사할 필요는 있을 것입니다.\nK-Pop 현상을 BTS를 중심으로 풀어낸 미국의 한 사설(https://www.dw.com/en/whats-behind-the-bts-phenomenon/a-62113315 )에서는, 한류 신화의 몇 가지 특징 중 하나로 ’개인적 문제에 대한 터부시’를 꼽고 있습니다. 이는 비단 K-Pop 만의 문제가 아닙니다. 고 이선균씨의 문제도 마찬가지입니다. 이는 한국의 문화적 배경과 맞닿아 있는 것으로 이해됩니다. 아마 미국처럼 자유롭게 될 가능성은 지극히 낮아 보입니다. 이는 역사적 배경과 민족성에서 분리될 수 없는 배경 때문일 것입니다.\n\n\n\n(NEXT K?) k-pop, k-beauty가 현 한류라고 생각하는데 혹시 다음 k가 붙을 문화는 무엇이라고 예측하시나요?\nK-POP이 세계적으로 확산되면서 한국의 콘텐츠 산업이 겪게 된 도전과 기회는 무엇이 있나요?\nK-pop의 시장 가치가 국내외로 높게 인정받고 있는데, 왜 엔터사의 평균 보수는 (대기업에 준하는 정도로) 높은 편이 아닌지 궁금합니다.\n\n\n한류의 새로운 미래는 FOOD라 생각합니다. 이제 먹거리에 대한 관심이 높아지고 있습니다. 진정으로 음식 문화에 대한 세계인의 관심이 전지구적으로 높아지고 있으며, 냉동 김밥, 만두, 김치 등에 대한 수요가 확산되고 있는 것이 시장에서도 관찰됩니다.\n엔터사의 평균 보수…. 는 계약직의 한계로서 방송사 계약 구조…. 의 불공정성 때문입니다. 이 모든 것들은 선배들의 잘못된 관행과 시장의 불균형성 때문입니다. 여러분들이 조금씩 바꿔나갈 수 있기를 고대해봅니다. 미안합니다. ^^;;\n\n\n디지털 시대의 콘텐츠 소비와 팬덤의 변화\n\n메타버스는 어떤 모양으로 발전하게 될까? 메타버스가 발전을 거듭나면, 언젠가 메타버스가 우리의 일상이 될 수 있을까?\n메타버스를 통한 K-pop의 가능성은 아이돌 그룹의 온라인 플랫폼 활성화 등을 통해 입증되고 있는 것 같습니다. 앞으로 이런 플랫폼을 비롯한 산업 전반이 글로벌적 관점에서 어떻게 변화할지 궁금합니다.\n점점 기술이 접목되고 있는 한류 시장에서 팬들이 이를 진정으로 선호할 수 있을까? 이에 따른 매출 변화에 어떻게 대응할 수 있을까? (ex. 에스파의 컨셉에 불호가 있었음)\n플레이브의 성공요인, 그리고 플레이브와 같은 가상 아이돌이 지속적으로 성공할 수 있을지에 대한 교수님의 인사이트가 궁금합니다.\n\n\n메타버스는 아직 놀이문화 중심으로 크게 자리잡지 못한 느낌입니다. VR/AR 시장이 정착되지 못한 것은 크게 두 가지 이유라고 생각되는데, 하나는 일상에서의 불편함(착용, 사용의 불편함 – 배터리 문제, 무게감, 이질감 등), 다른 하나는 충분한 앱의 부재 두 가지 이유라 사료됩니다. 반면 마이너한 한류 팬덤 시장에서 이를 접목할 수 있을 것인가에서, 반은 회의적이고, 반은 희망적일 수 있습니다. 국내 OTT 시장에서 유일하게 흑자를 내는 것이 바로 애니매이션 OTT 플랫폼입니다. 마이너한 플랫폼 소구층에서 독자적으로 소비층을 확보해낸다면 이를 시장으로 만들어내는 힘이 생기듯, 메타버스 시장도 충분히 확보해낼 수 있을 것입니다.\n플레이브, 저는 그 선조격인 아담… 부터 알던 세대로서, 다소간에 회의적이긴 합니다. 그럼에도 일본에서는 버츄얼 아이돌이 성공하고(최근 도쿄 출장을 다녀왔는데, 거리에서도, 또 모바일로 소비하는 사람들의 모습에서도 충분히 그 시장과 수요를 짐작할 수 있을 정도였습니다) 확장하는 모습을 볼 때 한국에서도 가능성을 점칠 수 있었습니다. 다만 한국에서는 보다 즉물적이고 현실적인 것들을 쫓는 인상이 강합니다. 한국의 감각적인 것들, 강렬한 것들을 찾는 민족성과 맞닿은 것이 아닐까 개인적으로 생각해 봅니다. 따라서 플레이브의 성공 그 다음을 짐작하기란 대단히 힘듭니다만, 그럼에도 보다 감각적이면서 더 즉물적인 무언가를 소구할 수 있는 가상의 아이돌이라면 소비할 수용자가 있지 않을까 짐작해 봅니다.\n\n\nOTT 와 진로\n\nott 플랫폼으로 한국 작품도 많이 국제적인 사랑을 받고 있다고 생각하는데, ott 관련해서 진로를 결정한다면 어떤 준비를 하면 좋을까요?”\n\n\n’한국 OTT 시청자의 OK를 받으면, 전세계에서 OK다’라는 우스갯소리가 있다고 합니다. 그만큼 한국 시장의 시청자 눈이 높습니다. 콘텐츠 큐레이터, 기획자, 플랫폼 중개업, 심지어는 카피라이터와 OTT 포스터 디자이너, OTT 플랫폼 유튜브 콘텐츠 관리자 까지 다양한 2차, 3차 생산자 영역까지 크리에이터 영역이 펼집니다. 컬쳐 앤 테크놀로지 분야의 전공자들이 뛰어들 수 있는 영토가 더욱 넓어지고 있습니다."
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/04_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/04_week.html",
    "title": "패션과 뉴테크",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 4: 패션과 뉴테크\n\nDate: 20240326\n수업자료\nPre-class video\n\nHow data is driving the future of fashion | Steve Brown | TED Institute\n\n\n\nThe New Reality of Fashion is Digital | Gala Marija Vrbanic | TED\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/02_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/02_week.html",
    "title": "융합 콘텐츠 기획과 제작",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 월요일 20:00까지\n\n\n\n\nWeek 2: 융합 콘텐츠 기획과 제작\n\nDate: 20240312\n수업자료\nPre-class video\n\n직업탐구- 별일입니다 - 버추얼 휴먼 매니저 | EBS\n\n\n\n덕후에서 콘텐츠 크리에이터가 되는 방법 | 대도서관\n\n\n\n좋은 컨텐츠를 만드는 법 | 에이틴 제작사\n\n\n\nWhy AI Will Spark Exponential Economic Growth | Cathie Wood | TED\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/grad_immersive/weekly/index.html#intro",
    "href": "teaching/grad_immersive/weekly/index.html#intro",
    "title": "Weekly Content",
    "section": "Intro",
    "text": "Intro\n\n\nWeek 1: Course Intro\n\nDate: 20240305\nClass: Course Introduction\n\nAn overview of the course"
  },
  {
    "objectID": "teaching/grad_immersive/weekly/index.html#part-i-콘텐츠-기획과-제작",
    "href": "teaching/grad_immersive/weekly/index.html#part-i-콘텐츠-기획과-제작",
    "title": "Weekly Content",
    "section": "Part I: 콘텐츠 기획과 제작",
    "text": "Part I: 콘텐츠 기획과 제작\n\n\nWeek 2: 융합 콘텐츠 기획과 제작\n\nDate: 20240312\nPre-class video\n\n덕후에서 콘텐츠 크리에이터가 되는 방법 | 대도서관\n좋은 컨텐츠를 만드는 법 | 에이틴 제작사\nWhy AI Will Spark Exponential Economic Growth | Cathie Wood | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 강혜원 교수(SKKU)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_immersive/weekly/index.html#part-ii-문화와-문화-산업",
    "href": "teaching/grad_immersive/weekly/index.html#part-ii-문화와-문화-산업",
    "title": "Weekly Content",
    "section": "Part II: 문화와 문화 산업",
    "text": "Part II: 문화와 문화 산업\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n소속사 없이 음원 유통하는 법\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 정헌섭 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 4: 패션과 뉴테크\n\nDate: 20240326\nPre-class video\n\nHow data is driving the future of fashion | Steve Brown | TED Institute\nThe New Reality of Fashion is Digital | Gala Marija Vrbanic | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 류현석 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 5: 게임 & 인터랙티브 디자인\n\nDate: 20240402\nPre-class video\n\n게임 과몰입, 게임 잘못일까요? | 한덕현 중앙대학교 병원 정신건강의학과 교수 | | 세바시\nDiscussion\n\nClass\n\nGuest Lecturer: 김수완 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 6: 한류와 팬덤\n\nDate: 20240409\nPre-class video\n\nAll I Really Need To Know, I Learned From KPOP | Donald Lim | TEDxYouth@SJCS\nHas K-pop prepared us for the metaverse? | Alex Karlsson | TEDxBrunkebergstorg\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이종명 교수(SKKU)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_immersive/weekly/index.html#part-iii-문화-기술",
    "href": "teaching/grad_immersive/weekly/index.html#part-iii-문화-기술",
    "title": "Weekly Content",
    "section": "Part III: 문화 기술",
    "text": "Part III: 문화 기술\n\n\nWeek 7: 데이터 시각화의 예술\n\nDate: 20240416\nPre-class video\n\nThe beauty of data visualization - David McCandless\nData Visualization Best Practices - Stephanie Evergreen\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 전서연 강사\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 8: 문화콘텐츠와 자연어 처리\n\nDate: 20240423\nPre-class video\n\nText 데이터를 활용한 개인화 음악 추천서비스 웹 데모 구축 (혁신성장청년인재양성사업 프로젝트 발표)\nNLP Project - Emotion In Text Classifier App with Streamlit and Python\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 구영은 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 9: 메타버스와 메타휴먼\n\nDate: 20240430\nPre-class video\n\n버추얼 아이돌 그룹 플레이브의 탄생 과정과 기술 이야기 (언리얼 페스트 2023 서울)\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 원종서 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 10: 인터랙션 사이언스, UX\n\nDate: 20240507\nPre-class video\n\nHuman-Computer Interaction and User Interface Design | Tony Tang\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이대호 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 11: 서비스 디자인, 데이터 드리븐 마케팅\n\nDate: 20240514\nPre-class video\n\nWhy Data Marketing So Important | 세바시\nAge of Experience, Data+Service | 설상훈 교수님\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 설상훈 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 12: 가상/증강현실 콘텐츠 제작\n\nDate: 20240521\nPre-class video\n\n상상이 현실화가 되는 세계, 그 중심에 서서 외치다 | 서동일 오큘러스VR코리아 지사장\nWhat is extended reality? | The Gadget Show\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 김태원 대표(RGB Makers)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_immersive/weekly/index.html#part-iv-문화-콘텐츠-경영",
    "href": "teaching/grad_immersive/weekly/index.html#part-iv-문화-콘텐츠-경영",
    "title": "Weekly Content",
    "section": "Part IV: 문화 콘텐츠 경영",
    "text": "Part IV: 문화 콘텐츠 경영\n\n\nWeek 13: 융합콘텐츠와 창업\n\nDate: 20240528\nPre-class video\n\nGPT 있는데 왜 또 만들어요? 손쉽게 이해하는 AI 개발 트렌드와 비하인드\n세상을 바꾸는 뉴콘텐츠\n콘텐츠 창업에서 가장 중요한 것 | CEO 노정석\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 윤영훈 대표 (주)ASSI\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 14: 엔터테인먼트 경영\n\nDate: 20240604\nPre-class video\n\nHow Netflix changed entertainment – and where it’s headed | Reed Hastings\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이동찬 경영총괄 (TEO Universe)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_immersive/weekly/index.html#conclusion",
    "href": "teaching/grad_immersive/weekly/index.html#conclusion",
    "title": "Weekly Content",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "teaching/grad_immersive/about/index.html",
    "href": "teaching/grad_immersive/about/index.html",
    "title": "Course description & Communication",
    "section": "",
    "text": "Course description\n\n개요\n\n\n\n\nGoal\n\n목적\n\n\n\n\nTime & Location\n\n(3h) Mon 15:00 ~ 17:50 @국제관 첨단강의실[9B217]\n\n\n\n\nWeekly Design\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nLecturer\nNote\n\n\n\n\n1\n09/02/2024\nCourse Intro\n\n\n\n\n2\n09/09/2024\n실감미디어 실험 디자인 개요\n이창준\n\n\n\n3\n09/16/2024\n추석(휴강)\n\n\n\n\n4\n09/23/2024\n실감미디어 콘텐츠 기획(1)\n김태원\n\n\n\n5\n09/30/2024\n실감미디어 콘텐츠 기획(2)\n김태원\n\n\n\n6\n10/07/2024\n실감미디어 콘텐츠 제작(1)\n김태원\n\n\n\n7\n10/14/2024\n실감미디어 콘텐츠 제작(2)\n김태원\n\n\n\n8\n10/21/2024\n콘텐츠 피드백 및 마무리\n이창준\n김태원\n\n\n\n9\n10/28/2024\n실험 디자인과 분석을 위한 통계\n이창준\n\n\n\n10\n11/04/2024\n실감미디어 콘텐츠를 활용한 실험 계획 발표와 피드백\n이창준\n\n\n\n11\n11/11/2024\n실감미디어 콘텐츠를 활용한 실험 계획 수정 발표와 피드백\n이창준\n\n\n\n12\n11/18/2024\n실감미디어 콘텐츠 실험 (1)\n이창준\n\n\n\n13\n11/25/2024\n실감미디어 콘텐츠 실험 (2)\n이창준\n\n\n\n14\n12/02/2024\n실험 분석 결과 토론 (1)\n이창준\n\n\n\n15\n12/09/2024\n개별 연구 미팅\n이창준\n\n\n\n16\n12/16/2024\n최종 연구 발표\n이창준\n김태원\n\n\n\n\n\n\n\nAbout Lecturer (Modulator)\n\nChangjun LEE\nAssociate Professor (Head of Culture & Tech)\nSchool of Convergence. SKKU.\nchangjunlee.com\n\nTaewon KIM\nCEO, RGB Makers\n\n\n\nTeam Teaching\n\n실감미디어 콘텐츠 제작: 김태원 대표\n실감미디어 콘텐츠 활용 실험 연구 디자인과 분석: 이창준 교수\n\n\n\n\nEvaluation\n\nAttendance & Attitude (30 %)\nContent creation (30%)\nFinal Research Paper (40%)\n\n\n\n\nCommunication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gkvMrhDg\n입장하셔서 이름을 꼭 출석부에 있는 이름으로 설정해주세요.\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "teaching/grad_immersive/index.html",
    "href": "teaching/grad_immersive/index.html",
    "title": "Immersive Media & CT",
    "section": "",
    "text": "About Course\n\n\n\nCourse materials"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/index.html",
    "href": "teaching/grad_immersive/weekly_2/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nWeek\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n2\n\n\n융합 콘텐츠 기획과 제작\n\n\n강혜원\n\n\n\n\n3\n\n\n음악, K-pop, 엔터테인먼트\n\n\n정헌섭\n\n\n\n\n4\n\n\n패션과 뉴테크\n\n\n류현석\n\n\n\n\n5\n\n\n게임 & 인터랙티브 디자인\n\n\n김수완\n\n\n\n\n6\n\n\n한류와 팬덤\n\n\n이종명\n\n\n\n\n7\n\n\n데이터 시각화의 예술\n\n\n전서연\n\n\n\n\n8\n\n\n문화콘텐츠와 자연어 처리\n\n\n구영은\n\n\n\n\n9\n\n\n메타버스와 메타휴먼\n\n\n원종서\n\n\n\n\n10\n\n\n인터랙션 사이언스, UX\n\n\n이대호\n\n\n\n\n11\n\n\n서비스 디자인, 데이터 드리븐 마케팅\n\n\n설상훈\n\n\n\n\n12\n\n\n가상/증강현실 콘텐츠 제작\n\n\n김태원\n\n\n\n\n13\n\n\n융합콘텐츠와 창업\n\n\n윤영훈\n\n\n\n\n14\n\n\n엔터테인먼트 경영\n\n\n이동찬\n\n\n\n\n15\n\n\n기말시험\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/03_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/03_week.html",
    "title": "음악, K-pop, 엔터테인먼트",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\n수업자료\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n\n\n\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n\n\n\n소속사 없이 음원 유통하는 법\n\n\n\nAI가 만든 앨범 자켓\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/05_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/05_week.html",
    "title": "게임 & 인터랙티브 디자인",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 5: 게임 & 인터랙티브 디자인\n\nDate: 20240402\n수업자료\nPre-class video\n\n게임 과몰입, 게임 잘못일까요? | 한덕현 중앙대학교 병원 정신건강의학과 교수 | | 세바시\n\n\n\nGoogles New STUNNING AGI Breakthrough “Genie 1.0” (Bigger Than You Think)\n\n\n\nDiscussion\n\n\n\n\nDiscussion\n\n\n\n로드 중…"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/07_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/07_week.html",
    "title": "데이터 시각화의 예술",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 7: 데이터 시각화의 예술\n\nDate: 20240416\n수업자료_1\n수업자료_2\n수업자료_3\n수업자료_4\nPre-class video\n\nThe beauty of data visualization - David McCandless\n\n\n\nData Visualization Best Practices - Stephanie Evergreen\n\n\n\n\n\nDiscussion\n\n\n\n&lt;p&gt;로드 중…&lt;/p&gt;\n\n\n\n\nBest Discussion\n\n#1\n\n고등학교 때부터 정보라는 과목이 정식으로 생기면서 배운 경험이 있습니다. 그 때 엑셀 활용 방법을 배우면서 데이터 자료를 정리하는 방법을 배웠습니다. 네이터 검색 데이터를 활용한 그래프 만들기를 했었는데, 그 때 네이터 검색 키워드로 데이터가 정렬되는 모습을 보며 현대 사회에서는 사람들을 군집으로 분류하기 편리해졌다고 생각했습니다. 저는 그 때, 우산의 검색량이 가장 높아지는 달은? 이라는 주제로 과제를 진행했고 7, 8월이 가장 압도적일 것이라 예상했는데 결과는 의외로 3월 혹은 6월이 많았습니다. 함께 검색된 키워드 데이터를 통해 봄비 혹은 장마를 대비하기 위해서라는 이유를 얻었습니다. 이처럼 현대 사회에서는 데이터의 시각화를 통해 사회 현상을 읽을 수 있으며 이제는 이를 상업적으로 이용해야지만 살아남을 수 있겠다는 생각을 했습니다.\n또한 영상을 통해 증거의 등급을 매길 수 있다는 점을 새롭게 알게 되었습니다. 사실 저는 데이터 시각화는 사람들의 주관적인 생각을 시각화했다고 생각했습니다. 그래서 많이 나올 수록 무조건 좋은 게 아니라고 생각했습니다. 물론 증거의 등급을 나눈다고 해서 많이 나온 데이터가 무조건 옳고, 좋은 것은 아니지만 좀 더 신뢰를 가질 수 있을 것이라 생각합니다. 앞으로도 데이터 시각화에 대해 좀 더 편리하고 보다 정확한 기술이 나온다면 자료 수집 과정에서 시간을 상당히 줄일 수 있을 것이라 예상합니다.\n\n\n\n데이터 시각화는 영상에서 설명하는 것과 같이 복잡한 데이터를 이해하기 쉽고, 효과적으로 사람들에게 전달하는 수단인 것 같습니다. 데이터를 시각화하는 가장 기초적인 방법은 차트나 그래프 등이 있습니다. 이는 우리가 접하기 가장 쉬운 방법 중 하나인데요, 영상에서 나온 것과 같이 다양한 디자인과 패턴으로도 시각화가 가능하다는 것을 새롭게 알게되었습니다. 작년 2학기 때 구글 코랩으로 데이터를 시각화하는 법을 배웠었는데요, 그 당시에도 그냥 숫자로만 데이터를 보는 것보다 시각화하는 것이 데이터를 습득하는 데 더 많은 도움이 된다고 생각했었습니다. 오늘 영상 시청 후, 아트에 관심이 있는 저는, 앞으로 데이터를 시각화하는 데에 있어 디자인과 패턴 등이 얼마나 더 다양해질 지 궁금해졌습니다.\n\n\n\n저는 데이터 시각화에 대해 더 깊이 이해하게 되었습니다. 데이터 시각화는 단순히 데이터를 제시하는 것이 아니라 예술과 과학의 결합이기도 합니다. 복잡한 데이터를 단순화하고 추세와 패턴을 강조하며 데이터 스토리를 설명하고 탐색을 지원하고 비교를 촉진하여 복잡한 정보를 효과적으로 전달할 수 있습니다. 데이터 시각화의 목적은 데이터를 표시하는 것뿐만 아니라 시각적 효과를 통해 일관된 이야기를 전달하고 청중이 논리적 순서로 주요 통찰력을 이해하도록 안내하는 것입니다. 대화형 시각화는 청중에게 자신의 관심사와 필요에 따라 관련 통찰력을 찾을 수 있는 독립적인 데이터 탐색 기회를 제공합니다. 이 정보 폭발의 시대에 데이터 시각화는 중요한 의사소통 도구가 되었습니다.간결하고 명확하며 적절한 시각화 유형 선택과 데이터 스토리텔링.. 탐색 지원 및 비교 촉진과 같은 모범 사례를 준수함으로써 데이터의 아름다움을 더 잘 발견하고 데이터를 더 깊이 이해하고 활용하며 의사 결정과 행동에 대한 보다 강력한 지원을 제공할 수 있습니다.\n\n\n\n#2\n\n이 영상에서는 데이터 전문가가 사용하는 데이터 시각화 툴에 대해 많이 배우게 되었던 것 같습니다. QuickBooks Online부터 Excel과 Tableau까지 정말 많은 시각화 툴이 발전되고 있으며, 더불어 많은 회사들도 해당 툴을 활용하여 경영을 발전하는 트렌드를 보이고 있는 점이 신선했습니다(Salesforce, Survey Monkey 등). 해외 회사 말고 국내 회사들은 어떤 시각화 툴을 사용하고 있는지 궁금증이 들게되었습니다! 더불어 영상을 보면서 데이터 시각화에 대해 조금 친근함을 느끼게 되었습니다. 어릴 때 자주했던 마인드 맵이나 to do list 작성 등등, 이 모든 것들도 나의 머릿 속 정보들을 시각화하여 정리하는 것이라고 말해주는 작가의 말 한마디 덕에, 멀게만 느껴졌던 데이터 시각화가 생각보다 우리의 삶 가까이에 활용되고 있었음을 새삼 깨닫게 되었습니다!\n\n\n\n데이터 시각화에 인간의 개입이 중요하다고 밝힌 점이 인상깊었다. 결국 시각화 결과물을 받아들이는 자는 인간이기에, 인간이 보기 용이하려면 당연히 인간의 개입이 중요할 수밖에 없으리라는 생각이 든다. 그러나 이렇게 당연한 점이 중요하게 작용하는 이유는, 다양한 기술이 활성화된 이 시대에, 그저 자동화된 프로그램이나 생성형 AI 등 시각화 툴이 그려주는 데이터 형상들을 정제 없이 그대로 받아들인다면 데이터에 대한 주도적인 인사이트를 얻지 못할 가능성이 매우 크기 때문이다. 인간이 스스로 시각화를 할 만한 가치가 있는 데이터의 특성들을 선택할 수 있는 능력을 가지고 있어야 사회에 필요한 요소들을 시각적으로 바로 받아들일 수 있기 때문이다.\n\n\n\n\n\nBest Questions\n1. 데이터 시각화의 한계와 가능성에 대한 깊은 고민\n데이터 시각화를 통해 일어난 일들을 분석하고, 앞으로 일어나는 일에 대해 추측하는부분에 있어 도움을 주는 것은 사실입니다. 하지만 역사를 보면 알 수 있듯이 생각지도 못한 일들이 우리 가운데에 일어나고 변화와 혁신을 주기도 합니다. 이러한 부분에서 데이터 시각화가 오히려 혁신을 불러일으키는 데에 방해를 줄 수도 있을 것이라 생각이 드는데, 어떻게 하면 혁신을 가로막지 않으면서도 데이터 시각화를 통해 영감을 줄 수 있을지 궁금합니다.\n\n2. 데이터 시각화의 목적에 대한 의문 제기로 출발점과 목적을 다시 생각하게 만드는 질문\n사회에서 데이터를 활용하는 것이 일반화되기까지 얼마 되지 않았다고 생각하는데, 데이터 시각화가 시작된 계기가 궁금합니다. 어떤 것에 활용하기 위해 시작되었나요?\n데이터는 사람들에 의해 형성이 된다고 하잖아요. 근데 반대로 데이터에 의해 만들어지는 사회 흐름, 상황으로 인해 사람들이 따라가는 것은 아닌가요? 예를 들어, 어떤 신규 카페의 검색량이 높아져 이 카페가 인기 카페로 분류되고, 사람들이 많이 방문하는 것처럼 말입니다. 만약 아니라고 해도 앞으로도 그러지 않을 것이라고 생각하시나요?\n\n3. 데이터 시각화의 신뢰성에 대한 중요한 이슈\n1. 사전 영상을 보고 나서 데이터 시각화에 대한 회의감이 좀 많이 들었던 것 같습니다. 두 영상의 작가는 모두 우리가 데이터를 통해 얻고자 하는 답과 통찰력만 가지고 있다면, 그것에 맞춰서 데이터를 시각화할 수 있다고 합니다.\n의도적으로 일부 데이터를 시각화 단계에서 제외를 하거나, 왜곡하는 방향으로 시각화 차트를 만들 수도 있어 오히려 의사결정에 혼란을 줄 수 있다면, 데이터는 일부 중요한 의사결정자에게만 공개하는게 낫지 않을까요?\n데이터를 시각화를 하는 것이 진실을 알려주기보다는 많은 사람에게 나의 의견을 뒷받침받기 위한 전략으로 보입니다.모든 사람들에게 시각화된 데이터를 공개를 해야하는 이유는 무엇인가요?\n2. 영상은 해외 데이터 전문가가 강연을 하여서 해외에서 사용하는 시각화 툴을 많이 알려주었다. 국내에서는 어떤 시각화 툴이 유행을 하고 있는지, 시각화를 전문적으로 하는 기업이 있는지, 교수님께서 눈여겨 보는 시각화 트렌드가 있는지 궁금증이 들게되었습니다!"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/09_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/09_week.html",
    "title": "메타버스와 메타휴먼",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 9: 메타버스와 메타휴먼\n\nDate: 20240430\n수업자료\nPre-class video\n\n버추얼 아이돌 그룹 플레이브의 탄생 과정과 기술 이야기 (언리얼 페스트 2023 서울)\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n기술 개발과 적용 당시 선풍적인 인기를 끌었던 실제 사람과 비슷한 모델을 만들고자 하는 것이 아닌 만화 형식을 발전시키는 선택을 했다는 것이 인상 깊었다. 메타버스 기술과 가상인간에 대한 접근이 늘어나면서, 이러한 기술을 만들고 일반 대중에게 공개할 때, 우리가 처음 느끼는 감정은 신기함보다는 실사화에 대한 어색함이라고 생각했다. 평소 이러한 부분에 대해 생각을 갖고 있었는데, 만화 형식을 차용한 부분에서 어색함에 대한 역치를 낮출 수 있는 좋은 시도라는 생각을 할 수 있었던 것 같다. 항상 우리가 새로운 것을 만들어 가는데 있어 앞으로 가져야 할 자세는 기존의 것과 동떨어진 신기한 것을 만들어 내는 것이 아니라, 기존의 것과 잘 융화될 수 있는 자세를 갖추는 것임을 다시금 알게 되었던 것 같다.\n\n\n\n왜 실제 사람도 아닌 모습으로 캐릭터를 구축했을까 의문이 들었습니다. 너무 캐릭터 같아서 애니메이션에 나오는 2D 캐릭터 같았는데, 너무 실사화 된다면 타겟층의 소비자들이 오히려 거부감이 들 수 있다는 것을 고려했다는 점을 새로 알게 되었습니다. 플레이브가 음악방송에 출현하는 모습을 보며 항상 생각했습니다. 촬영 감독 등의 스태프들은 그들의 원래 모습, 본 모습을 볼 수 있는 것 아닌가? 근데 왜 소문이 나지 않을까, 만약 본 모습을 들키게 된다면 어떤 파장을 일으켜올까? 등 너무 궁금했습니다. 이건 아직도 궁금한 점입니다. 최근 런닝맨 프로그램에서도 버추얼 아이돌 경험해보기로 직접 멤버들이 캐릭터를 만들고 실시간 방송을 한 적이 있었습니다. 그걸 보면서 버추얼 캐릭터를 이용해 어떻게 실시간으로 방송하는지 알게 되었다. 이전에는 모델의 모션을 따서 어색한 느낌을 주는 캐릭터만 구현 가능했다면 이제는 실시간으로 어떤 모션을 취하는지에 따라 완벽한 싱크를 보여주며 구현할 수 있다는 점이 AI 기술의 발전이 엄청나게 되었다는 점을 몸소 느끼게 해주었습니다.\n\n\n\nVFX 또는 SFX 등 CG 기술력의 발전에 놀라운 동시에 아이러니하게도 CG 기술력이 비전공자들에게 과대평가되고 있다는 생각도 하게 되었다. 영상에서 언급되었듯, 영화계의 비주얼 이펙트 기술은 대단하지만 후가공의 과정이 불가피하다는 것이다. 또한 플레이브와 같은 버추얼의 영역은 기존 영역과는 다르고 기존의 모션 캡쳐 기술력으로는 표현하기 부족하다는 사실을 알게 되었다. 이러한 사실을 비전공자들은 자세히 알기 어렵기 때문에 과대평가되고 있다고 생각하게 되었다. 그럼에도 블래스트는 언리얼 엔진을 이용한 시네마틱+라이브의 영역에 발을 들였고, 자체 기술을 개발해 성공적인 결과를 얻었다. 특히 높은 기술력으로 기존 버튜버 산업과 차별을 두고 대중성 강한 케이팝 장르와 접목한 점, 모션 캡쳐 과정에서 신체간섭문제나 리타게팅의 한계를 해결하고자 매 프레임마다 백터 계산을 적용한 것이 놀라웠다.\n\n\n\n최근 민희진 프로듀서의 기자회견을 통해 드러난 아이돌 업계의 현실을 알고 나서 다시 사전 영상을 보니, 버추얼 아이돌을 프로듀싱의 측면에서 바라보지 않을 수 없었다. 영상 전반에 걸쳐, 블래스트 관계자는 실시간 모션 캡처의 어려움과 기술적 극복과정을 설명한다. 놀라웠던 점은, 이 프로젝트가 수많은 장애물을 극복할 투자가치가 있다고 생각해 포기하지 않고 개발한 프로듀서의 선구안이었다. 안되는 이유에 비해 해야하는 이유는 너무나도 적었지만, 포기하지 않고 독자적인 기술을 발전한 블래스트팀의 노고가 대단하게 느껴지며, 이런 프로듀서가 성공하는구나 생각하였다!\n하지만 걱정되는 점은, 플래이브의 소속사인 블래스트는 영상에도 나왔듯이 기존에는 시네마틱 라이브 영상을 만들기 위해 설립된 VFX회사이다. 하지만 기술을 발전하고 수요를 파악하다 연예 기획사의 업무까지 겸업을 하게 되었다. 버추얼 아이돌의 경우, 100프로 AI가 활동하는 것이 아닌, AI의 필터를 쓴 인물들이 아이돌로 활동하는 것이다. 아무래도 ’얼굴없는가수’와 비슷한 계열로 활동을 하다보면, 팬층이 두꺼워져도 본인 그 자체를 좋아해주기보단 AI의 모습을 좋아해주는 것은 아닌지, 멘탈 관리가 힘들어질 것 같다. 하지만 소속사는 기술회사의 업무를 중점적으로 하고 있다보니 과연 소속 아이돌들의 멘탈 관리에 신경을 쓸 수 있는지, 아티스트 케어보다는 기술 발전에만 노력을 쏟고 있는 것은 아닌지 프로듀싱의 상황이 많이 궁금해지는 영상이었다.\n\n\n\n플레이브 대한 거부감의 원인을 알아보고, 플레이브가 사람들에게 친근하고 거부감 없게 다가가려면 어떤 식으로 기획하고 마케팅해야할 지 연구해본 적이 있는데, 이런 영상은 컬텍 학도로서는 관련 지식을 얻을 수 있어 정말 도움 되지만, 플레이브 자체를 마케팅하는 데 있어서는 역효과가 날 것이라고 생각했다. 플레이브의 차별점, 셀링 포인트는 ’춤을 추는 인형탈 알바, 서로 싸우는 야구 마스코트’처럼 자기가 쓴 탈에 몰입하고, 자기가 탈을 쓰고 있음을 알고 있음에도 나오는 뻔뻔함이라고 생각했기 때문이다. 멤버들이 나오는 영상을 보면, 상황에 몰입하거나, 그래픽으로 나오는 벌칙 등을 진짜 맞는 것처럼 오버액션 하는 등의 모습을 보이는데 이런 모습이 보는 이로 하여금 친근함을 느낄 수 있게 할 것이라고 생각했기 때문이다.\n\n\n\n디지털 전환이 가속화됨에 따라 메타버스가 미래 생활과 학습의 중요한 부분이 되고 있습니다. 가상 세계와 현실 세계의 융합은 엔터테인먼트 분야뿐만 아니라 교육, 업무 등 다양한 분야에서 새로운 가능성을 열어주고 있습니다. 또한 메타버스 환경에서의 메타휴먼이라는 가상 인물은 독특한 특징을 가지고 있습니다. 메타휴먼은 360도 전방위 모델링과 실시간 상호작용 능력을 갖추고 있으며, 이를 위해서는 3D 모델링, 모션 캡처, 실시간 렌더링 등 첨단 기술의 융합이 필요합니다. 더불어 메타휴먼이 엔터테인먼트 산업에 미치는 영향에 대해서도 생각해 보았습니다. 메타휴먼은 가상 환경에서 실시간 공연을 할 수 있어, 엔터테인먼트 소비 패턴에 새로운 가능성을 열어줄 것입니다. 학생으로서 저 또한 이 분야에서 미래의 발전 가능성을 고려해 보고 있습니다. 마지막으로 메타버스 환경에서는 문화적 차이에 대한 이해가 중요하다는 점을 깨달았습니다. 다양한 문화적 배경을 가진 사용자들의 요구를 존중하고 이해하는 것이 메타버스 애플리케이션의 광범위한 수용과 포용성을 위해 필수적입니다. 전반적으로 메타버스와 메타휴먼에 대한 학습을 통해 미래 발전 동향에 대한 깊이 있는 인식을 갖게 되었습니다. 또한 기술 학습, 산업 적용, 문화 적응 등 다양한 측면에서 새로운 아이디어를 얻게 되었습니다. 이를 바탕으로 학생으로서 메타버스 시대에 보다 잘 준비할 수 있을 것 같습니다.\n\n\n\n\nBest Questions\n\n30분이라는 긴 영상동안 블래스트 관계자가 설명하는 실시간 모션 캡처의 수많은 변수와 어려움, 그리고 이를 기술적 극복 과정을 과정이 정말 인상 깊었습니다. 불필요한 아트 에셋 누적 관리의 어려움, 시네마틱과 라이브 작업의 충돌, FK와 IK의 밸런스 조절 어려움, 후가공 단계의 소멸 등 이 프로젝트가 망할 이유는 수만가지가 있었습니다. 그럼에도 불구하고 블래스트 팀이 이 메타버스와 메타휴먼의 가능성을 믿고 프로젝트를 진행할 수 있었던 마음가짐은 어디서부터 나왔던 것인지, 교수님이 바라보셨을때 메타버스와 메타휴먼이 나아갈 수 있는 가능성은 어디까지인지 궁금합니다! 더 나아가 블래스트 외에 눈여겨 보시는 비슷한 계열의 회사가 있으신지 궁금합니다~\n\n\n\n앞으로 메타버스, 그리고 메타휴먼에 대한 개발에 있어 계속해서 이슈화 될 수 있는 내용은 필연적으로 ’현실성’과 관련이 있다고 생각한다. 1학년 교양 수업으로, ’창의적융합디자인’이라는 수업을 수강했던 경험이 있다. 이 때 당시, 제페토를 구현하며 학교 주변 건물과, 사회 문제의 해결을 위한 아이디어 제시에 활용했던 경험이 있다. 이 당시에 들었던 생각은, ’아직까지 많이 부족하다’는 생각이었다. 나의 기술적 숙련도가 부족한 것이 가장 큰 원인이었겠지만, 기술적으로 제대로 구현되었다 하더라도 인식 체계가 이를 쉽게 받아들이지 못할 것 같은 느낌이었다. 사람들의 인식 체계와 기술 사이의 괴리를 줄이기 위한 방안으로 어색함의 역치를 감소시키는 것과 같은 개념이 아닌, 새로운 것의 창조를 통한 해결 방안은 없을까?\n\n\n\n저는 오랫동안 즐겨한 RPG 게임이 있습니다… (조금 마이너하다고 생각해서… 이름을 써도 잘 모르실 것 같지만 &lt;파이널판타지14&gt;라는 게임입니다…) 게임을 하면서 제 캐릭터의 외관을 보기 좋게 만들고, 재화로 예쁜 옷을 사서 입히고, 전투적인 콘텐츠를 즐기지 않더라도 광장 같은 곳에 캐릭터를 함께 세워 두고 그냥 유저들과 대화를 하기도 합니다. 어떤 사람들은 공간을 꾸미는 ’하우징’이라는 기능을 이용해서 과제를 위한 스터디카페를 열고, 극장을 만들고, 게임 내의 기능으로 합을 맞춰 연주를 선보이기도 하고요… 그냥 예쁘게 만들어진 공간을 걸어다니며 사진 찍는 것을 좋아하는 유저도 있습니다. 그래서 몇년 전, 한창 ’메타버스’라는 개념이 뜨기 시작할 때, 유저들과 우스갯소리로 ’여기가 메타버스다. 여기로 와라.’라는 말을 주고받기도 했습니다. 그래서 하고자 하는 질문은… 메타버스와 RPG게임의 차이가 무엇인가요? 실제로 게임적 요소의 유무 외에는 메타버스와 RPG 게임 사이에 어떤 차이점이 있는지 잘 모르겠습니다. 게다가 메타버스를 표방하는 플랫폼에서는 콘텐츠로서 게임적인 요소를 추가하고 있기도 하고요. 기존에 존재하던 서비스와 효용이 크게 다르지 않다면, 왜 갑자기 메타버스라는 기술이 뜨고 관심을 받게 된 건지도 궁금합니다…"
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/11_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/11_week.html",
    "title": "서비스 디자인, 데이터 드리븐 마케팅",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 11: 서비스 디자인, 데이터 드리븐 마케팅\n\nDate: 20240514\n수업자료 1\n수업자료 2\nPre-class video\n\nWhy Data Marketing So Important | 세바시\n\n\n\nAge of Experience, Data+Service | 설상훈 교수님\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n글부터 미디어까지, 형태에 따라 의사소통의 해상도가 높아지며 경험의 전달률이 높아진다는 내용이 흥미로웠다. 동시에 모든 의사소통의 행위자가, 자신의 경험의 전달률을 높이고 싶은 목적으로 의사소통에 임하는 것은 아니라는 것 역시 느꼈다. 실제로 나는 영상 미디어보다 텍스트를 선호하는데, 영상 미디어에서는 그 영상의 기획/제작/공유자의 의도가 너무 직관적으로 다가와서 오히려 수용자로 하여금 거리감을 마련해주지 않다고 느껴지기 때문이다. 의사소통 과정에서 틈을, 이를테면 노이즈를 만들어주는 텍스트가 나에게는 해상도가 낮을 수는 있어도 의사소통을 더 편하게 해주는 형태인 것이다. 그래서 앞으로 데이터를 통한 경험 전달률을 무조건 극대화하는 게 아닌, 더 높은 해상도를 원하면서 동시에 수용할 수 있는 상대와 분야에 한정해 이뤄져야 하지 않을까 생각했다.\n\n\n\n현재 ‘우리는 인공지능에 의존하게 될 것이다.’ 혹은 ’인공지능이 인간을 대체할 것이다’라는 우려와 두려움에 조금씩 잠식되어 가는 중인 것 같습니다. 하지만, 영상에서도 언급하고 있는 내용처럼 인간이 할 수 있는 일, 인간이 더 잘할 수 있는 일인 “해석”과 그 결과로 새로운 통찰을 찾는 것이 있습니다. 역사적으로 인간과 비인간을 구분함에 인간은 항상 자신들만이 할 수 있는 것만을 찾아왔습니다. 인공지능이 발달한다고 해서 인간이 설 수 있는 자리가 없는 것은 아닐 것입니다. 인공지능을 활용해서 다만 우리가 현재로서는 느끼기 어려운 또 다른 인간의 장점들이 드러나고, 그것을 발전시키는 방향으로 변화할 것이라고 생각합니다. 양극단에 있는 데이터 역시도 간과해서는 안된다는 내용이 가장 기억에 남습니다. 특히, 경험에는 좋고 나쁜 것이 없으며 그것을 나눌 수도 없다는 부분이 인상깊었습니다. 비슷한 맥락에서 몇 주전 “자연어처리”를 주제로 수업을 진행했을 때, 장애인의 목소리가 되어주는 AI 기술과 관련된 영상이 제시되었는데, 이 역시도 일반적으로 우리가 지나칠 수 있는 이들의 경험, 그리고 이들의 경험을 더 나은 방향으로 발전시킬 수 있도록 하는 것이 또 하나의 기회로 작용할 수 있으며, 데이터는 이런 측면에서 우리가 쉽게 지나칠 수 있는 부분을 알려주는 수단이라고 생각합니다.\n\n\n\n데이터 기반 의사결정이 중요하다고 들었는데, 늘 ’어떤 데이터’를 가지고 ’어떤 가설’을 만들어야하는지 궁금했었다. 영상에서 예시로 나온 ’예측 정교화를 위한 여러 가설들’을 보니, 어쩌면 당연한 이야기들이었다. (ex. 긍정적인 바이럴과 판매량 비례하여 증가) 해당 가설을 검증하기 위해 필요한 데이터, 이 데이터를 수집하기 위한 키워드를 차례로 붙여보니 ’어떤 데이터’들이 필요한지 쉽게 와닿았다. 가설과 검증이라고 해서 처음부터 새롭고 신박하며 복잡하지만, 논리적으로 타당해야한다고 생각했는데, 창의력은 검증 과정이 아니라 검증 결과를 통해 마케팅 아이디어를 도출하는데에 필요하다는 것을 깨달았다. 경험데이터 정규분포표에 나타냈을 때, 양쪽 끝에 해당되는 이상치를 배제하지 않음으로서 아직 오지 않은 트렌드를 예측할 수 있다는 것을 알게 되었다. 이 데이터들을 입체적으로 분석해보면 지금 유행하고 있는 데이터와 아닌 것으로 구분할 수 있고, 메인 데이터를 제거하면 가치있는 데이터를 사용할 수 있게 된다. 결국 낮은 신뢰도의 데이터 구간 중에서도 신뢰도가 더 낮은 데이터들이 서비스 디자인에서는 유효하게 작용할 수 있다는 것을 알았다.\n\n\n\n강연에서 제시된 구체적인 사례를 보고 마케팅에서 데이터를 어떻게 활용할 수 있는지, 데이터가 어떤 효과를 가져올 수 있는지를 이해할 수 있었습니다. 그리고 사례를 보니 좋은 마케팅은 어느 산업 분야에서나 중요 요소라는 것이 새삼 느껴지는데, 추상적으로 존재했던 소비자들의 니즈나 평가를 데이터 수집을 통해 정량적으로 확인할 경우, 마케팅 계획이 더 효율적으로 이루어질 수 있을 것 같습니다. 그리고 모든 사람들이 데이터 마케팅의 중요함을 알고 채택할 경우, 더욱 중요해지는 것은 데이터 수집의 영역보다 해석 방식이 될 것이라는 점도 깨달았습니다. 경험을 전달하는 데에 있어서 어떤 방식이 의사소통의 해상도가 높은지 순서대로 나타낸 부분이 인상 깊었습니다. 가장 해상도가 높은 경험을 실현하는 방법으로는 지금까지 프로토 타입과 메타버스가 있다는 것을 알게 되었고, ’앞으로 그 이상 어떤 방법이 있을 것인가’에 대한 질문에 대해 깊게 생각해보게 되었습니다.\n\n\n\n\nBest Questions\n\n(그래서 어떤 데이터가 필요할까요?) 데이터 기반 접근이 마케팅 전략에 큰 영향을 주고 있다는 사실은 인지하고 있는데, 질문하고 싶은 것은 서비스 디자인과 데이터 드리븐 마케팅을 효과적으로 통합하기 위해서는 어떤 종류의 데이터가 필요한지 궁금합니다. 그리고 이러한 데이터를 수집하고 분석하여 서비스 디자인에 어떻게 반영할 수 있는지도 궁금합니다.\n\n\n\n(비주류 데이터를 바라보는 관점) 데이터 베이스의 마케팅에 초점을 두게 되면 두번째 영상에서 언급된 양 끝의 데이터는 점점 더 소외될 것이다. 데이터에 의해 주된 정보, 유행중인 문화는 더욱 부각되고 마케팅 됨으로써 더더욱 주류가 될 것이다. 하지만 이러한 현상이 지속된다면 문화가 단면적이게 되지 않을까라는 의문을 갖게 되었다. 실제로 세계적으로 문화 획일화가 기하급수적인 속도로 진행되고 있다.\n\n\n\n(크롤링의 어려움) 최근 구글을 비롯한 주요 웹사이트에서 서드 파티 쿠키를 제한함으로서 사용자들의 정보를 모으기가 점점 더 어려워진다고 생각합니다. 이러한 상황에 대해 어떻게 보시고, 데이터 드리븐 마케팅의 방식이 어떠한 방식으로 발전할 것이라거 보시는 지 궁금합니다."
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/13_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/13_week.html",
    "title": "융합콘텐츠와 창업",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\nDiscussion 제출 (pre-class 내용과 Question)\n\nSubmission due: 수업전 일요일 자정까지\n\n\n\n\nWeek 13: 융합콘텐츠와 창업\n\nDate: 20240528\nPre-class video\n\nGPT 있는데 왜 또 만들어요? 손쉽게 이해하는 AI 개발 트렌드와 비하인드\n\n\n\n세상을 바꾸는 뉴콘텐츠\n\n\n\n콘텐츠 창업에서 가장 중요한 것 | CEO 노정석\n\n\n\n\n\nDiscussion\n\n\n\n로드 중…\n\n\n\n\nBest Discussion\n\n\n나는 이 영상을 보기 전까지, AI를 창업에 가져온다는 생각은 너무나도 큰 자본과 기술력이 필요할 것이라는 편견에 휩싸여 생각조차 안했었다. 하지만 수직적 AI라는 개념을 알게 되면서, AI시장에서도 틈새시장이 존재한다는 점을 알게 되었다. 특정 산업 분야에서 문제를 해결하기 위해 AI를 활용하거나 창작하는 것은 생각보다 도전할만하며, 특히 여러 수직적 회사 중 런웨이(runway)라는 회사가 가장 인상깊었다.\n\n\n\n\n\nVR과 AR과 MR의 명확한 개념과 실제 적용 사례를 비교하며 의미를 이해할 수 있었다. 그중에서도 흥미로운 것은 VR툰이었다. 드라마 기획자를 꿈꾸는 입장에서, 드라마라는 콘텐츠 역시 VR, AR, MR과 같은 기술요소를 접목시키는 시기가 올 수 있다고 생각하는데, 웹툰 시장에서는 이미 그런 시도가 있다는 점에서 큰 인사이트가 되었다. 웹툰은 스토리를 이미지 단위로 전달하고, VR은 이미지를 실감나게 구현하는 기술이라는 점에서 콘텐츠 특성과 기술 특성이 잘 접목된 것 같다. 다만 드라마는 여러 이미지들이 연결된 장면이 스토리 단위라서, 동일한 접근으로 기술 접목을 하면 부자연스러울 수 있을 것 같다. 드라마 콘텐츠 기획자로서, 드라마의 특성과 기술의 특성을 모두 잘 이해해야 앞으로 드라마 시장에서의 새로운 콘텐츠를 상상하고 실제로 구현해낼 수 있을 것이라 생각한다. 때문에 기술 분야에 대한 심도 깊은 이해가 결국 앞으로의 콘텐츠 기획과 창작에 인사이트가 될 것임을 다시 인지할 수 있었던 영상이었다.\n“꾸준히 하나를 하고 싶어도, 큰 거 하나를 이루지 못했기에 끊임없이 도전하는 것이다.” 연쇄 창업가를 하는 이유에 대해 솔직하게 발언하신 게 인상적이다. 창업은 돈을 벌어야 하는 것과는 별개로, 계속 상상해내고 상상해낸 것을 구현해내고 구현해낸 것의 한계를 부딪혀야 하고, 그 한계를 계속 버텨내야 하고, 버텨내면서 극복해내야 하는 과정의 연속인 것 같다. 설령 극복이 안 되어도 사명감과 진정성으로 계속 견뎌내야 한다. 나 역시 이전에 청년협동조합 법인을 세우고, 사업자로서 2년 가량 활동하면서 나에 대한 발견을 할 수 있었다. 나는 스스로 창의적이며 창조하는 것을 좋아하는 사람인 줄 알았지만, 그것을 업으로 지속할 만큼의 집념과 진정성이 부족한 사람이었다. 이처럼 창업은 즐거움보다는 고통을 더 자주 느끼게 되는 과정임은 분명하지만, 그만큼 무엇이든 ‘보게 하는’ 기회임도 분명하다. 나 자신에 대해, 또 내가 관심 있던 주제에 대해, 나의 가치관에 대해, 폭넓게는 정말 사회에 대해서도. 뭐든 그전에는 보지 못했던 것을 보게 한다는 점에서, 영상의 발언자와는 다르게 대학생 시절 창업을 도전하는 것을 권장해보고 싶다.\n\n\n\n\n\n거대언어모델(LLM)이 중요한 것을 알고 있었지만, 이 영상을 통해 왜 중요한지를 좀 더 구체적으로 알게 되었다. 어떤 분야의 인공지능이던 결국 인간과 소통하려면 인간의 언어를 이해해서 아웃풋을 만들어내거나, 인공지능이 알아낸 정보를 자연어로 표현해야 하기 때문에 인공지능의 코어에 해당하는 것을 이해할 수 있었다.\nAI의 발전으로 인해 일자리가 줄어든다는 부정적인 예측들도 있고, 그것이 일정 부분 맞는 예측이긴 하지만 한 편으로는 개인들이 쉽고 저렴하게 더 퀄리티 있는 콘텐츠나 작업을 할 수 있도록 하는 것도 사실인 것 같다. NC의 LLM을 개발하신 분께서 마지막에 말씀하신 것처럼 인공지능을 활용해서 더 고차원적인 활동을 어떤 식으로 할 수 있을지 상상하고 고민하는 것이 중요한 것 같다. 구글과 같이 큰 회사에 자신의 회사를 매각하며 큰 성공을 거두었으면서도 끊임없이 창업을 하시는 노정석 CEO님의 열정이 가장 인상깊었다. 사람들은 사업을 단순히 돈벌이 수단으로 여기곤 하지만 하지만 노대표님의 경우 수익 창출 그 이상을 것을 위해 창업을 하시는다는 것을 그 짧은 영상을 통해서도 알 수 있었다. “이건 확실히 올 미래이고, 내가 하지 않으면 아무도 하지 않을 것 같아”라는 정신이 있어야 한다는 말씀을 하셨다. 이러한 생각을 해야 창업을 할 수 있을 것 같고, 이런 생각을 하려면 전문적인 지식과 더불어 시장을 읽는 능력, 보통 사람들이 알아차리지 못하는 문제를 인식하는 통찰력 등이 필요할 것 같다.\n\n\n\n\n\nBest Questions\n\nAI 기술이 콘텐츠 창작을 어느정도 자동화하고 창의력을 보조하는 도구로 활용될 수 있다는 점이 인상적이었습니다. 이러한 AI 기술을 이용한 콘텐츠 창작이 창업 초기 단계에서 어떤 이점을 제공할 수 있을까요? 또한, 창업을 준비하는 사람들이 AI 기술을 효과적으로 활용하기 위해 고려해야 할 중요한 요소는 무엇인가요?\n\n\n\n창업 시 자신이 하는 일이 성공할 것이라고 확신할 수 있는 방법은 무엇인가요?창업을 준비하는 과정에서 가장 큰 불확실성은 바로 성공 가능성입니다. 많은 시간과 자원을 투자하기 전에, 그 사업 아이디어가 성공할 가능성이 높은지 평가할 수 있는 방법을 알고 싶습니다. 이는 창업자들이 더 나은 의사결정을 하고 리스크를 최소화하는 데 도움이 될 것입니다. 성공 여부를 미리 판단할 수 있는 지표나 방법이 있다면, 창업 과정에서 자신감을 가질 수 있을 것입니다."
  },
  {
    "objectID": "teaching/grad_immersive/weekly_2/posts/15_week.html",
    "href": "teaching/grad_immersive/weekly_2/posts/15_week.html",
    "title": "기말시험",
    "section": "",
    "text": "Weekly content\n\n수업에 참석하기 전, 아래 활동을 완료하시기 바랍니다:\n\npre-class video 시청\n\n\n\nConclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "teaching/grad_stat/weekly/index.html#intro",
    "href": "teaching/grad_stat/weekly/index.html#intro",
    "title": "Weekly Content",
    "section": "Intro",
    "text": "Intro\n\n\nWeek 1: Course Intro\n\nDate: 20240305\nClass: Course Introduction\n\nAn overview of the course"
  },
  {
    "objectID": "teaching/grad_stat/weekly/index.html#part-i-콘텐츠-기획과-제작",
    "href": "teaching/grad_stat/weekly/index.html#part-i-콘텐츠-기획과-제작",
    "title": "Weekly Content",
    "section": "Part I: 콘텐츠 기획과 제작",
    "text": "Part I: 콘텐츠 기획과 제작\n\n\nWeek 2: 융합 콘텐츠 기획과 제작\n\nDate: 20240312\nPre-class video\n\n덕후에서 콘텐츠 크리에이터가 되는 방법 | 대도서관\n좋은 컨텐츠를 만드는 법 | 에이틴 제작사\nWhy AI Will Spark Exponential Economic Growth | Cathie Wood | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 강혜원 교수(SKKU)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_stat/weekly/index.html#part-ii-문화와-문화-산업",
    "href": "teaching/grad_stat/weekly/index.html#part-ii-문화와-문화-산업",
    "title": "Weekly Content",
    "section": "Part II: 문화와 문화 산업",
    "text": "Part II: 문화와 문화 산업\n\n\nWeek 3: 음악, K-pop, 엔터테인먼트\n\nDate: 20240319\nPre-class video\n\nWhat if You Could Sing in Your Favorite Musician’s Voice? | Holly Herndon | TED\n음악 한 곡을 발매하는데 돈이 얼마나 들까?\n소속사 없이 음원 유통하는 법\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 정헌섭 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 4: 패션과 뉴테크\n\nDate: 20240326\nPre-class video\n\nHow data is driving the future of fashion | Steve Brown | TED Institute\nThe New Reality of Fashion is Digital | Gala Marija Vrbanic | TED\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 류현석 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 5: 게임 & 인터랙티브 디자인\n\nDate: 20240402\nPre-class video\n\n게임 과몰입, 게임 잘못일까요? | 한덕현 중앙대학교 병원 정신건강의학과 교수 | | 세바시\nDiscussion\n\nClass\n\nGuest Lecturer: 김수완 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 6: 한류와 팬덤\n\nDate: 20240409\nPre-class video\n\nAll I Really Need To Know, I Learned From KPOP | Donald Lim | TEDxYouth@SJCS\nHas K-pop prepared us for the metaverse? | Alex Karlsson | TEDxBrunkebergstorg\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이종명 교수(SKKU)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_stat/weekly/index.html#part-iii-문화-기술",
    "href": "teaching/grad_stat/weekly/index.html#part-iii-문화-기술",
    "title": "Weekly Content",
    "section": "Part III: 문화 기술",
    "text": "Part III: 문화 기술\n\n\nWeek 7: 데이터 시각화의 예술\n\nDate: 20240416\nPre-class video\n\nThe beauty of data visualization - David McCandless\nData Visualization Best Practices - Stephanie Evergreen\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 전서연 강사\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 8: 문화콘텐츠와 자연어 처리\n\nDate: 20240423\nPre-class video\n\nText 데이터를 활용한 개인화 음악 추천서비스 웹 데모 구축 (혁신성장청년인재양성사업 프로젝트 발표)\nNLP Project - Emotion In Text Classifier App with Streamlit and Python\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 구영은 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 9: 메타버스와 메타휴먼\n\nDate: 20240430\nPre-class video\n\n버추얼 아이돌 그룹 플레이브의 탄생 과정과 기술 이야기 (언리얼 페스트 2023 서울)\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 원종서 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 10: 인터랙션 사이언스, UX\n\nDate: 20240507\nPre-class video\n\nHuman-Computer Interaction and User Interface Design | Tony Tang\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이대호 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 11: 서비스 디자인, 데이터 드리븐 마케팅\n\nDate: 20240514\nPre-class video\n\nWhy Data Marketing So Important | 세바시\nAge of Experience, Data+Service | 설상훈 교수님\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 설상훈 교수(SKKU)\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 12: 가상/증강현실 콘텐츠 제작\n\nDate: 20240521\nPre-class video\n\n상상이 현실화가 되는 세계, 그 중심에 서서 외치다 | 서동일 오큘러스VR코리아 지사장\nWhat is extended reality? | The Gadget Show\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 김태원 대표(RGB Makers)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_stat/weekly/index.html#part-iv-문화-콘텐츠-경영",
    "href": "teaching/grad_stat/weekly/index.html#part-iv-문화-콘텐츠-경영",
    "title": "Weekly Content",
    "section": "Part IV: 문화 콘텐츠 경영",
    "text": "Part IV: 문화 콘텐츠 경영\n\n\nWeek 13: 융합콘텐츠와 창업\n\nDate: 20240528\nPre-class video\n\nGPT 있는데 왜 또 만들어요? 손쉽게 이해하는 AI 개발 트렌드와 비하인드\n세상을 바꾸는 뉴콘텐츠\n콘텐츠 창업에서 가장 중요한 것 | CEO 노정석\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 윤영훈 대표 (주)ASSI\nModulator: 이창준 교수(SKKU)\n\n\n\n\n\nWeek 14: 엔터테인먼트 경영\n\nDate: 20240604\nPre-class video\n\nHow Netflix changed entertainment – and where it’s headed | Reed Hastings\nDiscussion & Questions Submission\n\nClass\n\nGuest Lecturer: 이동찬 경영총괄 (TEO Universe)\nModulator: 이창준 교수(SKKU)"
  },
  {
    "objectID": "teaching/grad_stat/weekly/index.html#conclusion",
    "href": "teaching/grad_stat/weekly/index.html#conclusion",
    "title": "Weekly Content",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n다가오는 인공지능의 시대\n\nAI 4대 석학’ 앤드류 응, 공개강연\n\n\n\nWeek 15: Wrap-up Exam\n\nDate: 20240611"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/02_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/02_week.html",
    "title": "HCI 실험 디자인 및 실제 연구 사례 소개",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/04_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/04_week.html",
    "title": "R의 데이터 유형 및 구조(1)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/06_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/06_week.html",
    "title": "기술 통계",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/08_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/08_week.html",
    "title": "추론 통계 (1)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/10_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/10_week.html",
    "title": "Wrap-up QZ",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/12_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/12_week.html",
    "title": "HCI 데이터를 활용한 데이터 시각화(2)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/14_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/14_week.html",
    "title": "실제 연구 replicate (2)",
    "section": "",
    "text": "Weekly content"
  },
  {
    "objectID": "teaching/ml101/about/index.html#course-description",
    "href": "teaching/ml101/about/index.html#course-description",
    "title": "About ML101",
    "section": "",
    "text": "Goal\n\n\n\nThis course covers the fundamentals of the field, including supervised and unsupervised learning algorithms, regression, classification, and clustering. The course may also cover topics such as model evaluation, feature selection, and regularization.\nIn a supervised learning setting, students learn about linear regression and logistic regression, as well as more complex algorithms such as Naive Bayes, decision trees, random forests, and kNN. They learn how to train models on a labeled dataset and make predictions on new data.\nIn an unsupervised learning setting, students learn about clustering algorithms such as k-means and Apriori. They learn how to extract meaningful structure from unlabeled data.\nThe course may also cover advanced topics such as natural language processing. Students learn how to implement and use these algorithms in R.\nThroughout the course, students work on practical projects and assignments to apply the concepts they have learned. By the end of the course, students should have a solid understanding of the basics of machine learning and be able to apply these concepts to real-world problems."
  },
  {
    "objectID": "teaching/ml101/about/index.html#syllabus",
    "href": "teaching/ml101/about/index.html#syllabus",
    "title": "About ML101",
    "section": "Syllabus",
    "text": "Syllabus\n\n\nWeek 1: Introduction to Data Science and R\nWeek 2: About ML & Modelling\nWeek 3: Public Holiday\n\n\n\nPART I: Classification\n\nWeek 4: Decision Tree\nWeek 5: Random Forest\nWeek 6: Naive Bayes\nWeek 7: kNN\nWeek 8: QZ #1\n\n\n\n\nPART II: Regression\n\nWeek 9: Linear regression\nWeek 10: Non-linear regression\n\n\n\n\nPART III: Unsupervised Learning\n\nWeek 11: Clustering\nWeek 12: Apriori\n\n\n\n\nPART IV: Model Improvement\n\nWeek 13: Performance Evaluation\nWeek 14: Wrap-up\nWeek 15: QZ #2\nWeek 16: Project Presentation"
  },
  {
    "objectID": "teaching/ml101/about/index.html#weekly-design",
    "href": "teaching/ml101/about/index.html#weekly-design",
    "title": "About ML101",
    "section": "Weekly Design",
    "text": "Weekly Design\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nPre-class\nClass\nPBL\nNote\n\n\n\n\n1\n09/03/2024\n\nCourse intro\n\nParticipate Ars Electronica\n&lt;Recorded Lecture&gt;\n\n\n2\n09/10/2024\nInstall R & R Studio\nAbout ML & Modelling\nPractice\n\n\n\n\n3\n09/17/2024\n\n\n\nThanks giving holiday\n\n\n4\n09/24/2024\nClassification\n\nDecision Tree\n\nPractice\nProblem description\n\n\n\n5\n10/01/2024\n\nRandom Forest\n\nPractice\nData introduction\n\n\n\n6\n10/08/2024\n\nNaive Bayes\n\nPractice\nTeam arrangement\n\n\n\n7\n10/15/2024\n\nkNN\n\nPractice\nTeam meeting #1\n\n\n\n8\n10/22/2024\nRegression\n\nLinear regression\n\nPractice\nTeam meeting #2\n\n\n\n9\n10/29/2024\nQZ #1\n\nTeam meeting #3\n\n\n\n10\n11/05/2024\n\nNon-linear regression\n\nPractice\nTeam meeting #4\n\n\n\n11\n11/12/2024\nUnsupervised learning\n\nClustering\n\nPractice\nTeam meeting #5\n\n\n\n12\n11/19/2024\n\nApriori\n\nPractice\nTeam meeting #6\n\n\n\n13\n11/26/2024\nModel improvement\nPractice\nTeam meeting #7\n\n\n\n14\n12/03/2024\nText mining & other skills\nPractice\nTeam consulting #1\n\n\n\n15\n12/10/2024\nQZ #2\n\nTeam consulting #2\n\n\n\n16\n12/17/2024\nProj Report\n\nProject Presentation"
  },
  {
    "objectID": "teaching/ml101/about/index.html#course-management",
    "href": "teaching/ml101/about/index.html#course-management",
    "title": "About ML101",
    "section": "Course management",
    "text": "Course management\n\n\nLecturer: Changjun Lee (Associate Professor in SKKU School of Convergence)\n\nchangjunlee@skku.edu\n\nTA: Haeyoon LEE (Ph.D. Student, SKKU Interaction Science)\n\nhaileysunny@naver.com\n\n\n\n\nTime:\n\n(1h): Flipped learning content\n(2h): Tue 09:00 ~ 10:50\n\nLocation: International Hall High-Tech e+ Lecture Room (9B312)\n\n\n\nClass consists of Pre-class, Class, and PBL project\n\n\nPre-class\n\nStudents will be required to watch the lecturer’s recorded lecture (or other given videos) before the off-line (or online streaming ZOOM) class and learn themselves\nVideo is about the concept of the data science and the programming language\n(Sometimes) Students are required to submit Discussions to check the level of their understanding\n\nClass\n\nLecturer summarize the pre-class lecture and explain more details\n\nAsk students about the pre-class content to check whether they learned themselves\nOK to answer incorrectly, but if you cannot answer at all, it will be reflected in your pre-class discussion score.\n\nStudents will practice with the advanced code\nA Quiz will be in the class to check the level of understanding\n\nPBL project\n\nStudents organize teams that meet several conditions.\n\n4~5 members in a team\nBackground diversity: no homogeneous majors in a team\nException: Allowed if persuasion is possible for sufficient reasons\n\nData will be given. Teams are going to choose the data they want to explore considering their interest\nTeams can offer a zoom meeting with lecturer if they need"
  },
  {
    "objectID": "teaching/ml101/about/index.html#final-outputs-an-example-not-limited",
    "href": "teaching/ml101/about/index.html#final-outputs-an-example-not-limited",
    "title": "About ML101",
    "section": "Final outputs (An example not limited)",
    "text": "Final outputs (An example not limited)\n\nData Preparing (or Collecting)\nExplore data (Descriptive stats)\nSet your hypothesis (or research questions)\nVisualize data to confirm your hypo or RQs\nExplain your findings\nExpanding your findings to implications"
  },
  {
    "objectID": "teaching/ml101/about/index.html#textbooks-for-the-course",
    "href": "teaching/ml101/about/index.html#textbooks-for-the-course",
    "title": "About ML101",
    "section": "Textbooks for the course",
    "text": "Textbooks for the course\n\nR4DS: R for Data Science (written by Hadley Wickham and Garrett Grolemund)\n\nis an excellent resource for learning data science using R, covering data manipulation, visualization, and modeling with R. The book is available as a free online resource.\n\nRC2E: R Cookbook (written by JD Long and Paul Teetor)\n\nis a comprehensive resource for data scientists, statisticians, and programmers who want to explore the capabilities of R programming for data analysis and visualization.\n\nRGC: R Graphic Cookbook (written by Winston Chang)\n\nis a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems\n\nMDR: Statistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\nis a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language.\n\nISR: Introductory Statistics with R (written by Peter Dalgaard)\n\nis a great resource for learning basic statistics with a focus on R programming. This book covers a wide range of statistical concepts, from descriptive statistic"
  },
  {
    "objectID": "teaching/ml101/about/index.html#grade",
    "href": "teaching/ml101/about/index.html#grade",
    "title": "About ML101",
    "section": "Grade",
    "text": "Grade\n\nAttendance & Participation (20 %)\nQZs (40 %)\nProject (40 %)"
  },
  {
    "objectID": "teaching/ml101/about/index.html#communication",
    "href": "teaching/ml101/about/index.html#communication",
    "title": "About ML101",
    "section": "Communication",
    "text": "Communication\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gli0lhDg\nWhen you enter, please make sure to enter your name as it is on the attendance sheet. (입장하셔서 이름을 꼭 출석부에 있는 이름으로 설정해주세요.)\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s8zTrYCf"
  },
  {
    "objectID": "teaching/grad_stat/weekly_2/posts/01_week.html",
    "href": "teaching/grad_stat/weekly_2/posts/01_week.html",
    "title": "Course Intro",
    "section": "",
    "text": "Weekly content"
  }
]