[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Changjun LEE",
    "section": "",
    "text": "I’m a data scientist but humanist who thinks caffeine is the nectar of the gods. Yes, I believe in God. I love discovering new gadgets, media, and content as much as I love discovering new places, but I prefer to live life on the edge with spontaneous trips. I have a strong faith in a higher power and I’m on a quest to find my purpose in this vast universe. In my free time, you can find me sipping coffee and pondering the meaning of life.\nAbout me more.. here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Changjun Lee",
    "section": "",
    "text": "Media and Social Informatics\nHanyang University ERICA Campus\nRecent CV(PDF)\n\n\nBio\n\nAs a computational social scientist, I bring a unique interdisciplinary perspective to the fields of economics, innovation studies, and media & communications. With a background in natural sciences, including a bachelor’s degree in biology and chemistry, I went on to earn a Ph.D. in technology management, economics, and policy. My global research experience includes working as a postdoctoral researcher at the National University of Singapore and University College Dublin. My research focuses on utilizing computational methods to tackle a wide range of social phenomena, including technology evolution & regional growth, knowledge management, and technology & media innovation. As an Associate Professor at Hanyang University in South Korea, I am responsible for the Department of Media & Social Informatics and the Institution of Creativity & Interaction. I am passionate about using technology and data to drive innovation and solve real-world problems.”\n\n\nResearch #Innovation #Media #Technology #Public Policy\nTeaching #DataScience #Computational Social Science\n\n\n\n\nEmployment\n\nHanyang University (ERICA), Ansan, Republic of Korea\nDepartment of Media & Social Informatics\n\nAssociate Professor (2022 ~ present)\nAssistant Professor (2019 ~ 2022.)\n\nUniversity College Dublin, Republic of Ireland\nSpatial Dynamics Lab\n\nSenior Postdoctoral Research Fellow (2017 ~ 2019)\n\nNational University of Singapore, Singapore\nLee Kuan Yew School of Public Policy\n\nPostdoctoral Research Associate (2015 ~ 2017)\n\n\n\n\n\nEducation\n\nPh.D. in Economics (Technology Management)\n\nSeoul National University, Seoul, Republic of Korea (2010 - 2015)\nTechnology Management, Economics, and Policy Program\nDissertation: Research on the Growth Mechanism of Platform Ecosystem\n\nB.Sc. in Biology & Chemistry\n\nYonsei University, Seoul, Republic of Korea (2004 - 2010)\n\n\n\n\n\nPublications\n\nPublication keywordsPublication listsCo-author network\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeachings\nGo to Course page\n\nData-Science101 (데이터사이언스의이해)\n\n2022-Fall, 2021-Fall, 2020-Fall, 2020-Spring, 2019-Fall\n\nMachine Learning 101 (데이터예측모델과기계학습의응용)\n\n2022-Spring, 2021-Spring, 2020-Fall\n\nDatabase101 (데이터베이스의원리와응용)\n\n2022-Spring, 2021-Spring, 2020-Spring\n\nBasic Math & Statistics for Data Science (데이터사이언스를위한기초수학과통계)\n\n2021-Spring\n\n(Grad) Media & Data-Science (미디어와데이터사이언스)\n\n2022-Spring, 2021-Spring\n\n(Grad) Media User Data Analysis (미디어이용자데이터분석)\n\n2022-Fall\n\nInformation Society Media Capstone Design 1 (정보사회캡스톤디자인)\n\n2021-Spring\n\n\n\n\n\nInternal and External Positions\n\n창의성과인터랙션연구소 연구소장 (2021~현재)\n정보사회와미디어 편집위원 (2023~현재)\n정보통신정책학회지 편집위원 (2022~현재)\n한국혁신학회지 부편집위원장 (2021~현재)\n미디어경영학회 총무이사 (2023~현재)\n사이버커뮤니케이션학회 기획이사 (2023~현재)\n한국언론학회 연구이사 (2023~현재)\n사이버커뮤니케이션학회 총무이사 (2022)\n미디어경영학회 연구이사 (2022)\n한국방송학회 연구이사 (2022)\n\n\n\n\nAchievements\n\n2022. 한국미디어경영학회 우수 논문 학술상\n2021. 한양대학교 에리카 국제연구부문 학술상 (신진교원상).\n2021. IC-PBL 루키 부문 우수상 수상.\n\n\n\n\nMedia\n\n21세기 우리가 알아야 할 필수 과목, 정보통신(The Science Times)\n“韓클라우드 시장, 美中 공룡기업 놀이터로” (매일경제)\n[매일경제] 이창준 교수, 클라우드 보안인증제도(CSAP) 완화에 대해 코멘트 (NewsH)\n[신문 읽어주는 교수님] 늦은 밤 넷플릭스 시청이 수면에 미치는 영향 (NewsH)\n수면 루틴 깨는 OTT…잠자리 26분 늦어지고 수면시간 30분 줄어 (디지털데일리)\n넷플릭스 유튜브 보느라 얼마나 늦게 잠드나요? (미디어오늘)\n서울대학교 기술경영경제정책 대학원 졸업생 멘토링 세미나\n에퀴즈 온 더 블럭 (사랑한대)\n한양대 슬기로운 새내기 생활(교수님편)\nERICA 방송국 VOH와 취중젠담 1편\nERICA 방송국 VOH와 취중젠담 2편\n알고리즘, 누구냐 넌! (한대신문)\n너란 SNS 광고 (한대신문)\n\n\n\n\nGraduate Alumni\n\n김종화(Ph.D. Student @University of Georgia)\n박지은(연구원 @미디어미래연구소)"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "CJ’s Blogs",
    "section": "",
    "text": "Going to post technical reports, research, and thoughts.\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the AdaBoost\n\n\n오답노트의 힘\n\n\n\n\n\n\n\n\n\n29 March 2023\n\n\nChangjun Lee\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Confusion Matrix and ROC Curve in R\n\n\n혼동이 찾아오는 혼돈행렬\n\n\n\n\n\n\n\n\n\n21 March 2023\n\n\nChangjun Lee\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are Regular Expressions and How to Use Them in R\n\n\n외계어가 아니에요!\n\n\n\n\n\n\n\n\n\n08 March 2023\n\n\nChangjun Lee\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Package in R\n\n\nStand on the shoulders of giants\n\n\n\n\n\n\n\n\n\n08 March 2023\n\n\nChangjun Lee\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto, the game changer\n\n\n날개 달고 날아가는 디지털 퍼블리싱\n\n\n누구나 퍼블리싱 하는 시대가 온다\n\n\n\n\n\n\n22 February 2023\n\n\nChangjun Lee\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/posts/1_first_post.html",
    "href": "blogs/posts/1_first_post.html",
    "title": "Quarto, the game changer",
    "section": "",
    "text": "R Markdown을 처음 접하고 “와 멋지다..!” 했었는데 이제는 Quarto 가 나와 Data communication 영역을 평정해버렸다. Quarto의 등장은 기술 블로그, 특히 데이터 사이언스를 업으로 삼고 있는 사람들에게는 큰 축복이 아닐 수 없다. 기록이 중요한 세상에서 손쉽게 데이터 작업의 흔적들을 남기고 포스팅 할 수 있으니 말이다.\n포스팅의 의미는 두 가지이다.\n\n하나는 내가 한 작업들을 잊지 않기 위한 포스팅,\n두 번째는 다른 사람들에게 보여주기 위한 포스팅\n\n둘 다 중요하지만 사실 첫 번째가 조금 더 중요하다. 기록용이라면 옵시디언이 더 좋지 않나? 라고 생각할 수 있는데 반은 맞고 반은 틀리다. 생각을 정리하고 엮어 나가는데 옵시디언이 가장 좋은 노트앱이이지만 결국엔 내 생각을 누구에게 보여준다는 생각으로 정리하지 않으면 잘 쌓이지 않는다는 단점이 있기 때문이다.\nQuarto의 가장 큰 장점은 아래와 같이 R이나 파이썬 또는 그 어떤 코드도 자유롭게 임베딩 할 수 있다는 것.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nmtcars %>% head\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n임베딩할 수 있을 뿐만 아니라 결과도 같이 레포팅해준다.\n\nmtcars %>% \n  mutate(cyl = as.factor(cyl)) %>% \n  ggplot(aes(x = wt, y = mpg)) + \n  geom_point(aes(color = cyl, size = qsec), \n             alpha = 0.5) +\n  scale_color_manual(values = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n  scale_size(range = c(0.5, 12))  # Adjust the range of points size\n\n\n\n\n정말 멋진 세상이 아닐 수 없다.\n이 홈페이지도 Quarto webpage 를 참고해서 아주 쉽게 만들 수 있었다. Shiny가 data communication 에 interactive 한 새로운 차원을 열었다면 Quarto 는 디지털 퍼블리싱의 높은 문턱을 한참 아래로 낮춘 혁신이라고 말할 수 있다.\n몸 담고 있는 한양대학교 정보사회미디어학과의 슬로건은 “데이터로 세상을 읽고 콘텐츠로 주장하는 지능정보사회의 저널리스트” 이다. 이 슬로건을 실행하기 위해서 앞으로는 R 과 Python과 같은 데이터 사이언스 언어와 분석 역량 뿐만 아니라 Shiny, Quarto와 같은 디지털 문해력을 기를 수 있는 교육이 보완되어야 하겠다."
  },
  {
    "objectID": "blogs/posts/2_about_a_package.html",
    "href": "blogs/posts/2_about_a_package.html",
    "title": "What is a Package in R",
    "section": "",
    "text": "R is a popular programming language for data analysis and statistical computing. One of the most powerful features of R is the availability of packages, which are collections of functions, data sets, and documentation that extend the functionality of the base R system. In this blog post, we will discuss what packages are in R, why they are so important, and provide some examples of how to use them in your code.\nA package in R is essentially a bundle of R code, data, and documentation that is designed to perform a specific task or set of tasks. Packages are created by individuals, organizations, and communities of developers who want to share their work with others. Some packages are developed for general use, while others are more specialized, designed to address specific problems or perform specific analyses.\nThe R community has created thousands of packages that cover a wide range of topics and applications. Some of the most popular packages in R include ggplot2 for data visualization, dplyr for data manipulation, and tidyr for data cleaning. These packages provide users with a wealth of tools and resources that they can use to perform complex analyses and visualizations with ease.\nOne of the most important benefits of packages in R is that they are easy to install and use. With just a few simple commands, users can download and install the packages they need, and then access the functions, data sets, and documentation that come with them. Here’s an example of how to install and load the ggplot2 package in R:\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nOnce you have installed and loaded the ggplot2 package, you can start using its functions to create beautiful data visualizations. Here’s a simple example of how to use the ggplot function to create a scatterplot:\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\nAnother benefit of packages in R is that they are highly customizable. Users can modify the code in packages to suit their specific needs, or even create their own packages from scratch. This allows users to tailor their work to their specific requirements and work more efficiently.\nIn conclusion, packages in R are essential tools for data analysis and statistical computing. They provide users with a wealth of resources that they can use to perform complex analyses and visualizations with ease, and they are easy to install and use. Whether you are a beginner or an experienced R programmer, packages are a must-have resource for your work. With the right packages, you can accomplish more in less time and get better results from your data."
  },
  {
    "objectID": "blogs/posts/3_regular_expression_part_1.html",
    "href": "blogs/posts/3_regular_expression_part_1.html",
    "title": "What are Regular Expressions and How to Use Them in R",
    "section": "",
    "text": "Regular expressions are a powerful tool for searching, matching, and manipulating text. They are a type of pattern language that can be used to describe sets of character strings, such as words or sentences, in a concise and flexible way. In this blog post, we will introduce what regular expressions are, why they are useful, and how to use them in R.\nA regular expression is a sequence of characters that define a search pattern. The pattern can be used to match (and sometimes replace) strings, or to perform some other manipulation of strings. Regular expressions are commonly used for searching for patterns in text data, such as extracting specific information from a document, validating input data, or matching and replacing specific characters.\nOne of the most powerful features of regular expressions is their ability to specify patterns using a compact and flexible syntax. For example, the regular expression a.b will match any string that contains an a followed by any single character followed by a b. The dot (.) is a special character that matches any single character. Similarly, the regular expression [abc] will match any string that contains one of the characters a, b, or c.\nIn R, there are several functions for working with regular expressions, including grep, grepl, sub, and gsub. These functions allow you to search for, match, and replace patterns in strings. For example, the grep function can be used to search for patterns in a vector of strings:\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^b\", text)\n\n[1] 2\n\n\nThis code uses the grep function to search for strings in the text vector that start with the letter b. The ^ symbol is a special character that matches the start of a string. The result of this code will be a vector of the indices of the strings in the text vector that match the pattern:\nThe grepl function is similar to grep, but returns a logical vector indicating which elements of the input vector match the pattern, rather than the indices:\n\ngrepl(\"^b\", text)\n\n[1] FALSE  TRUE FALSE\n\n\nThe sub and gsub functions can be used to replace matches in a string:\n\nsub(\"b\", \"B\", text)\n\n[1] \"apple\"  \"Banana\" \"cherry\"\n\ngsub(\"b\", \"B\", text)\n\n[1] \"apple\"  \"Banana\" \"cherry\"\n\n\nRegular expressions are a powerful tool for searching, matching, and manipulating text data. They are used to describe patterns in text data in a concise and flexible way, and can be used for a variety of purposes, such as extracting specific information from a document, validating input data, or matching and replacing specific characters. With the grep, grepl, sub, and gsub functions in R, you can easily search for and manipulate patterns in text data.\nOne of the key features of regular expressions is the ability to use special characters to define patterns. Here are some of the most commonly used special characters in regular expressions:\n\n[] (brackets): Matches any single character that is contained within the brackets. For example, [abc] matches any string that contains one of the characters a, b, or c.\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^[abc]\", text)\n\n[1] 1 2 3\n\n\nThis code uses the grep function to search for strings in the text vector that start with the characters a, b, or c. The result of this code will be a vector of the indices of the strings in the text vector that match the pattern:\n\n[^] (negated brackets): Matches any single character that is not contained within the brackets. For example, [^abc] matches any string that contains any character other than a, b, or c.\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^[^abc]\", text)\n\ninteger(0)\n\n\n\n^ (caret): Matches the start of a string. For example, ^b matches any string that starts with the letter b.\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"^b\", text)\n\n[1] 2\n\n\n\n$ (dollar sign ): Matches the end of a string. For example, b$ matches any string that ends with the letter b.\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"b$\", text)\n\ninteger(0)\n\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"e$\", text)\n\n[1] 1\n\n\n\n+(plus sign): Matches one or more occurrences of the preceding character or group. For example, a+ matches any string that contains one or more occurrences of the letter a.\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"a+\", text)\n\n[1] 1 2\n\n\n\n? (question mark): Matches zero or one occurrence of the preceding character or group. For example, a? matches any string that contains zero or one occurrences of the letter a.\n\n\ntext <- c(\"apple\", \"banana\", \"cherry\")\ngrep(\"a?\", text)\n\n[1] 1 2 3\n\n\nIf you want to learn more about regular expressions in R, check out this video tutorial: https://www.youtube.com/watch?v=q8SzNKib5-4. The video provides a comprehensive explanation of regular expressions and how to use them in R, with practical examples and hands-on exercises. Whether you’re a beginner or an advanced user, this video is a great resource to deepen your understanding of regular expressions and their applications in R.\n한글로 된 설명은 안도현 교수님의 eBook 참고: https://bookdown.org/ahn_media/bookdown-demo/cleantool.html#%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9Dregular-expressions"
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "",
    "text": "In this blog post, we will explore two essential concepts in evaluating classification models: the confusion matrix and the Receiver Operating Characteristic (ROC) curve. We will go through the basics, discuss how to interpret these metrics, and provide R code snippets to create and visualize them using the popular caret and pROC packages. We’ll demonstrate these concepts using the Titanic dataset."
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#loading-the-titanic-dataset",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#loading-the-titanic-dataset",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "Loading the Titanic Dataset",
    "text": "Loading the Titanic Dataset\nThe Titanic dataset is not available in R by default, but it can be loaded using the titanic package. First, install and load the package:\n\n# Install the titanic package if you haven't already\n# if (!requireNamespace(\"titanic\", quietly = TRUE)) {\n#   install.packages(\"titanic\")\n# }\n\n# Load the titanic package\nlibrary(titanic)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Load the Titanic dataset\ndata(\"titanic_train\")\ndata(\"titanic_test\")\n\n# Combine the training and testing datasets\ntitanic_data <- bind_rows(titanic_train, titanic_test)\n\nBefore we proceed, let’s preprocess the dataset by selecting relevant features and handling missing values:\n\n# Select relevant features and remove rows with missing values\ntitanic_data <- titanic_data[, c(\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\")]\ntitanic_data <- na.omit(titanic_data)\n\n# Convert the 'Sex' variable to a factor\ntitanic_data$Sex <- as.factor(titanic_data$Sex)"
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#confusion-matrix",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#confusion-matrix",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nA confusion matrix is a tabular representation of the predictions made by a classification model, showing the number of correct and incorrect predictions for each class. It is a useful tool to evaluate a model’s performance and identify its strengths and weaknesses.\n\nCreating a Confusion Matrix in R\nTo demonstrate the confusion matrix, we will use the preprocessed Titanic dataset and create a logistic regression model. First, let’s load the required packages and split the dataset into training (80%) and testing (20%) sets:\n\n# Load required packages\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Split the dataset into training (80%) and testing (20%) sets\nset.seed(42)\ntrain_index <- createDataPartition(titanic_data$Survived, p = 0.8, list = FALSE)\ntitanic_train <- titanic_data[train_index, ]\ntitanic_test <- titanic_data[-train_index, ]\n\nNow, let’s create a logistic regression model and make predictions on the test set:\n\n# Create the logistic regression model\nmodel <- glm(Survived ~ ., data = titanic_train, family = \"binomial\")\n\n# Make predictions on the test dataset\npredicted_probs <- predict(model, titanic_test, type = \"response\")\npredicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)\n\nNext, we will create the confusion matrix using the caret package:\n\n# Create the confusion matrix\ncm <- confusionMatrix(table(predicted_classes, titanic_test$Survived))\nprint(cm)\n\nConfusion Matrix and Statistics\n\n                 \npredicted_classes  0  1\n                0 78 17\n                1 11 36\n                                          \n               Accuracy : 0.8028          \n                 95% CI : (0.7278, 0.8648)\n    No Information Rate : 0.6268          \n    P-Value [Acc > NIR] : 4.43e-06        \n                                          \n                  Kappa : 0.5687          \n                                          \n Mcnemar's Test P-Value : 0.3447          \n                                          \n            Sensitivity : 0.8764          \n            Specificity : 0.6792          \n         Pos Pred Value : 0.8211          \n         Neg Pred Value : 0.7660          \n             Prevalence : 0.6268          \n         Detection Rate : 0.5493          \n   Detection Prevalence : 0.6690          \n      Balanced Accuracy : 0.7778          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nInterpreting the Confusion Matrix\nThe confusion matrix displays the following information:\n\nTrue Positives (TP): 78, the number of correctly predicted non-survivors (class 0)\nTrue Negatives (TN): 36, the number of correctly predicted survivors (class 1)\nFalse Negatives (FN): 11, the number of non-survivors incorrectly predicted as survivors (class 1)\nFalse Positives (FP): 17, the number of survivors incorrectly predicted as non-survivors (class 0)\n\nStatistics:\n\nAccuracy: 0.8028 (80.28%), the proportion of correct predictions (both true positives and true negatives) among the total number of cases. The 95% CI (confidence interval) for the accuracy is (0.7278, 0.8648), meaning we can be 95% confident that the true accuracy lies within this range.\nNo Information Rate (NIR): 0.6268, the accuracy that could be obtained by always predicting the majority class (class 0 in this case).\nP-Value [Acc > NIR]: 4.43e-06, the p-value for a statistical test comparing the accuracy of the model to the NIR. A small p-value (typically less than 0.05) indicates that the model’s accuracy is significantly better than the NIR.\nKappa: 0.5687, a metric that considers both the true positive rate and the false positive rate, providing a more balanced assessment of the model’s performance. Kappa ranges from -1 to 1, with 0 indicating no better than random chance, and 1 indicating perfect agreement between predictions and true values.\nMcnemar's Test P-Value: 0.3447, the p-value for a statistical test comparing the number of false positives and false negatives. A large p-value (typically greater than 0.05) indicates that there is no significant difference between the number of false positives and false negatives.\n\nSensitivity, Specificity, and Other Metrics:\n\nSensitivity (Recall or True Positive Rate): 0.8764, the proportion of actual positive cases (survivors) that were correctly identified by the model.\nSpecificity: 0.6792, the proportion of actual negative cases (non-survivors) that were correctly identified by the model.\nPositive Predictive Value (PPV): 0.8211, the proportion of positive predictions (predicted survivors) that were actually positive (true survivors).\nNegative Predictive Value (NPV): 0.7660, the proportion of negative predictions (predicted non-survivors) that were actually negative (true non-survivors).\nPrevalence: 0.6268, the proportion of the true positive cases (survivors) in the dataset.\nDetection Rate: 0.5493, the proportion of true positive cases that were correctly detected by the model.\nDetection Prevalence: 0.6690, the proportion of cases predicted as positive (survivors) by the model.\nBalanced Accuracy: 0.7778, the average of sensitivity and specificity, providing a balanced assessment of the model’s performance across both classes.\n\nThe ‘Positive’ class is set to 0 (non-survivors) in this analysis."
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#roc-curve",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#roc-curve",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "ROC Curve",
    "text": "ROC Curve\nThe Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier’s performance across all possible decision thresholds. It plots the True Positive Rate (TPR, also known as sensitivity or recall) against the False Positive Rate (FPR, or 1 - specificity) at various threshold settings.\n\nCreating an ROC Curve in R\nTo create an ROC curve, we will use the pROC package. First, let’s install and load the required package:\n\n# Install the pROC package if you haven't already\n# if (!requireNamespace(\"pROC\", quietly = TRUE)) {\n#   install.packages(\"pROC\")\n# }\n\n# Load the pROC package\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nNow, let’s create the ROC curve using the predicted probabilities from our logistic regression model:\n\n# Create the ROC curve\nroc_obj <- roc(titanic_test$Survived, predicted_probs)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n# Plot the ROC curve\nplot(roc_obj, main = \"ROC Curve for the Logistic Regression Model\")\nabline(0, 1, lty = 2, col = \"gray\")  # Add a reference line for a random classifier\n\n\n\n\n\n\nInterpreting the ROC Curve\nThe ROC curve helps us visualize the trade-off between sensitivity (True Positive Rate) and specificity (1 - False Positive Rate) for various threshold values. A perfect classifier would have an ROC curve that passes through the top-left corner of the plot (100% sensitivity and 100% specificity). In contrast, a random classifier would have an ROC curve that follows the diagonal reference line (gray dashed line in our plot).\nThe area under the ROC curve (AUC) is a scalar value that summarizes the performance of the classifier. An AUC of 1.0 indicates a perfect classifier, while an AUC of 0.5 suggests that the classifier is no better than random chance. We can calculate the AUC using the auc function from the pROC package:\n\n# Calculate the AUC\nauc_value <- auc(roc_obj)\ncat(\"AUC:\", auc_value, \"\\n\")\n\nAUC: 0.8181047 \n\n\nLet’s go through the process of drawing dots and lines in the ROC curve step by step.\n\nUnderstand the components of the ROC curve: The ROC curve consists of several points (dots) that represent the true positive rate (sensitivity) and false positive rate (1 - specificity) at various decision thresholds. To draw the curve, you need to connect these points with lines.\nDetermine decision thresholds: You must first identify the decision thresholds you want to use. These thresholds represent the probability cut-off points for classifying an observation as positive or negative. In most cases, you can use the unique predicted probabilities in your dataset as the thresholds.\n\n\n# Extract unique predicted probabilities\nthresholds <- unique(predicted_probs)\n\n# Sort the thresholds in descending order\nthresholds <- sort(thresholds, decreasing = TRUE)\n\n\nCalculate TPR and FPR for each threshold: For each threshold, calculate the true positive rate (sensitivity) and false positive rate (1 - specificity).\n\n\n# Initialize empty vectors for TPR and FPR\ntpr <- numeric(length(thresholds))\nfpr <- numeric(length(thresholds))\n\n# Calculate TPR and FPR for each threshold\nfor (i in seq_along(thresholds)) {\n\n  threshold <- thresholds[i]\n\n  # Classify observations based on the current threshold\n  predicted_classes <- as.integer(predicted_probs >= threshold)\n\n  # Create a confusion matrix\n  cm <- table(Predicted = predicted_classes, \n              Actual = titanic_test$Survived)\n\n  # Calculate TPR and FPR\n  if(sum(dim(cm))==4){\n    tpr[i] <- cm[\"1\", \"1\"] / (cm[\"1\", \"1\"] + cm[\"0\", \"1\"])\n    fpr[i] <- cm[\"1\", \"0\"] / (cm[\"1\", \"0\"] + cm[\"0\", \"0\"])\n  }\n  \n}\n\n\nPlot the ROC curve: Now that you have calculated the TPR and FPR for each threshold, you can plot the ROC curve by connecting the dots (points) with lines.\n\n\n# Create a data frame for plotting\nroc_df <- data.frame(Threshold = thresholds, TPR = tpr, FPR = fpr)\n\n# Create the ROC plot\nroc_plot <- ggplot(roc_df, aes(x = FPR, y = TPR)) +\n  geom_point(size = 2, color = \"red\") +  # Add points (dots)\n  geom_line(color = \"blue\", size = 1) +  # Connect the points with lines\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    title = \"ROC Curve for the Logistic Regression Model\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Display the plot\nprint(roc_plot)\n\n\n\n\nThis will create an ROC curve with dots (points) representing each decision threshold and lines connecting these dots. The resulting plot allows you to visualize the trade-offs between sensitivity and specificity at various threshold settings.\nLet’s see the process of drawing the ROC curve step by step.\n\nlibrary(gganimate)\n\nanimated_roc_plot <- ggplot(roc_df, aes(x = FPR, y = TPR)) +\n  geom_point(aes(group = seq_along(Threshold), \n                 color = as.factor(seq_along(Threshold))),\n             size = 2, show.legend = FALSE) +\n  geom_line(aes(group = seq_along(Threshold), \n                color = as.factor(seq_along(Threshold))),\n            size = 1, show.legend = FALSE) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    title = \"ROC Curve for the Logistic Regression Model\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\"\n  ) +\n  theme_minimal() +\n  transition_states(states = seq_along(roc_df$Threshold), transition_length = 2, state_length = 1) +\n  enter_fade() + exit_fade() +\n  shadow_mark(alpha = 0.5, size = 1) # Add the trajectory\n\n# Display the animated plot\nanimate(animated_roc_plot, nframes = 200, end_pause = 50)"
  },
  {
    "objectID": "blogs/posts/4_confusion_mat_and_ROC.html#conclusion",
    "href": "blogs/posts/4_confusion_mat_and_ROC.html#conclusion",
    "title": "Understanding the Confusion Matrix and ROC Curve in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have explored the confusion matrix and ROC curve as valuable tools for evaluating classification models. We demonstrated how to create and interpret these metrics using R code snippets and the Titanic dataset. With a solid understanding of these concepts, you can better assess the performance of your classification models and make informed decisions about model selection and tuning."
  },
  {
    "objectID": "proj/index.html",
    "href": "proj/index.html",
    "title": "CJL Lab",
    "section": "",
    "text": "가상세계 멀티 페르소나 성향과 사용자의 인지 강화\n\n한국연구재단. 신진연구자지원사업(인문사회)\n2022S1A5A805107011\n2022.05.01 ~ 2023.04.30\n\n플랫폼 산업의 경제 효용 추정: 자국 검색 플랫폼이 온라인 산업에 미치는 영향\n\n한국연구재단. 일반공동연구지원사업(인문사회)\n2020S1A5A2A0304148012\n2020.07.01 ~ 2023.06.30\n\n미래 기술 정책을 위한 한국형 스마트 특성화 전략 모델 구축\n\n한국연구재단. 이공분야기초. 생애첫연구사업\n2020R1G1A101245313\n2020.03.01 ~ 2023.02.28"
  },
  {
    "objectID": "proj/index.html#projects-completed",
    "href": "proj/index.html#projects-completed",
    "title": "CJL Lab",
    "section": "Projects completed",
    "text": "Projects completed\n\n탄소시장 대응 녹색기술 이전 및 기후변화 대응 기술사업모델을 통한 탄소시장 진출의 전략 대응도 및 전략 마련 : 특허분석을 기반으로\n\n녹색기술센터\n2022.06.23 ~ 2022.10.30\n\n육군 인력획득 홍보 대상의 성향 분석 빅데이터 모형 연구\n\n국방부 육군\n2022.02.01 ~ 2022.07.31\n\n건강 형평성 파악을 위한 안산시 건강지도 제작 연구\n\n안산시지속가능발전협의회\n2021.06.01 ~ 2021.12.22\n\nTechnology Evolution in Regional Economies\n\nEuropean Research Council\nERC 715631, TechEvo\n2017.03.01 ~ 2023.02.28\n\nWorking Hours and Familial Supports\n\nNUS Humanities and Social Sciences Research Fund from the National University of Singapore.\n2015.09.01 ~ 2017.08.31"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Publications",
    "section": "",
    "text": "International SSCI or SCIE\n\nLiu, H. S., Lee, C., Kim, K., Lee, J., Moon, A., Lee, D., & Park, M. (2023). An Analysis of Factors Influencing the Intention to Use” Untact” Services by Service Type. Sustainability, 15(4), 2870. https://www.mdpi.com/2071-1050/15/4/2870\nShim, D., Lee, C., & Oh, I. (2022). Analysis of OTT Users’ Watching Behavior for Identifying a Profitable Niche: Latent Class Regression Approach. Journal of Theoretical and Applied Electronic Commerce Research, 17(4), 1564-1580. https://www.mdpi.com/0718-1876/17/4/79\nKim, K., Kogler, D. F., Lee, C., & Kang, T.* (2022). Changes in regional knowledge bases and its effect on local labour markets in the midst of transition: Evidence from France over 1985–2015. Applied Spatial Analysis and Policy, 1-22. https://doi.org/10.1007/s12061-022-09444-4\nShon, M., Lee, D. & Lee, C.(2022). Inward or Outward? Direction of Knowledge Flow and Firm Efficiency. International Journal of Technology Management. 90(1-2), 102-121. https://doi.org/10.1504/IJTM.2022.124617\nPark, I., Lee, J., Lee, D., Lee, C., & Chung, W. Y.* (2022). Changes in consumption patterns during the COVID-19 pandemic: Analyzing the revenge spending motivations of different emotional groups. Journal of Retailing and Consumer Services, 65, 102874. https://doi.org/10.1016/j.jretconser.2021.102874\nLee, C., Shin, H., Kim, K., & Kogler, D. F.* (2022). The effects of regional capacity in knowledge recombination on production efficiency. Technological Forecasting and Social Change. 180, 121669. https://doi.org/10.1016/j.techfore.2022.121669\nJung, E., Lee, C.*, & Hwang, J. (2022). Effective strategies to attract crowdfunding investment based on the novelty of business ideas. Technological Forecasting and Social Change. 178, 121558. https://doi.org/10.1016/j.techfore.2022.121558\nJeon, H., & Lee, C.* (2022). Internet of Things Technology: Balancing Privacy Concerns with Convenience. Telematics and Informatics. 70, 101816. https://doi.org/10.1016/j.tele.2022.101816\nKogler, D. F., Davies, R. B., Lee, C., & Kim, K.* (2022). Regional knowledge spaces: the interplay of entry-relatedness and entry-potential for technological change and growth. The Journal of Technology Transfer. 1-24. https://doi.org/10.1007/s10961-022-09924-2\nTóth, G., Elekes, Z., Whittle, A., Lee, C.*, & Kogler, D. F. (2022). Technology network structure conditions the economic resilience of regions. Economic Geography. 98(4), 1-24. https://doi.org/10.1080/00130095.2022.2035715\nJo, H., Park, S., Shin, D., Shin, J.*, & Lee, C. (2022). Estimating cost of fighting against fake news during catastrophic situations. Telematics and Informatics. 66, 101734. https://doi.org/10.1016/j.tele.2021.101734\nPark, I., Shim, H., Kim, J., Lee, C., & Lee, D.* (2022). The Effects of Popularity Metrics in News Comments on the Formation of Public Opinion: Evidence from an Internet Portal Site. The Social Science Journal. doi:https://doi.org/10.1080/03623319.2020.1768485\nKim, K., Lee, J., & Lee, C.(2022). Which innovation type is better for production efficiency? A comparison between product/service, process, organizational, and marketing innovations using stochastic frontier and meta-frontier analysis. Technology Analysis & Strategic Management. doi: https://doi.org/10.1080/09537325.2021.1965979\nRocchetta, S., Mina, A., Lee, C., & Kogler, F. D. (2022). Technological Knowledge Space and the Resilience of European Regions. Journal of Economic Geography. 22(1), 27-51.* doi: https://doi.org/10.1093/jeg/lbab001\nLee, C., Cho, H., & Lee, D.* (2021). The mechanism of innovation spill-over across sub-layers in the ICT industry. Asian Journal of Technology Innovation. 29(2), 159-179. doi:https://doi.org/10.1080/19761597.2020.1796725\nJung, S., Kim, K. & Lee, C.(2021). The nature of ICT in technology convergence: A knowledge-based network analysis. PLOS ONE. 16(7): e0254424. https://doi.org/10.1371/journal.pone.0254424\nNa, C., Lee, D., Hwang, J., & Lee, C.* (2021). Strategic Groups Emerged by Selecting R&D Collaboration Partners and Firms’ Efficiency. Asian Journal of Technology Innovation. 29(1), 109-133. doi:https://doi.org/10.1080/19761597.2020.1788957\nLee, C., Lee, D., & Shon, M.* (2020). Effect of efficient triple-helix collaboration on organizations based on their stage of growth. Journal of Engineering and Technology Management. 58, 101604. https://doi.org/10.1016/j.jengtecman.2020.101604\nLee, C., Kogler, D.F., & Lee, D.* (2019). Capturing Information on Technology Convergence, International Collaboration, and Knowledge Flow from Patent Document: A Case of Information and Communication Technology. Information Processing & Management. 56(4), 1576-1591. doi:1016/j.ipm.2018.09.007\nKim, E.H.W.* & Lee, C. (2019). Does Working Long Hours Cause Marital Dissolution? Evidence from the Reduction in South Korea’s Workweek Standard. Asian Population Studies. 15(1), 87-104. doi:1080/17441730.2019.1565131\nKim E.H.W.*, Lee, C., & Do, Y.K. (2019). The Effect of Adult Children’s Working Times on Visiting with Elderly Parents: A Natural Experiment in Korea. Population Research and Policy Review. 38(1), 53-72. doi:1007/s11113-018-9486-0\n\nFeatured in: Straits Times, Lianhe Zaobao (In Chinese)\n\nLee, C.* & Hwang, J. (2018). The Influence of Giant Platform on Content Diversity. Technological Forecasting and Social Change. 136, 157-165. doi:1016/j.techfore.2016.11.029\nKim, E.H.W.*, Lee, C., & Do Y.K. (2018). The Effect of a Reduced Statutory Workweek on Familial Long-Term Care in Korea. Journal of Aging and Health. 30(10), 1620-1641. doi:1177/0898264318797469\n\nPoster Session Winner, 2018 Population Association of America Conference\n\n\n\n\nLee, C. & Kim, H.* (2018). The Evolutionary Trajectory of ICT Ecosystem: A Network Analysis based on Media User Data. Information & Management. 55(6), 795-805. doi:1016/j.im.2018.03.008\n\nBest Paper Award, 2015 Korea Media Panel Conference\n\nLee, C., Shin, J., & Hong, A.* (2018). Does Social Media Use Really Make People Politically Polarized? Direct and Indirect Effects of Social Media Use on Political Polarization. Telematics and Informatics. 35(1), 245-254. doi:1016/j.tele.2017.11.005\nNa, H.S., Lee, D., Hwang, J., & Lee, C.(2018). Research on the Mutual Relations between ISP and ASP Efficiency Changes for the Sustainable Growth of Internet Industry. Applied Economics. 50(11), 1238-1253. doi:1080/00036846.2017.1358443\nLee, C., Kim, J.H., & Lee, D.* (2017). Intra-industry Innovation, Spillovers, and Industry Evolution: Evidence from the Korean ICT industry. Telematics and Informatics. 34(8), 1503-1513. doi:1016/j.tele.2017.06.013\nLee, C., Jung, S., & Kim, K.O.* (2017). Effect of a Policy Intervention on Handset Subsidies on the Intention to Change Handsets and Households’ Expenses in Mobile Telecommunications. Telematics and Informatics. 34(8), 1524-1531. doi:1016/j.tele.2017.06.017\nLee, C., Kim, H., & Hong, A.* (2017). Ex-post Evaluation of Illegalizing Juvenile Online Game after Midnight: A Case of Shutdown Policy in South Korea. Telematics and Informatics. 34(8), 1597-1606. doi:1016/j.tele.2017.07.006\n\nCited in Nature Editorial http://www.nature.com/news/put-cult-online-games-to-the-test-1.22343\n\nLee, C., Lee, K., & Lee, D.* (2017). Mobile Healthcare Applications and Gamification for Sustained Health Maintenance. 9(5), 772. doi:10.3390/su9050772\nLee, C., Lee, D.*, & Hwang, J. (2015). Platform Openness and the Productivity of Content Providers: A meta-frontier analysis. Telecommunications Policy. 39(7). 553-562. doi:1016/j.telpol.2014.06.010\n\n\n\n\nKorean Citation Index (KCI) Journals\n\nChoi, M. & Lee, C.* (2022). The effect of Online Community Activities in Non-face-to-face Situations on Life Satisfaction : Focusing on the Comparison between Before(2017) and After(2021) COVID-19. Information Society & Media. 23(3), 83-124. https://doi.org/52558/ISM.2022.12.23.3.83\nYeom, J., Lee, S., & Lee, C.* (2022). Analysis of the Characteristics of Extreme Patriotism and Psychological Motives by Understanding the Cyber Conflict Between Chinese Fandom Patriotism and Hallyu Fandom. Journal of Cybercommunication Academic Society. 39(4). 5-49. https://doi.org/10.36494/JCAS.2022.12.39.4.5\nKang, S., Seo, Y., & Lee, C.* (2022). A Study of the Development of Sectoral Digital Transformation Conflict Indicator. Information Society & Media. 23(1), 41-68. https://doi.org/52558/ISM.2022.04.23.1.41\n\nBest Paper Award, 2022 Korea Media Management Association\n\nKim, K., Lee, J., & Lee, C.* (2021). Exploratory Analysis of Knowledge Structure and Evolutionary Trajectory in Korean Artificial Intelligence for Effective Technology Policy. Korean Innovation Study, 16(3). DOI:https://doi.org/10.46251/INNOS.2021.8.16.3.139\nJo, H., Oh, M., Shin, J.*, & Lee, C. (2021). Identifying Fake news in the Disastrous situations: Video versus Text. Journal of Cybercommunication Academic Society, 38(2).  DOI: https://doi.org/10.36494/JCAS.2021.06.38.2.83\nRoh, T., Jo, G., & Lee, C.* (2021). Coincidence Analysis with International Patent Classification (IPC) Network of US ICT Companies: Focusing on Technology Similarity and Application. Korean Innovation Study, 16(2).  doi:https://doi.org/10.46251/INNOS.2021.5.16.2.237\nLee, C., Woo, H.J., & Park, S.B. (2020). Factors of newly entering into the purchase on TV home-shopping: Random Forest Analysis based on users’ media repertoire data. Korean Innovation Study, 15(2), 113-149. doi:https://doi.org/10.46251/INNOS.2020.05.15.2.113\nKim, M.K., Lee, C., & Hong, A. (2016). The Analysis of Media Usage Pattern for Effective Diffusion of Information. Information Society & Media. 17(1). 77-113. doi:https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE06667546\n\n\n\n\nBook chapters\n\nLee, C. & Bae, Y. (2019). Development of Information Communication Technology Industry and Public Policy in South Korea, In Ahn, M.J., & Kim, Y. (Eds.), Public Administration and Public Policy in Korea. Springer. Doi: https://doi.org/10.1007/978-3-319-31816-5_3801-1"
  },
  {
    "objectID": "research/working.html",
    "href": "research/working.html",
    "title": "Working in Progress",
    "section": "",
    "text": "Under Review\n\nDominant and Spark Technologies in Business Methods Innovation\n\nwith Keungoui Kim, Dieter F. Kogler, Junmin Lee\n\nGrandma’s digital play goes to FinTech\n\nwith Yoonwoo Choi\n\nMovie-Country Relatedness Density and Entry TOP20\n\nwith Sung Wook Ji\n\nOTT Watching Time and Sleep Duration & Pattern\n\nwith Keungoui Kim & Chungho Na\n\nSocial Distancing, Innovation Adoption, and Privacy Concern\n\nwith Keungoui Kim, Dongnyuk Shim, and Sira Maliphol\n\n메타버스의 의미를 통한 네트워크 의제설정 작용원리 탐색 연구\n\nwith 박영주, 박세진\n\n시니어 SNS 적극적 활용 요인 분석\n\nwith 박지은\n\n\n\n\n\nWorking papers\n\nRegional Knowledge Transformation\nMedia Relatedness & Political Polarization\nRegional Knowledge Structure & Recombination\nGovernment Financial Support on Innovation Performance_Mediating role of Collaboration with Science-based partners\n디지털 트랜스포메이션 생태계 분석\n메타버스 사회적 자본이 아바타 커스터마이징에 끼치는 영향\n요일과 시간효과를 고려한 OTT vs. TV 선택 모형\n블록체인 기반 미디어 플랫폼 선택 모형"
  },
  {
    "objectID": "teaching/ds101/index.html",
    "href": "teaching/ds101/index.html",
    "title": "Data Science 101",
    "section": "",
    "text": "a <- \"Data\"\nb <- \"Science\"\npaste0(\"Welcome to \", a,\" \",b,\" \",100+1)\n\n[1] \"Welcome to Data Science 101\"\n\n\n\nIntroduction\nWelcome to Data Science 101 course, designed to equip you with the essential skills to analyze, visualize, and communicate data effectively. Over the course of 15 weeks, you will delve into the fundamentals of data science, master the power of R programming, and learn how to create interactive visualizations and websites to showcase your findings.\nThroughout the course, you will learn how to import, manipulate, and explore data using R and the tidyverse. You will gain hands-on experience with data cleaning, transformation, and aggregation techniques. Additionally, you’ll dive deep into data visualization with ggplot2 and learn how to create advanced, interactive plots using Shiny and plotly.\nBy the end of the course, you will have completed a data science project that demonstrates your ability to analyze, visualize, and communicate complex data insights. You will also learn the importance of collaboration, version control, and reproducible research in data science projects. With a solid understanding of the concepts and tools covered, you will be well-prepared to apply your skills in various real-world applications.\n\n\n\nSyllabus\nWeek 1: Introduction to Data Science and R\n\nWhat is Data Science?\nIntroduction to R and RStudio\nR syntax and basic operations\nData types and structures in R\n\nWeek 2: Data Import and Export\n\nReading and writing data in R (CSV, Excel, JSON, etc.)\nData from APIs and web scraping\nHandling missing data and errors\n\nWeek 3: Data Manipulation I\n\nIntroduction to tidyverse\nData cleaning with dplyr and tidyr\nData filtering and aggregation\n\nWeek 4: Data Manipulation II\n\nData transformation with dplyr\nGrouping and summarizing data\nJoining datasets\n\nWeek 5: Data Exploration\n\nDescriptive statistics\nExploratory data analysis (EDA)\nIntroduction to ggplot2 for data visualization\n\nWeek 6: Data Visualization I\n\nGrammar of graphics with ggplot2\nCustomizing plots with themes and scales\nAdding labels, titles, and legends\n\nWeek 7: Data Visualization II\n\nAdvanced ggplot2 techniques\nCreating different types of plots (scatter plots, bar plots, etc.)\nVisualizing distributions and relationships\n\nWeek 8: Data Visualization III\n\nFaceting and multi-panel plots\nPlotting time series data\nInteractive plots with plotly or ggplotly\n\nWeek 9: Mid-term QZ\nWeek 10: Introduction to Shiny\n\nWhat is Shiny?\nCreating Shiny apps with R\nAdding interactivity to data visualizations\n\nWeek 11: Version Control and Collaboration\n\nIntroduction to Git and GitHub\nCollaborating with others using version control\nBest practices for organizing and documenting data science projects\nWorking with AI (feat. ChatGPT)\n\nWeek 12: Reproducible Research\n\nIntroduction to R Markdown\nCreating reports and presentations with R Markdown\nEmbedding code, visualizations, and results in R Markdown documents\n\nWeek 13: Creating Websites with Quarto\n\nIntroduction to Quarto\nCreating a Quarto website with R Markdown\nCustomizing the website layout and design\nPublishing and sharing your Quarto website\n\nWeek 14: Team project consultation\nWeek 15: Project presentation\n\n\n\n\nTextbooks for the course\n\nStatistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\nis a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language.\n\nR for Data Science (written by Hadley Wickham and Garrett Grolemund)\n\nis an excellent resource for learning data science using R, covering data manipulation, visualization, and modeling with R. The book is available as a free online resource.\n\nIntroductory Statistics with R (written by Peter Dalgaard)\n\nis a great resource for learning basic statistics with a focus on R programming. This book covers a wide range of statistical concepts, from descriptive statistics to hypothesis testing and linear regression, along with R code examples.\n\nR Cookbook (written by JD Long and Paul Teetor)\n\nis a comprehensive resource for data scientists, statisticians, and programmers who want to explore the capabilities of R programming for data analysis and visualization.\n\nR Graphic Cookbook (written by Winston Chang)\n\nis a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "All courses",
    "section": "",
    "text": "Undergraduate courses (학부 수업)\n\nData Science 101 (데이터사이언스의이해)\nMachine Learning 101 (데이터예측모델과기계학습의응용)\nData Journalism (데이터저널리즘)\nCulture & Technology (문화와기술의이해)\nStat for Data Science (데이터사이언스를위한통계학기초)\n\n\n\n\nGraduate courses (대학원 수업)\n\nMedia & Data-Science\nNetwork Analysis in Social Science\nComputer Vision and Unstructured Data Analysis for Social Science Research"
  },
  {
    "objectID": "teaching/media_ds/about/cov_anal.html",
    "href": "teaching/media_ds/about/cov_anal.html",
    "title": "Cov19 visualization practice",
    "section": "",
    "text": "Import libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lubridate)\n\nLoading required package: timechange\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(stringdist)\n\n\nAttaching package: 'stringdist'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n존스홉킨스 대학의 COVID19 데이터를 가져오는 코드\n\nclean_jhd_to_long <- function(df) {\n  df_str <- deparse(substitute(df))\n  var_str <- substr(df_str, 1, str_length(df_str) - 4)\n  \n  df %>% group_by(`Country/Region`) %>%\n    filter(`Country/Region` != \"Cruise Ship\") %>%\n    select(-`Province/State`, -Lat, -Long) %>%\n    mutate_at(vars(-group_cols()), sum) %>% \n    distinct() %>%\n    ungroup() %>%\n    rename(country = `Country/Region`) %>%\n    pivot_longer(\n      -country, \n      names_to = \"date_str\", \n      values_to = var_str\n    ) %>%\n    mutate(date = mdy(date_str)) %>%\n    select(country, date, !! sym(var_str)) \n}\n\nconfirmed_raw <- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndeaths_raw <- read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nhead(confirmed_raw)\n\n# A tibble: 6 × 1,147\n  Province…¹ Count…²   Lat  Long 1/22/…³ 1/23/…⁴ 1/24/…⁵ 1/25/…⁶ 1/26/…⁷ 1/27/…⁸\n  <chr>      <chr>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 <NA>       Afghan…  33.9 67.7        0       0       0       0       0       0\n2 <NA>       Albania  41.2 20.2        0       0       0       0       0       0\n3 <NA>       Algeria  28.0  1.66       0       0       0       0       0       0\n4 <NA>       Andorra  42.5  1.52       0       0       0       0       0       0\n5 <NA>       Angola  -11.2 17.9        0       0       0       0       0       0\n6 <NA>       Antarc… -71.9 23.3        0       0       0       0       0       0\n# … with 1,137 more variables: `1/28/20` <dbl>, `1/29/20` <dbl>,\n#   `1/30/20` <dbl>, `1/31/20` <dbl>, `2/1/20` <dbl>, `2/2/20` <dbl>,\n#   `2/3/20` <dbl>, `2/4/20` <dbl>, `2/5/20` <dbl>, `2/6/20` <dbl>,\n#   `2/7/20` <dbl>, `2/8/20` <dbl>, `2/9/20` <dbl>, `2/10/20` <dbl>,\n#   `2/11/20` <dbl>, `2/12/20` <dbl>, `2/13/20` <dbl>, `2/14/20` <dbl>,\n#   `2/15/20` <dbl>, `2/16/20` <dbl>, `2/17/20` <dbl>, `2/18/20` <dbl>,\n#   `2/19/20` <dbl>, `2/20/20` <dbl>, `2/21/20` <dbl>, `2/22/20` <dbl>, …\n\n\n\nconfirmed_raw[confirmed_raw$'Country/Region'==\"US\",]\n\n# A tibble: 1 × 1,147\n  Province…¹ Count…²   Lat  Long 1/22/…³ 1/23/…⁴ 1/24/…⁵ 1/25/…⁶ 1/26/…⁷ 1/27/…⁸\n  <chr>      <chr>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 <NA>       US         40  -100       1       1       2       2       5       5\n# … with 1,137 more variables: `1/28/20` <dbl>, `1/29/20` <dbl>,\n#   `1/30/20` <dbl>, `1/31/20` <dbl>, `2/1/20` <dbl>, `2/2/20` <dbl>,\n#   `2/3/20` <dbl>, `2/4/20` <dbl>, `2/5/20` <dbl>, `2/6/20` <dbl>,\n#   `2/7/20` <dbl>, `2/8/20` <dbl>, `2/9/20` <dbl>, `2/10/20` <dbl>,\n#   `2/11/20` <dbl>, `2/12/20` <dbl>, `2/13/20` <dbl>, `2/14/20` <dbl>,\n#   `2/15/20` <dbl>, `2/16/20` <dbl>, `2/17/20` <dbl>, `2/18/20` <dbl>,\n#   `2/19/20` <dbl>, `2/20/20` <dbl>, `2/21/20` <dbl>, `2/22/20` <dbl>, …\n\n\n\nconfirmed_raw %>% names\n\n   [1] \"Province/State\" \"Country/Region\" \"Lat\"            \"Long\"          \n   [5] \"1/22/20\"        \"1/23/20\"        \"1/24/20\"        \"1/25/20\"       \n   [9] \"1/26/20\"        \"1/27/20\"        \"1/28/20\"        \"1/29/20\"       \n  [13] \"1/30/20\"        \"1/31/20\"        \"2/1/20\"         \"2/2/20\"        \n  [17] \"2/3/20\"         \"2/4/20\"         \"2/5/20\"         \"2/6/20\"        \n  [21] \"2/7/20\"         \"2/8/20\"         \"2/9/20\"         \"2/10/20\"       \n  [25] \"2/11/20\"        \"2/12/20\"        \"2/13/20\"        \"2/14/20\"       \n  [29] \"2/15/20\"        \"2/16/20\"        \"2/17/20\"        \"2/18/20\"       \n  [33] \"2/19/20\"        \"2/20/20\"        \"2/21/20\"        \"2/22/20\"       \n  [37] \"2/23/20\"        \"2/24/20\"        \"2/25/20\"        \"2/26/20\"       \n  [41] \"2/27/20\"        \"2/28/20\"        \"2/29/20\"        \"3/1/20\"        \n  [45] \"3/2/20\"         \"3/3/20\"         \"3/4/20\"         \"3/5/20\"        \n  [49] \"3/6/20\"         \"3/7/20\"         \"3/8/20\"         \"3/9/20\"        \n  [53] \"3/10/20\"        \"3/11/20\"        \"3/12/20\"        \"3/13/20\"       \n  [57] \"3/14/20\"        \"3/15/20\"        \"3/16/20\"        \"3/17/20\"       \n  [61] \"3/18/20\"        \"3/19/20\"        \"3/20/20\"        \"3/21/20\"       \n  [65] \"3/22/20\"        \"3/23/20\"        \"3/24/20\"        \"3/25/20\"       \n  [69] \"3/26/20\"        \"3/27/20\"        \"3/28/20\"        \"3/29/20\"       \n  [73] \"3/30/20\"        \"3/31/20\"        \"4/1/20\"         \"4/2/20\"        \n  [77] \"4/3/20\"         \"4/4/20\"         \"4/5/20\"         \"4/6/20\"        \n  [81] \"4/7/20\"         \"4/8/20\"         \"4/9/20\"         \"4/10/20\"       \n  [85] \"4/11/20\"        \"4/12/20\"        \"4/13/20\"        \"4/14/20\"       \n  [89] \"4/15/20\"        \"4/16/20\"        \"4/17/20\"        \"4/18/20\"       \n  [93] \"4/19/20\"        \"4/20/20\"        \"4/21/20\"        \"4/22/20\"       \n  [97] \"4/23/20\"        \"4/24/20\"        \"4/25/20\"        \"4/26/20\"       \n [101] \"4/27/20\"        \"4/28/20\"        \"4/29/20\"        \"4/30/20\"       \n [105] \"5/1/20\"         \"5/2/20\"         \"5/3/20\"         \"5/4/20\"        \n [109] \"5/5/20\"         \"5/6/20\"         \"5/7/20\"         \"5/8/20\"        \n [113] \"5/9/20\"         \"5/10/20\"        \"5/11/20\"        \"5/12/20\"       \n [117] \"5/13/20\"        \"5/14/20\"        \"5/15/20\"        \"5/16/20\"       \n [121] \"5/17/20\"        \"5/18/20\"        \"5/19/20\"        \"5/20/20\"       \n [125] \"5/21/20\"        \"5/22/20\"        \"5/23/20\"        \"5/24/20\"       \n [129] \"5/25/20\"        \"5/26/20\"        \"5/27/20\"        \"5/28/20\"       \n [133] \"5/29/20\"        \"5/30/20\"        \"5/31/20\"        \"6/1/20\"        \n [137] \"6/2/20\"         \"6/3/20\"         \"6/4/20\"         \"6/5/20\"        \n [141] \"6/6/20\"         \"6/7/20\"         \"6/8/20\"         \"6/9/20\"        \n [145] \"6/10/20\"        \"6/11/20\"        \"6/12/20\"        \"6/13/20\"       \n [149] \"6/14/20\"        \"6/15/20\"        \"6/16/20\"        \"6/17/20\"       \n [153] \"6/18/20\"        \"6/19/20\"        \"6/20/20\"        \"6/21/20\"       \n [157] \"6/22/20\"        \"6/23/20\"        \"6/24/20\"        \"6/25/20\"       \n [161] \"6/26/20\"        \"6/27/20\"        \"6/28/20\"        \"6/29/20\"       \n [165] \"6/30/20\"        \"7/1/20\"         \"7/2/20\"         \"7/3/20\"        \n [169] \"7/4/20\"         \"7/5/20\"         \"7/6/20\"         \"7/7/20\"        \n [173] \"7/8/20\"         \"7/9/20\"         \"7/10/20\"        \"7/11/20\"       \n [177] \"7/12/20\"        \"7/13/20\"        \"7/14/20\"        \"7/15/20\"       \n [181] \"7/16/20\"        \"7/17/20\"        \"7/18/20\"        \"7/19/20\"       \n [185] \"7/20/20\"        \"7/21/20\"        \"7/22/20\"        \"7/23/20\"       \n [189] \"7/24/20\"        \"7/25/20\"        \"7/26/20\"        \"7/27/20\"       \n [193] \"7/28/20\"        \"7/29/20\"        \"7/30/20\"        \"7/31/20\"       \n [197] \"8/1/20\"         \"8/2/20\"         \"8/3/20\"         \"8/4/20\"        \n [201] \"8/5/20\"         \"8/6/20\"         \"8/7/20\"         \"8/8/20\"        \n [205] \"8/9/20\"         \"8/10/20\"        \"8/11/20\"        \"8/12/20\"       \n [209] \"8/13/20\"        \"8/14/20\"        \"8/15/20\"        \"8/16/20\"       \n [213] \"8/17/20\"        \"8/18/20\"        \"8/19/20\"        \"8/20/20\"       \n [217] \"8/21/20\"        \"8/22/20\"        \"8/23/20\"        \"8/24/20\"       \n [221] \"8/25/20\"        \"8/26/20\"        \"8/27/20\"        \"8/28/20\"       \n [225] \"8/29/20\"        \"8/30/20\"        \"8/31/20\"        \"9/1/20\"        \n [229] \"9/2/20\"         \"9/3/20\"         \"9/4/20\"         \"9/5/20\"        \n [233] \"9/6/20\"         \"9/7/20\"         \"9/8/20\"         \"9/9/20\"        \n [237] \"9/10/20\"        \"9/11/20\"        \"9/12/20\"        \"9/13/20\"       \n [241] \"9/14/20\"        \"9/15/20\"        \"9/16/20\"        \"9/17/20\"       \n [245] \"9/18/20\"        \"9/19/20\"        \"9/20/20\"        \"9/21/20\"       \n [249] \"9/22/20\"        \"9/23/20\"        \"9/24/20\"        \"9/25/20\"       \n [253] \"9/26/20\"        \"9/27/20\"        \"9/28/20\"        \"9/29/20\"       \n [257] \"9/30/20\"        \"10/1/20\"        \"10/2/20\"        \"10/3/20\"       \n [261] \"10/4/20\"        \"10/5/20\"        \"10/6/20\"        \"10/7/20\"       \n [265] \"10/8/20\"        \"10/9/20\"        \"10/10/20\"       \"10/11/20\"      \n [269] \"10/12/20\"       \"10/13/20\"       \"10/14/20\"       \"10/15/20\"      \n [273] \"10/16/20\"       \"10/17/20\"       \"10/18/20\"       \"10/19/20\"      \n [277] \"10/20/20\"       \"10/21/20\"       \"10/22/20\"       \"10/23/20\"      \n [281] \"10/24/20\"       \"10/25/20\"       \"10/26/20\"       \"10/27/20\"      \n [285] \"10/28/20\"       \"10/29/20\"       \"10/30/20\"       \"10/31/20\"      \n [289] \"11/1/20\"        \"11/2/20\"        \"11/3/20\"        \"11/4/20\"       \n [293] \"11/5/20\"        \"11/6/20\"        \"11/7/20\"        \"11/8/20\"       \n [297] \"11/9/20\"        \"11/10/20\"       \"11/11/20\"       \"11/12/20\"      \n [301] \"11/13/20\"       \"11/14/20\"       \"11/15/20\"       \"11/16/20\"      \n [305] \"11/17/20\"       \"11/18/20\"       \"11/19/20\"       \"11/20/20\"      \n [309] \"11/21/20\"       \"11/22/20\"       \"11/23/20\"       \"11/24/20\"      \n [313] \"11/25/20\"       \"11/26/20\"       \"11/27/20\"       \"11/28/20\"      \n [317] \"11/29/20\"       \"11/30/20\"       \"12/1/20\"        \"12/2/20\"       \n [321] \"12/3/20\"        \"12/4/20\"        \"12/5/20\"        \"12/6/20\"       \n [325] \"12/7/20\"        \"12/8/20\"        \"12/9/20\"        \"12/10/20\"      \n [329] \"12/11/20\"       \"12/12/20\"       \"12/13/20\"       \"12/14/20\"      \n [333] \"12/15/20\"       \"12/16/20\"       \"12/17/20\"       \"12/18/20\"      \n [337] \"12/19/20\"       \"12/20/20\"       \"12/21/20\"       \"12/22/20\"      \n [341] \"12/23/20\"       \"12/24/20\"       \"12/25/20\"       \"12/26/20\"      \n [345] \"12/27/20\"       \"12/28/20\"       \"12/29/20\"       \"12/30/20\"      \n [349] \"12/31/20\"       \"1/1/21\"         \"1/2/21\"         \"1/3/21\"        \n [353] \"1/4/21\"         \"1/5/21\"         \"1/6/21\"         \"1/7/21\"        \n [357] \"1/8/21\"         \"1/9/21\"         \"1/10/21\"        \"1/11/21\"       \n [361] \"1/12/21\"        \"1/13/21\"        \"1/14/21\"        \"1/15/21\"       \n [365] \"1/16/21\"        \"1/17/21\"        \"1/18/21\"        \"1/19/21\"       \n [369] \"1/20/21\"        \"1/21/21\"        \"1/22/21\"        \"1/23/21\"       \n [373] \"1/24/21\"        \"1/25/21\"        \"1/26/21\"        \"1/27/21\"       \n [377] \"1/28/21\"        \"1/29/21\"        \"1/30/21\"        \"1/31/21\"       \n [381] \"2/1/21\"         \"2/2/21\"         \"2/3/21\"         \"2/4/21\"        \n [385] \"2/5/21\"         \"2/6/21\"         \"2/7/21\"         \"2/8/21\"        \n [389] \"2/9/21\"         \"2/10/21\"        \"2/11/21\"        \"2/12/21\"       \n [393] \"2/13/21\"        \"2/14/21\"        \"2/15/21\"        \"2/16/21\"       \n [397] \"2/17/21\"        \"2/18/21\"        \"2/19/21\"        \"2/20/21\"       \n [401] \"2/21/21\"        \"2/22/21\"        \"2/23/21\"        \"2/24/21\"       \n [405] \"2/25/21\"        \"2/26/21\"        \"2/27/21\"        \"2/28/21\"       \n [409] \"3/1/21\"         \"3/2/21\"         \"3/3/21\"         \"3/4/21\"        \n [413] \"3/5/21\"         \"3/6/21\"         \"3/7/21\"         \"3/8/21\"        \n [417] \"3/9/21\"         \"3/10/21\"        \"3/11/21\"        \"3/12/21\"       \n [421] \"3/13/21\"        \"3/14/21\"        \"3/15/21\"        \"3/16/21\"       \n [425] \"3/17/21\"        \"3/18/21\"        \"3/19/21\"        \"3/20/21\"       \n [429] \"3/21/21\"        \"3/22/21\"        \"3/23/21\"        \"3/24/21\"       \n [433] \"3/25/21\"        \"3/26/21\"        \"3/27/21\"        \"3/28/21\"       \n [437] \"3/29/21\"        \"3/30/21\"        \"3/31/21\"        \"4/1/21\"        \n [441] \"4/2/21\"         \"4/3/21\"         \"4/4/21\"         \"4/5/21\"        \n [445] \"4/6/21\"         \"4/7/21\"         \"4/8/21\"         \"4/9/21\"        \n [449] \"4/10/21\"        \"4/11/21\"        \"4/12/21\"        \"4/13/21\"       \n [453] \"4/14/21\"        \"4/15/21\"        \"4/16/21\"        \"4/17/21\"       \n [457] \"4/18/21\"        \"4/19/21\"        \"4/20/21\"        \"4/21/21\"       \n [461] \"4/22/21\"        \"4/23/21\"        \"4/24/21\"        \"4/25/21\"       \n [465] \"4/26/21\"        \"4/27/21\"        \"4/28/21\"        \"4/29/21\"       \n [469] \"4/30/21\"        \"5/1/21\"         \"5/2/21\"         \"5/3/21\"        \n [473] \"5/4/21\"         \"5/5/21\"         \"5/6/21\"         \"5/7/21\"        \n [477] \"5/8/21\"         \"5/9/21\"         \"5/10/21\"        \"5/11/21\"       \n [481] \"5/12/21\"        \"5/13/21\"        \"5/14/21\"        \"5/15/21\"       \n [485] \"5/16/21\"        \"5/17/21\"        \"5/18/21\"        \"5/19/21\"       \n [489] \"5/20/21\"        \"5/21/21\"        \"5/22/21\"        \"5/23/21\"       \n [493] \"5/24/21\"        \"5/25/21\"        \"5/26/21\"        \"5/27/21\"       \n [497] \"5/28/21\"        \"5/29/21\"        \"5/30/21\"        \"5/31/21\"       \n [501] \"6/1/21\"         \"6/2/21\"         \"6/3/21\"         \"6/4/21\"        \n [505] \"6/5/21\"         \"6/6/21\"         \"6/7/21\"         \"6/8/21\"        \n [509] \"6/9/21\"         \"6/10/21\"        \"6/11/21\"        \"6/12/21\"       \n [513] \"6/13/21\"        \"6/14/21\"        \"6/15/21\"        \"6/16/21\"       \n [517] \"6/17/21\"        \"6/18/21\"        \"6/19/21\"        \"6/20/21\"       \n [521] \"6/21/21\"        \"6/22/21\"        \"6/23/21\"        \"6/24/21\"       \n [525] \"6/25/21\"        \"6/26/21\"        \"6/27/21\"        \"6/28/21\"       \n [529] \"6/29/21\"        \"6/30/21\"        \"7/1/21\"         \"7/2/21\"        \n [533] \"7/3/21\"         \"7/4/21\"         \"7/5/21\"         \"7/6/21\"        \n [537] \"7/7/21\"         \"7/8/21\"         \"7/9/21\"         \"7/10/21\"       \n [541] \"7/11/21\"        \"7/12/21\"        \"7/13/21\"        \"7/14/21\"       \n [545] \"7/15/21\"        \"7/16/21\"        \"7/17/21\"        \"7/18/21\"       \n [549] \"7/19/21\"        \"7/20/21\"        \"7/21/21\"        \"7/22/21\"       \n [553] \"7/23/21\"        \"7/24/21\"        \"7/25/21\"        \"7/26/21\"       \n [557] \"7/27/21\"        \"7/28/21\"        \"7/29/21\"        \"7/30/21\"       \n [561] \"7/31/21\"        \"8/1/21\"         \"8/2/21\"         \"8/3/21\"        \n [565] \"8/4/21\"         \"8/5/21\"         \"8/6/21\"         \"8/7/21\"        \n [569] \"8/8/21\"         \"8/9/21\"         \"8/10/21\"        \"8/11/21\"       \n [573] \"8/12/21\"        \"8/13/21\"        \"8/14/21\"        \"8/15/21\"       \n [577] \"8/16/21\"        \"8/17/21\"        \"8/18/21\"        \"8/19/21\"       \n [581] \"8/20/21\"        \"8/21/21\"        \"8/22/21\"        \"8/23/21\"       \n [585] \"8/24/21\"        \"8/25/21\"        \"8/26/21\"        \"8/27/21\"       \n [589] \"8/28/21\"        \"8/29/21\"        \"8/30/21\"        \"8/31/21\"       \n [593] \"9/1/21\"         \"9/2/21\"         \"9/3/21\"         \"9/4/21\"        \n [597] \"9/5/21\"         \"9/6/21\"         \"9/7/21\"         \"9/8/21\"        \n [601] \"9/9/21\"         \"9/10/21\"        \"9/11/21\"        \"9/12/21\"       \n [605] \"9/13/21\"        \"9/14/21\"        \"9/15/21\"        \"9/16/21\"       \n [609] \"9/17/21\"        \"9/18/21\"        \"9/19/21\"        \"9/20/21\"       \n [613] \"9/21/21\"        \"9/22/21\"        \"9/23/21\"        \"9/24/21\"       \n [617] \"9/25/21\"        \"9/26/21\"        \"9/27/21\"        \"9/28/21\"       \n [621] \"9/29/21\"        \"9/30/21\"        \"10/1/21\"        \"10/2/21\"       \n [625] \"10/3/21\"        \"10/4/21\"        \"10/5/21\"        \"10/6/21\"       \n [629] \"10/7/21\"        \"10/8/21\"        \"10/9/21\"        \"10/10/21\"      \n [633] \"10/11/21\"       \"10/12/21\"       \"10/13/21\"       \"10/14/21\"      \n [637] \"10/15/21\"       \"10/16/21\"       \"10/17/21\"       \"10/18/21\"      \n [641] \"10/19/21\"       \"10/20/21\"       \"10/21/21\"       \"10/22/21\"      \n [645] \"10/23/21\"       \"10/24/21\"       \"10/25/21\"       \"10/26/21\"      \n [649] \"10/27/21\"       \"10/28/21\"       \"10/29/21\"       \"10/30/21\"      \n [653] \"10/31/21\"       \"11/1/21\"        \"11/2/21\"        \"11/3/21\"       \n [657] \"11/4/21\"        \"11/5/21\"        \"11/6/21\"        \"11/7/21\"       \n [661] \"11/8/21\"        \"11/9/21\"        \"11/10/21\"       \"11/11/21\"      \n [665] \"11/12/21\"       \"11/13/21\"       \"11/14/21\"       \"11/15/21\"      \n [669] \"11/16/21\"       \"11/17/21\"       \"11/18/21\"       \"11/19/21\"      \n [673] \"11/20/21\"       \"11/21/21\"       \"11/22/21\"       \"11/23/21\"      \n [677] \"11/24/21\"       \"11/25/21\"       \"11/26/21\"       \"11/27/21\"      \n [681] \"11/28/21\"       \"11/29/21\"       \"11/30/21\"       \"12/1/21\"       \n [685] \"12/2/21\"        \"12/3/21\"        \"12/4/21\"        \"12/5/21\"       \n [689] \"12/6/21\"        \"12/7/21\"        \"12/8/21\"        \"12/9/21\"       \n [693] \"12/10/21\"       \"12/11/21\"       \"12/12/21\"       \"12/13/21\"      \n [697] \"12/14/21\"       \"12/15/21\"       \"12/16/21\"       \"12/17/21\"      \n [701] \"12/18/21\"       \"12/19/21\"       \"12/20/21\"       \"12/21/21\"      \n [705] \"12/22/21\"       \"12/23/21\"       \"12/24/21\"       \"12/25/21\"      \n [709] \"12/26/21\"       \"12/27/21\"       \"12/28/21\"       \"12/29/21\"      \n [713] \"12/30/21\"       \"12/31/21\"       \"1/1/22\"         \"1/2/22\"        \n [717] \"1/3/22\"         \"1/4/22\"         \"1/5/22\"         \"1/6/22\"        \n [721] \"1/7/22\"         \"1/8/22\"         \"1/9/22\"         \"1/10/22\"       \n [725] \"1/11/22\"        \"1/12/22\"        \"1/13/22\"        \"1/14/22\"       \n [729] \"1/15/22\"        \"1/16/22\"        \"1/17/22\"        \"1/18/22\"       \n [733] \"1/19/22\"        \"1/20/22\"        \"1/21/22\"        \"1/22/22\"       \n [737] \"1/23/22\"        \"1/24/22\"        \"1/25/22\"        \"1/26/22\"       \n [741] \"1/27/22\"        \"1/28/22\"        \"1/29/22\"        \"1/30/22\"       \n [745] \"1/31/22\"        \"2/1/22\"         \"2/2/22\"         \"2/3/22\"        \n [749] \"2/4/22\"         \"2/5/22\"         \"2/6/22\"         \"2/7/22\"        \n [753] \"2/8/22\"         \"2/9/22\"         \"2/10/22\"        \"2/11/22\"       \n [757] \"2/12/22\"        \"2/13/22\"        \"2/14/22\"        \"2/15/22\"       \n [761] \"2/16/22\"        \"2/17/22\"        \"2/18/22\"        \"2/19/22\"       \n [765] \"2/20/22\"        \"2/21/22\"        \"2/22/22\"        \"2/23/22\"       \n [769] \"2/24/22\"        \"2/25/22\"        \"2/26/22\"        \"2/27/22\"       \n [773] \"2/28/22\"        \"3/1/22\"         \"3/2/22\"         \"3/3/22\"        \n [777] \"3/4/22\"         \"3/5/22\"         \"3/6/22\"         \"3/7/22\"        \n [781] \"3/8/22\"         \"3/9/22\"         \"3/10/22\"        \"3/11/22\"       \n [785] \"3/12/22\"        \"3/13/22\"        \"3/14/22\"        \"3/15/22\"       \n [789] \"3/16/22\"        \"3/17/22\"        \"3/18/22\"        \"3/19/22\"       \n [793] \"3/20/22\"        \"3/21/22\"        \"3/22/22\"        \"3/23/22\"       \n [797] \"3/24/22\"        \"3/25/22\"        \"3/26/22\"        \"3/27/22\"       \n [801] \"3/28/22\"        \"3/29/22\"        \"3/30/22\"        \"3/31/22\"       \n [805] \"4/1/22\"         \"4/2/22\"         \"4/3/22\"         \"4/4/22\"        \n [809] \"4/5/22\"         \"4/6/22\"         \"4/7/22\"         \"4/8/22\"        \n [813] \"4/9/22\"         \"4/10/22\"        \"4/11/22\"        \"4/12/22\"       \n [817] \"4/13/22\"        \"4/14/22\"        \"4/15/22\"        \"4/16/22\"       \n [821] \"4/17/22\"        \"4/18/22\"        \"4/19/22\"        \"4/20/22\"       \n [825] \"4/21/22\"        \"4/22/22\"        \"4/23/22\"        \"4/24/22\"       \n [829] \"4/25/22\"        \"4/26/22\"        \"4/27/22\"        \"4/28/22\"       \n [833] \"4/29/22\"        \"4/30/22\"        \"5/1/22\"         \"5/2/22\"        \n [837] \"5/3/22\"         \"5/4/22\"         \"5/5/22\"         \"5/6/22\"        \n [841] \"5/7/22\"         \"5/8/22\"         \"5/9/22\"         \"5/10/22\"       \n [845] \"5/11/22\"        \"5/12/22\"        \"5/13/22\"        \"5/14/22\"       \n [849] \"5/15/22\"        \"5/16/22\"        \"5/17/22\"        \"5/18/22\"       \n [853] \"5/19/22\"        \"5/20/22\"        \"5/21/22\"        \"5/22/22\"       \n [857] \"5/23/22\"        \"5/24/22\"        \"5/25/22\"        \"5/26/22\"       \n [861] \"5/27/22\"        \"5/28/22\"        \"5/29/22\"        \"5/30/22\"       \n [865] \"5/31/22\"        \"6/1/22\"         \"6/2/22\"         \"6/3/22\"        \n [869] \"6/4/22\"         \"6/5/22\"         \"6/6/22\"         \"6/7/22\"        \n [873] \"6/8/22\"         \"6/9/22\"         \"6/10/22\"        \"6/11/22\"       \n [877] \"6/12/22\"        \"6/13/22\"        \"6/14/22\"        \"6/15/22\"       \n [881] \"6/16/22\"        \"6/17/22\"        \"6/18/22\"        \"6/19/22\"       \n [885] \"6/20/22\"        \"6/21/22\"        \"6/22/22\"        \"6/23/22\"       \n [889] \"6/24/22\"        \"6/25/22\"        \"6/26/22\"        \"6/27/22\"       \n [893] \"6/28/22\"        \"6/29/22\"        \"6/30/22\"        \"7/1/22\"        \n [897] \"7/2/22\"         \"7/3/22\"         \"7/4/22\"         \"7/5/22\"        \n [901] \"7/6/22\"         \"7/7/22\"         \"7/8/22\"         \"7/9/22\"        \n [905] \"7/10/22\"        \"7/11/22\"        \"7/12/22\"        \"7/13/22\"       \n [909] \"7/14/22\"        \"7/15/22\"        \"7/16/22\"        \"7/17/22\"       \n [913] \"7/18/22\"        \"7/19/22\"        \"7/20/22\"        \"7/21/22\"       \n [917] \"7/22/22\"        \"7/23/22\"        \"7/24/22\"        \"7/25/22\"       \n [921] \"7/26/22\"        \"7/27/22\"        \"7/28/22\"        \"7/29/22\"       \n [925] \"7/30/22\"        \"7/31/22\"        \"8/1/22\"         \"8/2/22\"        \n [929] \"8/3/22\"         \"8/4/22\"         \"8/5/22\"         \"8/6/22\"        \n [933] \"8/7/22\"         \"8/8/22\"         \"8/9/22\"         \"8/10/22\"       \n [937] \"8/11/22\"        \"8/12/22\"        \"8/13/22\"        \"8/14/22\"       \n [941] \"8/15/22\"        \"8/16/22\"        \"8/17/22\"        \"8/18/22\"       \n [945] \"8/19/22\"        \"8/20/22\"        \"8/21/22\"        \"8/22/22\"       \n [949] \"8/23/22\"        \"8/24/22\"        \"8/25/22\"        \"8/26/22\"       \n [953] \"8/27/22\"        \"8/28/22\"        \"8/29/22\"        \"8/30/22\"       \n [957] \"8/31/22\"        \"9/1/22\"         \"9/2/22\"         \"9/3/22\"        \n [961] \"9/4/22\"         \"9/5/22\"         \"9/6/22\"         \"9/7/22\"        \n [965] \"9/8/22\"         \"9/9/22\"         \"9/10/22\"        \"9/11/22\"       \n [969] \"9/12/22\"        \"9/13/22\"        \"9/14/22\"        \"9/15/22\"       \n [973] \"9/16/22\"        \"9/17/22\"        \"9/18/22\"        \"9/19/22\"       \n [977] \"9/20/22\"        \"9/21/22\"        \"9/22/22\"        \"9/23/22\"       \n [981] \"9/24/22\"        \"9/25/22\"        \"9/26/22\"        \"9/27/22\"       \n [985] \"9/28/22\"        \"9/29/22\"        \"9/30/22\"        \"10/1/22\"       \n [989] \"10/2/22\"        \"10/3/22\"        \"10/4/22\"        \"10/5/22\"       \n [993] \"10/6/22\"        \"10/7/22\"        \"10/8/22\"        \"10/9/22\"       \n [997] \"10/10/22\"       \"10/11/22\"       \"10/12/22\"       \"10/13/22\"      \n[1001] \"10/14/22\"       \"10/15/22\"       \"10/16/22\"       \"10/17/22\"      \n[1005] \"10/18/22\"       \"10/19/22\"       \"10/20/22\"       \"10/21/22\"      \n[1009] \"10/22/22\"       \"10/23/22\"       \"10/24/22\"       \"10/25/22\"      \n[1013] \"10/26/22\"       \"10/27/22\"       \"10/28/22\"       \"10/29/22\"      \n[1017] \"10/30/22\"       \"10/31/22\"       \"11/1/22\"        \"11/2/22\"       \n[1021] \"11/3/22\"        \"11/4/22\"        \"11/5/22\"        \"11/6/22\"       \n[1025] \"11/7/22\"        \"11/8/22\"        \"11/9/22\"        \"11/10/22\"      \n[1029] \"11/11/22\"       \"11/12/22\"       \"11/13/22\"       \"11/14/22\"      \n[1033] \"11/15/22\"       \"11/16/22\"       \"11/17/22\"       \"11/18/22\"      \n[1037] \"11/19/22\"       \"11/20/22\"       \"11/21/22\"       \"11/22/22\"      \n[1041] \"11/23/22\"       \"11/24/22\"       \"11/25/22\"       \"11/26/22\"      \n[1045] \"11/27/22\"       \"11/28/22\"       \"11/29/22\"       \"11/30/22\"      \n[1049] \"12/1/22\"        \"12/2/22\"        \"12/3/22\"        \"12/4/22\"       \n[1053] \"12/5/22\"        \"12/6/22\"        \"12/7/22\"        \"12/8/22\"       \n[1057] \"12/9/22\"        \"12/10/22\"       \"12/11/22\"       \"12/12/22\"      \n[1061] \"12/13/22\"       \"12/14/22\"       \"12/15/22\"       \"12/16/22\"      \n[1065] \"12/17/22\"       \"12/18/22\"       \"12/19/22\"       \"12/20/22\"      \n[1069] \"12/21/22\"       \"12/22/22\"       \"12/23/22\"       \"12/24/22\"      \n[1073] \"12/25/22\"       \"12/26/22\"       \"12/27/22\"       \"12/28/22\"      \n[1077] \"12/29/22\"       \"12/30/22\"       \"12/31/22\"       \"1/1/23\"        \n[1081] \"1/2/23\"         \"1/3/23\"         \"1/4/23\"         \"1/5/23\"        \n[1085] \"1/6/23\"         \"1/7/23\"         \"1/8/23\"         \"1/9/23\"        \n[1089] \"1/10/23\"        \"1/11/23\"        \"1/12/23\"        \"1/13/23\"       \n[1093] \"1/14/23\"        \"1/15/23\"        \"1/16/23\"        \"1/17/23\"       \n[1097] \"1/18/23\"        \"1/19/23\"        \"1/20/23\"        \"1/21/23\"       \n[1101] \"1/22/23\"        \"1/23/23\"        \"1/24/23\"        \"1/25/23\"       \n[1105] \"1/26/23\"        \"1/27/23\"        \"1/28/23\"        \"1/29/23\"       \n[1109] \"1/30/23\"        \"1/31/23\"        \"2/1/23\"         \"2/2/23\"        \n[1113] \"2/3/23\"         \"2/4/23\"         \"2/5/23\"         \"2/6/23\"        \n[1117] \"2/7/23\"         \"2/8/23\"         \"2/9/23\"         \"2/10/23\"       \n[1121] \"2/11/23\"        \"2/12/23\"        \"2/13/23\"        \"2/14/23\"       \n[1125] \"2/15/23\"        \"2/16/23\"        \"2/17/23\"        \"2/18/23\"       \n[1129] \"2/19/23\"        \"2/20/23\"        \"2/21/23\"        \"2/22/23\"       \n[1133] \"2/23/23\"        \"2/24/23\"        \"2/25/23\"        \"2/26/23\"       \n[1137] \"2/27/23\"        \"2/28/23\"        \"3/1/23\"         \"3/2/23\"        \n[1141] \"3/3/23\"         \"3/4/23\"         \"3/5/23\"         \"3/6/23\"        \n[1145] \"3/7/23\"         \"3/8/23\"         \"3/9/23\"        \n\n\n확진자 데이터 프레임을 만들어 보자.\n\nconfirmed_raw %>% \n  filter(`Country/Region` %in% c(\"China\", \"Italy\", \"Japan\", \"United Kingdom\", \"US\", \"Korea, South\",\n                                 \"Spain\")) %>%\n  select(-c(`Province/State`, Lat, Long)) %>% \n  group_by(`Country/Region`) %>% summarise_all(sum) -> test\n\nnames(test)[1]<-\"country\"\n\nmelt(data = test, id.vars = \"country\", measure.vars = names(test)[-1]) %>% \n  separate(variable, into = c(\"mon\", \"day\", \"year\"), sep='/', extra = \"merge\") %>% \n  filter(day %in% c(1)) %>%\n  arrange(mon, day) %>% \n  mutate(date=as.Date(with(.,paste(mon, day, year, sep=\"/\")), format = \"%m/%d/%y\")) %>% \n  dcast(country ~ date) -> df.conf.case\n\n\ndf.conf.case\n\n         country 2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\n1          China      11891      79932      84002      86850      87520\n2          Italy          2       1694     110574     207428     233197\n3          Japan         20        259       2535      14558      16778\n4   Korea, South         12       3736       9887      10780      11541\n5          Spain          1         84     104118     215216     239638\n6 United Kingdom          2         94      43755     183500     258979\n7             US          8         32     227903    1115972    1809384\n  2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01 2020-12-01 2021-01-01\n1      88344      91690      94363      95568      97250      99336     102649\n2     240760     247832     270189     317409     709335    1620901    2129376\n3      18732      37790      69018      84212     101936     150857     239005\n4      12904      14366      20449      23952      26732      35163      62593\n5     249659     288522     470973     778607    1185678    1656444    1928265\n6     285276     305558     339403     462780    1038056    1647165    2549671\n7    2698127    4605921    6088458    7292562    9254490   13866746   20397398\n  2021-02-01 2021-03-01 2021-04-01 2021-05-01 2021-06-01 2021-07-01 2021-08-01\n1     107902     109034     110169     111325     112329     113614     115473\n2    2560957    2938371    3607083    4035617    4220304    4260788    4355348\n3     392533     433334     477691     598754     749126     801337     936852\n4      78844      90372     104194     123240     141476     158549     201002\n5    2822805    3204531    3291394    3524077    3682778    3821305    4447044\n6    3846807    4194287    4364544    4434156    4506331    4844879    5907641\n7   26482919   28814420   30656330   32516226   33407540   33797251   35152818\n  2021-09-01 2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01 2022-03-01\n1     117991     119790     121477     123725     128132     134564     456582\n2    4546487    4675758    4774783    5043620    6266939   11116422   12829972\n3    1514400    1706516    1722427    1726913    1733835    2825414    5078276\n4     255401     316020     367974     457612     639083     884310    3492686\n5    4861883    4961128    5011148    5174720    6294745   10039126   11036085\n6    6856890    7878555    9140352   10333452   13174530   17543963   19120746\n7   39585475   43694428   46163201   48743340   55099948   75570589   79228450\n  2022-04-01 2022-05-01 2022-06-01 2022-07-01 2022-08-01 2022-09-01 2022-10-01\n1    1400358    2024284    2097282    2137169    2265424    2510703    2762150\n2   14719394   16504791   17440232   18610011   21059545   21888255   22500346\n3    6614278    7910179    8876113    9355427   12935010   19116684   21329519\n4   13639915   17295733   18129313   18379552   19932439   23417425   24819611\n5   11551574   11896152   12360256   12818184   13226579   13342530   13422984\n6   21379545   22214004   22492903   22941360   23515928   23738035   23893496\n7   80252748   81483804   84556267   87832253   91515236   94659072   96369625\n  2022-11-01 2022-12-01 2023-01-01 2023-02-01 2023-03-01\n1    2959481    3764783    4612203    4903498    4903524\n2   23531023   24260660   25143705   25453789   25576852\n3   22389872   24933509   29321601   32610584   33241180\n4   25670407   27208800   29139535   30213928   30533573\n5   13511768   13595504   13684258   13731478   13763336\n6   24122922   24251636   24365688   24507372   24603450\n7   97540736   98903928  100769628  102479379  103533872\n\n\n사망자 데이터 프레임을 만들어 보자.\n\ndeaths_raw %>% \n  filter(`Country/Region` %in% c(\"China\", \"Italy\", \"Japan\", \"United Kingdom\", \"US\", \"Korea, South\",\n                                 \"Spain\")) %>%\n  select(-c(`Province/State`, Lat, Long)) %>% \n  group_by(`Country/Region`) %>% summarise_all(sum) -> test\n\nnames(test)[1]<-\"country\"\n\nmelt(data = test, id.vars = \"country\", measure.vars = names(test)[-1]) %>% \n  separate(variable, into = c(\"mon\", \"day\", \"year\"), sep='/', extra = \"merge\") %>% \n  filter(day %in% c(1)) %>%\n  arrange(mon, day) %>% \n  mutate(date=as.Date(with(.,paste(mon, day, year, sep=\"/\")), format = \"%m/%d/%y\")) %>% \n  dcast(country ~ date) -> df.death.case\n\n\ndf.death.case\n\n         country 2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\n1          China        259       2872       3332       4698       4708\n2          Italy          0         34      13155      28236      33475\n3          Japan          0          6         72        510        900\n4   Korea, South          0         17        165        250        272\n5          Spain          0          0       9387      24543      27127\n6 United Kingdom          1          3       6070      39849      52768\n7             US          0          1       6996      68518     108624\n  2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01 2020-12-01 2021-01-01\n1       4713       4737       4797       4813       4814       4830       4884\n2      34788      35146      35491      35918      38826      56361      74621\n3        976       1013       1314       1583       1776       2193       3541\n4        282        301        326        416        468        526        942\n5      28364      28445      29152      31973      35878      45511      50837\n6      56338      57454      57995      58946      64667      78184      95917\n7     128134     155059     183855     206852     231054     273099     352844\n  2021-02-01 2021-03-01 2021-04-01 2021-05-01 2021-06-01 2021-07-01 2021-08-01\n1       4966       5011       5023       5031       5031       5033       5047\n2      88845      97945     109847     121033     126221     127587     128068\n3       5833       7948       9194      10326      13160      14808      15198\n4       1435       1606       1737       1833       1965       2024       2099\n5      59081      69609      75541      78216      79983      80883      81486\n6     132799     148935     153012     154085     154509     155010     156941\n7     448381     513045     549448     572904     590904     600972     609715\n  2021-09-01 2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01 2022-03-01\n1       5056       5069       5081       5089       5103       5119       5901\n2     129290     130973     132120     133931     137513     146925     155000\n3      16138      17685      18274      18361      18392      18885      23908\n4       2303       2504       2874       3705       5694       6787       8266\n5      84472      86463      87368      88080      89405      93633      99883\n6     160317     164780     169438     173903     178046     184840     188681\n7     639812     699021     746135     781422     825870     892252     952086\n  2022-04-01 2022-05-01 2022-06-01 2022-07-01 2022-08-01 2022-09-01 2022-10-01\n1      12869      14697      14899      14928      15052      15251      15719\n2     159537     163612     166756     168425     172207     175663     177130\n3      28202      29605      30659      31302      32707      40245      45023\n4      16929      22958      24212      24562      25084      26940      28489\n5     102541     104456     106493     108111     110719     112600     114179\n6     193232     198276     200347     201869     205574     207875     209346\n7     983972     996109    1007741    1017872    1030654    1046956    1059542\n  2022-11-01 2022-12-01 2023-01-01 2023-02-01 2023-03-01\n1      15965      16001      17167      97668     101051\n2     179101     181098     184642     186833     188094\n3      46817      49834      57521      68407      72494\n4      29239      30621      32272      33522      34003\n5     115078     115901     117095     118434     119380\n6     212435     214234     217175     220129     220721\n7    1070821    1081153    1092779    1109996    1120897\n\n\n확진자, 사망자 데이터 프레임을 행렬로 만들어보자.¶\n\n# country.name<-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \ncountry.name<-unlist(df.conf.case[c(1)])\n\n#str(df.conf.case)\nm.conf.case<-as.matrix(df.conf.case[-1])\nrow.names(m.conf.case)<-country.name\n\nm.death.case<-as.matrix(df.death.case[-1])\nrow.names(m.death.case)=country.name\n\nm.death.rate<-round(m.death.case/m.conf.case, 2)\n\n\n확진자 행렬: m.conf.case\n사망자 행렬: m.death.case\n\n\nm.conf.case\n\n               2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\nChina               11891      79932      84002      86850      87520\nItaly                   2       1694     110574     207428     233197\nJapan                  20        259       2535      14558      16778\nKorea, South           12       3736       9887      10780      11541\nSpain                   1         84     104118     215216     239638\nUnited Kingdom          2         94      43755     183500     258979\nUS                      8         32     227903    1115972    1809384\n               2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01\nChina               88344      91690      94363      95568      97250\nItaly              240760     247832     270189     317409     709335\nJapan               18732      37790      69018      84212     101936\nKorea, South        12904      14366      20449      23952      26732\nSpain              249659     288522     470973     778607    1185678\nUnited Kingdom     285276     305558     339403     462780    1038056\nUS                2698127    4605921    6088458    7292562    9254490\n               2020-12-01 2021-01-01 2021-02-01 2021-03-01 2021-04-01\nChina               99336     102649     107902     109034     110169\nItaly             1620901    2129376    2560957    2938371    3607083\nJapan              150857     239005     392533     433334     477691\nKorea, South        35163      62593      78844      90372     104194\nSpain             1656444    1928265    2822805    3204531    3291394\nUnited Kingdom    1647165    2549671    3846807    4194287    4364544\nUS               13866746   20397398   26482919   28814420   30656330\n               2021-05-01 2021-06-01 2021-07-01 2021-08-01 2021-09-01\nChina              111325     112329     113614     115473     117991\nItaly             4035617    4220304    4260788    4355348    4546487\nJapan              598754     749126     801337     936852    1514400\nKorea, South       123240     141476     158549     201002     255401\nSpain             3524077    3682778    3821305    4447044    4861883\nUnited Kingdom    4434156    4506331    4844879    5907641    6856890\nUS               32516226   33407540   33797251   35152818   39585475\n               2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01\nChina              119790     121477     123725     128132     134564\nItaly             4675758    4774783    5043620    6266939   11116422\nJapan             1706516    1722427    1726913    1733835    2825414\nKorea, South       316020     367974     457612     639083     884310\nSpain             4961128    5011148    5174720    6294745   10039126\nUnited Kingdom    7878555    9140352   10333452   13174530   17543963\nUS               43694428   46163201   48743340   55099948   75570589\n               2022-03-01 2022-04-01 2022-05-01 2022-06-01 2022-07-01\nChina              456582    1400358    2024284    2097282    2137169\nItaly            12829972   14719394   16504791   17440232   18610011\nJapan             5078276    6614278    7910179    8876113    9355427\nKorea, South      3492686   13639915   17295733   18129313   18379552\nSpain            11036085   11551574   11896152   12360256   12818184\nUnited Kingdom   19120746   21379545   22214004   22492903   22941360\nUS               79228450   80252748   81483804   84556267   87832253\n               2022-08-01 2022-09-01 2022-10-01 2022-11-01 2022-12-01\nChina             2265424    2510703    2762150    2959481    3764783\nItaly            21059545   21888255   22500346   23531023   24260660\nJapan            12935010   19116684   21329519   22389872   24933509\nKorea, South     19932439   23417425   24819611   25670407   27208800\nSpain            13226579   13342530   13422984   13511768   13595504\nUnited Kingdom   23515928   23738035   23893496   24122922   24251636\nUS               91515236   94659072   96369625   97540736   98903928\n               2023-01-01 2023-02-01 2023-03-01\nChina             4612203    4903498    4903524\nItaly            25143705   25453789   25576852\nJapan            29321601   32610584   33241180\nKorea, South     29139535   30213928   30533573\nSpain            13684258   13731478   13763336\nUnited Kingdom   24365688   24507372   24603450\nUS              100769628  102479379  103533872\n\n\n\nm.death.case\n\n               2020-02-01 2020-03-01 2020-04-01 2020-05-01 2020-06-01\nChina                 259       2872       3332       4698       4708\nItaly                   0         34      13155      28236      33475\nJapan                   0          6         72        510        900\nKorea, South            0         17        165        250        272\nSpain                   0          0       9387      24543      27127\nUnited Kingdom          1          3       6070      39849      52768\nUS                      0          1       6996      68518     108624\n               2020-07-01 2020-08-01 2020-09-01 2020-10-01 2020-11-01\nChina                4713       4737       4797       4813       4814\nItaly               34788      35146      35491      35918      38826\nJapan                 976       1013       1314       1583       1776\nKorea, South          282        301        326        416        468\nSpain               28364      28445      29152      31973      35878\nUnited Kingdom      56338      57454      57995      58946      64667\nUS                 128134     155059     183855     206852     231054\n               2020-12-01 2021-01-01 2021-02-01 2021-03-01 2021-04-01\nChina                4830       4884       4966       5011       5023\nItaly               56361      74621      88845      97945     109847\nJapan                2193       3541       5833       7948       9194\nKorea, South          526        942       1435       1606       1737\nSpain               45511      50837      59081      69609      75541\nUnited Kingdom      78184      95917     132799     148935     153012\nUS                 273099     352844     448381     513045     549448\n               2021-05-01 2021-06-01 2021-07-01 2021-08-01 2021-09-01\nChina                5031       5031       5033       5047       5056\nItaly              121033     126221     127587     128068     129290\nJapan               10326      13160      14808      15198      16138\nKorea, South         1833       1965       2024       2099       2303\nSpain               78216      79983      80883      81486      84472\nUnited Kingdom     154085     154509     155010     156941     160317\nUS                 572904     590904     600972     609715     639812\n               2021-10-01 2021-11-01 2021-12-01 2022-01-01 2022-02-01\nChina                5069       5081       5089       5103       5119\nItaly              130973     132120     133931     137513     146925\nJapan               17685      18274      18361      18392      18885\nKorea, South         2504       2874       3705       5694       6787\nSpain               86463      87368      88080      89405      93633\nUnited Kingdom     164780     169438     173903     178046     184840\nUS                 699021     746135     781422     825870     892252\n               2022-03-01 2022-04-01 2022-05-01 2022-06-01 2022-07-01\nChina                5901      12869      14697      14899      14928\nItaly              155000     159537     163612     166756     168425\nJapan               23908      28202      29605      30659      31302\nKorea, South         8266      16929      22958      24212      24562\nSpain               99883     102541     104456     106493     108111\nUnited Kingdom     188681     193232     198276     200347     201869\nUS                 952086     983972     996109    1007741    1017872\n               2022-08-01 2022-09-01 2022-10-01 2022-11-01 2022-12-01\nChina               15052      15251      15719      15965      16001\nItaly              172207     175663     177130     179101     181098\nJapan               32707      40245      45023      46817      49834\nKorea, South        25084      26940      28489      29239      30621\nSpain              110719     112600     114179     115078     115901\nUnited Kingdom     205574     207875     209346     212435     214234\nUS                1030654    1046956    1059542    1070821    1081153\n               2023-01-01 2023-02-01 2023-03-01\nChina               17167      97668     101051\nItaly              184642     186833     188094\nJapan               57521      68407      72494\nKorea, South        32272      33522      34003\nSpain              117095     118434     119380\nUnited Kingdom     217175     220129     220721\nUS                1092779    1109996    1120897\n\n\n선정된 국가에 대한 인구 벡터를 만들어 봅시다. 국가 이름 벡터. 인구 벡터에 이름을 붙여주기 위해 생성\n\ncountry.name\n\n        country1         country2         country3         country4 \n         \"China\"          \"Italy\"          \"Japan\"   \"Korea, South\" \n        country5         country6         country7 \n         \"Spain\" \"United Kingdom\"             \"US\" \n\n\n선정된 국가 순서대로 인구 수를 입력한 벡터\n\npop<-c(1439323776, 60461826, 126476461, 51269185, 46754778, 67886011, 331002651)\npop\n\n[1] 1439323776   60461826  126476461   51269185   46754778   67886011  331002651\n\n\n아직은 벡터 값들에 이름이 붙어 있지 않은 것을 알 수 있다.\n\nnames(pop)\n\nNULL\n\n\npop 벡터에 각 인구수가 어느 국가이 인구수인지 names() 함수로 지정해줌\n\npop<-c(1439323776, 60461826, 126476461, 51269185, 46754778, 67886011, 331002651)\nnames(pop)<-country.name\npop\n\n         China          Italy          Japan   Korea, South          Spain \n    1439323776       60461826      126476461       51269185       46754778 \nUnited Kingdom             US \n      67886011      331002651 \n\n\nGDP 벡터를 만들어 봅시다 - 마찬가지로 names() 함수로 GDP가 어느 국가에 해당하는 GDP인지에 대한 정보를 준다.\n\n# round(m.conf.case/pop*1000, 2)\ncountry.name<-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \nGDP<-c(12237700479375,\n1943835376342,\n4872415104315,\n1530750923149,\n1314314164402,\n2637866340434,\n19485394000000)\nnames(GDP)<-country.name\n\nGDP\n\n       China        Italy        Japan        Korea        Spain           UK \n1.223770e+13 1.943835e+12 4.872415e+12 1.530751e+12 1.314314e+12 2.637866e+12 \n          US \n1.948539e+13 \n\n\n선정된 국가에 대한 인구밀도 벡터를 만들어 봅시다.\n\ncountry.name<-c(\"China\",\"Italy\",\"Japan\",\"Korea\",\"Spain\",\"UK\",\"US\")  \npop.density<-c(148, 205, 347, 530, 94, 275, 36)\nnames(pop.density)<-country.name\n\n각국의 GDP 시각화를 해보자.\n\nbarplot(GDP)\n\n\n\n\n\nbarplot(sort(GDP))\n\n\n\n\n\nbarplot(sort(GDP, decreasing = T))"
  },
  {
    "objectID": "teaching/media_ds/about/index.html",
    "href": "teaching/media_ds/about/index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Week 1: Course Intro\n\nDate: 20230302\nClass\n\nCourse Introduction PDF\nInstall R, RStudio, & Rtools\n\n\n\n\n\nWeek 2: R Basic Syntax (1)\n\nDate: 20230309\nPre-class: Basic syntax, Vector, Array\nClass: Hands-on practice PDF\n\nData in use: COV19_data\nProcess of creating the data above [Code]\n\n\n\n\nWeek 3: R Basic Syntax (2)\n\nDate: 20230316\nPre-class: Data.frame, List\nClass: Hands-on practice PDF\n\nData in use: List_KMP\n\n\n\n\nWeek 4: R Basic Skillset (1)\n\nDate: 20230323\nPre-class: Read, Write, Condition, Repetition\nClass: Hands-on practice [Code]\n\n\n\nWeek 5: R Basic Skillset (2)\n\nDate: 20230330\nPre-class: Function, Missing values, Outliers\nClass: Hands-on practice (Review Week 1 to 5)\n데이터사이언스를 활용한 미디어연구에 대한 고찰 [PDF]\n\nKCI 논문 검색 dbpia\nSSCI 논문 검색 google scholar\n미디어 분야에서 데이터사이언스 활용한 논문 목록 리스트 [XLSX]\n\n\n\n\n\nWeek 6: Data manipulation\n\nDate: 20230406\nPre-class: Data wrangling (Base R & Tidyverse)\nClass: Hands-on practice\n\nIntroducing tidyverse [click]\nGo to Posit cheatsheets [click]\n\nRecommended books for the further study\n\nStatistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\nis a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language.\n\nR for Data Science (written by Hadley Wickham and Garrett Grolemund)\n\nis an excellent resource for learning data science using R, covering data manipulation, visualization, and modeling with R. The book is available as a free online resource.\n\n\n\n\n\nWeek 7: Data visualization\n\nDate: 20230413\nPre-class: Data-visualization, ggplot2, and (ggplot practice, optional)\nClass: Hands-on practice\n\nggplot2 world [click]\nggplot2 extension gallery [click]\n\nRecommended books for the further study\n\nR Graphic Cookbook (written by Winston Chang)\n\nis a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems\n\n\n\n\n\n\nWeek 8: Text mining (1)\n\nDate: 20230420\n전처리, 빈도 분석, 형태소 분석\nCode for the class [here]\n\n\n\nWeek 9: Text mining (2)\n\nDate: 20230427\nTF-IDF, 감정 분석\nCode for the class [here]\n\n\n\nWeek 10: Text mining (3)\n\nDate: 20230504\n동시출현 분석\nCode for the class [here]\n\n\n\nWeek 11: Text mining (4)\n\nDate: 20230511\n토픽 모델, 총정리 및 실습\nCode for the class [here]\n\n\n\n\nWeek 12: 데이터사이언스를 활용한 논문 발제 수업 (1)\n\nDate: 20230518\nClass: 데이터사이언스를 활용한 논문을 찾아 5분 요약 소개 발표\n\n\n\nWeek 13: 데이터사이언스를 활용한 프로포절\n\nDate: 20230525\nClass: 데이터사이언스를 활용해서 풀고 싶은 내용 5분 발표\n\n\n\n\nWeek 14: 프로포절 개인 미팅\n\nDate: 20230601\n\n\n\nWeek 15: 최종 과제 제출\n\nDate: 20230608"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_1.html",
    "href": "teaching/media_ds/about/NLP_1.html",
    "title": "NLP",
    "section": "",
    "text": "Download example data\n\nspeech_moon.txt\nspeech_park.txt\n\n\n\n\n\nText Pre-processing\n\n\nInstall required packages\n\n# install.packages('strigr') # 최신 R 버전에서 다운로드 불가능, 아래 코드들을 실행하여 수동설치로 대체\n# install.packages('magrittr') # 만약 반복해서 'Updating Loaded Packages' 알림 발생 시, '아니요' 선택\n# install.packages('glue')\n# install.packages('stringi')\n# install.packages('https://cran.r-project.org/src/contrib/Archive/stringr/stringr_1.4.1.tar.gz', repos = NULL, type = 'source')\n# install.packages('tidytext')\n\n\n\nImport required libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(stringr)\nlibrary(tidytext)\n\n\n\nText Pre-processing (텍스트 전처리)\nImport speeches\n\n# Set to the path within the file where the current R script exists\nraw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8')\nhead(raw_moon)\n\n[1] \"정권교체 하겠습니다!\"                                                                                                                                                            \n[2] \"  정치교체 하겠습니다!\"                                                                                                                                                          \n[3] \"  시대교체 하겠습니다!\"                                                                                                                                                          \n[4] \"  \"                                                                                                                                                                              \n[5] \"  ‘불비불명(不飛不鳴)’이라는 고사가 있습니다. 남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새. 그러나 그 새는 한번 날면 하늘 끝까지 날고, 한번 울면 천지를 뒤흔듭니다.\"\n[6] \"\"                                                                                                                                                                                \n\n\nRemove unnecessary characters - str_replace_all\n\n# Learn how it works with sample text\ntxt <- \"치킨은!! 맛있다. xyz 정말 맛있다!@#\"\ntxt\n\n[1] \"치킨은!! 맛있다. xyz 정말 맛있다!@#\"\n\n# string = 처리할 텍스트, \n# pattern = 규칙, \n# replacement = 바꿀 문자\nstr_replace_all(string = txt, pattern = '[^가-힣]', replacement = ' ')\n\n[1] \"치킨은   맛있다      정말 맛있다   \"\n\n\n\n# raw_moon의 불필요한 문자 제거하기\nmoon <- raw_moon %>%\n  str_replace_all('[^가-힣]', ' ')\nhead(moon)\n\n[1] \"정권교체 하겠습니다 \"                                                                                                                                                        \n[2] \"  정치교체 하겠습니다 \"                                                                                                                                                      \n[3] \"  시대교체 하겠습니다 \"                                                                                                                                                      \n[4] \"  \"                                                                                                                                                                          \n[5] \"   불비불명       이라는 고사가 있습니다  남쪽 언덕 나뭇가지에 앉아   년 동안 날지도 울지도 않는 새  그러나 그 새는 한번 날면 하늘 끝까지 날고  한번 울면 천지를 뒤흔듭니다 \"\n[6] \"\"                                                                                                                                                                            \n\n\nRemove Consecutive Spaces\n\ntxt <- \"치킨은 맛있다 정말 맛있다 \"\ntxt\n\n[1] \"치킨은 맛있다 정말 맛있다 \"\n\nstr_squish(txt)\n\n[1] \"치킨은 맛있다 정말 맛있다\"\n\n\n\n# moon에 있는 연속된 공백 제거하기\nmoon <- moon %>% \n  str_squish()\nhead(moon)\n\n[1] \"정권교체 하겠습니다\"                                                                                                                                          \n[2] \"정치교체 하겠습니다\"                                                                                                                                          \n[3] \"시대교체 하겠습니다\"                                                                                                                                          \n[4] \"\"                                                                                                                                                             \n[5] \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울지도 않는 새 그러나 그 새는 한번 날면 하늘 끝까지 날고 한번 울면 천지를 뒤흔듭니다\"\n[6] \"\"                                                                                                                                                             \n\n\nConvert data to tibble structure - as_tibble()\n\nmoon <- dplyr::as_tibble(moon)\nmoon\n\n# A tibble: 117 × 1\n   value                                                                        \n   <chr>                                                                        \n 1 \"정권교체 하겠습니다\"                                                        \n 2 \"정치교체 하겠습니다\"                                                        \n 3 \"시대교체 하겠습니다\"                                                        \n 4 \"\"                                                                           \n 5 \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울…\n 6 \"\"                                                                           \n 7 \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치로 불러냈습…\n 8 \"\"                                                                           \n 9 \"\"                                                                           \n10 \"우리나라 대통령 이 되겠습니다\"                                              \n# … with 107 more rows\n\n\nPre-processing at once (feat. %>%)\n\nmoon <- raw_moon %>% \n  str_replace_all('[^가-힣]', ' ') %>% # 한글만 남기기\n  str_squish() %>% # 연속된 공백 제거\n  as_tibble() # tibble로 변환\n\n\n\nTokenization - unnest_tokens()\nPractice with sample data\n\n# 샘플 텍스트로 작동 원리 알아보기\ntext <- tibble(value = \"대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\")\ntext\n\n# A tibble: 1 × 1\n  value                                                                        \n  <chr>                                                                        \n1 대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민…\n\ntext %>% # 문장 기준 토큰화\n  unnest_tokens(input = value, # 토큰화할 텍스트\n                output = word, # 토큰을 담을 변수명\n                token = 'sentences') # 문장 기준\n\n# A tibble: 2 × 1\n  word                                                             \n  <chr>                                                            \n1 대한민국은 민주공화국이다.                                       \n2 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n\ntext %>% # 띄어쓰기 기준 토큰화\n  unnest_tokens(input = value,\n                output = word,\n                token = 'words')\n\n# A tibble: 10 × 1\n   word          \n   <chr>         \n 1 대한민국은    \n 2 민주공화국이다\n 3 대한민국의    \n 4 주권은        \n 5 국민에게      \n 6 있고          \n 7 모든          \n 8 권력은        \n 9 국민으로부터  \n10 나온다        \n\ntext %>% # 문자 기준 토큰화\n  unnest_tokens(input = value,\n                output = word,\n                token = 'characters')\n\n# A tibble: 40 × 1\n   word \n   <chr>\n 1 대   \n 2 한   \n 3 민   \n 4 국   \n 5 은   \n 6 민   \n 7 주   \n 8 공   \n 9 화   \n10 국   \n# … with 30 more rows\n\n# 연설문 토큰화하기 \nword_space <- moon %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = 'words')\nword_space\n\n# A tibble: 2,025 × 1\n   word      \n   <chr>     \n 1 정권교체  \n 2 하겠습니다\n 3 정치교체  \n 4 하겠습니다\n 5 시대교체  \n 6 하겠습니다\n 7 불비불명  \n 8 이라는    \n 9 고사가    \n10 있습니다  \n# … with 2,015 more rows\n\n\n\n\nWord frequency visualization\n\n# 단어 빈도 구하기 - count()\ntemp_word_space <- word_space %>% \n  count(word, sort = T)\ntemp_word_space\n\n# A tibble: 1,440 × 2\n   word             n\n   <chr>        <int>\n 1 합니다          27\n 2 수              16\n 3 있습니다        13\n 4 저는            13\n 5 등              12\n 6 있는            12\n 7 함께            12\n 8 만들겠습니다    11\n 9 일자리          10\n10 국민의           9\n# … with 1,430 more rows\n\n# 한 글자로 된 단어 제거하기 - filter(str_count())\n# str_count = 문자열의 글자 수 구하기\nstr_count('배')\n\n[1] 1\n\nstr_count('사과')\n\n[1] 2\n\n# 두 글자 이상만 남기기\ntemp_word_space <- temp_word_space %>% \n  filter(str_count(word) > 1)\ntemp_word_space\n\n# A tibble: 1,384 × 2\n   word             n\n   <chr>        <int>\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n# … with 1,374 more rows\n\n# 한 번에 작업하기\nword_space <- word_space %>% \n  count(word, sort = T) %>% \n  filter(str_count(word) > 1)\nword_space\n\n# A tibble: 1,384 × 2\n   word             n\n   <chr>        <int>\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n# … with 1,374 more rows\n\n# 자주 사용된 단어 추출하기\ntop20 <- word_space %>% \n  head(20)\ntop20\n\n# A tibble: 20 × 2\n   word             n\n   <chr>        <int>\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n11 새로운           8\n12 위해             8\n13 그리고           7\n14 나라             7\n15 나라가           7\n16 지금             7\n17 낡은             6\n18 대통령이         6\n19 되겠습니다       6\n20 없는             6\n\n\n\n# 막대 그래프 만들기 - geom_col()\n# mac 사용자, 그래프에 한글 지원폰트로 변경\n# theme_set(theme_gray(base_family = \"AppleGothic\"))\nggplot2::ggplot(top20, aes(x = reorder(word, n), y = n)) + # 단어 빈도순 정렬\n  geom_col() +\n  coord_flip() # 회전\n\n\n\n\n\n# 그래프 다듬기\nggplot(top20, aes(x = reorder(word, n), y = n)) +\n  geom_col() + \n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) +\n  labs(title = '문재인 대통령 출마 연설문 단어 빈도',\n       x = NULL, y = NULL) +\n  theme(title = element_text(size = 12))\n\n\n\n# 워드 클라우드 만들기 - geom_text_wordcloud() \n# install.packages('ggwordcloud')\nlibrary(ggwordcloud)\n\nggplot(word_space, aes(label = word, size = n)) +\n  geom_text_wordcloud(seed = 1234) +\n  scale_radius(limits = c(3, NA), # 최소, 최대 단어 빈도\n               range = c(3, 30)) # 최소, 최대 글자 크기\n\n\n\n# 그래프 다듬기\nggplot(word_space, \n       aes(label = word, \n           size = n,\n           col = n)) + # 빈도에 따라 색깔 표현\n  geom_text_wordcloud(seed = 1234) +\n  scale_radius(limits = c(3, NA),\n               range = c(3, 30)) +\n  scale_color_gradient(low = '#66aaf2', # 최소 빈도 색깔\n                       high = '#004EA1') + # 최대 빈도 색깔\n  theme_minimal() # 배경 없는 테마 적용\n\n\n\n\n\n\n\nMorphological analysis\n형태소 단위 분석\n\n# 그래프 폰트 바꾸기\n# 1. 구글 폰트 불러오기 - font_add_google()\n# install.packages('showtext')\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n# install.packages('jsonlite')\n# install.packages('curl')\n\nfont_add_google(name = 'Nanum Gothic', family = 'nanumgothic')\nshowtext_auto()\n\n# 2. 그래프에 폰트 지정하기\nggplot(word_space,\n       aes(label = word,\n           size = n,\n           col = n)) +\n  geom_text_wordcloud(seed = 1234,\n                      family = 'nanumgothic') + # 폰트 적용\n  scale_radius(limits = c(3,NA),\n               range = c(3,30)) +\n  scale_color_gradient(low = '#66aaf2',\n                       high = '#004EA1') +\n  theme_minimal()\n\n\n\n# '검은고딕' 폰트 적용\nfont_add_google(name = 'Black Han Sans', family = 'blackhansans')\nshowtext_auto()\n\nggplot(word_space,\n       aes(label = word,\n           size = n,\n           col = n)) +\n  geom_text_wordcloud(seed = 1234,\n                      family = 'blackhansans') + # 폰트 적용\n  scale_radius(limits = c(3,NA),\n               range = c(3,30)) +\n  scale_color_gradient(low = '#66aaf2',\n                       high = '#004EA1') +\n  theme_minimal()\n\n\n\n# 3. ggplot2 패키지로 만든 그래프의 폰트 바꾸기\nfont_add_google(name = 'Gamja Flower', family = 'gamjaflower')\nshowtext_auto()\n\nggplot(top20, aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) + \n  labs(title = '문재인 대통령 출마 연설문 단어 빈도',\n       x = NULL, y = NULL) +\n  theme(title = element_text(size = 12), text = element_text(family = 'gamjaflower')) # 폰트 적용\n\n\n\n# ggplot2 기본 테마 폰트 변경하기 --------------------------------------------------------\n# 매번 theme()를 이용해 폰트를 지정하는게 번거롭다면 ggplot2 패키지 기본 테마 폰트 설정\ntheme_set(theme_gray(base_family = 'nanumgothic'))\n\nKoNLP 한글 형태소 분석 패키지 설치하기\nhttps://github.com/youngwoos/Doit_R/blob/master/FAQ/install_KoNLP.md\n\n# 1. 자바와 rJava 패키지 설치하기\n# install.packages('multilinguer')\nlibrary(multilinguer)\n\n반드시 Amazon Corretto 설치 후, RStudio 종료 + 재시작하기\n\n# install_jdk()\n\nKoNLP 의존성 패키지 설치하기\n\n# install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = 'binary')\n\nKoNLP 패키지 설치하기\n\n# install.packages('remotes')\n# remotes::install_github('haven-jeon/KoNLP',\n#                         upgrade = 'never',\n#                         INSTALL_opts = c('--no-multiarch'))\n\n\n# 'scala-library-2.11.8.jar' 에러 발생 시, download.file 코드 실행\n\n# download.file(url = \"https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar\",\n#               destfile = paste0(.libPaths()[1], \"/KoNLP/Java/scala-library-2.11.8.jar\"))\n\n\nlibrary(KoNLP) # Fail to locate \n\nChecking user defined dictionary!\n\n# Checking user defined dictionary! <- 해당 문구는 에러가 아니고 아래 useNIADic() 실행하여 사전을 설정\n\n# useNIADic() \n# 다운로드 항목 출력 시, 'All' 선택하여 다운로드\n\n\n형태소 분석기를 이용해 토큰화하기 - 명사 추출 샘플 텍스트로 작동 원리 알아보기\n\ntext <- tibble(\n  value = c(\"대한민국은 민주공화국이다.\",\n            \"대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\"))\ntext\n\n# A tibble: 2 × 1\n  value                                                            \n  <chr>                                                            \n1 대한민국은 민주공화국이다.                                       \n2 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n\n# extraNoun(): 문장에서 추출한 명사를 list 구조로 출력\nextractNoun(text$value)\n\n[[1]]\n[1] \"대한민국\"   \"민주공화국\"\n\n[[2]]\n[1] \"대한민국\" \"주권\"     \"국민\"     \"권력\"     \"국민\"    \n\n# unnest_tokens()를 이용해 명사 추출하기, 다루기 쉬운 tibble 구조로 명사 출력\nlibrary(tidytext)\n\ntext %>% \n  unnest_tokens(input = value, # 분석 대상\n                output = word, # 출력 변수명\n                token = extractNoun) # 토큰화 함수\n\n# A tibble: 7 × 1\n  word      \n  <chr>     \n1 대한민국  \n2 민주공화국\n3 대한민국  \n4 주권      \n5 국민      \n6 권력      \n7 국민      \n\n# 띄어쓰기 기준 추출\ntext %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = 'words')\n\n# A tibble: 10 × 1\n   word          \n   <chr>         \n 1 대한민국은    \n 2 민주공화국이다\n 3 대한민국의    \n 4 주권은        \n 5 국민에게      \n 6 있고          \n 7 모든          \n 8 권력은        \n 9 국민으로부터  \n10 나온다        \n\n# 명사 추출\ntext %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\n\n# A tibble: 7 × 1\n  word      \n  <chr>     \n1 대한민국  \n2 민주공화국\n3 대한민국  \n4 주권      \n5 국민      \n6 권력      \n7 국민      \n\n# 연설문에서 명사 추출하기\n# 문재인 대통령 연설문 불러오기\n\nraw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8')\n\n# 기본적인 전처리\nlibrary(stringr)\n# install.packages('textclean')\nlibrary(textclean)\n\nmoon <- raw_moon %>% \n  str_replace_all('[^가-힣]', ' ') %>% \n  str_squish() %>% \n  as_tibble()\nmoon\n\n# A tibble: 117 × 1\n   value                                                                        \n   <chr>                                                                        \n 1 \"정권교체 하겠습니다\"                                                        \n 2 \"정치교체 하겠습니다\"                                                        \n 3 \"시대교체 하겠습니다\"                                                        \n 4 \"\"                                                                           \n 5 \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안 날지도 울…\n 6 \"\"                                                                           \n 7 \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치로 불러냈습…\n 8 \"\"                                                                           \n 9 \"\"                                                                           \n10 \"우리나라 대통령 이 되겠습니다\"                                              \n# … with 107 more rows\n\n# 명사 기준 토큰화\nword_noun <- moon %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\nword_noun\n\n# A tibble: 1,757 × 1\n   word      \n   <chr>     \n 1 \"정권교체\"\n 2 \"하겠습니\"\n 3 \"정치\"    \n 4 \"교체\"    \n 5 \"하겠습니\"\n 6 \"시대\"    \n 7 \"교체\"    \n 8 \"하겠습니\"\n 9 \"\"        \n10 \"불비불명\"\n# … with 1,747 more rows\n\n\n\n# 단어 빈도 구하기\nword_noun <- word_noun %>% \n  count(word, sort = T) %>% # 단어 빈도 구해 내림차순 정렬\n  filter(str_count(word) > 1) # 두 글자 이상만 남기기\nword_noun\n\n# A tibble: 704 × 2\n   word       n\n   <chr>  <int>\n 1 국민      21\n 2 일자리    21\n 3 나라      19\n 4 우리      17\n 5 경제      15\n 6 사회      14\n 7 성장      13\n 8 대통령    12\n 9 정치      12\n10 하게      12\n# … with 694 more rows\n\n# 띄어쓰기 기준 추출\nmoon %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = 'words') %>% \n  count(word, sort = T) %>% \n  filter(str_count(word) > 1)\n\n# A tibble: 1,384 × 2\n   word             n\n   <chr>        <int>\n 1 합니다          27\n 2 있습니다        13\n 3 저는            13\n 4 있는            12\n 5 함께            12\n 6 만들겠습니다    11\n 7 일자리          10\n 8 국민의           9\n 9 우리             9\n10 우리나라         9\n# … with 1,374 more rows\n\n# 명사 추출\nmoon %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun) %>% \n  count(word, sort = T) %>% \n  filter(str_count(word) > 1)\n\n# A tibble: 704 × 2\n   word       n\n   <chr>  <int>\n 1 국민      21\n 2 일자리    21\n 3 나라      19\n 4 우리      17\n 5 경제      15\n 6 사회      14\n 7 성장      13\n 8 대통령    12\n 9 정치      12\n10 하게      12\n# … with 694 more rows\n\n# 상위 20개 단어 추출\ntop20 <- word_noun %>% \n  head(20)\ntop20\n\n# A tibble: 20 × 2\n   word         n\n   <chr>    <int>\n 1 국민        21\n 2 일자리      21\n 3 나라        19\n 4 우리        17\n 5 경제        15\n 6 사회        14\n 7 성장        13\n 8 대통령      12\n 9 정치        12\n10 하게        12\n11 대한민국    11\n12 평화        11\n13 복지        10\n14 우리나라    10\n15 확대        10\n16 들이         9\n17 사람         9\n18 산업         9\n19 정부         9\n20 복지국가     8\n\n# 막대 그래프 만들기\nlibrary(showtext)\nfont_add_google(name = 'Nanum Gothic', family = 'nanumgothic')\nshowtext_auto()\n\nggplot(top20, aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) +\n  labs(x = NULL) +\n  theme(text = element_text(family = 'nanumgothic'))\n\n\n\n# 워드 클라우드 만들기\n# 폰트 설정\nfont_add_google(name = 'Black Han Sans', family = 'blackhansans')\nshowtext_auto()\n\nlibrary(ggwordcloud)\nggplot(word_noun, aes(label = word, size = n, col = n)) +\n  geom_text_wordcloud(seed = 1234, family = 'blackhansans') +\n  scale_radius(limits = c(3,NA),\n               range = c(3,15)) +\n  scale_color_gradient(low = '#66aaf2', high = '#004EA1') +\n  theme_minimal()\n\n\n\n\n\n# 문장 기준으로 토큰화하기\nsentences_moon <- raw_moon %>% \n  str_squish() %>% \n  as_tibble() %>% \n  unnest_tokens(input = value,\n                output = sentence,\n                token = 'sentences')\nsentences_moon\n\n# A tibble: 207 × 1\n   sentence                                                               \n   <chr>                                                                  \n 1 정권교체 하겠습니다!                                                   \n 2 정치교체 하겠습니다!                                                   \n 3 시대교체 하겠습니다!                                                   \n 4 ‘불비불명(不飛不鳴)’이라는 고사가 있습니다.                            \n 5 남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새.             \n 6 그러나 그 새는 한번 날면 하늘 끝까지 날고, 한번 울면 천지를 뒤흔듭니다.\n 7 그 동안 정치와 거리를 둬 왔습니다.                                     \n 8 그러나 암울한 시대가 저를 정치로 불러냈습니다.                         \n 9 더 이상 남쪽 나뭇가지에 머무를 수 없었습니다.                          \n10 이제 저는 국민과 함께 높이 날고 크게 울겠습니다.                       \n# … with 197 more rows\n\n# 특정 단어가 사용된 문장 추출하기 - str_detect()\n# 예시\nstr_detect('치킨은 맛있다', '치킨')\n\n[1] TRUE\n\nstr_detect('치킨은 맛있다', '피자')\n\n[1] FALSE\n\n# 특정 단어가 사용된 문장 추출하기, '국민'\nsentences_moon %>% \n  filter(str_detect(sentence, '국민'))\n\n# A tibble: 19 × 1\n   sentence                                                                     \n   <chr>                                                                        \n 1 이제 저는 국민과 함께 높이 날고 크게 울겠습니다.                             \n 2 오늘 저는 제18대 대통령선거 출마를 국민 앞에 엄숙히 선언합니다.              \n 3 존경하는 국민 여러분!                                                        \n 4 국민이 모두 아픕니다.                                                        \n 5 국민 한 사람 한 사람이 모두 아픕니다.                                        \n 6 국민들에게 희망을 주는 정치가 절실하게 필요합니다.                           \n 7 국민의 뜻이 대통령의 길입니다.                                               \n 8 저는 대선출마를 결심하고 국민 여러분께 출마선언문을 함께 쓰자고 제안 드렸습… \n 9 시민의 한숨과 눈물을 닦아주지 못하는 정치가 있었고, 오히려 국민의 걱정거리가…\n10 상식이 통하는 사회, 권한과 책임이 비례하는 사회, 다름을 인정하는 세상, 개천… \n11 그러나 거창하게만 들리는 이 국가비전 역시 국민의 마음속에 있었습니다.        \n12 더욱 낮아지고 겸손해져서 국민의 마음속으로 들어가라.                         \n13 국민들이 제게 준 가르침입니다.                                               \n14 국민의 뜻에서 대통령의 길을 찾겠습니다.                                      \n15 문화혁신을 통해 모든 국민의 창조성을 높이고 이를 통해 기술혁신과 신산업 형성…\n16 이렇게 하면 국민의 살림이 서서히 나아질 것이며 5년 뒤에는 큰 성과가 나타날 … \n17 이명박 정부의 방해에도 불구하고 끝내 국민이 지켜준 세종시, 혁신도시를 지방 … \n18 존경하는 국민 여러분!                                                        \n19 국민의 마음에서 길을 찾는 우리나라 대통령이 되겠습니다.                      \n\n# 특정 단어가 사용된 문장 추출하기, '일자리'\nsentences_moon %>% \n  filter(str_detect(sentence, '일자리'))\n\n# A tibble: 18 × 1\n   sentence                                                                     \n   <chr>                                                                        \n 1 빚 갚기 힘들어서, 아이 키우기 힘들어서, 일자리가 보이지 않아서 아픕니다.     \n 2 상생과 평화의 대한민국은 공평과 정의에 바탕을 두고, 성장의 과실을 함께 누리… \n 3 복지의 확대를 통해 보육, 교육, 의료, 요양 등 사회서비스 부문에 수많은 일자리…\n 4 결국 복지국가로 가는 길은 사람에 대한 투자, 일자리 창출, 자영업 고통 경감, … \n 5 ‘일자리 정부’로 ‘일자리 혁명’을 이루겠습니다.                                \n 6 복지의 확대와 함께 저는 강력한 ‘일자리 혁명’을 이루고자 합니다.              \n 7 지금 너무나 많은 젊은이들과 실업자, 비정규직 종사자, 근로능력이 있는 고령자… \n 8 좋은 일자리 창출을 위해 비정규직의 정규직 전환 촉진, 비정규직에 대한 차별철… \n 9 또한 정보통신 산업, 바이오산업, 나노 산업, 신재생에너지 산업, 문화산업과 콘… \n10 그리고 앞에서 말한 보육, 교육, 의료, 복지 등 사회서비스 부문은 무궁무진한 잠…\n11 일자리 없는 곳에서 희망을 찾을 수 없습니다.                                  \n12 지방 일자리에 대해 특별한 노력을 기울이겠습니다.                             \n13 지역균형발전은 곧 산업 균형, 일자리 균형이 목표입니다.                       \n14 이명박 정부의 방해에도 불구하고 끝내 국민이 지켜준 세종시, 혁신도시를 지방 … \n15 이 모든 정책의 실효성을 담보하기 위해 대통령이 되면 저는 가장 먼저 대통령 직…\n16 저는 먼 훗날 ‘일자리 혁명을 일으킨 대통령’으로 평가받기를 희망합니다.        \n17 또한 좋은 일자리와 산업혁신을 위해서는 평생학습체제가 뒷받침되어야 합니다.   \n18 노인 일자리를 늘리고, 특히 그 연륜과 경험을 지역사회에 활용할 수 있는 방안도…"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_2.html",
    "href": "teaching/media_ds/about/NLP_2.html",
    "title": "NLP",
    "section": "",
    "text": "Comparing the frequency of words\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nraw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8') # 문재인 대통령 연설문 불러오기\nmoon <- raw_moon %>% \n  as_tibble() %>% \n  mutate(president = 'moon')\n\nraw_park <- readLines('data/speech_park.txt', encoding = 'UTF-8')\npark <- raw_park %>% \n  as_tibble() %>% \n  mutate(president = 'park')\n\n# 데이터 합치기\nbind_speeches <- bind_rows(moon, park) %>% \n  select(president, value)\n\nhead(bind_speeches)\n\n# A tibble: 6 × 2\n  president value                                                               \n  <chr>     <chr>                                                               \n1 moon      \"정권교체 하겠습니다!\"                                              \n2 moon      \"  정치교체 하겠습니다!\"                                            \n3 moon      \"  시대교체 하겠습니다!\"                                            \n4 moon      \"  \"                                                                \n5 moon      \"  ‘불비불명(不飛不鳴)’이라는 고사가 있습니다. 남쪽 언덕 나뭇가지에…\n6 moon      \"\"                                                                  \n\ntail(bind_speeches)\n\n# A tibble: 6 × 2\n  president value                                                              \n  <chr>     <chr>                                                              \n1 park      \"국민들이 꿈으로만 가졌던 행복한 삶을 실제로 이룰 수 있도록 도와드…\n2 park      \"\"                                                                 \n3 park      \"감사합니다.\"                                                      \n4 park      \"\"                                                                 \n5 park      \"2012년 7월 10일\"                                                  \n6 park      \"새누리당 예비후보 박근혜\"                                         \n\n# 집단별 단어 빈도 구하기\n# 1. 기본적인 전처리 및 토큰화\n# 기본적인 전처리\nlibrary(stringr)\nspeeches <- bind_speeches %>% \n  mutate(value = str_replace_all(value, '[^가-힣]', ' '),\n         value = str_squish(value))\nspeeches\n\n# A tibble: 213 × 2\n   president value                                                              \n   <chr>     <chr>                                                              \n 1 moon      \"정권교체 하겠습니다\"                                              \n 2 moon      \"정치교체 하겠습니다\"                                              \n 3 moon      \"시대교체 하겠습니다\"                                              \n 4 moon      \"\"                                                                 \n 5 moon      \"불비불명 이라는 고사가 있습니다 남쪽 언덕 나뭇가지에 앉아 년 동안…\n 6 moon      \"\"                                                                 \n 7 moon      \"그 동안 정치와 거리를 둬 왔습니다 그러나 암울한 시대가 저를 정치… \n 8 moon      \"\"                                                                 \n 9 moon      \"\"                                                                 \n10 moon      \"우리나라 대통령 이 되겠습니다\"                                    \n# … with 203 more rows\n\n# 토큰화\nlibrary(tidytext)\nlibrary(KoNLP)\n\nChecking user defined dictionary!\n\nspeeches <- speeches %>% \n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\nspeeches\n\n# A tibble: 2,997 × 2\n   president word      \n   <chr>     <chr>     \n 1 moon      \"정권교체\"\n 2 moon      \"하겠습니\"\n 3 moon      \"정치\"    \n 4 moon      \"교체\"    \n 5 moon      \"하겠습니\"\n 6 moon      \"시대\"    \n 7 moon      \"교체\"    \n 8 moon      \"하겠습니\"\n 9 moon      \"\"        \n10 moon      \"불비불명\"\n# … with 2,987 more rows\n\n# 하위 집단별 단어 빈도 구하기 - count()\n# 샘플 텍스트로 작동 원리 알아보기\ndf <- tibble(class = c('a','a','a','b','b','b'),\n             sex = c('female','male','female','male','male','female'))\ndf\n\n# A tibble: 6 × 2\n  class sex   \n  <chr> <chr> \n1 a     female\n2 a     male  \n3 a     female\n4 b     male  \n5 b     male  \n6 b     female\n\ndf %>% count(class, sex)\n\n# A tibble: 4 × 3\n  class sex        n\n  <chr> <chr>  <int>\n1 a     female     2\n2 a     male       1\n3 b     female     1\n4 b     male       2\n\n# 두 연설문의 단어 빈도 구하기\nfrequency <- speeches %>% \n  count(president, word) %>% \n  filter(str_count(word) > 1)\nhead(frequency)\n\n# A tibble: 6 × 3\n  president word         n\n  <chr>     <chr>    <int>\n1 moon      가동         1\n2 moon      가사         1\n3 moon      가슴         2\n4 moon      가족         1\n5 moon      가족구조     1\n6 moon      가지         4\n\n# 자주 사용된 단어 추출하기, dplyr::slice_max() - 값이 큰 상위 n개의 행을 추출해 내림차순 정렬\n# 샘플 텍스트로 작동 원리 알아보기\ndf <- tibble(x = c(1:100))\ndf\n\n# A tibble: 100 × 1\n       x\n   <int>\n 1     1\n 2     2\n 3     3\n 4     4\n 5     5\n 6     6\n 7     7\n 8     8\n 9     9\n10    10\n# … with 90 more rows\n\ndf %>% slice_max(x, n = 3)\n\n# A tibble: 3 × 1\n      x\n  <int>\n1   100\n2    99\n3    98\n\n# 연설문에 가장 많이 사용된 단어 추출하기\ntop10 <- frequency %>% \n  group_by(president) %>% \n  slice_max(n, n = 10)\ntop10\n\n# A tibble: 22 × 3\n# Groups:   president [2]\n   president word       n\n   <chr>     <chr>  <int>\n 1 moon      국민      21\n 2 moon      일자리    21\n 3 moon      나라      19\n 4 moon      우리      17\n 5 moon      경제      15\n 6 moon      사회      14\n 7 moon      성장      13\n 8 moon      대통령    12\n 9 moon      정치      12\n10 moon      하게      12\n# … with 12 more rows\n\n# 단어 빈도 동점 처리 제외하고 추출하기, slice_max(with_ties = F) - 원본 데이터의 정렬 순서에 따라 행 추출\ntop10 %>% \n  filter(president == 'park') # 박근혜 전 대통령의 연설문은 동점 처리로 인해, 단어 12개가 모두 추출되어버림\n\n# A tibble: 12 × 3\n# Groups:   president [1]\n   president word       n\n   <chr>     <chr>  <int>\n 1 park      국민      72\n 2 park      행복      23\n 3 park      여러분    20\n 4 park      정부      17\n 5 park      경제      15\n 6 park      신뢰      11\n 7 park      국가      10\n 8 park      우리      10\n 9 park      교육       9\n10 park      사람       9\n11 park      사회       9\n12 park      일자리     9\n\n# 샘플 데이터로 작동 원리 알아보기\ndf <- tibble(x = c('A','B','C','D'), y = c(4,3,2,2))\ndf %>% \n  slice_max(y, n = 3)\n\n# A tibble: 4 × 2\n  x         y\n  <chr> <dbl>\n1 A         4\n2 B         3\n3 C         2\n4 D         2\n\ndf %>% \n  slice_max(y, n = 3, with_ties = F)\n\n# A tibble: 3 × 2\n  x         y\n  <chr> <dbl>\n1 A         4\n2 B         3\n3 C         2\n\n# 연설문에 적용하기\ntop10 <- frequency %>% \n  group_by(president) %>% \n  slice_max(n, n = 10, with_ties = F)\ntop10\n\n# A tibble: 20 × 3\n# Groups:   president [2]\n   president word       n\n   <chr>     <chr>  <int>\n 1 moon      국민      21\n 2 moon      일자리    21\n 3 moon      나라      19\n 4 moon      우리      17\n 5 moon      경제      15\n 6 moon      사회      14\n 7 moon      성장      13\n 8 moon      대통령    12\n 9 moon      정치      12\n10 moon      하게      12\n11 park      국민      72\n12 park      행복      23\n13 park      여러분    20\n14 park      정부      17\n15 park      경제      15\n16 park      신뢰      11\n17 park      국가      10\n18 park      우리      10\n19 park      교육       9\n20 park      사람       9\n\n# 막대 그래프 만들기\n# 1. 변수의 항목별로 그래프 만들기 - facet_wrap()\nggplot(top10, aes(x = reorder(word, n),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president)\n\n\n\n# 2. 그래프별 y축 설정하기\nggplot(top10, aes(x = reorder(word, n),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, # president별 그래프 생성\n             scales = 'free_y') # y축 통일하지 않음\n\n\n\n# 3. 특정 단어 제외하고 막대 그래프 만들기\ntop10 <- frequency %>% \n  filter(word != '국민') %>% \n  group_by(president) %>% \n  slice_max(n, n = 10, with_ties = F)\n\nggplot(top10, aes(x = reorder(word, n),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = 'free_y')\n\n\n\n# 4. 축 정렬하기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = 'free_y')\n\n\n\n# 5. 변수 항목 제거하기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = \"free_y\") +\n  scale_x_reordered() +\n  labs(x = NULL) \n\n\n\n\n\n\nOdds Ratio\n상대적으로 중요한 단어 찾기\n\n# Long form을 Wide form으로 변환하기\n# Long form 데이터 살펴보기\ndf_long <- frequency %>% \n  group_by(president) %>% \n  slice_max(n, n = 10) %>% \n  filter(word %in% c('국민','우리','정치','행복'))\ndf_long\n\n# A tibble: 6 × 3\n# Groups:   president [2]\n  president word      n\n  <chr>     <chr> <int>\n1 moon      국민     21\n2 moon      우리     17\n3 moon      정치     12\n4 park      국민     72\n5 park      행복     23\n6 park      우리     10\n\n# Long form을 Wide form으로 변형하기\ndf_wide <- df_long %>% \n  pivot_wider(names_from = president,\n              values_from = n)\ndf_wide\n\n# A tibble: 4 × 3\n  word   moon  park\n  <chr> <int> <int>\n1 국민     21    72\n2 우리     17    10\n3 정치     12    NA\n4 행복     NA    23\n\n# NA를 0으로 바꾸기\ndf_wide <- df_long %>% \n  pivot_wider(names_from = president,\n              values_from = n,\n              values_fill = list(n = 0))\ndf_wide\n\n# A tibble: 4 × 3\n  word   moon  park\n  <chr> <int> <int>\n1 국민     21    72\n2 우리     17    10\n3 정치     12     0\n4 행복      0    23\n\n# 연설문 단어 빈도를 Wide form으로 변환하기\nfrequency_wide <- frequency %>% \n  pivot_wider(names_from = president,\n              values_from = n,\n              values_fill = list(n = 0))\nfrequency_wide\n\n# A tibble: 955 × 3\n   word      moon  park\n   <chr>    <int> <int>\n 1 가동         1     0\n 2 가사         1     0\n 3 가슴         2     0\n 4 가족         1     1\n 5 가족구조     1     0\n 6 가지         4     0\n 7 가치         3     1\n 8 각종         1     0\n 9 감당         1     0\n10 강력         3     0\n# … with 945 more rows\n\n# 오즈비 구하기\n# 1. 단어의 비중을 나타낸 변수 추가하기\nfrequency_wide <- frequency_wide %>%\n  mutate(ratio_moon = ((moon)/(sum(moon))), # moon 에서 단어의 비중\n         ratio_park = ((park)/(sum(park)))) # park 에서 단어의 비중\nfrequency_wide\n\n# A tibble: 955 × 5\n   word      moon  park ratio_moon ratio_park\n   <chr>    <int> <int>      <dbl>      <dbl>\n 1 가동         1     0   0.000749    0      \n 2 가사         1     0   0.000749    0      \n 3 가슴         2     0   0.00150     0      \n 4 가족         1     1   0.000749    0.00117\n 5 가족구조     1     0   0.000749    0      \n 6 가지         4     0   0.00299     0      \n 7 가치         3     1   0.00225     0.00117\n 8 각종         1     0   0.000749    0      \n 9 감당         1     0   0.000749    0      \n10 강력         3     0   0.00225     0      \n# … with 945 more rows\n\n# 어떤 단어가 한 연설문에 전혀 사용되지 않으면 빈도/오즈비 0, 단어 비중 비교 불가, 빈도가 0보다 큰 값이 되도록 모든 값에 +1\nfrequency_wide <- frequency_wide %>%\n  mutate(ratio_moon = ((moon + 1)/(sum(moon + 1))), # moon에서 단어의 비중\n         ratio_park = ((park + 1)/(sum(park + 1)))) # park에서 단어의 비중\nfrequency_wide\n\n# A tibble: 955 × 5\n   word      moon  park ratio_moon ratio_park\n   <chr>    <int> <int>      <dbl>      <dbl>\n 1 가동         1     0   0.000873   0.000552\n 2 가사         1     0   0.000873   0.000552\n 3 가슴         2     0   0.00131    0.000552\n 4 가족         1     1   0.000873   0.00110 \n 5 가족구조     1     0   0.000873   0.000552\n 6 가지         4     0   0.00218    0.000552\n 7 가치         3     1   0.00175    0.00110 \n 8 각종         1     0   0.000873   0.000552\n 9 감당         1     0   0.000873   0.000552\n10 강력         3     0   0.00175    0.000552\n# … with 945 more rows\n\n# 2. 오즈비 변수 추가하기\nfrequency_wide <- frequency_wide %>% \n  mutate(odds_ratio = ratio_moon / ratio_park)\nfrequency_wide\n\n# A tibble: 955 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>\n 1 가동         1     0   0.000873   0.000552      1.58 \n 2 가사         1     0   0.000873   0.000552      1.58 \n 3 가슴         2     0   0.00131    0.000552      2.37 \n 4 가족         1     1   0.000873   0.00110       0.791\n 5 가족구조     1     0   0.000873   0.000552      1.58 \n 6 가지         4     0   0.00218    0.000552      3.96 \n 7 가치         3     1   0.00175    0.00110       1.58 \n 8 각종         1     0   0.000873   0.000552      1.58 \n 9 감당         1     0   0.000873   0.000552      1.58 \n10 강력         3     0   0.00175    0.000552      3.17 \n# … with 945 more rows\n\nfrequency_wide %>% \n  arrange(-odds_ratio) # moon에서 상대적인 비중이 클수록 1보다 큰 값\n\n# A tibble: 955 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>\n 1 복지국가     8     0    0.00393   0.000552       7.12\n 2 세상         6     0    0.00306   0.000552       5.54\n 3 여성         6     0    0.00306   0.000552       5.54\n 4 정의         6     0    0.00306   0.000552       5.54\n 5 강자         5     0    0.00262   0.000552       4.75\n 6 공평         5     0    0.00262   0.000552       4.75\n 7 대통령의     5     0    0.00262   0.000552       4.75\n 8 보통         5     0    0.00262   0.000552       4.75\n 9 상생         5     0    0.00262   0.000552       4.75\n10 지방         5     0    0.00262   0.000552       4.75\n# … with 945 more rows\n\nfrequency_wide %>% \n  arrange(odds_ratio) # park에서 상대적인 비중이 클수록 1보다 작은 값\n\n# A tibble: 955 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>\n 1 박근혜       0     8   0.000436    0.00496     0.0879\n 2 여러분       2    20   0.00131     0.0116      0.113 \n 3 행복         3    23   0.00175     0.0132      0.132 \n 4 실천         0     5   0.000436    0.00331     0.132 \n 5 정보         0     5   0.000436    0.00331     0.132 \n 6 투명         0     5   0.000436    0.00331     0.132 \n 7 과제         0     4   0.000436    0.00276     0.158 \n 8 국정운영     0     4   0.000436    0.00276     0.158 \n 9 시작         0     4   0.000436    0.00276     0.158 \n10 지식         0     4   0.000436    0.00276     0.158 \n# … with 945 more rows\n\nfrequency_wide %>% \n  arrange(abs(1-odds_ratio)) # 두 연설문에서 단어 비중이 같으면 1\n\n# A tibble: 955 × 6\n   word    moon  park ratio_moon ratio_park odds_ratio\n   <chr>  <int> <int>      <dbl>      <dbl>      <dbl>\n 1 때문       4     3    0.00218    0.00221      0.989\n 2 강화       3     2    0.00175    0.00165      1.06 \n 3 부담       3     2    0.00175    0.00165      1.06 \n 4 세계       3     2    0.00175    0.00165      1.06 \n 5 책임       3     2    0.00175    0.00165      1.06 \n 6 협력       3     2    0.00175    0.00165      1.06 \n 7 거대       2     1    0.00131    0.00110      1.19 \n 8 교체       2     1    0.00131    0.00110      1.19 \n 9 근본적     2     1    0.00131    0.00110      1.19 \n10 기반       2     1    0.00131    0.00110      1.19 \n# … with 945 more rows\n\n# 상대적으로 중요한 단어 추출하기\n# 오즈비가 가장 높거나 가장 낮은 단어 추출하기\ntop10 <- frequency_wide %>% \n  filter(rank(odds_ratio) <= 10 | rank(-odds_ratio) <= 10)\n\ntop10 %>% \n  arrange(-odds_ratio)\n\n# A tibble: 20 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>\n 1 복지국가     8     0   0.00393    0.000552     7.12  \n 2 세상         6     0   0.00306    0.000552     5.54  \n 3 여성         6     0   0.00306    0.000552     5.54  \n 4 정의         6     0   0.00306    0.000552     5.54  \n 5 강자         5     0   0.00262    0.000552     4.75  \n 6 공평         5     0   0.00262    0.000552     4.75  \n 7 대통령의     5     0   0.00262    0.000552     4.75  \n 8 보통         5     0   0.00262    0.000552     4.75  \n 9 상생         5     0   0.00262    0.000552     4.75  \n10 지방         5     0   0.00262    0.000552     4.75  \n11 과제         0     4   0.000436   0.00276      0.158 \n12 국정운영     0     4   0.000436   0.00276      0.158 \n13 시작         0     4   0.000436   0.00276      0.158 \n14 지식         0     4   0.000436   0.00276      0.158 \n15 행복         3    23   0.00175    0.0132       0.132 \n16 실천         0     5   0.000436   0.00331      0.132 \n17 정보         0     5   0.000436   0.00331      0.132 \n18 투명         0     5   0.000436   0.00331      0.132 \n19 여러분       2    20   0.00131    0.0116       0.113 \n20 박근혜       0     8   0.000436   0.00496      0.0879\n\n# 막대 그래프 만들기\n# 1. 비중이 큰 연설문을 나타낸 변수 추가하기\ntop10 <- top10 %>%\n  mutate(president = ifelse(odds_ratio > 1, \"moon\", \"park\"),\n         n = ifelse(odds_ratio > 1, moon, park))\ntop10\n\n# A tibble: 20 × 8\n   word      moon  park ratio_moon ratio_park odds_ratio president     n\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl> <chr>     <int>\n 1 강자         5     0   0.00262    0.000552     4.75   moon          5\n 2 공평         5     0   0.00262    0.000552     4.75   moon          5\n 3 대통령의     5     0   0.00262    0.000552     4.75   moon          5\n 4 보통         5     0   0.00262    0.000552     4.75   moon          5\n 5 복지국가     8     0   0.00393    0.000552     7.12   moon          8\n 6 상생         5     0   0.00262    0.000552     4.75   moon          5\n 7 세상         6     0   0.00306    0.000552     5.54   moon          6\n 8 여러분       2    20   0.00131    0.0116       0.113  park         20\n 9 여성         6     0   0.00306    0.000552     5.54   moon          6\n10 정의         6     0   0.00306    0.000552     5.54   moon          6\n11 지방         5     0   0.00262    0.000552     4.75   moon          5\n12 행복         3    23   0.00175    0.0132       0.132  park         23\n13 과제         0     4   0.000436   0.00276      0.158  park          4\n14 국정운영     0     4   0.000436   0.00276      0.158  park          4\n15 박근혜       0     8   0.000436   0.00496      0.0879 park          8\n16 시작         0     4   0.000436   0.00276      0.158  park          4\n17 실천         0     5   0.000436   0.00331      0.132  park          5\n18 정보         0     5   0.000436   0.00331      0.132  park          5\n19 지식         0     4   0.000436   0.00276      0.158  park          4\n20 투명         0     5   0.000436   0.00331      0.132  park          5\n\n# 2. 막대 그래프 만들기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = 'free_y') +\n  scale_x_reordered()\n\n\n\n# 3. 그래프별로 축 설정하기\nggplot(top10, aes(x = reorder_within(word, n, president),\n                  y = n,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ president, scales = \"free\") +\n  scale_x_reordered() +\n  labs(x = NULL) \n\n\n\n# 주요 단어가 사용된 문장 살펴보기\n# 1. 원문을 문장 기준으로 토큰화하기\nspeeches_sentence <- bind_speeches %>% \n  as_tibble() %>% \n  unnest_tokens(input = value,\n                output = sentence,\n                token = 'sentences')\nspeeches_sentence\n\n# A tibble: 329 × 2\n   president sentence                                                          \n   <chr>     <chr>                                                             \n 1 moon      \"정권교체 하겠습니다!\"                                            \n 2 moon      \"정치교체 하겠습니다!\"                                            \n 3 moon      \"시대교체 하겠습니다!\"                                            \n 4 moon      \"\"                                                                \n 5 moon      \"‘불비불명(不飛不鳴)’이라는 고사가 있습니다.\"                     \n 6 moon      \"남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새.\"      \n 7 moon      \"그러나 그 새는 한번 날면 하늘 끝까지 날고, 한번 울면 천지를 뒤흔…\n 8 moon      \"그 동안 정치와 거리를 둬 왔습니다.\"                              \n 9 moon      \"그러나 암울한 시대가 저를 정치로 불러냈습니다.\"                  \n10 moon      \"더 이상 남쪽 나뭇가지에 머무를 수 없었습니다.\"                   \n# … with 319 more rows\n\nhead(speeches_sentence)\n\n# A tibble: 6 × 2\n  president sentence                                                    \n  <chr>     <chr>                                                       \n1 moon      \"정권교체 하겠습니다!\"                                      \n2 moon      \"정치교체 하겠습니다!\"                                      \n3 moon      \"시대교체 하겠습니다!\"                                      \n4 moon      \"\"                                                          \n5 moon      \"‘불비불명(不飛不鳴)’이라는 고사가 있습니다.\"               \n6 moon      \"남쪽 언덕 나뭇가지에 앉아, 3년 동안 날지도 울지도 않는 새.\"\n\ntail(speeches_sentence)\n\n# A tibble: 6 × 2\n  president sentence                                                           \n  <chr>     <chr>                                                              \n1 park      국민 여러분의 행복이 곧 저의 행복입니다.                           \n2 park      사랑하는 조국 대한민국과 국민 여러분을 위해, 앞으로 머나 먼 길, 끝…\n3 park      그 길을 함께 해주시길 부탁드립니다.                                \n4 park      감사합니다.                                                        \n5 park      2012년 7월 10일                                                    \n6 park      새누리당 예비후보 박근혜                                           \n\n# 2. 주요 단어가 사용된 문장 추출하기 - str_detect()\nspeeches_sentence %>% \n  filter(president == 'moon' & str_detect(sentence, '복지국가'))\n\n# A tibble: 8 × 2\n  president sentence                                                            \n  <chr>     <chr>                                                               \n1 moon      ‘강한 복지국가’를 향해 담대하게 나아가겠습니다.                     \n2 moon      2백 년 전 이와 같은 소득재분배, 복지국가의 사상을 가진 위정자가 지… \n3 moon      이제 우리는 복지국가를 향해 담대하게 나아갈 때입니다.               \n4 moon      부자감세, 4대강 사업 같은 시대착오적 과오를 청산하고, 하루빨리 복지…\n5 moon      우리는 지금 복지국가로 가느냐, 양극화의 분열된 국가로 가느냐 하는 … \n6 moon      강한 복지국가일수록 국가 경쟁력도 더 높습니다.                      \n7 moon      결국 복지국가로 가는 길은 사람에 대한 투자, 일자리 창출, 자영업 고… \n8 moon      우리는 과감히 강한 보편적 복지국가로 가야 합니다.                   \n\n# 중요도가 비슷한 단어 살펴보기\nfrequency_wide %>% \n  arrange(abs(1 - odds_ratio)) %>% \n  head(10)\n\n# A tibble: 10 × 6\n   word    moon  park ratio_moon ratio_park odds_ratio\n   <chr>  <int> <int>      <dbl>      <dbl>      <dbl>\n 1 때문       4     3    0.00218    0.00221      0.989\n 2 강화       3     2    0.00175    0.00165      1.06 \n 3 부담       3     2    0.00175    0.00165      1.06 \n 4 세계       3     2    0.00175    0.00165      1.06 \n 5 책임       3     2    0.00175    0.00165      1.06 \n 6 협력       3     2    0.00175    0.00165      1.06 \n 7 거대       2     1    0.00131    0.00110      1.19 \n 8 교체       2     1    0.00131    0.00110      1.19 \n 9 근본적     2     1    0.00131    0.00110      1.19 \n10 기반       2     1    0.00131    0.00110      1.19 \n\n# 중요도가 비슷하면서 빈도가 높은 단어\nfrequency_wide %>% \n  filter(moon >= 5 & park >= 5) %>%\n  arrange(abs(1 - odds_ratio)) %>%\n  head(10)\n\n# A tibble: 10 × 6\n   word      moon  park ratio_moon ratio_park odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>\n 1 사회        14     9    0.00655    0.00552      1.19 \n 2 사람         9     9    0.00436    0.00552      0.791\n 3 경제        15    15    0.00698    0.00883      0.791\n 4 지원         5     5    0.00262    0.00331      0.791\n 5 우리        17    10    0.00786    0.00607      1.29 \n 6 불안         7     8    0.00349    0.00496      0.703\n 7 산업         9     5    0.00436    0.00331      1.32 \n 8 대한민국    11     6    0.00524    0.00386      1.36 \n 9 국가         7    10    0.00349    0.00607      0.576\n10 교육         6     9    0.00306    0.00552      0.554\n\n\n\n\nLog Odds Ratio\n\n# 로그 오즈비 구하기\nfrequency_wide <- frequency_wide %>% \n  mutate(log_odds_ratio = log(odds_ratio))\nfrequency_wide\n\n# A tibble: 955 × 7\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>          <dbl>\n 1 가동         1     0   0.000873   0.000552      1.58           0.459\n 2 가사         1     0   0.000873   0.000552      1.58           0.459\n 3 가슴         2     0   0.00131    0.000552      2.37           0.865\n 4 가족         1     1   0.000873   0.00110       0.791         -0.234\n 5 가족구조     1     0   0.000873   0.000552      1.58           0.459\n 6 가지         4     0   0.00218    0.000552      3.96           1.38 \n 7 가치         3     1   0.00175    0.00110       1.58           0.459\n 8 각종         1     0   0.000873   0.000552      1.58           0.459\n 9 감당         1     0   0.000873   0.000552      1.58           0.459\n10 강력         3     0   0.00175    0.000552      3.17           1.15 \n# … with 945 more rows\n\n# moon에서 비중이 큰 단어, 0보다 큰 양수\nfrequency_wide %>%\n  arrange(-log_odds_ratio)\n\n# A tibble: 955 × 7\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>          <dbl>\n 1 복지국가     8     0    0.00393   0.000552       7.12           1.96\n 2 세상         6     0    0.00306   0.000552       5.54           1.71\n 3 여성         6     0    0.00306   0.000552       5.54           1.71\n 4 정의         6     0    0.00306   0.000552       5.54           1.71\n 5 강자         5     0    0.00262   0.000552       4.75           1.56\n 6 공평         5     0    0.00262   0.000552       4.75           1.56\n 7 대통령의     5     0    0.00262   0.000552       4.75           1.56\n 8 보통         5     0    0.00262   0.000552       4.75           1.56\n 9 상생         5     0    0.00262   0.000552       4.75           1.56\n10 지방         5     0    0.00262   0.000552       4.75           1.56\n# … with 945 more rows\n\n# park에서 비중이 큰 단어, 0보다 작은 음수\nfrequency_wide %>%\n  arrange(log_odds_ratio)\n\n# A tibble: 955 × 7\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>          <dbl>\n 1 박근혜       0     8   0.000436    0.00496     0.0879          -2.43\n 2 여러분       2    20   0.00131     0.0116      0.113           -2.18\n 3 행복         3    23   0.00175     0.0132      0.132           -2.03\n 4 실천         0     5   0.000436    0.00331     0.132           -2.03\n 5 정보         0     5   0.000436    0.00331     0.132           -2.03\n 6 투명         0     5   0.000436    0.00331     0.132           -2.03\n 7 과제         0     4   0.000436    0.00276     0.158           -1.84\n 8 국정운영     0     4   0.000436    0.00276     0.158           -1.84\n 9 시작         0     4   0.000436    0.00276     0.158           -1.84\n10 지식         0     4   0.000436    0.00276     0.158           -1.84\n# … with 945 more rows\n\n# 비중이 비슷한 단어, 0에 가까운\nfrequency_wide %>%\n  arrange(abs(log_odds_ratio))\n\n# A tibble: 955 × 7\n   word    moon  park ratio_moon ratio_park odds_ratio log_odds_ratio\n   <chr>  <int> <int>      <dbl>      <dbl>      <dbl>          <dbl>\n 1 때문       4     3    0.00218    0.00221      0.989        -0.0109\n 2 강화       3     2    0.00175    0.00165      1.06          0.0537\n 3 부담       3     2    0.00175    0.00165      1.06          0.0537\n 4 세계       3     2    0.00175    0.00165      1.06          0.0537\n 5 책임       3     2    0.00175    0.00165      1.06          0.0537\n 6 협력       3     2    0.00175    0.00165      1.06          0.0537\n 7 거대       2     1    0.00131    0.00110      1.19          0.171 \n 8 교체       2     1    0.00131    0.00110      1.19          0.171 \n 9 근본적     2     1    0.00131    0.00110      1.19          0.171 \n10 기반       2     1    0.00131    0.00110      1.19          0.171 \n# … with 945 more rows\n\n# 로그 오즈비를 이용해 중요한 단어 비교하기\ntop10 <- frequency_wide %>% \n  group_by(president = ifelse(log_odds_ratio > 0, 'moon', 'park')) %>% \n  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)\ntop10\n\n# A tibble: 20 × 8\n# Groups:   president [2]\n   word      moon  park ratio_moon ratio_park odds_ratio log_odds_ratio presid…¹\n   <chr>    <int> <int>      <dbl>      <dbl>      <dbl>          <dbl> <chr>   \n 1 복지국가     8     0   0.00393    0.000552     7.12             1.96 moon    \n 2 세상         6     0   0.00306    0.000552     5.54             1.71 moon    \n 3 여성         6     0   0.00306    0.000552     5.54             1.71 moon    \n 4 정의         6     0   0.00306    0.000552     5.54             1.71 moon    \n 5 강자         5     0   0.00262    0.000552     4.75             1.56 moon    \n 6 공평         5     0   0.00262    0.000552     4.75             1.56 moon    \n 7 대통령의     5     0   0.00262    0.000552     4.75             1.56 moon    \n 8 보통         5     0   0.00262    0.000552     4.75             1.56 moon    \n 9 상생         5     0   0.00262    0.000552     4.75             1.56 moon    \n10 지방         5     0   0.00262    0.000552     4.75             1.56 moon    \n11 박근혜       0     8   0.000436   0.00496      0.0879          -2.43 park    \n12 여러분       2    20   0.00131    0.0116       0.113           -2.18 park    \n13 행복         3    23   0.00175    0.0132       0.132           -2.03 park    \n14 실천         0     5   0.000436   0.00331      0.132           -2.03 park    \n15 정보         0     5   0.000436   0.00331      0.132           -2.03 park    \n16 투명         0     5   0.000436   0.00331      0.132           -2.03 park    \n17 과제         0     4   0.000436   0.00276      0.158           -1.84 park    \n18 국정운영     0     4   0.000436   0.00276      0.158           -1.84 park    \n19 시작         0     4   0.000436   0.00276      0.158           -1.84 park    \n20 지식         0     4   0.000436   0.00276      0.158           -1.84 park    \n# … with abbreviated variable name ¹​president\n\n# 주요 변수 추출\ntop10 %>%\n  arrange(-log_odds_ratio) %>%\n  select(word, log_odds_ratio, president)\n\n# A tibble: 20 × 3\n# Groups:   president [2]\n   word     log_odds_ratio president\n   <chr>             <dbl> <chr>    \n 1 복지국가           1.96 moon     \n 2 세상               1.71 moon     \n 3 여성               1.71 moon     \n 4 정의               1.71 moon     \n 5 강자               1.56 moon     \n 6 공평               1.56 moon     \n 7 대통령의           1.56 moon     \n 8 보통               1.56 moon     \n 9 상생               1.56 moon     \n10 지방               1.56 moon     \n11 과제              -1.84 park     \n12 국정운영          -1.84 park     \n13 시작              -1.84 park     \n14 지식              -1.84 park     \n15 행복              -2.03 park     \n16 실천              -2.03 park     \n17 정보              -2.03 park     \n18 투명              -2.03 park     \n19 여러분            -2.18 park     \n20 박근혜            -2.43 park     \n\n# 막대 그래프 만들기\nggplot(top10, aes(x = reorder(word, log_odds_ratio),\n                  y = log_odds_ratio,\n                  fill = president)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL)\n\n\n\n\n\n\nTD-IDF\n\nraw_speeches <- readr::read_csv(\"data/speeches_presidents.csv\")\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): president, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_speeches\n\n# A tibble: 4 × 2\n  president value                                                               \n  <chr>     <chr>                                                               \n1 문재인    \"정권교체 하겠습니다!   정치교체 하겠습니다!   시대교체 하겠습니다!…\n2 박근혜    \"존경하는 국민 여러분! 저는 오늘, 국민 한 분 한 분의 꿈이 이루어지… \n3 이명박    \"존경하는 국민 여러분, 사랑하는 한나라당 당원 동지 여러분! 저는 오… \n4 노무현    \"어느때인가 부터 제가 대통령이 되겠다고 말을 하기 시작했습니다. 많… \n\n# 기본적인 전처리\nspeeches <- raw_speeches %>%\n  mutate(value = str_replace_all(value, \"[^가-힣]\", \" \"),\n         value = str_squish(value))\n# 토큰화\nspeeches <- speeches %>%\n  unnest_tokens(input = value,\n                output = word,\n                token = extractNoun)\n# 단어 빈도 구하기\nfrequecy <- speeches %>%\n  count(president, word) %>%\n  filter(str_count(word) > 1)\nfrequecy\n\n# A tibble: 1,513 × 3\n   president word      n\n   <chr>     <chr> <int>\n 1 노무현    가슴      2\n 2 노무현    가훈      2\n 3 노무현    갈등      1\n 4 노무현    감옥      1\n 5 노무현    강자      1\n 6 노무현    개편      4\n 7 노무현    개혁      4\n 8 노무현    건국      1\n 9 노무현    경선      1\n10 노무현    경쟁      1\n# … with 1,503 more rows\n\n# 3.4.2 TF-IDF 구하기\nlibrary(tidytext)\nfrequecy <- frequecy %>%\n  bind_tf_idf(term = word, # 단어\n              document = president, # 텍스트 구분 기준\n              n = n) %>% # 단어 빈도\n  arrange(-tf_idf)\nfrequecy\n\n# A tibble: 1,513 × 6\n   president word         n      tf   idf tf_idf\n   <chr>     <chr>    <int>   <dbl> <dbl>  <dbl>\n 1 노무현    공식         6 0.0163  1.39  0.0227\n 2 노무현    비젼         6 0.0163  1.39  0.0227\n 3 노무현    정계         6 0.0163  1.39  0.0227\n 4 이명박    리더십       6 0.0158  1.39  0.0219\n 5 노무현    권력         9 0.0245  0.693 0.0170\n 6 노무현    개편         4 0.0109  1.39  0.0151\n 7 이명박    당원         4 0.0105  1.39  0.0146\n 8 이명박    동지         4 0.0105  1.39  0.0146\n 9 이명박    일류국가     4 0.0105  1.39  0.0146\n10 박근혜    박근혜       8 0.00962 1.39  0.0133\n# … with 1,503 more rows\n\n# TF-IDF가 높은 단어 살펴보기\nfrequecy %>% filter(president == \"문재인\")\n\n# A tibble: 688 × 6\n   president word         n      tf   idf  tf_idf\n   <chr>     <chr>    <int>   <dbl> <dbl>   <dbl>\n 1 문재인    복지국가     8 0.00608 1.39  0.00843\n 2 문재인    여성         6 0.00456 1.39  0.00633\n 3 문재인    공평         5 0.00380 1.39  0.00527\n 4 문재인    담쟁이       5 0.00380 1.39  0.00527\n 5 문재인    대통령의     5 0.00380 1.39  0.00527\n 6 문재인    보통         5 0.00380 1.39  0.00527\n 7 문재인    상생         5 0.00380 1.39  0.00527\n 8 문재인    우리나라    10 0.00760 0.693 0.00527\n 9 문재인    지방         5 0.00380 1.39  0.00527\n10 문재인    확대        10 0.00760 0.693 0.00527\n# … with 678 more rows\n\nfrequecy %>% filter(president == \"박근혜\")\n\n# A tibble: 407 × 6\n   president word         n      tf   idf  tf_idf\n   <chr>     <chr>    <int>   <dbl> <dbl>   <dbl>\n 1 박근혜    박근혜       8 0.00962 1.39  0.0133 \n 2 박근혜    정보         5 0.00601 1.39  0.00833\n 3 박근혜    투명         5 0.00601 1.39  0.00833\n 4 박근혜    행복        23 0.0276  0.288 0.00795\n 5 박근혜    교육         9 0.0108  0.693 0.00750\n 6 박근혜    국정운영     4 0.00481 1.39  0.00666\n 7 박근혜    정부        17 0.0204  0.288 0.00588\n 8 박근혜    개개인       3 0.00361 1.39  0.00500\n 9 박근혜    개인         3 0.00361 1.39  0.00500\n10 박근혜    공개         3 0.00361 1.39  0.00500\n# … with 397 more rows\n\nfrequecy %>% filter(president == \"이명박\")\n\n# A tibble: 202 × 6\n   president word         n      tf   idf  tf_idf\n   <chr>     <chr>    <int>   <dbl> <dbl>   <dbl>\n 1 이명박    리더십       6 0.0158  1.39  0.0219 \n 2 이명박    당원         4 0.0105  1.39  0.0146 \n 3 이명박    동지         4 0.0105  1.39  0.0146 \n 4 이명박    일류국가     4 0.0105  1.39  0.0146 \n 5 이명박    한나라       7 0.0184  0.693 0.0128 \n 6 이명박    나라        15 0.0395  0.288 0.0114 \n 7 이명박    도약         3 0.00789 1.39  0.0109 \n 8 이명박    일하         3 0.00789 1.39  0.0109 \n 9 이명박    사랑         5 0.0132  0.693 0.00912\n10 이명박    인생         5 0.0132  0.693 0.00912\n# … with 192 more rows\n\nfrequecy %>% filter(president == \"노무현\")\n\n# A tibble: 216 × 6\n   president word         n      tf   idf  tf_idf\n   <chr>     <chr>    <int>   <dbl> <dbl>   <dbl>\n 1 노무현    공식         6 0.0163  1.39  0.0227 \n 2 노무현    비젼         6 0.0163  1.39  0.0227 \n 3 노무현    정계         6 0.0163  1.39  0.0227 \n 4 노무현    권력         9 0.0245  0.693 0.0170 \n 5 노무현    개편         4 0.0109  1.39  0.0151 \n 6 노무현    국회의원     3 0.00817 1.39  0.0113 \n 7 노무현    남북대화     3 0.00817 1.39  0.0113 \n 8 노무현    총리         3 0.00817 1.39  0.0113 \n 9 노무현    가훈         2 0.00545 1.39  0.00755\n10 노무현    개혁         4 0.0109  0.693 0.00755\n# … with 206 more rows\n\n# TF-IDF가 낮은 단어 살펴보기, 4개가 동일한 값으로 출력됨\nfrequecy %>%\n  filter(president == \"문재인\") %>%\n  arrange(tf_idf)\n\n# A tibble: 688 × 6\n   president word       n       tf   idf tf_idf\n   <chr>     <chr>  <int>    <dbl> <dbl>  <dbl>\n 1 문재인    경쟁       6 0.00456      0      0\n 2 문재인    경제      15 0.0114       0      0\n 3 문재인    고통       4 0.00304      0      0\n 4 문재인    과거       1 0.000760     0      0\n 5 문재인    국민      21 0.0160       0      0\n 6 문재인    기회       5 0.00380      0      0\n 7 문재인    대통령    12 0.00913      0      0\n 8 문재인    동안       2 0.00152      0      0\n 9 문재인    들이       9 0.00684      0      0\n10 문재인    마음       2 0.00152      0      0\n# … with 678 more rows\n\nfrequecy %>%\n  filter(president == \"박근혜\") %>%\n  arrange(tf_idf)\n\n# A tibble: 407 × 6\n   president word       n      tf   idf tf_idf\n   <chr>     <chr>  <int>   <dbl> <dbl>  <dbl>\n 1 박근혜    경쟁       1 0.00120     0      0\n 2 박근혜    경제      15 0.0180      0      0\n 3 박근혜    고통       4 0.00481     0      0\n 4 박근혜    과거       2 0.00240     0      0\n 5 박근혜    국민      72 0.0865      0      0\n 6 박근혜    기회       1 0.00120     0      0\n 7 박근혜    대통령     3 0.00361     0      0\n 8 박근혜    동안       3 0.00361     0      0\n 9 박근혜    들이       3 0.00361     0      0\n10 박근혜    마음       3 0.00361     0      0\n# … with 397 more rows\n\nfrequecy %>%\n  filter(president == \"이명박\") %>%\n  arrange(tf_idf)\n\n# A tibble: 202 × 6\n   president word       n      tf   idf tf_idf\n   <chr>     <chr>  <int>   <dbl> <dbl>  <dbl>\n 1 이명박    경쟁       3 0.00789     0      0\n 2 이명박    경제       5 0.0132      0      0\n 3 이명박    고통       1 0.00263     0      0\n 4 이명박    과거       1 0.00263     0      0\n 5 이명박    국민      13 0.0342      0      0\n 6 이명박    기회       3 0.00789     0      0\n 7 이명박    대통령     4 0.0105      0      0\n 8 이명박    동안       1 0.00263     0      0\n 9 이명박    들이       1 0.00263     0      0\n10 이명박    마음       1 0.00263     0      0\n# … with 192 more rows\n\nfrequecy %>%\n  filter(president == \"노무현\") %>%\n  arrange(tf_idf)\n\n# A tibble: 216 × 6\n   president word       n      tf   idf tf_idf\n   <chr>     <chr>  <int>   <dbl> <dbl>  <dbl>\n 1 노무현    경쟁       1 0.00272     0      0\n 2 노무현    경제       1 0.00272     0      0\n 3 노무현    고통       1 0.00272     0      0\n 4 노무현    과거       1 0.00272     0      0\n 5 노무현    국민       7 0.0191      0      0\n 6 노무현    기회       1 0.00272     0      0\n 7 노무현    대통령     6 0.0163      0      0\n 8 노무현    동안       2 0.00545     0      0\n 9 노무현    들이       4 0.0109      0      0\n10 노무현    마음       1 0.00272     0      0\n# … with 206 more rows\n\n# 막대 그래프 만들기\n# 주요 단어 추출\ntop10 <- frequecy %>%\n  group_by(president) %>%\n  slice_max(tf_idf, n = 10, with_ties = F)\ntop10\n\n# A tibble: 40 × 6\n# Groups:   president [4]\n   president word         n      tf   idf  tf_idf\n   <chr>     <chr>    <int>   <dbl> <dbl>   <dbl>\n 1 노무현    공식         6 0.0163  1.39  0.0227 \n 2 노무현    비젼         6 0.0163  1.39  0.0227 \n 3 노무현    정계         6 0.0163  1.39  0.0227 \n 4 노무현    권력         9 0.0245  0.693 0.0170 \n 5 노무현    개편         4 0.0109  1.39  0.0151 \n 6 노무현    국회의원     3 0.00817 1.39  0.0113 \n 7 노무현    남북대화     3 0.00817 1.39  0.0113 \n 8 노무현    총리         3 0.00817 1.39  0.0113 \n 9 노무현    가훈         2 0.00545 1.39  0.00755\n10 노무현    개혁         4 0.0109  0.693 0.00755\n# … with 30 more rows\n\n# 그래프 순서 정하기\ntop10$president <- factor(top10$president,\n                          levels = c(\"문재인\", \"박근혜\", \"이명박\", \"노무현\"))\n\n# 막대 그래프 만들기\nlibrary(ggplot2)\nlibrary(showtext)\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nggplot(top10, aes(x = reorder_within(word, tf_idf, president),\n                  y = tf_idf,\n                  fill = president)) +\n  geom_col(show.legend = F) +\n  coord_flip() +\n  facet_wrap(~ president, scales = \"free\", ncol = 2) +\n  scale_x_reordered() +\n  labs(x = NULL)\n\n\n\n\n\n\nSentimental Analysis\n\n# 감정 사전 불러오기\ndic <- read_csv('data/knu_sentiment_lexicon.csv')\n\nRows: 14854 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): word\ndbl (1): polarity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 긍정 단어\ndic %>%\n  filter(polarity == 2) %>%\n  arrange(word)\n\n# A tibble: 2,602 × 2\n   word              polarity\n   <chr>                <dbl>\n 1 가능성이 늘어나다        2\n 2 가능성이 있다고          2\n 3 가능하다                 2\n 4 가볍고 상쾌하다          2\n 5 가볍고 상쾌한            2\n 6 가볍고 시원하게          2\n 7 가볍고 편안하게          2\n 8 가볍고 환하게            2\n 9 가운데에서 뛰어남        2\n10 가장 거룩한              2\n# … with 2,592 more rows\n\n# 부정 단어\ndic %>%\n  filter(polarity == -2) %>%\n  arrange(word)\n\n# A tibble: 4,799 × 2\n   word            polarity\n   <chr>              <dbl>\n 1 가난                  -2\n 2 가난뱅이              -2\n 3 가난살이              -2\n 4 가난살이하다          -2\n 5 가난설음              -2\n 6 가난에                -2\n 7 가난에 쪼들려서       -2\n 8 가난하게              -2\n 9 가난하고              -2\n10 가난하고 어렵다       -2\n# … with 4,789 more rows\n\n# 감정 단어의 종류 살펴보기\ndic %>%\n  filter(word %in% c(\"좋은\", \"나쁜\"))\n\n# A tibble: 2 × 2\n  word  polarity\n  <chr>    <dbl>\n1 좋은         2\n2 나쁜        -2\n\ndic %>%\n  filter(word %in% c(\"기쁜\", \"슬픈\"))\n\n# A tibble: 2 × 2\n  word  polarity\n  <chr>    <dbl>\n1 슬픈        -2\n2 기쁜         2\n\n# 이모티콘\nlibrary(stringr)\ndic %>%\n  filter(!str_detect(word, \"[가-힣]\")) %>%\n  arrange(word)\n\n# A tibble: 77 × 2\n   word  polarity\n   <chr>    <dbl>\n 1 (-;          1\n 2 (-_-)       -1\n 3 (;_;)       -1\n 4 (T_T)       -1\n 5 (^-^)        1\n 6 (^^)         1\n 7 (^^*         1\n 8 (^_^)        1\n 9 (^_^;       -1\n10 (^o^)        1\n# … with 67 more rows\n\n# 총 14,854개 단어\ndic %>%\n  mutate(sentiment = ifelse(polarity >= 1, \"pos\",\n                            ifelse(polarity <= -1, \"neg\", \"neu\"))) %>%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  <chr>     <int>\n1 neg        9829\n2 neu         154\n3 pos        4871\n\n# 문장의 감정 점수 구하기\n# 1. 단어 기준으로 토큰화하기\ndf <- tibble(sentence = c(\"디자인 예쁘고 마감도 좋아서 만족스럽다.\",\n                          \"디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다.\"))\n\nlibrary(tidytext)\ndf <- df %>%\n  unnest_tokens(input = sentence,\n                output = word,\n                token = \"words\",\n                drop = F)\ndf\n\n# A tibble: 12 × 2\n   sentence                                             word      \n   <chr>                                                <chr>     \n 1 디자인 예쁘고 마감도 좋아서 만족스럽다.              디자인    \n 2 디자인 예쁘고 마감도 좋아서 만족스럽다.              예쁘고    \n 3 디자인 예쁘고 마감도 좋아서 만족스럽다.              마감도    \n 4 디자인 예쁘고 마감도 좋아서 만족스럽다.              좋아서    \n 5 디자인 예쁘고 마감도 좋아서 만족스럽다.              만족스럽다\n 6 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 디자인은  \n 7 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 괜찮다    \n 8 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 그런데    \n 9 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 마감이    \n10 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 나쁘고    \n11 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 가격도    \n12 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 비싸다    \n\n# 2. 단어에 감정 점수 부여하기\ndf <- df %>%\n  left_join(dic, by = \"word\") %>%\n  mutate(polarity = ifelse(is.na(polarity), 0, polarity))\ndf\n\n# A tibble: 12 × 3\n   sentence                                             word       polarity\n   <chr>                                                <chr>         <dbl>\n 1 디자인 예쁘고 마감도 좋아서 만족스럽다.              디자인            0\n 2 디자인 예쁘고 마감도 좋아서 만족스럽다.              예쁘고            2\n 3 디자인 예쁘고 마감도 좋아서 만족스럽다.              마감도            0\n 4 디자인 예쁘고 마감도 좋아서 만족스럽다.              좋아서            2\n 5 디자인 예쁘고 마감도 좋아서 만족스럽다.              만족스럽다        2\n 6 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 디자인은          0\n 7 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 괜찮다            1\n 8 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 그런데            0\n 9 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 마감이            0\n10 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 나쁘고           -2\n11 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 가격도            0\n12 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다. 비싸다           -2\n\n# 3. 문장별로 감정 점수 합산하기\nscore_df <- df %>%\n  group_by(sentence) %>%\n  summarise(score = sum(polarity))\nscore_df\n\n# A tibble: 2 × 2\n  sentence                                             score\n  <chr>                                                <dbl>\n1 디자인 예쁘고 마감도 좋아서 만족스럽다.                  6\n2 디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다.    -3\n\n\n\n\nSentimental Analysis for the comments\n댓글 감성 분석\n\n# 데이터 불러오기\nraw_news_comment <- read_csv(\"data/news_comment_parasite.csv\")\n\nRows: 4150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): reply, press, title, url\ndttm (1): reg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_news_comment\n\n# A tibble: 4,150 × 5\n   reg_time            reply                                   press title url  \n   <dttm>              <chr>                                   <chr> <chr> <chr>\n 1 2020-02-10 16:59:02 \"정말 우리 집에 좋은 일이 생겨 기쁘고 … MBC   '기…  http…\n 2 2020-02-10 13:32:24 \"와 너무 기쁘다! 이 시국에 정말 내 일…  SBS   [영…  http…\n 3 2020-02-10 12:30:09 \"우리나라의 영화감독분들 그리고 앞으로… 한겨… ‘기…  http…\n 4 2020-02-10 13:08:22 \"봉준호 감독과 우리나라 대한민국 모두 … 한겨… ‘기…  http…\n 5 2020-02-10 16:25:41 \"노벨상 탄느낌이네요\\r\\n축하축하 합니…  한겨… ‘기…  http…\n 6 2020-02-10 12:31:45 \"기생충 상 받을때 박수 쳤어요.감독상도… 한겨… ‘기…  http…\n 7 2020-02-10 12:31:33 \"대한민국 영화사를 새로 쓰고 계시네요 … 한겨… ‘기…  http…\n 8 2020-02-11 09:20:52 \"저런게 아카데미상 받으면  '태극기 휘…  한겨… ‘기…  http…\n 9 2020-02-10 20:53:27 \"다시한번 보여주세요 영화관에서 보고싶… 한겨… ‘기…  http…\n10 2020-02-10 20:22:41 \"대한민국 BTS와함께  봉준호감독님까지\\… 한겨… ‘기…  http…\n# … with 4,140 more rows\n\n# 기본적인 전처리\nlibrary(textclean)\nnews_comment <- raw_news_comment %>%\n  mutate(id = row_number(),\n         reply = str_squish(replace_html(reply)))\nnews_comment\n\n# A tibble: 4,150 × 6\n   reg_time            reply                             press title url      id\n   <dttm>              <chr>                             <chr> <chr> <chr> <int>\n 1 2020-02-10 16:59:02 정말 우리 집에 좋은 일이 생겨 기… MBC   '기…  http…     1\n 2 2020-02-10 13:32:24 와 너무 기쁘다! 이 시국에 정말 …  SBS   [영…  http…     2\n 3 2020-02-10 12:30:09 우리나라의 영화감독분들 그리고 …  한겨… ‘기…  http…     3\n 4 2020-02-10 13:08:22 봉준호 감독과 우리나라 대한민국 … 한겨… ‘기…  http…     4\n 5 2020-02-10 16:25:41 노벨상 탄느낌이네요 축하축하 합…  한겨… ‘기…  http…     5\n 6 2020-02-10 12:31:45 기생충 상 받을때 박수 쳤어요.감…  한겨… ‘기…  http…     6\n 7 2020-02-10 12:31:33 대한민국 영화사를 새로 쓰고 계시… 한겨… ‘기…  http…     7\n 8 2020-02-11 09:20:52 저런게 아카데미상 받으면 '태극기… 한겨… ‘기…  http…     8\n 9 2020-02-10 20:53:27 다시한번 보여주세요 영화관에서 …  한겨… ‘기…  http…     9\n10 2020-02-10 20:22:41 대한민국 BTS와함께 봉준호감독님…  한겨… ‘기…  http…    10\n# … with 4,140 more rows\n\n# 데이터 구조 확인\nglimpse(news_comment)\n\nRows: 4,150\nColumns: 6\n$ reg_time <dttm> 2020-02-10 16:59:02, 2020-02-10 13:32:24, 2020-02-10 12:30:0…\n$ reply    <chr> \"정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일…\n$ press    <chr> \"MBC\", \"SBS\", \"한겨레\", \"한겨레\", \"한겨레\", \"한겨레\", \"한겨레…\n$ title    <chr> \"'기생충' 아카데미 작품상까지 4관왕…영화사 새로 썼다\", \"[영상…\n$ url      <chr> \"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=1…\n$ id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n\n# 단어 기준으로 토큰화하고 감정 점수 부여하기\n# 토큰화\nword_comment <- news_comment %>%\n  unnest_tokens(input = reply,\n                output = word,\n                token = \"words\",\n                drop = F)\n\nword_comment %>%\n  select(word, reply)\n\n# A tibble: 37,718 × 2\n   word   reply                                                                \n   <chr>  <chr>                                                                \n 1 정말   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 2 우리   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 3 집에   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 4 좋은   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 5 일이   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 6 생겨   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 7 기쁘고 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 8 행복한 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n 9 것처럼 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n10 나의   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행…\n# … with 37,708 more rows\n\n# 감정 점수 부여\nword_comment <- word_comment %>%\n  left_join(dic, by = \"word\") %>%\n  mutate(polarity = ifelse(is.na(polarity), 0, polarity))\n\nword_comment %>%\n  select(word, polarity)\n\n# A tibble: 37,718 × 2\n   word   polarity\n   <chr>     <dbl>\n 1 정말          0\n 2 우리          0\n 3 집에          0\n 4 좋은          2\n 5 일이          0\n 6 생겨          0\n 7 기쁘고        2\n 8 행복한        2\n 9 것처럼        0\n10 나의          0\n# … with 37,708 more rows\n\n# 자주 사용된 감정 단어 살펴보기\n# 1. 감정 분류하기\nword_comment <- word_comment %>%\n  mutate(sentiment = ifelse(polarity == 2, \"pos\",\n                            ifelse(polarity == -2, \"neg\", \"neu\")))\n\nword_comment %>%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  <chr>     <int>\n1 neg         285\n2 neu       36671\n3 pos         762\n\n# 2. 막대 그래프 만들기\ntop10_sentiment <- word_comment %>%\n  filter(sentiment != \"neu\") %>%\n  count(sentiment, word) %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10)\n\ntop10_sentiment\n\n# A tibble: 22 × 3\n# Groups:   sentiment [2]\n   sentiment word       n\n   <chr>     <chr>  <int>\n 1 neg       소름      56\n 2 neg       소름이    16\n 3 neg       아니다    15\n 4 neg       우울한     9\n 5 neg       해         8\n 6 neg       미친       7\n 7 neg       가난한     5\n 8 neg       어려운     5\n 9 neg       힘든       5\n10 neg       더러운     4\n# … with 12 more rows\n\nggplot(top10_sentiment, aes(x = reorder(word, n),\n                            y = n,\n                            fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.3) +\n  facet_wrap(~ sentiment, scales = \"free\") +\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +\n  labs(x = NULL) \n\n\n\n# 댓글별 감정 점수 구하고 댓글 살펴보기\n# 1. 댓글별 감정 점수 구하기\nscore_comment <- word_comment %>%\n  group_by(id, reply) %>%\n  summarise(score = sum(polarity)) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nscore_comment %>%\n  select(score, reply)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   <dbl> <chr>                                                                  \n 1     6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복…\n 2     6 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요…\n 3     4 우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 영감… \n 4     3 봉준호 감독과 우리나라 대한민국 모두 자랑스럽다. 세계 어디를 가고 우리…\n 5     0 노벨상 탄느낌이네요 축하축하 합니다                                    \n 6     0 기생충 상 받을때 박수 쳤어요.감독상도 기대해요.봉준호 감독 화이팅^^    \n 7     0 대한민국 영화사를 새로 쓰고 계시네요 ㅊㅊㅊ                            \n 8     0 저런게 아카데미상 받으면 '태극기 휘날리며'' '광해' '명량''은 전부문 휩…\n 9     0 다시한번 보여주세요 영화관에서 보고싶은디                              \n10     2 대한민국 BTS와함께 봉준호감독님까지 대단하고 한국의 문화에 자긍심을 가…\n# … with 4,130 more rows\n\n# 2. 감정 점수 높은 댓글 살펴보기\n# 긍정 댓글\nscore_comment %>%\n  select(score, reply) %>%\n  arrange(-score)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   <dbl> <chr>                                                                  \n 1    11 아니 다른상을 받은것도 충분히 대단하고 굉장하지만 최고의 영예인 작품상…\n 2     9 봉준호의 위대한 업적은 진보 영화계의 위대한 업적이고 대한민국의 업적입…\n 3     7 이 수상소식을 듣고 억수로 기뻐하는 가족이 있을것 같다. SNS를 통해 자기…\n 4     7 감사 감사 감사 수상 소감도 3관왕 답네요                                \n 5     6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복…\n 6     6 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요…\n 7     6 축하 축하 축하 모두들 수고 하셨어요 기생충 화이팅                      \n 8     6 축하!!!! 축하!!!!! 오스카의 정복은 무엇보다 시나리오의 힘이다. 작가의 …\n 9     6 조여정 ㆍ예쁜얼굴때문에 연기력을 제대로 평가받지 못해 안타깝던 내가 좋…\n10     6 좋은 걸 좋다고 말하지 못하는 인간들이 참 불쌍해지네....댓글 보니 인생… \n# … with 4,130 more rows\n\n# 부정 댓글\nscore_comment %>%\n  select(score, reply) %>%\n  arrange(score)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   <dbl> <chr>                                                                  \n 1    -7 기생충 영화 한국인 으로써 싫다 대단히 싫다!! 가난한 서민들의 마지막 자…\n 2    -6 이 페미민국이 잘 되는 게 아주 싫다. 최악의 나쁜일들과 불운, 불행, 어둡…\n 3    -5 특정 인물의 성공을 국가의 부흥으로 연관짓는 것은 미개한 발상이다. 봉준…\n 4    -4 좌파들이 나라 망신 다 시킨다..ㅠ 설레발 오지게 치더니..꼴랑 각본상 하… \n 5    -4 부패한 386 민주화 세대 정권의 무분별한 포퓰리즘으로 탄생한 좀비들의 살…\n 6    -4 기생충 내용은 좋은데 제목이 그래요. 극 중 송강호가족이 부잣집에 대해서…\n 7    -4 이런 감독과 이런 배우보고 좌좀 이라고 지1랄하던 그분들 다 어디계시냐? …\n 8    -4 축하합니다. 근데 현실 세계인 한국에선 그보다 훨씬 나쁜 넘인 조로남불 … \n 9    -4 큰일이다....국제적 망신이다...전 세계사람들이 우리나라를 기생충으로 보…\n10    -4 더럽고 추잡한 그들만의 리그                                            \n# … with 4,130 more rows\n\n# 감정 경향 살펴보기\n# 1. 감정 점수 빈도 구하기\nscore_comment %>%\n  count(score)\n\n# A tibble: 17 × 2\n   score     n\n   <dbl> <int>\n 1    -7     1\n 2    -6     1\n 3    -5     1\n 4    -4    17\n 5    -3    35\n 6    -2   175\n 7    -1   206\n 8     0  2897\n 9     1   222\n10     2   432\n11     3    57\n12     4    71\n13     5     7\n14     6    14\n15     7     2\n16     9     1\n17    11     1\n\n# 2. 감정 분류하고 막대 그래프 만들기\n# 감정 분류하기\nscore_comment <- score_comment %>%\n  mutate(sentiment = ifelse(score >= 1, \"pos\",\n                            ifelse(score <= -1, \"neg\", \"neu\")))\n\n# 감정 빈도와 비율 구하기\nfrequency_score <- score_comment %>%\n  count(sentiment) %>%\n  mutate(ratio = n/sum(n)*100)\nfrequency_score\n\n# A tibble: 3 × 3\n  sentiment     n ratio\n  <chr>     <int> <dbl>\n1 neg         436  10.5\n2 neu        2897  70.0\n3 pos         807  19.5\n\n# 막대 그래프 만들기\nggplot(frequency_score, aes(x = sentiment, y = n, fill = sentiment)) +\n  geom_col() +\n  geom_text(aes(label = n), vjust = -0.3) +\n  scale_x_discrete(limits = c(\"pos\", \"neu\", \"neg\"))\n\n\n\n# 3. 비율 누적 막대 그래프 만들기\ndf <- tibble(contry = c(\"Korea\", \"Korea\", \"Japen\", \"Japen\"), # 축\n             sex = c(\"M\", \"F\", \"M\", \"F\"), # 누적 막대\n             ratio = c(60, 40, 30, 70)) # 값\ndf\n\n# A tibble: 4 × 3\n  contry sex   ratio\n  <chr>  <chr> <dbl>\n1 Korea  M        60\n2 Korea  F        40\n3 Japen  M        30\n4 Japen  F        70\n\nggplot(df, aes(x = contry, y = ratio, fill = sex)) + geom_col()\n\n\n\nggplot(df, aes(x = contry, y = ratio, fill = sex)) +\n  geom_col() +\n  geom_text(aes(label = paste0(ratio, \"%\")), # % 표시\n            position = position_stack(vjust = 0.5)) # 가운데 표시\n\n\n\n# 댓글의 감정 비율로 누적 막대 그래프 만들기\n# 더미 변수 생성\nfrequency_score$dummy <- 0\nfrequency_score\n\n# A tibble: 3 × 4\n  sentiment     n ratio dummy\n  <chr>     <int> <dbl> <dbl>\n1 neg         436  10.5     0\n2 neu        2897  70.0     0\n3 pos         807  19.5     0\n\nggplot(frequency_score, aes(x = dummy, y = ratio, fill = sentiment)) +\n  geom_col() +\n  geom_text(aes(label = paste0(round(ratio, 1), \"%\")),\n            position = position_stack(vjust = 0.5)) +\n  theme(axis.title.x = element_blank(), # x축 이름 삭제\n        axis.text.x = element_blank(), # x축 값 삭제\n        axis.ticks.x = element_blank()) # x축 눈금 삭제\n\n\n\n\n\n\nWord Frequency by Sentiment Category\n감정 범주별 단어 빈도\n\n# 감정 범주별 단어 빈도 구하기\n# 1. 토큰화하고 두 글자 이상 한글 단어만 남기기\ncomment <- score_comment %>%\n  unnest_tokens(input = reply, # 단어 기준 토큰화\n                output = word,\n                token = \"words\",\n                drop = F) %>%\n  filter(str_detect(word, \"[가-힣]\") & # 한글 추출\n           str_count(word) >= 2) # 두 글자 이상 추출\n\n# 2. 감정 범주별 빈도 구하기\nfrequency_word <- comment %>%\n  filter(str_count(word) >= 2) %>%\n  count(sentiment, word, sort = T)\nfrequency_word\n\n# A tibble: 19,223 × 3\n   sentiment word               n\n   <chr>     <chr>          <int>\n 1 neu       축하합니다       214\n 2 neu       봉준호           203\n 3 neu       기생충           164\n 4 neu       축하드립니다     155\n 5 neu       정말             146\n 6 neu       대박             134\n 7 neu       진짜             121\n 8 pos       봉준호           106\n 9 pos       정말              97\n10 neu       자랑스럽습니다    96\n# … with 19,213 more rows\n\n# 긍정 댓글 고빈도 단어\nfrequency_word %>%\n  filter(sentiment == \"pos\")\n\n# A tibble: 5,234 × 3\n   sentiment word           n\n   <chr>     <chr>      <int>\n 1 pos       봉준호       106\n 2 pos       정말          97\n 3 pos       대단하다      83\n 4 pos       진짜          79\n 5 pos       자랑스럽다    78\n 6 pos       축하          63\n 7 pos       대한민국      61\n 8 pos       영화          58\n 9 pos       멋지다        55\n10 pos       기생충        53\n# … with 5,224 more rows\n\n# 부정 댓글 고빈도 단어\nfrequency_word %>%\n  filter(sentiment == \"neg\")\n\n# A tibble: 4,080 × 3\n   sentiment word             n\n   <chr>     <chr>        <int>\n 1 neg       소름            49\n 2 neg       봉준호          47\n 3 neg       기생충          33\n 4 neg       이런            33\n 5 neg       정말            32\n 6 neg       진짜            26\n 7 neg       좌빨            21\n 8 neg       너무            20\n 9 neg       블랙리스트에    19\n10 neg       영화            18\n# … with 4,070 more rows\n\n# 상대적으로 자주 사용된 단어 비교하기\n# 1. 로그 오즈비 구하기\n# wide form으로 변환\ncomment_wide <- frequency_word %>%\n  filter(sentiment != \"neu\") %>%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = list(n = 0))\ncomment_wide\n\n# A tibble: 8,380 × 3\n   word         pos   neg\n   <chr>      <int> <int>\n 1 봉준호       106    47\n 2 정말          97    32\n 3 대단하다      83     1\n 4 진짜          79    26\n 5 자랑스럽다    78     1\n 6 축하          63     0\n 7 대한민국      61     4\n 8 영화          58    18\n 9 멋지다        55     0\n10 기생충        53    33\n# … with 8,370 more rows\n\n# 로그 오즈비 구하기\ncomment_wide <- comment_wide %>%\n  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /\n                                ((neg + 1) / (sum(neg + 1)))))\ncomment_wide\n\n# A tibble: 8,380 × 4\n   word         pos   neg log_odds_ratio\n   <chr>      <int> <int>          <dbl>\n 1 봉준호       106    47          0.589\n 2 정말          97    32          0.876\n 3 대단하다      83     1          3.52 \n 4 진짜          79    26          0.873\n 5 자랑스럽다    78     1          3.46 \n 6 축하          63     0          3.95 \n 7 대한민국      61     4          2.30 \n 8 영화          58    18          0.920\n 9 멋지다        55     0          3.81 \n10 기생충        53    33          0.250\n# … with 8,370 more rows\n\n# 2. 로그 오즈비가 가장 큰 단어 10개씩 추출하기\ntop10 <- comment_wide %>%\n  group_by(sentiment = ifelse(log_odds_ratio > 0, \"pos\", \"neg\")) %>%\n  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)\ntop10\n\n# A tibble: 20 × 5\n# Groups:   sentiment [2]\n   word         pos   neg log_odds_ratio sentiment\n   <chr>      <int> <int>          <dbl> <chr>    \n 1 소름           2    49          -3.03 neg      \n 2 좌빨           1    21          -2.61 neg      \n 3 못한           0     7          -2.29 neg      \n 4 미친           0     7          -2.29 neg      \n 5 좌좀           0     6          -2.16 neg      \n 6 소름이         1    12          -2.08 neg      \n 7 가난한         0     5          -2.00 neg      \n 8 모르는         0     5          -2.00 neg      \n 9 아쉽다         0     5          -2.00 neg      \n10 닭그네         0     4          -1.82 neg      \n11 축하          63     0           3.95 pos      \n12 멋지다        55     0           3.81 pos      \n13 대단한        47     0           3.66 pos      \n14 좋은          42     0           3.55 pos      \n15 대단하다      83     1           3.52 pos      \n16 자랑스럽다    78     1           3.46 pos      \n17 최고          27     0           3.12 pos      \n18 세계적인      24     0           3.01 pos      \n19 최고의        23     0           2.97 pos      \n20 위대한        22     0           2.92 pos      \n\n# 3. 막대 그래프 만들기\nggplot(top10, aes(x = reorder(word, log_odds_ratio),\n                  y = log_odds_ratio,\n                  fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL)\n\n\n\n\n\n\nModify Emotion Words\n감정 단어 수정\n\n# 감정 단어가 사용된 원문 살펴보기\n# \"소름\"이 사용된 댓글\nscore_comment %>%\n  filter(str_detect(reply, \"소름\")) %>%\n  select(reply)\n\n# A tibble: 131 × 1\n   reply                                                                       \n   <chr>                                                                       \n 1 소름돋네요                                                                  \n 2 와..진짜소름 저 소리처음질렀어요 눈물나요.. ㅠㅠ                            \n 3 생중계 보며 봉준호 할 때 소름이~~~!! ㅠㅠ 수상소감들으며 함께 가슴이 벅차네…\n 4 와 보다가 소름 짝 수고들하셨어요                                            \n 5 한국어 소감 듣는데 소름돋네 축하드립니다                                    \n 6 대단하다!! 봉준호 이름 나오자마자 소름                                      \n 7 와우 브라보~ 키아누리브스의 봉준호, 순간 소름이.. 멋지십니다.               \n 8 소름 돋네요. 축하합니다                                                     \n 9 소름.... 기생충 각본집 산거 다시한번 잘했다는 생각이ㅠㅠㅠ 축하해요!!!!!!   \n10 소름끼쳤어요 너무 멋집니다 ^^!!!!                                           \n# … with 121 more rows\n\n# \"미친\"이 사용된 댓글\nscore_comment %>%\n  filter(str_detect(reply, \"미친\")) %>%\n  select(reply)\n\n# A tibble: 15 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 와 3관왕 미친                                                                \n 2 미친거야 이건~~                                                              \n 3 Korea 대단합니다 김연아 방탄 봉준호 스포츠 음악 영화 못하는게 없어요 좌빨 감…\n 4 청룡영화제에서 다른나라가 상을 휩쓴거죠? 와..미쳤다 미국영화제에서 한국이 빅…\n 5 설마했는데 감독상, 작품상, 각본상을 죄다 휩쓸어버릴 줄이야. 이건 미친 꿈이야…\n 6 완전 완전...미친촌재감~이런게 바로 애국이지~ 존경합니다~                     \n 7 이세상엔 참 미 친 인간들이 많다는걸 댓글에서 다시한번 느낀다..모두가 축하해… \n 8 올해 아카데미 최다 수상작이기도 하다 이건 진짜 미친사건이다                  \n 9 CJ회장이 저기서 왜 언급되는지... 미친 부회장.. 공과사 구분 못하는 정권의 홍… \n10 미친봉                                                                       \n11 미친 3관왕 ㄷㄷㄷㄷㄷ                                                        \n12 진짜 미친일...                                                               \n13 나도모르게 보다가 육성으로 미친...ㅋㅋㅋㅋ 대박ㅜ                            \n14 헐...감독상...미친...미쳤다..소름돋는다...                                   \n15 인정할건인정하자 봉감독 송배우 이배우 조배우등 인정하자 또 가로세로 ㅆㄹㄱ들…\n\ndic %>% filter(word %in% c(\"소름\", \"소름이\", \"미친\"))\n\n# A tibble: 3 × 2\n  word   polarity\n  <chr>     <dbl>\n1 소름이       -2\n2 소름         -2\n3 미친         -2\n\n# 감정 사전 수정하기\nnew_dic <- dic %>%\n  mutate(polarity = ifelse(word %in% c(\"소름\", \"소름이\", \"미친\"), 2, polarity))\nnew_dic %>% filter(word %in% c(\"소름\", \"소름이\", \"미친\"))\n\n# A tibble: 3 × 2\n  word   polarity\n  <chr>     <dbl>\n1 소름이        2\n2 소름          2\n3 미친          2\n\n# 수정한 사전으로 감정 점수 부여하기\nnew_word_comment <- word_comment %>%\n  select(-polarity) %>%\n  left_join(new_dic, by = \"word\") %>%\n  mutate(polarity = ifelse(is.na(polarity), 0, polarity))\n\n# 댓글별 감정 점수 구하기\nnew_score_comment <- new_word_comment %>%\n  group_by(id, reply) %>%\n  summarise(score = sum(polarity)) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nnew_score_comment %>%\n  select(score, reply) %>%\n  arrange(-score)\n\n# A tibble: 4,140 × 2\n   score reply                                                                  \n   <dbl> <chr>                                                                  \n 1    11 아니 다른상을 받은것도 충분히 대단하고 굉장하지만 최고의 영예인 작품상…\n 2     9 봉준호의 위대한 업적은 진보 영화계의 위대한 업적이고 대한민국의 업적입…\n 3     8 소름 소름 진짜 멋지다 대단하다                                         \n 4     7 이 수상소식을 듣고 억수로 기뻐하는 가족이 있을것 같다. SNS를 통해 자기…\n 5     7 감사 감사 감사 수상 소감도 3관왕 답네요                                \n 6     6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복…\n 7     6 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요…\n 8     6 축하 축하 축하 모두들 수고 하셨어요 기생충 화이팅                      \n 9     6 생중계 보며 봉준호 할 때 소름이~~~!! ㅠㅠ 수상소감들으며 함께 가슴이 … \n10     6 축하!!!! 축하!!!!! 오스카의 정복은 무엇보다 시나리오의 힘이다. 작가의 …\n# … with 4,130 more rows\n\n# 전반적인 감정 경향 살펴보기\n# 1. 감정 분류하기\n# 1점 기준으로 긍정 중립 부정 분류\nnew_score_comment <- new_score_comment %>%\n  mutate(sentiment = ifelse(score >= 1, \"pos\",\n                            ifelse(score <= -1, \"neg\", \"neu\")))\n\n# 2. 감정 범주별 빈도와 비율 구하기\n# 원본 감정 사전 활용\nscore_comment %>%\n  count(sentiment) %>%\n  mutate(ratio = n/sum(n)*100)\n\n# A tibble: 3 × 3\n  sentiment     n ratio\n  <chr>     <int> <dbl>\n1 neg         436  10.5\n2 neu        2897  70.0\n3 pos         807  19.5\n\n# 수정한 감정 사전 활용\nnew_score_comment %>%\n  count(sentiment) %>%\n  mutate(ratio = n/sum(n)*100)\n\n# A tibble: 3 × 3\n  sentiment     n ratio\n  <chr>     <int> <dbl>\n1 neg         368  8.89\n2 neu        2890 69.8 \n3 pos         882 21.3 \n\n# 3. 분석 결과 비교하기\nword <- \"소름|소름이|미친\"\n\n# 원본 감정 사전 활용\nscore_comment %>%\n  filter(str_detect(reply, word)) %>%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  <chr>     <int>\n1 neg          73\n2 neu          63\n3 pos           9\n\n# 수정한 감정 사전 활용\nnew_score_comment %>%\n  filter(str_detect(reply, word)) %>%\n  count(sentiment)\n\n# A tibble: 3 × 2\n  sentiment     n\n  <chr>     <int>\n1 neg           5\n2 neu          56\n3 pos          84\n\n# 감정 범주별 주요 단어 살펴보기\n# 1. 두 글자 이상 한글 단어만 남기고 단어 빈도 구하기\n# 토큰화 및 전처리\nnew_comment <- new_score_comment %>%\n  unnest_tokens(input = reply,\n                output = word,\n                token = \"words\",\n                drop = F) %>%\n  filter(str_detect(word, \"[가-힣]\") &\n           str_count(word) >= 2)\n\n# 감정 및 단어별 빈도 구하기\nnew_frequency_word <- new_comment %>%\n  count(sentiment, word, sort = T)\n\n# 2. 로그 오즈비 구하기\n# Wide form으로 변환\nnew_comment_wide <- new_frequency_word %>%\n  filter(sentiment != \"neu\") %>%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = list(n = 0))\n\n# 로그 오즈비 구하기\nnew_comment_wide <- new_comment_wide %>%\n  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /\n                                ((neg + 1) / (sum(neg + 1)))))\n\n# 3. 로그 오즈비가 큰 단어로 막대 그래프 만들기\nnew_top10 <- new_comment_wide %>%\n  group_by(sentiment = ifelse(log_odds_ratio > 0, \"pos\", \"neg\")) %>%\n  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)\n\nggplot(new_top10, aes(x = reorder(word, log_odds_ratio),\n                      y = log_odds_ratio,\n                      fill = sentiment)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL) \n\n\n\n# 4. 주요 단어가 사용된 댓글 살펴보기\n# 긍정 댓글 원문\nnew_score_comment %>%\n  filter(sentiment == \"pos\" & str_detect(reply, \"축하\")) %>%\n  select(reply)\n\n# A tibble: 189 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼!! 나의 일인 양 행복합니다…\n 2 와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요 진심… \n 3 우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 영감을 주시…\n 4 아카데미나 다른 상이나 지들만의 잔치지~ 난 대한민국에서 받는 상이 제일 가치 …\n 5 정부에 빨대 꼽은 정치시민단체 기생충들이 득실거리는 떼한민국애서 훌륭한 영화…\n 6 대단해요 나는 안봤는데 그렇게 잘 만들어 한국인의 기백을 세계에 알리는 큰 일… \n 7 나한테 돌아오는게 하나도 없는데 왜이렇게 자랑스럽지?ㅎㅎㅎ 축하 합니다~작품… \n 8 한국영화 100년사에 한횟을 긋네요. 정말 축하 합니다                           \n 9 와 대단하다 진짜 축하드려요!!! 대박 진짜                                     \n10 각본상, 국제 영화상 수상 축하. 편집상은 꽝남.                                \n# … with 179 more rows\n\nnew_score_comment %>%\n  filter(sentiment == \"pos\" & str_detect(reply, \"소름\")) %>%\n  select(reply)\n\n# A tibble: 77 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 생중계 보며 봉준호 할 때 소름이~~~!! ㅠㅠ 수상소감들으며 함께 가슴이 벅차네… \n 2 와 보다가 소름 짝 수고들하셨어요                                             \n 3 대단하다!! 봉준호 이름 나오자마자 소름                                       \n 4 와우 브라보~ 키아누리브스의 봉준호, 순간 소름이.. 멋지십니다.                \n 5 소름 돋네요. 축하합니다                                                      \n 6 소름.... 기생충 각본집 산거 다시한번 잘했다는 생각이ㅠㅠㅠ 축하해요!!!!!!    \n 7 봉준호 아저씨 우리나라 자랑입니다 헐리웃 배우들과 화면에 같이 비춰지는게 아… \n 8 추카해요. 봉준호하는데 막 완전 소름 돋았어요.                                \n 9 소름돋아서 닭살돋고.. 그냥 막 감동이라 눈물이 나려했어요.. 대단하고 자랑스럽…\n10 한국 영화 최초 아카데미상 수상, 92년 역사의 국제 장편 영화상과 최우수작품상 …\n# … with 67 more rows\n\n# 부정 댓글 원문\nnew_score_comment %>%\n  filter(sentiment == \"neg\" & str_detect(reply, \"좌빨\")) %>%\n  select(reply)\n\n# A tibble: 34 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 자칭 보수들은 분노의 타이핑중 ㅋㅋㅋㅋㅋㅋ전세계를 좌빨로 몰수는 없고 자존심…\n 2 자칭보수 왈 : 미국에 로비했다 ㅋㅋ좌빨영화가 상받을리 없다 ㅋㅋㅋㅋㅋㅋㅋ 본…\n 3 좌빨 봉준호 영화는 쳐다도 안본다 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ                      \n 4 봉준호 그렇게 미국 싫어하는데 상은 쳐 받으러 가는 좌빨 수준ㅋㅋㅋ            \n 5 좌빨 기생충들은 댓글도 달지마라 미국 영화제 수상이 니들하고 뭔상관인데.      \n 6 얘들은 왜 인정을 안하냐? ㅋㅋ 니들 이미 변호인 찍을대 부터 송강호 욕해대고 … \n 7 넷상 보수들 만큼 이중적인 새1끼들 없음. 봉준호 송강호 보고 종북좌빨 홍어드립…\n 8 우선 축하합니다.그리고 다음에는 조씨가족을 모델로한 뻔뻔하고 거짓말을 밥 먹… \n 9 Korea 대단합니다 김연아 방탄 봉준호 스포츠 음악 영화 못하는게 없어요 좌빨 감…\n10 좌빨 감독이라고 블랙리스트에 올랐던 사람을 세계인이 인정해주네. 방구석에 앉… \n# … with 24 more rows\n\nnew_score_comment %>%\n  filter(sentiment == \"neg\" & str_detect(reply, \"못한\")) %>%\n  select(reply)\n\n# A tibble: 7 × 1\n  reply                                                                         \n  <chr>                                                                         \n1 한번도경험하지. 못한 조국가족사기단기생충. 개봉박두                           \n2 여기서 정치얘기하는 건 학창시절 공부 못한 거 인증하는 꼴... 주제좀 벗어나지 … \n3 이 기사를 반문으로 먹고 사는 자유왜국당과, mb아바타 간철수 댓글알바들이 매우 …\n4 한국미국일본 vs 주적북한,중국러시아 이 구도인 현 시대 상황 속에서, 미국 일본… \n5 친일수꼴 들과 자한당넘들이 나라에 경사만 있으면 엄청 싫어합니다, 맨날 사고만 …\n6 각본상,국제상,감독상 ...어디서 듣도보도 못한 아차상 같은 쩌리처리용 상 아닌가…\n7 난 밥을 먹고 기생충은 오스카를 먹다, 기생충은 대한민국의 국격을 높였는데 난 … \n\n# 5. 분석 결과 비교하기\n# 수정한 감정 사전 활용\nnew_top10 %>%\n  select(-pos, -neg) %>%\n  arrange(-log_odds_ratio)\n\n# A tibble: 20 × 3\n# Groups:   sentiment [2]\n   word       log_odds_ratio sentiment\n   <chr>               <dbl> <chr>    \n 1 축하                 3.88 pos      \n 2 멋지다               3.76 pos      \n 3 소름                 3.76 pos      \n 4 대단한               3.59 pos      \n 5 대단하다             3.49 pos      \n 6 좋은                 3.48 pos      \n 7 자랑스럽다           3.40 pos      \n 8 최고                 3.09 pos      \n 9 세계적인             2.94 pos      \n10 최고의               2.90 pos      \n11 닭그네              -1.89 neg      \n12 못하고              -1.89 neg      \n13 사회적              -1.89 neg      \n14 싫다                -1.89 neg      \n15 가난한              -2.07 neg      \n16 모르는              -2.07 neg      \n17 아쉽다              -2.07 neg      \n18 좌좀                -2.22 neg      \n19 못한                -2.36 neg      \n20 좌빨                -2.68 neg      \n\n# 원본 감정 사전 활용\ntop10 %>%\n  select(-pos, -neg) %>%\n  arrange(-log_odds_ratio)\n\n# A tibble: 20 × 3\n# Groups:   sentiment [2]\n   word       log_odds_ratio sentiment\n   <chr>               <dbl> <chr>    \n 1 축하                 3.95 pos      \n 2 멋지다               3.81 pos      \n 3 대단한               3.66 pos      \n 4 좋은                 3.55 pos      \n 5 대단하다             3.52 pos      \n 6 자랑스럽다           3.46 pos      \n 7 최고                 3.12 pos      \n 8 세계적인             3.01 pos      \n 9 최고의               2.97 pos      \n10 위대한               2.92 pos      \n11 닭그네              -1.82 neg      \n12 가난한              -2.00 neg      \n13 모르는              -2.00 neg      \n14 아쉽다              -2.00 neg      \n15 소름이              -2.08 neg      \n16 좌좀                -2.16 neg      \n17 못한                -2.29 neg      \n18 미친                -2.29 neg      \n19 좌빨                -2.61 neg      \n20 소름                -3.03 neg      \n\n# 수정 감정 사전 활용 시 \"미친\"이 목록에서 사라짐, 로그 오즈비가 10위 안에 들지 못할 정도로 낮아지기 때문\nnew_comment_wide %>%\n  filter(word == \"미친\")\n\n# A tibble: 1 × 4\n  word    pos   neg log_odds_ratio\n  <chr> <int> <int>          <dbl>\n1 미친      7     0           1.80"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_3.html",
    "href": "teaching/media_ds/about/NLP_3.html",
    "title": "NLP",
    "section": "",
    "text": "동시 출현 단어 분석\n\n# 기본적인 전처리\n# 기생충 기사 댓글 불러오기\nraw_news_comment <- readr::read_csv(\"data/news_comment_parasite.csv\")\n\nRows: 4150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): reply, press, title, url\ndttm (1): reg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 전처리\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(textclean)\nnews_comment <- raw_news_comment %>%\n  select(reply) %>%\n  mutate(reply = str_replace_all(reply, \"[^가-힣]\", \" \"),\n         reply = str_squish(reply),\n         id = row_number())\n\n# 토큰화하기\n# 1. 형태소 분석기를 이용해 품사 기준으로 토큰화하기\nlibrary(tidytext)\nlibrary(KoNLP)\n\nChecking user defined dictionary!\n\ncomment_pos <- news_comment %>%\n  unnest_tokens(input = reply,\n                output = word,\n                token = SimplePos22,\n                drop = F)\n\ncomment_pos %>%\n  select(reply, word)\n\n# A tibble: 39,956 × 2\n   reply                                                                   word \n   <chr>                                                                   <chr>\n 1 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 정말…\n 2 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 우리…\n 3 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 집/n…\n 4 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 좋/p…\n 5 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 일/n…\n 6 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 생기…\n 7 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 기쁘…\n 8 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 행복…\n 9 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 것/n…\n10 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복합… 나/n…\n# … with 39,946 more rows\n\n# 2. 품사 분리하여 행 구성하기\nlibrary(tidyr)\ncomment_pos <- comment_pos %>%\n  separate_rows(word, sep = \"[+]\")\n\ncomment_pos %>%\n  select(word, reply)\n\n# A tibble: 70,553 × 2\n   word    reply                                                                \n   <chr>   <chr>                                                                \n 1 정말/ma 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 2 우리/np 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 3 집/nc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 4 에/jc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 5 좋/pa   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 6 은/et   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 7 일/nc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 8 이/jc   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 9 생기/pv 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n10 어/ec   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n# … with 70,543 more rows\n\n# 3. 품사 추출하기\n# 명사 추출하기\nnoun <- comment_pos %>%\n  filter(str_detect(word, \"/n\")) %>%\n  mutate(word = str_remove(word, \"/.*$\"))\n\nnoun %>%\n  select(word, reply)\n\n# A tibble: 27,457 × 2\n   word   reply                                                                \n   <chr>  <chr>                                                                \n 1 우리   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 2 집     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 3 일     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 4 행복한 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 5 것     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 6 나     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 7 일     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 8 양     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n 9 행복   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n10 행복   정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 행복…\n# … with 27,447 more rows\n\n# 명사 빈도 구하기\nnoun %>%\n  count(word, sort = T)\n\n# A tibble: 8,069 × 2\n   word         n\n   <chr>    <int>\n 1 영화       463\n 2 기생충     445\n 3 봉준호     372\n 4 것         353\n 5 아카데미   252\n 6 축하       232\n 7 나         230\n 8 대한민국   226\n 9 자랑       222\n10 작품상     218\n# … with 8,059 more rows\n\n# 동사, 형용사 추출하기\npvpa <- comment_pos %>%\n  filter(str_detect(word, \"/pv|/pa\")) %>% # \"/pv\", \"/pa\" 추출\n  mutate(word = str_replace(word, \"/.*$\", \"다\")) # \"/\"로 시작 문자를 \"다\"로 바꾸기\n\npvpa %>%\n  select(word, reply)\n\n# A tibble: 5,317 × 2\n   word       reply                                                             \n   <chr>      <chr>                                                             \n 1 좋다       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 2 생기다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 3 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 4 축하드리다 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 5 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 … \n 6 기쁘다     와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려… \n 7 기쁘다     와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려… \n 8 축하드리다 와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려… \n 9 불다       우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 …\n10 크다       우리나라의 영화감독분들 그리고 앞으로 그 꿈을 그리는 분들에게 큰 …\n# … with 5,307 more rows\n\n# 품사 결합\ncomment <- bind_rows(noun, pvpa) %>%\n  filter(str_count(word) >= 2) %>%\n  arrange(id)\n\ncomment %>%\n  select(word, reply)\n\n# A tibble: 26,860 × 2\n   word       reply                                                            \n   <chr>      <chr>                                                            \n 1 우리       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 2 행복한     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 3 행복       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 4 행복       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 5 좋다       정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 6 생기다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 7 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 8 축하드리다 정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n 9 기쁘다     정말 우리 집에 좋은 일이 생겨 기쁘고 행복한 것처럼 나의 일인 양 …\n10 시국       와 너무 기쁘다 이 시국에 정말 내 일같이 기쁘고 감사하다 축하드려…\n# … with 26,850 more rows\n\n# ***명사, 동사, 형용사를 한 번에 추출하기\ncomment_new <- comment_pos %>%\n  separate_rows(word, sep = \"[+]\") %>%\n  filter(str_detect(word, \"/n|/pv|/pa\")) %>%\n  mutate(word = ifelse(str_detect(word, \"/pv|/pa\"),\n                       str_replace(word, \"/.*$\", \"다\"),\n                       str_remove(word, \"/.*$\"))) %>%\n  filter(str_count(word) >= 2) %>%\n  arrange(id)\n\n# 단어 동시 출현 빈도 구하기\n# install.packages(\"widyr\")\nlibrary(widyr)\npair <- comment %>%\n  pairwise_count(item = word,\n                 feature = id,\n                 sort = T)\npair\n\n# A tibble: 245,920 × 3\n   item1      item2      n\n   <chr>      <chr>  <dbl>\n 1 영화       기생충   111\n 2 기생충     영화     111\n 3 감독       봉준호    86\n 4 봉준호     감독      86\n 5 감독님     봉준호    66\n 6 봉준호     감독님    66\n 7 만들다     영화      57\n 8 영화       만들다    57\n 9 기생충     봉준호    54\n10 블랙리스트 감독      54\n# … with 245,910 more rows\n\n# 특정 단어와 자주 함께 사용된 단어 살펴보기\npair %>% filter(item1 == \"영화\")\n\n# A tibble: 2,313 × 3\n   item1 item2        n\n   <chr> <chr>    <dbl>\n 1 영화  기생충     111\n 2 영화  만들다      57\n 3 영화  봉준호      52\n 4 영화  받다        48\n 5 영화  한국        46\n 6 영화  아카데미    42\n 7 영화  같다        41\n 8 영화  감독        39\n 9 영화  아니다      38\n10 영화  좋다        35\n# … with 2,303 more rows\n\npair %>% filter(item1 == \"봉준호\")\n\n# A tibble: 1,579 × 3\n   item1  item2          n\n   <chr>  <chr>      <dbl>\n 1 봉준호 감독          86\n 2 봉준호 감독님        66\n 3 봉준호 기생충        54\n 4 봉준호 영화          52\n 5 봉준호 블랙리스트    48\n 6 봉준호 대한민국      38\n 7 봉준호 자랑          33\n 8 봉준호 축하드리다    30\n 9 봉준호 송강호        30\n10 봉준호 축하          25\n# … with 1,569 more rows\n\n\n\n\n동시 출현 네트워크\n\n# 네트워크 그래프 데이터 만들기\n# install.packages(\"tidygraph\")\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ngraph_comment <- pair %>%\n  filter(n >= 25) %>%\n  as_tbl_graph()\ngraph_comment \n\n# A tbl_graph: 30 nodes and 108 edges\n#\n# A directed simple graph with 2 components\n#\n# Node Data: 30 × 1 (active)\n  name  \n  <chr> \n1 영화  \n2 기생충\n3 감독  \n4 봉준호\n5 감독님\n6 만들다\n# … with 24 more rows\n#\n# Edge Data: 108 × 3\n   from    to     n\n  <int> <int> <dbl>\n1     1     2   111\n2     2     1   111\n3     3     4    86\n# … with 105 more rows\n\n# 네트워크 그래프 만들기\n# install.packages(\"ggraph\")\nlibrary(ggraph)\n\nLoading required package: ggplot2\n\nggraph(graph_comment) +\n  geom_edge_link() + # 엣지\n  geom_node_point() + # 노드\n  geom_node_text(aes(label = name)) # 텍스트\n\nUsing \"stress\" as default layout\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n# 그래프 다듬기\n# 한글 폰트 설정\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nfont_add_google(name = \"Nanum Gothic\", family = \"nanumgothic\")\nshowtext_auto()\n\n# 엣지와 노드의 색깔, 크기, 텍스트 위치 수정\nset.seed(1234) # 난수 고정\n\nggraph(graph_comment, layout = \"fr\") + # 레이아웃\n  geom_edge_link(color = \"gray50\", # 엣지 색깔\n                 alpha = 0.5) + # 엣지 명암\n  geom_node_point(color = \"lightcoral\", # 노드 색깔\n                  size = 5) + # 노드 크기\n  geom_node_text(aes(label = name), # 텍스트 표시\n                 repel = T, # 노드밖 표시\n                 size = 5, # 텍스트 크기\n                 family = \"nanumgothic\") + # 폰트\n  theme_graph() # 배경 삭제\n\n\n\n# 네트워크 그래프 함수 만들기\nword_network <- function(x) {\n  ggraph(x, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\",\n                   alpha = 0.5) +\n    geom_node_point(color = \"lightcoral\",\n                    size = 5) +\n    geom_node_text(aes(label = name),\n                   repel = T,\n                   size = 5,\n                   family = \"nanumgothic\") +\n    theme_graph()\n}\n\nset.seed(1234)\nword_network(graph_comment)\n\n\n\n# 유의어 처리하기\ncomment <- comment %>%\n  mutate(word = ifelse(str_detect(word, \"감독\") &\n                         !str_detect(word, \"감독상\"), \"봉준호\", word),\n         word = ifelse(word == \"오르다\", \"올리다\", word),\n         word = ifelse(str_detect(word, \"축하\"), \"축하\", word))\n\n# 단어 동시 출현 빈도 구하기\npair <- comment %>%\n  pairwise_count(item = word,\n                 feature = id,\n                 sort = T)\n\n# 네트워크 그래프 데이터 만들기\ngraph_comment <- pair %>%\n  filter(n >= 25) %>%\n  as_tbl_graph()\n\n# 네트워크 그래프 만들기\nset.seed(1234)\nword_network(graph_comment)\n\n\n\n# 연결중심성과 커뮤니티 표현하기\n# 1. 네트워크 그래프 데이터에 연결 중심성, 커뮤니티 변수 추가하기\nset.seed(1234)\ngraph_comment <- pair %>%\n  filter(n >= 25) %>%\n  as_tbl_graph(directed = F) %>%\n  mutate(centrality = centrality_degree(), # 연결 중심성\n         group = as.factor(group_infomap())) # 커뮤니티\ngraph_comment\n\n# A tbl_graph: 36 nodes and 152 edges\n#\n# An undirected multigraph with 1 component\n#\n# Node Data: 36 × 3 (active)\n  name       centrality group\n  <chr>           <dbl> <fct>\n1 봉준호             62 4    \n2 축하               34 2    \n3 영화               26 3    \n4 블랙리스트          6 6    \n5 기생충             26 1    \n6 대한민국           10 3    \n# … with 30 more rows\n#\n# Edge Data: 152 × 3\n   from    to     n\n  <int> <int> <dbl>\n1     1     2   198\n2     1     2   198\n3     1     3   119\n# … with 149 more rows\n\n# 2. 네트워크 그래프에 연결 중심성, 커뮤니티 표현하기\nset.seed(1234)\nggraph(graph_comment, layout = \"fr\") + # 레이아웃\n  geom_edge_link(color = \"gray50\", # 엣지 색깔\n                 alpha = 0.5) + # 엣지 명암\n  geom_node_point(aes(size = centrality, # 노드 크기\n                      color = group), # 노드 색깔\n                  show.legend = F) + # 범례 삭제\n  scale_size(range = c(5, 15)) + # 노드 크기 범위\n  geom_node_text(aes(label = name), # 텍스트 표시\n                 repel = T, # 노드밖 표시\n                 size = 5) + # 폰트\n  theme_graph() # 배경 삭제\n\n\n\n# 3. 네트워크의 주요 단어 살펴보기\ngraph_comment %>%\n  filter(name == \"봉준호\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# An unrooted tree\n#\n# Node Data: 1 × 3 (active)\n  name   centrality group\n  <chr>       <dbl> <fct>\n1 봉준호         62 4    \n#\n# Edge Data: 0 × 3\n# … with 3 variables: from <int>, to <int>, n <dbl>\n\n# 같은 커뮤니티로 분류된 단어 살펴보기\ngraph_comment %>%\n  filter(group == 4) %>%\n  arrange(-centrality) %>%\n  data.frame()\n\n    name centrality group\n1 봉준호         62     4\n2   받다         10     4\n3   자랑          6     4\n4 만들다          4     4\n\n# 연결 중심성이 높은 주요 단어 살펴보기\ngraph_comment %>%\n  arrange(-centrality)\n\n# A tbl_graph: 36 nodes and 152 edges\n#\n# An undirected multigraph with 1 component\n#\n# Node Data: 36 × 3 (active)\n  name     centrality group\n  <chr>         <dbl> <fct>\n1 봉준호           62 4    \n2 축하             34 2    \n3 영화             26 3    \n4 기생충           26 1    \n5 작품상           14 5    \n6 대한민국         10 3    \n# … with 30 more rows\n#\n# Edge Data: 152 × 3\n   from    to     n\n  <int> <int> <dbl>\n1     1     2   198\n2     1     2   198\n3     1     3   119\n# … with 149 more rows\n\n# \"2번\" 커뮤니티로 분류된 단어\ngraph_comment %>%\n  filter(group == 2) %>%\n  arrange(-centrality) %>%\n  data.frame()\n\n    name centrality group\n1   축하         34     2\n2   좋다          8     2\n3   진심          4     2\n4   수상          4     2\n5   없다          4     2\n6   대단          2     2\n7 기쁘다          2     2\n\n# 4. 주요 단어가 사용된 원문 살펴보기\nnews_comment %>%\n  filter(str_detect(reply, \"봉준호\") & str_detect(reply, \"대박\")) %>%\n  select(reply)\n\n# A tibble: 19 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 대박 대박 진짜 대박 봉준호 감독님과 우리 배우들 너무 다랑스러워요            \n 2 내가 죽기전에 아카데미에서 한국어를 들을줄이야 봉준호대박 기생충대박         \n 3 대박 관왕이라니 축하합니다 봉준호를 배출한 충무로 그리고 문화강국 대한 민국  \n 4 우와 대박 진자 대단하다 봉준호                                               \n 5 봉준호 경사났네 대박중에 대에박 축하합니다                                   \n 6 봉준호 작품상 탔다 대박                                                      \n 7 봉준호 군대 면제시켜도될듯 대박 여윽시 위대한 한국에는 위대한 봉준호 형님이 …\n 8 아니 다른상을 받은것도 충분히 대단하고 굉장하지만 최고의 영예인 작품상을 받… \n 9 봉준호 군대 면제시켜도될듯 대박 여윽시 위대한 한국에는 위대한 봉준호 형님이 …\n10 봉준호감독님대박 축하합니다                                                  \n11 와 봉준호 대박 축하드려요                                                    \n12 대박이다 감격의 한해입니다 봉준호 감독님 정말 축하드립니다                   \n13 좌파영화인 봉준호가 좌파영화로 아카데미 작품상 대박 배 아프겠다 안보겄다던 … \n14 각본상 외국여영화상 수상 대박입니다 축하하고 잠시후에 봉준호 감독상과 영어 … \n15 한국 역사상 최초인 오스카상 관왕 진짜 대박 대한민국 위상과 국격을 세계인들에…\n16 미쳣다 감독상은 진짜 예상못햇는데 마틴 스콜쎄지 퀜틴타란티노 스티븐스필버그… \n17 봉준호감독 짱 가 대박났네                                                    \n18 와 대박 소름돋아 으악 봉준호 감독님 너무너무 축하드려요                      \n19 와 진짜 대박이다 봉준호 언젠가 정말 세계적으로 인정 받는 날이 올줄은 알았지… \n\nnews_comment %>%\n  filter(str_detect(reply, \"박근혜\") & str_detect(reply, \"블랙리스트\")) %>%\n  select(reply)\n\n# A tibble: 63 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 일베와 자한당이 싫어하는 봉준호 감독이 아카데미에서 상받으니 쪽바리들처럼 엄…\n 2 박근혜 블랙리스트 로 낙인찍은 봉준호 감독님이 아시아 최초로 오스카에서 상을 …\n 3 우리나라에서만 좌파다 빨갱이다 라고 비하함 박근혜 때 이런 세계적 감독을 블랙…\n 4 박근혜 최순실 블랙리스트에 오른 훌륭하신 감독님 축하합니다                   \n 5 박근혜정부가 얼마나 썩고 무능했냐면 각종 영화제에서 최고상 수상을 받는 유능… \n 6 넷상 보수들 만큼 이중적인 새 끼들 없음 봉준호 송강호 보고 종북좌빨 홍어드립 …\n 7 박근혜 자한당 독재시절 봉준호 송강호를 블랙리스트 올려놓고 활동 방해 감시하… \n 8 대단합니다 김연아 방탄 봉준호 스포츠 음악 영화 못하는게 없어요 좌빨 감독이라…\n 9 송강호 봉준호 박근혜 이명박 시절 블랙리스트 이제 어떻게 깔려구               \n10 이명박근혜정권당시 좌파감독이라고 블랙리스트까지 올랏던 봉준호 역사적위업을 …\n# … with 53 more rows\n\nnews_comment %>%\n  filter(str_detect(reply, \"기생충\") & str_detect(reply, \"조국\")) %>%\n  select(reply)\n\n# A tibble: 64 × 1\n   reply                                                                        \n   <chr>                                                                        \n 1 조국이가 받아야 한다 기생충 스토리 제공                                      \n 2 한번도경험하지 못한 조국가족사기단기생충 개봉박두                            \n 3 와 조국 가족 사기단 부제 기생충 최고                                         \n 4 문재인과 조국 기생충 리얼                                                    \n 5 기생충은 좌좀 조국 가족을 패러디한 영화라서 우파들도 열광하고 있는 것이다 같…\n 6 조국 가족이 기생충 영화를 꼭 봐야되는데                                      \n 7 좌파 인생영화인데 좌파 기생충들에게 이 상을 받쳐라 조국 서울대 문서위조학과 …\n 8 기생충 조국 봉준호 만세                                                      \n 9 봉준호감독님 글로벌 영화계 큰상수상을 진심으로 축하합니다 다만 기생충 작품은…\n10 기생충보면서 조국생각난사람 나쁜일라나 봉준호 감독님이 현 시대를 참 잘 반영… \n# … with 54 more rows\n\n\n\n\n단어 상관 분석\n\n# 파이 계수 구하기\nword_cors <- comment %>%\n  add_count(word) %>%\n  filter(n >= 20) %>%\n  pairwise_cor(item = word,\n               feature = id,\n               sort = T)\nword_cors\n\n# A tibble: 26,732 × 3\n   item1      item2      correlation\n   <chr>      <chr>            <dbl>\n 1 올리다     블랙리스트       0.478\n 2 블랙리스트 올리다           0.478\n 3 역사       쓰다             0.370\n 4 쓰다       역사             0.370\n 5 박근혜     블랙리스트       0.322\n 6 블랙리스트 박근혜           0.322\n 7 가족       조국             0.306\n 8 조국       가족             0.306\n 9 작품상     감독상           0.276\n10 감독상     작품상           0.276\n# … with 26,722 more rows\n\n# 특정 단어와 관련성이 큰 단어 살펴보기\nword_cors %>%\n  filter(item1 == \"대한민국\")\n\n# A tibble: 163 × 3\n   item1    item2  correlation\n   <chr>    <chr>        <dbl>\n 1 대한민국 국민        0.182 \n 2 대한민국 자랑        0.158 \n 3 대한민국 위상        0.149 \n 4 대한민국 국격        0.129 \n 5 대한민국 위대한      0.100 \n 6 대한민국 세계        0.0910\n 7 대한민국 문화        0.0757\n 8 대한민국 감사합      0.0724\n 9 대한민국 나라        0.0715\n10 대한민국 오늘        0.0715\n# … with 153 more rows\n\nword_cors %>%\n  filter(item1 == \"역사\")\n\n# A tibble: 163 × 3\n   item1 item2    correlation\n   <chr> <chr>          <dbl>\n 1 역사  쓰다          0.370 \n 2 역사  최초          0.117 \n 3 역사  한국          0.0982\n 4 역사  순간          0.0910\n 5 역사  한국영화      0.0821\n 6 역사  아니다        0.0774\n 7 역사  감사          0.0654\n 8 역사  영광          0.0640\n 9 역사  영화제        0.0596\n10 역사  오스카        0.0593\n# … with 153 more rows\n\n# 파이 계수로 막대 그래프 만들기\n# 1. 관심 단어별로 파이 계수가 큰 단어 추출하기\n# 관심 단어 목록 생성\ntarget <- c(\"대한민국\", \"역사\", \"수상소감\", \"조국\", \"박근혜\", \"블랙리스트\")\ntop_cors <- word_cors %>%\n  filter(item1 %in% target) %>%\n  group_by(item1) %>%\n  slice_max(correlation, n = 8)\n\n# 2. 막대 그래프 만들기\n# 그래프 순서 정하기\ntop_cors$item1 <- factor(top_cors$item1, levels = target)\nlibrary(ggplot2)\nggplot(top_cors, aes(x = reorder_within(item2, correlation, item1),\n                     y = correlation,\n                     fill = item1)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(x = NULL) +\n  theme(text = element_text(family = \"nanumgothic\"))\n\n\n\n# 파이 계수로 네트워크 그래프 만들기\n# 1. 네트워크 그래프 데이터 만들기, 연결 중심성과 커뮤니티 추가하기\nset.seed(1234)\ngraph_cors <- word_cors %>%\n  filter(correlation >= 0.15) %>%\n  as_tbl_graph(directed = F) %>%\n  mutate(centrality = centrality_degree(),\n         group = as.factor(group_infomap()))\n\n# 2. 네트워크 그래프 만들기\nset.seed(1234)\nggraph(graph_cors, layout = \"fr\") +\n  geom_edge_link(color = \"gray50\",\n                 aes(edge_alpha = correlation, # 엣지 명암\n                     edge_width = correlation), # 엣지 두께\n                 show.legend = F) + # 범례 삭제\n  scale_edge_width(range = c(1, 4)) + # 엣지 두께 범위\n  geom_node_point(aes(size = centrality,\n                      color = group),\n                  show.legend = F) +\n  scale_size(range = c(5, 10)) +\n  geom_node_text(aes(label = name),\n                 repel = T,\n                 size = 5,\n                 family = \"nanumgothic\") +\n  theme_graph()\n\n\n\n\n\n\n연이어 사용된 단어쌍\n\n# 엔그램으로 토큰화하기\ntext <- tibble(value = \"대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\")\ntext\n\n# A tibble: 1 × 1\n  value                                                                        \n  <chr>                                                                        \n1 대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민…\n\n# 바이그램 토큰화\ntext %>%\n  unnest_tokens(input = value,\n                output = word,\n                token = \"ngrams\",\n                n = 2)\n\n# A tibble: 9 × 1\n  word                     \n  <chr>                    \n1 대한민국은 민주공화국이다\n2 민주공화국이다 대한민국의\n3 대한민국의 주권은        \n4 주권은 국민에게          \n5 국민에게 있고            \n6 있고 모든                \n7 모든 권력은              \n8 권력은 국민으로부터      \n9 국민으로부터 나온다      \n\n# 트라이그램 토큰화\ntext %>%\n  unnest_tokens(input = value,\n                output = word,\n                token = \"ngrams\",\n                n = 3)\n\n# A tibble: 8 × 1\n  word                                \n  <chr>                               \n1 대한민국은 민주공화국이다 대한민국의\n2 민주공화국이다 대한민국의 주권은    \n3 대한민국의 주권은 국민에게          \n4 주권은 국민에게 있고                \n5 국민에게 있고 모든                  \n6 있고 모든 권력은                    \n7 모든 권력은 국민으로부터            \n8 권력은 국민으로부터 나온다          \n\n# 유니그램 토큰화 = 단어 기준 토큰화\ntext %>%\n  unnest_tokens(input = value,\n                output = word,\n                token = \"ngrams\",\n                n = 1)\n\n# A tibble: 10 × 1\n   word          \n   <chr>         \n 1 대한민국은    \n 2 민주공화국이다\n 3 대한민국의    \n 4 주권은        \n 5 국민에게      \n 6 있고          \n 7 모든          \n 8 권력은        \n 9 국민으로부터  \n10 나온다        \n\n# 기사 댓글로 바이그램 만들기\n# 1. 명사, 동사, 형용사 추출하기\ncomment_new <- comment_pos %>%\n  separate_rows(word, sep = \"[+]\") %>%\n  filter(str_detect(word, \"/n|/pv|/pa\")) %>%\n  mutate(word = ifelse(str_detect(word, \"/pv|/pa\"),\n                       str_replace(word, \"/.*$\", \"다\"),\n                       str_remove(word, \"/.*$\"))) %>%\n  filter(str_count(word) >= 2) %>%\n  arrange(id)\n\n# 2. 유의어 처리하기\ncomment_new <- comment_new %>%\n  mutate(word = ifelse(str_detect(word, \"감독\") &\n                         !str_detect(word, \"감독상\"), \"봉준호\", word),\n         word = ifelse(word == \"오르다\", \"올리다\", word),\n         word = ifelse(str_detect(word, \"축하\"), \"축하\", word))\n\n# 3. 한 댓글이 하나의 행이 되도록 결합하기\ncomment_new %>%\n  select(word)\n\n# A tibble: 26,860 × 1\n   word  \n   <chr> \n 1 우리  \n 2 좋다  \n 3 생기다\n 4 기쁘다\n 5 행복한\n 6 행복  \n 7 축하  \n 8 행복  \n 9 기쁘다\n10 기쁘다\n# … with 26,850 more rows\n\nline_comment <- comment_new %>%\n  group_by(id) %>%\n  summarise(sentence = paste(word, collapse = \" \"))\n\nline_comment\n\n# A tibble: 4,007 × 2\n      id sentence                                                               \n   <int> <chr>                                                                  \n 1     1 우리 좋다 생기다 기쁘다 행복한 행복 축하 행복 기쁘다                   \n 2     2 기쁘다 시국 기쁘다 감사하다 축하 진심                                  \n 3     3 우리나라 봉준호 불다 크다 영감 봉준호 공동각본쓴 한진 작가님 축하 축하…\n 4     4 봉준호 봉준호 우리나라 대한민국 자랑 세계 어디 우리 한국인 힘내다 삽시 \n 5     5 노벨상 탄느낌이네요 축하                                               \n 6     6 기생충 받다 박수 치다 감독상 기대다 봉준호 봉준호                      \n 7     7 대한민국 영화사 쓰다 계시다                                            \n 8     8 아카데미상 받다 태극기 휘날리다 광해 명량 전부문 휩쓸어야겠            \n 9     9 다시한번 보이다 영화관                                                 \n10    10 대한민국 봉준호 대단 한국의 문화 자긍심 가지                           \n# … with 3,997 more rows\n\n# 4. 바이그램으로 토큰화하기\nbigram_comment <- line_comment %>%\n  unnest_tokens(input = sentence,\n                output = bigram,\n                token = \"ngrams\",\n                n = 2)\nbigram_comment\n\n# A tibble: 23,348 × 2\n      id bigram       \n   <int> <chr>        \n 1     1 우리 좋다    \n 2     1 좋다 생기다  \n 3     1 생기다 기쁘다\n 4     1 기쁘다 행복한\n 5     1 행복한 행복  \n 6     1 행복 축하    \n 7     1 축하 행복    \n 8     1 행복 기쁘다  \n 9     2 기쁘다 시국  \n10     2 시국 기쁘다  \n# … with 23,338 more rows\n\n# 연이어 사용된 단어쌍 빈도 구하기\n# 1. 바이그램 분리하기\nbigram_seprated <- bigram_comment %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\nbigram_seprated\n\n# A tibble: 23,348 × 3\n      id word1  word2 \n   <int> <chr>  <chr> \n 1     1 우리   좋다  \n 2     1 좋다   생기다\n 3     1 생기다 기쁘다\n 4     1 기쁘다 행복한\n 5     1 행복한 행복  \n 6     1 행복   축하  \n 7     1 축하   행복  \n 8     1 행복   기쁘다\n 9     2 기쁘다 시국  \n10     2 시국   기쁘다\n# … with 23,338 more rows\n\n# 2. 단어쌍 빈도 구하기\npair_bigram <- bigram_seprated %>%\n  count(word1, word2, sort = T) %>%\n  na.omit()\npair_bigram\n\n# A tibble: 19,030 × 3\n   word1      word2          n\n   <chr>      <chr>      <int>\n 1 봉준호     봉준호       155\n 2 블랙리스트 올리다        64\n 3 진심       축하          64\n 4 봉준호     축하          57\n 5 봉준호     송강호        34\n 6 영화       만들다        31\n 7 축하       봉준호        31\n 8 대단       축하          27\n 9 봉준호     블랙리스트    27\n10 대박       축하          26\n# … with 19,020 more rows\n\n# 3. 단어쌍 살펴보기\n# 동시 출현 단어쌍\npair %>%\n  filter(item1 == \"대한민국\")\n\n# A tibble: 1,010 × 3\n   item1    item2        n\n   <chr>    <chr>    <dbl>\n 1 대한민국 봉준호      70\n 2 대한민국 축하        54\n 3 대한민국 자랑        44\n 4 대한민국 영화        30\n 5 대한민국 기생충      27\n 6 대한민국 국민        22\n 7 대한민국 세계        16\n 8 대한민국 아카데미    16\n 9 대한민국 위상        15\n10 대한민국 좋다        14\n# … with 1,000 more rows\n\n# 바이그램 단어쌍\npair_bigram %>%\n  filter(word1 == \"대한민국\")\n\n# A tibble: 109 × 3\n   word1    word2      n\n   <chr>    <chr>  <int>\n 1 대한민국 국민      21\n 2 대한민국 자랑      15\n 3 대한민국 영화      11\n 4 대한민국 국격       8\n 5 대한민국 위상       7\n 6 대한민국 만세       6\n 7 대한민국 봉준호     5\n 8 대한민국 문화       4\n 9 대한민국 영광       4\n10 대한민국 기생충     3\n# … with 99 more rows\n\n# 엔그램으로 네트워크 그래프 만들기\n# 네트워크 그래프 데이터 만들기\ngraph_bigram <- pair_bigram %>%\n  filter(n >= 8) %>%\n  as_tbl_graph()\n\n# 네트워크 그래프 만들기\nset.seed(1234)\nword_network(graph_bigram)\n\n\n\n# 유의어 통일하고 네트워크 그래프 다시 만들기\n# 유의어 처리\nbigram_seprated <- bigram_seprated %>%\n  mutate(word1 = ifelse(str_detect(word1, \"대단\"), \"대단\", word1),\n         word2 = ifelse(str_detect(word2, \"대단\"), \"대단\", word2),\n         word1 = ifelse(str_detect(word1, \"자랑\"), \"자랑\", word1),\n         word2 = ifelse(str_detect(word2, \"자랑\"), \"자랑\", word2),\n         word1 = ifelse(str_detect(word1, \"짝짝짝\"), \"짝짝짝\", word1),\n         word2 = ifelse(str_detect(word2, \"짝짝짝\"), \"짝짝짝\", word2)) %>%\n  filter(word1 != word2) # 같은 단어 연속 제거\n\n# 단어쌍 빈도 구하기\npair_bigram <- bigram_seprated %>%\n  count(word1, word2, sort = T) %>%\n  na.omit()\n\n# 네트워크 그래프 데이터 만들기\nset.seed(1234)\ngraph_bigram <- pair_bigram %>%\n  filter(n >= 8) %>%\n  as_tbl_graph(directed = F) %>%\n  mutate(centrality = centrality_degree(), # 중심성\n         group = as.factor(group_infomap())) # 커뮤니티\n\n# 네트워크 그래프 만들기\nset.seed(1234)\nggraph(graph_bigram, layout = \"fr\") + # 레이아웃\n  geom_edge_link(color = \"gray50\", # 엣지 색깔\n                 alpha = 0.5) + # 엣지 명암\n  geom_node_point(aes(size = centrality, # 노드 크기\n                      color = group), # 노드 색깔\n                  show.legend = F) + # 범례 삭제\n  scale_size(range = c(4, 8)) + # 노드 크기 범위\n  geom_node_text(aes(label = name), # 텍스트 표시\n                 repel = T, # 노드밖 표시\n                 size = 5, # 텍스트 크기\n                 family = \"nanumgothic\") + # 폰트\n  theme_graph() # 배경 삭제"
  },
  {
    "objectID": "teaching/media_ds/about/NLP_4.html",
    "href": "teaching/media_ds/about/NLP_4.html",
    "title": "NLP",
    "section": "",
    "text": "LDA Model\n\n# 전처리하기\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nraw_news_comment <- read_csv(\"data/news_comment_parasite.csv\") %>%\n  mutate(id = row_number())\n\nRows: 4150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): reply, press, title, url\ndttm (1): reg_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_news_comment\n\n# A tibble: 4,150 × 6\n   reg_time            reply                             press title url      id\n   <dttm>              <chr>                             <chr> <chr> <chr> <int>\n 1 2020-02-10 16:59:02 \"정말 우리 집에 좋은 일이 생겨 …  MBC   '기…  http…     1\n 2 2020-02-10 13:32:24 \"와 너무 기쁘다! 이 시국에 정말 … SBS   [영…  http…     2\n 3 2020-02-10 12:30:09 \"우리나라의 영화감독분들 그리고 … 한겨… ‘기…  http…     3\n 4 2020-02-10 13:08:22 \"봉준호 감독과 우리나라 대한민국… 한겨… ‘기…  http…     4\n 5 2020-02-10 16:25:41 \"노벨상 탄느낌이네요\\r\\n축하축하… 한겨… ‘기…  http…     5\n 6 2020-02-10 12:31:45 \"기생충 상 받을때 박수 쳤어요.감… 한겨… ‘기…  http…     6\n 7 2020-02-10 12:31:33 \"대한민국 영화사를 새로 쓰고 계…  한겨… ‘기…  http…     7\n 8 2020-02-11 09:20:52 \"저런게 아카데미상 받으면  '태극… 한겨… ‘기…  http…     8\n 9 2020-02-10 20:53:27 \"다시한번 보여주세요 영화관에서 … 한겨… ‘기…  http…     9\n10 2020-02-10 20:22:41 \"대한민국 BTS와함께  봉준호감독…  한겨… ‘기…  http…    10\n# … with 4,140 more rows\n\n# 기본적인 전처리\nlibrary(stringr)\nlibrary(textclean)\nnews_comment <- raw_news_comment %>%\n  mutate(reply = str_replace_all(reply, \"[^가-힣]\", \" \"),\n         reply = str_squish(reply)) %>%\n  distinct(reply, .keep_all = T) %>% # 중복 댓글 제거\n  filter(str_count(reply, boundary(\"word\")) >= 3) # 짧은 문서 제거, 3단어 이상 추출\n\n# 2. 명사 추출하기\nlibrary(tidytext)\nlibrary(KoNLP)\n\nChecking user defined dictionary!\n\n# 명사 추출\ncomment <- news_comment %>%\n  unnest_tokens(input = reply,\n                output = word,\n                token = extractNoun,\n                drop = F) %>%\n  filter(str_count(word) > 1) %>%\n  group_by(id) %>% # 댓글 내 중복 단어 제거\n  distinct(word, .keep_all = T) %>%\n  ungroup() %>%\n  select(id, word) \ncomment\n\n# A tibble: 21,457 × 2\n      id word    \n   <int> <chr>   \n 1     1 우리    \n 2     1 행복    \n 3     2 시국    \n 4     2 감사    \n 5     2 하다    \n 6     2 진심    \n 7     3 우리나라\n 8     3 영화감독\n 9     3 영감    \n10     3 봉감    \n# … with 21,447 more rows\n\n# 3. 빈도 높은 단어 제거하기\ncount_word <- comment %>%\n  add_count(word) %>%\n  filter(n <= 200) %>%\n  select(-n)\n\n# 4. 불용어 제거하기, 유의어 처리하기\n# 불용어, 유의어 확인하기\ncount_word %>%\n  count(word, sort = T) %>%\n  print(n = 200)\n\n# A tibble: 6,022 × 2\n    word             n\n    <chr>        <int>\n  1 작품상         200\n  2 자랑           193\n  3 블랙리스트     173\n  4 조국           170\n  5 한국           165\n  6 대박           148\n  7 세계           140\n  8 수상           135\n  9 미국           128\n 10 들이           123\n 11 정치           108\n 12 역사           102\n 13 오스카         101\n 14 우리나라        96\n 15 감독상          93\n 16 진심            93\n 17 좌파            90\n 18 작품            87\n 19 한국영화        87\n 20 사람            86\n 21 배우            85\n 22 박근혜          84\n 23 국민            80\n 24 하다            80\n 25 최고            79\n 26 호감            79\n 27 우리            78\n 28 문화            75\n 29 생각            71\n 30 수상소감        68\n 31 감사            67\n 32 가족            65\n 33 나라            65\n 34 오늘            63\n 35 시상식          61\n 36 문재인          60\n 37 자랑스럽습니    60\n 38 송강호          59\n 39 소름            57\n 40 정권            54\n 41 각본상          53\n 42 감동            53\n 43 댓글            51\n 44 빨갱이          51\n 45 인정            48\n 46 소식            47\n 47 자한            47\n 48 소감            45\n 49 이미경          44\n 50 하나            43\n 51 한국인          43\n 52 대통령          42\n 53 정부            42\n 54 아카데미상      39\n 55 하게            39\n 56 위상            38\n 57 문재            37\n 58 쾌거            37\n 59 감격            36\n 60 순간            36\n 61 외국            36\n 62 전세계          36\n 63 호가            36\n 64 하면            35\n 65 눈물            34\n 66 보수            34\n 67 와우            34\n 68 현실            34\n 69 기사            33\n 70 영광            33\n 71 영화계          33\n 72 경사            32\n 73 사회            31\n 74 한국의          31\n 75 국제            30\n 76 누구            30\n 77 때문            29\n 78 마지막          29\n 79 얘기            29\n 80 인간            29\n 81 자랑스럽        29\n 82 해서            29\n 83 이번            28\n 84 훌륭            28\n 85 그네            27\n 86 기분            27\n 87 로컬            27\n 88 사람들          27\n 89 영화제          27\n 90 정도            27\n 91 뉴스            26\n 92 왜구            26\n 93 하네            26\n 94 자유            25\n 95 추카            25\n 96 기생            24\n 97 반미            24\n 98 영화상          24\n 99 이야기          24\n100 정경            24\n101 해요            24\n102 내용            23\n103 당신            23\n104 세상            23\n105 수준            23\n106 위대            23\n107 이것            23\n108 일본            23\n109 국위선양        22\n110 니들            22\n111 다시            22\n112 중국            22\n113 진정            22\n114 계획            21\n115 국가            21\n116 네이버          21\n117 숟가락          21\n118 쓰레기          21\n119 왕이            21\n120 재미            21\n121 정신            21\n122 존경            21\n123 행복            21\n124 국격            20\n125 문화계          20\n126 예술            20\n127 코로나          20\n128 하기            20\n129 하지            20\n130 가슴            19\n131 강국            19\n132 사건            19\n133 아시아          19\n134 완전            19\n135 우파            19\n136 중요            19\n137 최초            19\n138 부회장          18\n139 사실            18\n140 소리            18\n141 제작            18\n142 각본            17\n143 발전            17\n144 스텝            17\n145 시절            17\n146 실화            17\n147 올해            17\n148 의미            17\n149 자기            17\n150 자신            17\n151 천재            17\n152 토착            17\n153 한거            17\n154 한번            17\n155 해주            17\n156 그것            16\n157 노벨상          16\n158 다들            16\n159 다음            16\n160 모두            16\n161 박수            16\n162 상상            16\n163 시대            16\n164 어디            16\n165 여기            16\n166 오스카상        16\n167 최우수작품상    16\n168 한국어          16\n169 후보            16\n170 고생            15\n171 기대            15\n172 덕분            15\n173 발표            15\n174 상은            15\n175 예상            15\n176 월드컵          15\n177 응원            15\n178 이해            15\n179 조선            15\n180 한류            15\n181 해외            15\n182 까지            14\n183 대한            14\n184 리스            14\n185 모습            14\n186 바이러스        14\n187 생중계          14\n188 여자            14\n189 예전            14\n190 이거            14\n191 이름            14\n192 장면            14\n193 표현            14\n194 하신            14\n195 한국적          14\n196 한마디          14\n197 황금종려상      14\n198 김연아          13\n199 만큼            13\n200 방탄            13\n# … with 5,822 more rows\n\n# 불용어 목록 만들기\nstopword <- c(\"들이\", \"하다\", \"하게\", \"하면\", \"해서\", \"이번\", \"하네\",\n              \"해요\", \"이것\", \"니들\", \"하기\", \"하지\", \"한거\", \"해주\",\n              \"그것\", \"어디\", \"여기\", \"까지\", \"이거\", \"하신\", \"만큼\")\n\n# 불용어, 유의어 처리하기\ncount_word <- count_word %>%\n  filter(!word %in% stopword) %>%\n  mutate(word = recode(word,\n                       \"자랑스럽습니\" = \"자랑\",\n                       \"자랑스럽\" = \"자랑\",\n                       \"자한\" = \"자유한국당\",\n                       \"문재\" = \"문재인\",\n                       \"한국의\" = \"한국\",\n                       \"그네\" = \"박근혜\",\n                       \"추카\" = \"축하\",\n                       \"정경\" = \"정경심\",\n                       \"방탄\" = \"방탄소년단\"))\n\n# ***불용어 목록을 파일로 만들어 활용하기\n# tibble 구조로 불용어 목록 만들기\nstopword <- tibble(word = c(\"들이\", \"하다\", \"하게\", \"하면\", \"해서\", \"이번\", \"하네\",\n                            \"해요\", \"이것\", \"니들\", \"하기\", \"하지\", \"한거\", \"해주\",\n                            \"그것\", \"어디\", \"여기\", \"까지\", \"이거\", \"하신\", \"만큼\"))\n# 불용어 목록 저장하기\n# readr::write_csv(stopword, \"stopword.csv\")\n\n# 불용어 목록 불러오기\n# stopword <- read_csv(\"stopword.csv\")\n\n# 불용어 제거하기 - filter()\ncount_word <- count_word %>%\n  filter(!word %in% stopword$word)\n\n# 불용어 제거하기 - dplyr::anti_join()\ncount_word <- count_word %>%\n  anti_join(stopword, by = \"word\")\n\n# LDA 모델 만들기\n# 1. DTM 만들기\n# 문서별 단어 빈도 구하기\ncount_word_doc <- count_word %>%\n  count(id, word, sort = T)\ncount_word_doc\n\n# A tibble: 17,592 × 3\n      id word           n\n   <int> <chr>      <int>\n 1    35 한국           2\n 2   206 자랑           2\n 3   566 자랑           2\n 4   578 자랑           2\n 5   598 자랑           2\n 6  1173 한국           2\n 7  1599 한국           2\n 8  1762 한국           2\n 9  2240 한국           2\n10  2307 방탄소년단     2\n# … with 17,582 more rows\n\n# DTM 만들기\n# install.packages(\"tm\")\ndtm_comment <- count_word_doc %>%\n  cast_dtm(document = id, term = word, value = n)\ndtm_comment\n\n<<DocumentTermMatrix (documents: 3203, terms: 5995)>>\nNon-/sparse entries: 17592/19184393\nSparsity           : 100%\nMaximal term length: 35\nWeighting          : term frequency (tf)\n\n# 2. LDA 모델 만들기\n# install.packages(\"topicmodels\")\n\n# 토픽 모델 만들기\nlibrary(topicmodels)\nlda_model <- LDA(dtm_comment,\n                 k = 8,\n                 method = \"Gibbs\",\n                 control = list(seed = 1234))\n\n# 모델 내용 확인\nglimpse(lda_model)\n\nFormal class 'LDA_Gibbs' [package \"topicmodels\"] with 16 slots\n  ..@ seedwords      : NULL\n  ..@ z              : int [1:17604] 8 8 4 3 7 4 3 1 1 1 ...\n  ..@ alpha          : num 6.25\n  ..@ call           : language LDA(x = dtm_comment, k = 8, method = \"Gibbs\", control = list(seed = 1234))\n  ..@ Dim            : int [1:2] 3203 5995\n  ..@ control        :Formal class 'LDA_Gibbscontrol' [package \"topicmodels\"] with 14 slots\n  ..@ k              : int 8\n  ..@ terms          : chr [1:5995] \"한국\" \"자랑\" \"방탄소년단\" \"박근혜\" ...\n  ..@ documents      : chr [1:3203] \"35\" \"206\" \"566\" \"578\" ...\n  ..@ beta           : num [1:8, 1:5995] -7.81 -10.22 -10.25 -5.83 -10.25 ...\n  ..@ gamma          : num [1:3203, 1:8] 0.151 0.15 0.11 0.114 0.11 ...\n  ..@ wordassignments:List of 5\n  .. ..$ i   : int [1:17592] 1 1 1 1 1 1 1 1 1 1 ...\n  .. ..$ j   : int [1:17592] 1 98 99 100 101 102 103 104 105 106 ...\n  .. ..$ v   : num [1:17592] 8 4 3 7 4 3 7 2 8 6 ...\n  .. ..$ nrow: int 3203\n  .. ..$ ncol: int 5995\n  .. ..- attr(*, \"class\")= chr \"simple_triplet_matrix\"\n  ..@ loglikelihood  : num -126429\n  ..@ iter           : int 2000\n  ..@ logLiks        : num(0) \n  ..@ n              : int 17604\n\n\n\n\n토픽별 주요 단어\n\n# 토픽별 단어 확률, beta 추출하기\nterm_topic <- tidy(lda_model, matrix = \"beta\")\nterm_topic\n\n# A tibble: 47,960 × 3\n   topic term       beta\n   <int> <chr>     <dbl>\n 1     1 한국  0.000405 \n 2     2 한국  0.0000364\n 3     3 한국  0.0000353\n 4     4 한국  0.00295  \n 5     5 한국  0.0000353\n 6     6 한국  0.0000356\n 7     7 한국  0.00661  \n 8     8 한국  0.0593   \n 9     1 자랑  0.0181   \n10     2 자랑  0.00440  \n# … with 47,950 more rows\n\n# 토픽별 단어 수\nterm_topic %>%\n  count(topic)\n\n# A tibble: 8 × 2\n  topic     n\n  <int> <int>\n1     1  5995\n2     2  5995\n3     3  5995\n4     4  5995\n5     5  5995\n6     6  5995\n7     7  5995\n8     8  5995\n\n# 토픽 1의 beta 합계\nterm_topic %>%\n  filter(topic == 1) %>%\n  summarise(sum_beta = sum(beta))\n\n# A tibble: 1 × 1\n  sum_beta\n     <dbl>\n1        1\n\n# 특정 단어의 토픽별 확률 살펴보기\nterm_topic %>%\n  filter(term == \"작품상\")\n\n# A tibble: 8 × 3\n  topic term        beta\n  <int> <chr>      <dbl>\n1     1 작품상 0.0000368\n2     2 작품상 0.000763 \n3     3 작품상 0.0000353\n4     4 작품상 0.0000364\n5     5 작품상 0.0000353\n6     6 작품상 0.0695   \n7     7 작품상 0.000727 \n8     8 작품상 0.000388 \n\n# 특정 토픽에서 beta가 높은 단어 살펴보기\nterm_topic %>%\n  filter(topic == 6) %>%\n  arrange(-beta)\n\n# A tibble: 5,995 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     6 작품상   0.0695 \n 2     6 감독상   0.0318 \n 3     6 한국영화 0.0228 \n 4     6 수상     0.0214 \n 5     6 각본상   0.0154 \n 6     6 나라     0.0143 \n 7     6 호감     0.0136 \n 8     6 감격     0.0129 \n 9     6 순간     0.0125 \n10     6 눈물     0.00788\n# … with 5,985 more rows\n\n# 모든 토픽의 주요 단어 살펴보기\nterms(lda_model, 20) %>%\n  data.frame()\n\n    Topic.1 Topic.2 Topic.3    Topic.4    Topic.5  Topic.6    Topic.7\n1      작품    대박    조국       역사       자랑   작품상 블랙리스트\n2      진심  시상식  문재인   우리나라       우리   감독상     박근혜\n3      정치    오늘    가족       세계       최고 한국영화       사람\n4      자랑    국민    문화     오스카       감사     수상     송강호\n5  수상소감    소름  대통령       수상       생각   각본상     이미경\n6      댓글    정치    자랑     빨갱이       소식     나라 자유한국당\n7      외국    배우    때문     영화계   국위선양     호감       정권\n8      경사    계획    인정 아카데미상       감동     감격       소감\n9      훌륭    축하    정부       인간       하나     순간       보수\n10     좌파    위상    강국       얘기 방탄소년단     눈물       인정\n11     왜구    최고    호감       로컬     영화상   전세계     마지막\n12     배우    한번    와우       내용   한국영화     진정       기생\n13     예술    쾌거    사건       좌파       정도   노벨상       하나\n14   전세계    생각    국격       정신       와우     소식     네이버\n15   아시아    중국    고생       의미       조선     기사       한국\n16     호감    다음    덕분       생각       존경     문화     이야기\n17     토착    기분    기대       상상       후보     국제     부회장\n18     발전    왕이    정말       국민       우한     각본     쓰레기\n19   사람들    세상    해도       나라       시대     다들       좌파\n20     수준    자랑    눈물       정부       행복     발표       영광\n        Topic.8\n1          한국\n2          미국\n3        한국인\n4          세계\n5          좌파\n6          배우\n7          감동\n8          누구\n9          사회\n10         자유\n11         현실\n12         영광\n13         위대\n14       영화제\n15         이해\n16         자신\n17 최우수작품상\n18         예상\n19   황금종려상\n20         이유\n\n# 토픽별 주요 단어 시각화하기\n# 1. 토픽별로 beta가 가장 높은 단어 추출하기\n# 토픽별 beta 상위 10개 단어 추출\ntop_term_topic <- term_topic %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10)\n\n# 2. 막대 그래프 만들기\n# install.packages(\"scales\") # restart 알림 발생 시, '아니요' 선택\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nggplot(top_term_topic,\n       aes(x = reorder_within(term, beta, topic),\n           y = beta,\n           fill = factor(topic))) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 4) +\n  coord_flip() +\n  scale_x_reordered() +\n  scale_y_continuous(n.breaks = 4,\n                     labels = number_format(accuracy = .01)) +\n  labs(x = NULL) \n\n\n\n\n\n\n문서를 토픽별로 분류\n\n# 문서별 토픽 확률 gamma 추출하기\ndoc_topic <- tidy(lda_model, matrix = \"gamma\")\ndoc_topic\n\n# A tibble: 25,624 × 3\n   document topic  gamma\n   <chr>    <int>  <dbl>\n 1 35           1 0.151 \n 2 206          1 0.15  \n 3 566          1 0.110 \n 4 578          1 0.114 \n 5 598          1 0.110 \n 6 1173         1 0.110 \n 7 1599         1 0.114 \n 8 1762         1 0.0962\n 9 2240         1 0.125 \n10 2307         1 0.135 \n# … with 25,614 more rows\n\n# gamma 살펴보기\ndoc_topic %>%\n  count(topic)\n\n# A tibble: 8 × 2\n  topic     n\n  <int> <int>\n1     1  3203\n2     2  3203\n3     3  3203\n4     4  3203\n5     5  3203\n6     6  3203\n7     7  3203\n8     8  3203\n\n# 문서 1의 gamma 합계\ndoc_topic %>%\n  filter(document == 1) %>%\n  summarise(sum_gamma = sum(gamma))\n\n# A tibble: 1 × 1\n  sum_gamma\n      <dbl>\n1         1\n\n# 문서별 확률이 가장 높은 토픽으로 분류하기\n# 1. 문서별로 확률이 가장 높은 토픽 추출하기\ndoc_class <- doc_topic %>%\n  group_by(document) %>%\n  slice_max(gamma, n = 1)\ndoc_class\n\n# A tibble: 5,328 × 3\n# Groups:   document [3,203]\n   document topic gamma\n   <chr>    <int> <dbl>\n 1 1            5 0.159\n 2 10           8 0.168\n 3 100          5 0.153\n 4 1000         7 0.15 \n 5 1001         1 0.137\n 6 1001         3 0.137\n 7 1001         7 0.137\n 8 1002         3 0.137\n 9 1002         7 0.137\n10 1002         8 0.137\n# … with 5,318 more rows\n\n# 2. 원문에 확률이 가장 높은 토픽 번호 부여하기\n# integer로 변환\ndoc_class$document <- as.integer(doc_class$document)\n\n# 원문에 토픽 번호 부여\nnews_comment_topic <- raw_news_comment %>%\n  left_join(doc_class, by = c(\"id\" = \"document\"))\n\nWarning in left_join(., doc_class, by = c(id = \"document\")): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 2 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n# 결합 확인\nnews_comment_topic %>%\n  select(id, topic)\n\n# A tibble: 6,275 × 2\n      id topic\n   <int> <int>\n 1     1     5\n 2     2     1\n 3     2     5\n 4     2     6\n 5     3     2\n 6     3     8\n 7     4     4\n 8     5     5\n 9     5     6\n10     6     3\n# … with 6,265 more rows\n\n# 3. 토픽별 문서 수 살펴보기\nnews_comment_topic %>%\n  count(topic)\n\n# A tibble: 9 × 2\n  topic     n\n  <int> <int>\n1     1   660\n2     2   704\n3     3   663\n4     4   609\n5     5   708\n6     6   690\n7     7   649\n8     8   645\n9    NA   947\n\n# topic이 NA인 문서 제거\nnews_comment_topic <- news_comment_topic %>%\n  na.omit()\n\nnews_comment_topic %>%\n  count(topic)\n\n# A tibble: 8 × 2\n  topic     n\n  <int> <int>\n1     1   660\n2     2   704\n3     3   663\n4     4   609\n5     5   708\n6     6   690\n7     7   649\n8     8   645\n\n# 토픽별 문서 수와 단어 시각화하기\n# 1. 토픽별 주요 단어 목록 만들기\ntop_terms <- term_topic %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 6, with_ties = F) %>%\n  summarise(term = paste(term, collapse = \", \"))\ntop_terms\n\n# A tibble: 8 × 2\n  topic term                                                \n  <int> <chr>                                               \n1     1 작품, 진심, 정치, 자랑, 수상소감, 댓글              \n2     2 대박, 시상식, 오늘, 국민, 소름, 정치                \n3     3 조국, 문재인, 가족, 문화, 대통령, 자랑              \n4     4 역사, 우리나라, 세계, 오스카, 수상, 빨갱이          \n5     5 자랑, 우리, 최고, 감사, 생각, 소식                  \n6     6 작품상, 감독상, 한국영화, 수상, 각본상, 나라        \n7     7 블랙리스트, 박근혜, 사람, 송강호, 이미경, 자유한국당\n8     8 한국, 미국, 한국인, 세계, 좌파, 배우                \n\n# 2. 토픽별 문서 빈도 구하기\ncount_topic <- news_comment_topic %>%\n  count(topic)\ncount_topic\n\n# A tibble: 8 × 2\n  topic     n\n  <int> <int>\n1     1   660\n2     2   704\n3     3   663\n4     4   609\n5     5   708\n6     6   690\n7     7   649\n8     8   645\n\n# 3. 문서 빈도에 주요 단어 결합하기\ncount_topic_word <- count_topic %>%\n  left_join(top_terms, by = \"topic\") %>%\n  mutate(topic_name = paste(\"Topic\", topic))\ncount_topic_word\n\n# A tibble: 8 × 4\n  topic     n term                                                 topic_name\n  <int> <int> <chr>                                                <chr>     \n1     1   660 작품, 진심, 정치, 자랑, 수상소감, 댓글               Topic 1   \n2     2   704 대박, 시상식, 오늘, 국민, 소름, 정치                 Topic 2   \n3     3   663 조국, 문재인, 가족, 문화, 대통령, 자랑               Topic 3   \n4     4   609 역사, 우리나라, 세계, 오스카, 수상, 빨갱이           Topic 4   \n5     5   708 자랑, 우리, 최고, 감사, 생각, 소식                   Topic 5   \n6     6   690 작품상, 감독상, 한국영화, 수상, 각본상, 나라         Topic 6   \n7     7   649 블랙리스트, 박근혜, 사람, 송강호, 이미경, 자유한국당 Topic 7   \n8     8   645 한국, 미국, 한국인, 세계, 좌파, 배우                 Topic 8   \n\n# 4. 토픽별 문서 수와 주요 단어로 막대 그래프 만들기\nggplot(count_topic_word,\n       aes(x = reorder(topic_name, n),\n           y = n,\n           fill = topic_name)) +\n  geom_col(show.legend = F) +\n  coord_flip() +\n  geom_text(aes(label = n) , # 문서 빈도 표시\n            hjust = -0.2) + # 막대 밖에 표시\n  geom_text(aes(label = term), # 주요 단어 표시\n            hjust = 1.03, # 막대 안에 표시\n            col = \"white\", # 색깔\n            fontface = \"bold\", # 두껍게\n            family = \"nanumgothic\") + # 폰트\n  scale_y_continuous(expand = c(0, 0), # y축-막대 간격 줄이기\n                     limits = c(0, 820)) + # y축 범위\n  labs(x = NULL)\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n토픽 이름 짓기\n\n# 토픽별 주요 문서 살펴보고 토픽 이름 짓기\n# 1. 원문을 읽기 편하게 전처리하기, gamma가 높은 순으로 정렬하기\ncomment_topic <- news_comment_topic %>%\n  mutate(reply = str_squish(replace_html(reply))) %>%\n  arrange(-gamma)\n\ncomment_topic %>%\n  select(gamma, reply)\n\n# A tibble: 5,328 × 2\n   gamma reply                                                                  \n   <dbl> <chr>                                                                  \n 1 0.264 \"도서관서 여자화장실서 나오는 남자사서보고 카메라있는지없는지 확인했다…\n 2 0.260 \"봉준호 송강호 블랙리스트 올리고 CJ 이미경 대표는 박근혜가 보기싫다는 …\n 3 0.239 \"보수정권때 블랙리스트에 오른 봉준호 송강호가 보기싫다는 박근혜의 말한…\n 4 0.238 \"도서관서 여자화장실서 나오는 남자사서보고 카메라있는지없는지 확인했다…\n 5 0.235 \"당초 \\\"1917\\\"과 \\\"기생충\\\"의 접전을[초기엔 1917이 훨씬 우세]예상했지… \n 6 0.234 \"박그네 밑에서 블랙리스트 있었는데 ㅋㅋㅋㅋㅋㅋㅋ 이미경이는 박근혜가 …\n 7 0.226 \"위대한 박정희 삼성이 대한민국을 세계에 우뚝 세워 놨기에 가능한 일....…\n 8 0.225 \"기생충 영화보고 좌빨이 얼마나 기생충인지 못느낀 사람 제정신이야? 좌빨…\n 9 0.225 \"봉준호 감독과 송강호 배우는 이명박그네 정권 시절 문화계 블랙리스트 였…\n10 0.224 \"나중에 기생충 정부 영화 한편 나오겠네. 남자 주연 문xx ,여자주연 김정x…\n# … with 5,318 more rows\n\n# 2. 주요 단어가 사용된 문서 살펴보기\n# 토픽 1 내용 살펴보기\ncomment_topic %>%\n  filter(topic == 1 & str_detect(reply, \"작품\")) %>%\n  head(50) %>%\n  pull(reply)\n\n [1] \"봉감독의 'local'이라는 말에 발끈했나요? 미국 아카데미의 놀라운 변화입니다. 기생충이란 영화의 작품적 우수성 뿐만 아니라 '봉준호'라는 개인의 네임벨류와 인간적 매력과 천재성이 이번 아카데미 수상에 큰 역할을 한 것 같네요. 그의 각종 수상소감을 보면 면면히 드러나네요~~\"                                                                                           \n [2] \"이 작품을 기준으로 앞으로도 계속 쓰여질 것입니다. 진심으로 축하드립니다^^\"                                                                                                                                                                                                                                                                                         \n [3] \"이런 위대한 작품과 감독을 블랙리스트에 올려 대중에게서 뺏어 묻어버릴려고 했던 쥐닥정권 그걸 찬양하는 소시오패스일베충 벌래에게 무한한 저주가 함께하길 기원합니다.^^\"                                                                                                                                                                                               \n [4] \"폐쇄적이라는 평가를 받아온 오스카가 외국영화에 작품상을 주는걸 보니 또다른 의미의 권위가 느껴진다\"                                                                                                                                                                                                                                                                 \n [5] \"봉준호감독 대단하다 열등감이있는외모 안된다는 편견 자신과의 싸움 결국 그럭게 말하는자 들은 뭐하고사는지?우스꽝스럽다고 비꼬고 놀렸을만한 모습이지만 그들은 이런상 한번이라도 받을수있는 자격이있는지 암튼 대다하고 작품활동열심히 하셔서 멋진사람으로 기억되길,.,,외모비하하는 찌질이들은 아무재능없는 소인배가되고 결국 계속 악플 올렸다간 따돌림이나 당하겠지...\"\n [6] \"이런 감독을 박그네 토착왜구 정부에서는 좌파 블랙리스트에 올렸었지 ㅋㅋㅋ 근데 정작 미국에선 작품상 ㅋㅋㅋ\"                                                                                                                                                                                                                                                         \n [7] \"축하합니다!! 자랑스럽고 멋져요^^ 앞으로도 멋진 작품 부탁드립니다!\"                                                                                                                                                                                                                                                                                                 \n [8] \"정말 대단하다는 말밖에는~ 진심으로 축하드립니다. 앞으로도 좋은 작품 많이 만들어주세요~^^\"                                                                                                                                                                                                                                                                          \n [9] \"한류의 또다른 이정표를 봉준호가 해내는구가 오스카상 특히 작품상은 비영어권 국가가 수상하기 어려운 상당히 보수적인데 대단하다 썩은 정치로 한숨쉬는 국민들에게 또 다른 희망과 자부심을 심어줘서 고맙다\"                                                                                                                                                              \n[10] \"작품이라고 할 수 없는 습작 정도의 물건을 상을 줬으니, 그동안 오스카 상이 얼마나 추접한 상인지 증명하는 것임.\"                                                                                                                                                                                                                                                      \n[11] \"기생충을 재밌게는봤지만,작품성이 그리대단한줄은 몰랐네요,축하합니다\"                                                                                                                                                                                                                                                                                               \n[12] \"완전감동이네요..제2의기생충같은 작품 많이많이 만들어주세요..\"                                                                                                                                                                                                                                                                                                      \n[13] \"봉감독님 너무 축하합니다! 앞으로도 훌륭한 작품들 기대합니다♥\"                                                                                                                                                                                                                                                                                                      \n[14] \"작품도 뛰어났지만 국가 위상이 그만큼 높아진 것. 촛불시민혁명 국민들이 비폭력적으로 정권교체 하는데 성공하고 문 대통령이 남북정상회담 멋지게 해내셨고 그 과정을 거치면서 대한민국이 서구 사회에 유명해진 것. 분위기가 뒷받침 해주지 않으면 아무리 작품이 좋아도 이런 성과는 못냄.\"                                                                                  \n[15] \"각본상받았다고 하길래 그게 끝인줄 알았더니 시작에 불과했었네. 한국영화의 쾌거로구나. 헐리웃영화가 아닌 작품이 작품상을 받은게 최초라는데 역사를 썼다. 와우~\"                                                                                                                                                                                                       \n[16] \"작품..감독..배우는 말할 것도 없고 샤론최님까지 통역을 잘 해주셔서 외국사람들으 현지호응도..작품 이해도가 높아져서.. 분위기 잘 잡아주셨나보네요.\"                                                                                                                                                                                                                   \n[17] \"개인적으론 봉감독 작품중에 여전히 살인의 추억이 가장 인상깊지만... 기생충은 전세계적으로 화두가 된 주제선정이 탁월했고 풀어내는 모양새도 능수능란하게 과하지도 덜하지도 않게 딱 좋았다. 여유있게 유머를 가지고 오스카를 향한 긴 레이스에서 정치[?]도 잘했다.\"                                                                                                      \n[18] \"백인우월주의가 팽배한 그 나라에서 조차 인정할 수 밖에 없을만큼 대단한 작품이었다는 반증인거지. 그리고 저 영화 보고 기분 더러웠다는 건 제대로 느낀거 아닐까? 기생충 자체가 사회풍자가 들어있으니까. 원래 너무 현실적인게 가장 불편함. 극구 부정하고 싶거든.\"                                                                                                        \n[19] \"각본상 준거보니 작품상은 1917 아니면 조커다\"                                                                                                                                                                                                                                                                                                                       \n[20] \"축하드려요. 당신은 우리의 자랑입니다. 좋은 작품 앞으로도 기대할게요^^ 짝짝짝\"                                                                                                                                                                                                                                                                                      \n[21] \"오스카 작품상 탔다 비영어권처음이다\"                                                                                                                                                                                                                                                                                                                               \n[22] \"축하합니다~^^. 작품상까지 받은 작품인데 주연상이 없음이 좀 아이러니지만....\"                                                                                                                                                                                                                                                                                       \n[23] \"김기덕 감독님 영화는 이렇게 해야됩니다. 맨날 어둡고 보기 힘든 영화만 만들지 마시고 이런 작품도 만들어 주세요! 봉 감독님 진심 축하드립니다.\"                                                                                                                                                                                                                        \n[24] \"솔직히 작품상은 조커가 타는 게 맞는데 동양인 쿼터 준다고 많이 배려해 준 듯.\"                                                                                                                                                                                                                                                                                       \n[25] \"솔직히 눈물난다ㅡ내가 내 인생영화로 꼽았던 작품! 개봉첫날 두번 봤다 ㅜ.ㅜ\"                                                                                                                                                                                                                                                                                         \n[26] \"와...햐늘을 찌르는 작품성을 바탕으로 아카데미는 로컬영화제라고 인터뷰하던 그대의 고귀한 자태는정말 멋잇엇어요....자랑스럽습니다\"                                                                                                                                                                                                                                   \n[27] \"남들은 남감독 작품이 해외서 상을 타니 입에 침이 마르도록 찬양을 한다. 왜 저렇게 남자일을 자기일처럼 좋아할까? 저 자리에 여성은 없고 향후 몇 십년 동안 없을 수도 있단 생각에 난 웃음도 나질 않는다. ㅉ\"                                                                                                                                                             \n[28] \"우리나라 영화제..10년도 넘은 배우한테..신인신ㅇ 주고 공동수상 남발하고..감독상 받으면 작품상은 포기해야 하는데..외국의 냉철함 너무 보기 좋네\"                                                                                                                                                                                                                      \n[29] \"축하합니다. 어깨가 으쓱해집니다. 작품상까지 수상하시길 응원합니다.\"                                                                                                                                                                                                                                                                                                \n[30] \"꿈 아니냐?? 작품상??? 전 세계에서 한해 제일 잘만들면서 대중성도 있는 작품에 주는 그상 맞냐? World no.1 ㄷ ㄷ ㄷ ㄷ ㄷ 진짜 개소름\"                                                                                                                                                                                                                                 \n[31] \"근데 나는 봉준호 송강호도 대단하지만 이선균이 진심 대단함. 이선균 필모는 넘사네 진짜. 작품도 다양하고.\"                                                                                                                                                                                                                                                            \n[32] \"정말 자랑스럽습니다! 눈물흘리며 같이 환호했습니다! 앞으로도 역사에 남을 작품 많이 남겨주세요!\"                                                                                                                                                                                                                                                                     \n[33] \"다른 작품들도 진짜 쟁쟁했는데.... 이건 대한민국 영화의 새로운 역사입니다!!! 봉 감독님 진짜 축하드려요!!! 그리고 수구꼴통 토착왜구 새끼들아!!! 제발 이런걸 정치적인거랑 엮지마라!!!!! 늬들의 행태 너무나 역겹다!!!!!!\"                                                                                                                                              \n[34] \"아카데미에서 큰상받은건 축하하고 대한민국인으로서 자랑스럽긴한데, 영알못은 기생충의 어떤점이 작품성이 있는지 잘 모르겠다.\"                                                                                                                                                                                                                                         \n[35] \"일하다가 눈물날뻔..^^ 감격스럽네요 자랑스럽네요 봉감독님의 더 많은 작품, 훌륭한 작품 기대합니다~~^^\"                                                                                                                                                                                                                                                               \n[36] \"얼마전 괴물 다시보는데도 진짜 봉감독님 작품 너무 재밌어요! 수상 축하드려요!\"                                                                                                                                                                                                                                                                                       \n[37] \"작품성과 흥행성을 모두 거머쥔 시대상을 딱 반영하는 상이다. 오스카도 반영했을듯 하네ㅋ\"                                                                                                                                                                                                                                                                             \n[38] \"자본주의 민주주의 아메리카가 인정한 작품. 기생충이 짱개의 유사사회주의 찬양한것처럼 달창 거지기생충들 그냥 좋댄다 ㅋㅋㅋㅋㅋ\"                                                                                                                                                                                                                                      \n[39] \"작품상은 가능성이 희박하다는 기사를 봤었는데... 기적이네요.... 투표 방식이 이번에 바껴서 그런가\"                                                                                                                                                                                                                                                                   \n[40] \"봉준호 감독님! 대한민국을 널리 알려주셔서 감사합니다. 수상소감 말씀하실때 눈물이 핑 돌더군요. 앞으로도 좋은 작품 세계적인 영화를 기대합니다. 축하드립니다.기생충! 봉준호 감독님! 화이팅입니다 ^^\"                                                                                                                                                                  \n[41] \"아마 반백년내에는 한국에서 이런 영화가 나오긴 힘들듯.. 칸 황금종려상과 아카데미 작품상을 동시에 수상한것도 전세계적으로 60년만에 일인거고.. 진짜 어메이징한 올해 한국의 최대 이변일듯..\"                                                                                                                                                                           \n[42] \"예전 살인의 추억을 극장에서 보고 작품,상업성을 다 갖춘 감독의 출현에 지금껏 응원해왔어요 정말 뭉클합니다\"                                                                                                                                                                                                                                                          \n[43] \"블랙리스트랑 작품의 완성도랑 무'슨상'관임? 예술적으로 완성도 높은 작품을 만들었으니 상을 받은거지. 문화계 블랙리스트가 뭐 능력떨어지는 예술인들에 대한 리스트도 아니고.\"                                                                                                                                                                                           \n[44] \"축하 합니다 쭉 좋은 작품 부탁 합니다.\"                                                                                                                                                                                                                                                                                                                             \n[45] \"감독님 정말 멋집니다!!! 헐리우드작품 감독님의 영화도 한번 보고싶습니다!! 화이팅~~~~~~\"                                                                                                                                                                                                                                                                             \n[46] \"작품상 받을 포스인데...\"                                                                                                                                                                                                                                                                                                                                           \n[47] \"축하합니다. 앞으로도 좋은 작품 기대하겠습니다.\"                                                                                                                                                                                                                                                                                                                    \n[48] \"축하드립니다~ 국제적 작품이 되엇어요 ♡~♡\"                                                                                                                                                                                                                                                                                                                          \n[49] \"황금종려상에 아카데미 작품상까지 정말 대단합니다\"                                                                                                                                                                                                                                                                                                                  \n[50] \"대단~^^ 작품상까지~ 올들어 젤 기쁜 기사네요\"                                                                                                                                                                                                                                                                                                                       \n\ncomment_topic %>%\n  filter(topic == 1 & str_detect(reply, \"진심\")) %>%\n  head(50) %>%\n  pull(reply)\n\n [1] \"한국문화는 1등수준. 정치는 개돼지 3등수준. 대한민국 문화수준을 엎 그레이드 한 봉준호감독에게 존경을 표한다. 월드컵4강.봉준호감독 오스카수상. 한국국민들 절대로 잊혀지지않는 대사건이다. 정말 진심으로 축하드립니다.특히 CJ가 한국영화 산업에 큰 발전에 국민의 한 사람으로 감사드린다.\"\n [2] \"진심 축하드립니다. 대한민국 예술처럼 정치, 경제도 발전해서 살기좋은 나라가 되었으면 좋겠네요\"                                                                                                                                                                                         \n [3] \"이 작품을 기준으로 앞으로도 계속 쓰여질 것입니다. 진심으로 축하드립니다^^\"                                                                                                                                                                                                            \n [4] \"소름이 돋을 정도로 믿기지 않습니다! 진심으로 축하드립니다 살인의추억을 보고 우리나라에도 이렇게 훌륭한 영화를 만드는 감독이 있구나 느꼈고 그 이후로 계속 봉감독님 영화를 챙겨보게 되었는데 이제는 세계인도 인정해주는 감독이 되어 너무나도 벅찹니다 다시한번 진심으로 축하드립니다\"   \n [5] \"봉감독님 진심 축하드립니다! 아카데미에서 외국 영화 기생충이 4관왕을 차지했다는 것은 아카데미도 기류가 바뀌고 있다는 ... 험지에서 대단한 업적을 남긴 감독, 배우들, 스텝들에게 찬사를 보냅니다!\"                                                                                        \n [6] \"기생충!! 우와~~^^이게 실화인가요? 진심으로 축하드려요\"                                                                                                                                                                                                                                \n [7] \"정말 대단하다는 말밖에는~ 진심으로 축하드립니다. 앞으로도 좋은 작품 많이 만들어주세요~^^\"                                                                                                                                                                                             \n [8] \"진심으로 축하드립니다. 자랑스럽습니다.\"                                                                                                                                                                                                                                               \n [9] \"진심 축하드려요 봉준호감독님 진심 응원합니다\"                                                                                                                                                                                                                                         \n[10] \"한국영화계에 이런일이 생길 줄은 꿈에도 몰랐는데 4관왕 진심으로 축하드립니다.\"                                                                                                                                                                                                         \n[11] \"영화 보면서 봉준호는 천재다 라고 외쳤는데~ 진심 자랑스럽다 축하합니다\"                                                                                                                                                                                                                \n[12] \"와 진심 대박 ㅜㅜ 화이트 파티인 아카데미에서 ㅠㅜ\"                                                                                                                                                                                                                                    \n[13] \"와.. 대박 진심 호명순간 소리질렀어여\"                                                                                                                                                                                                                                                 \n[14] \"김기덕 감독님 영화는 이렇게 해야됩니다. 맨날 어둡고 보기 힘든 영화만 만들지 마시고 이런 작품도 만들어 주세요! 봉 감독님 진심 축하드립니다.\"                                                                                                                                           \n[15] \"세계가 인정하는 대한민국 예술! 진심으로 자랑스럽다ㅠㅠ\"                                                                                                                                                                                                                               \n[16] \"근데 나는 봉준호 송강호도 대단하지만 이선균이 진심 대단함. 이선균 필모는 넘사네 진짜. 작품도 다양하고.\"                                                                                                                                                                               \n[17] \"이걸 정치적으로 까는거보면 뇌가있나 싶다 진심으로 /\"                                                                                                                                                                                                                                  \n[18] \"축하드립니다. 대한민국 사람으로서 봉준호감독님 세계적인 거장에 한걸음 다가가신걸 진심으로 축하드려요~~^^\"                                                                                                                                                                             \n[19] \"우와 !!!!! 진심으로 축하드려요 !!!!\"                                                                                                                                                                                                                                                  \n[20] \"어~~ 진짜 되네? 진심으로 축하드려요\"                                                                                                                                                                                                                                                  \n[21] \"진심 축하드립니다~~~ 너무 멋져요!!\"                                                                                                                                                                                                                                                   \n[22] \"대박입니다~~ 진심으로 축하합니다~!!!\"                                                                                                                                                                                                                                                 \n[23] \"와 대박이네요 ~ 진심으로 축하드립니다.\"                                                                                                                                                                                                                                               \n[24] \"아카데미 4관왕 거머쥐신 봉준호 감독님 진심 추카드려요\"                                                                                                                                                                                                                                \n[25] \"미쳤다 진심 비명지르고\"                                                                                                                                                                                                                                                               \n[26] \"진심 대박이다.와\"                                                                                                                                                                                                                                                                     \n[27] \"캬 역사를만드셨네 진심 축하드립니다\"                                                                                                                                                                                                                                                  \n[28] \"와...진심 미쳤다이건.\"                                                                                                                                                                                                                                                                \n[29] \"진심 축하드려요 영화못봤는데 봐야겠네요\"                                                                                                                                                                                                                                              \n[30] \"진짜 자랑스럽고 하고싶은 말이 많은데 벅차서 글이 안써지네요 봉감독님~♡ 진심으로 축하드려요\"                                                                                                                                                                                           \n[31] \"너무 자랑스럽다. 진심. 레알.\"                                                                                                                                                                                                                                                         \n[32] \"진심 소름~~축하해요!\"                                                                                                                                                                                                                                                                 \n[33] \"봉준호 감독관님 진심으로 축하드립니다.\"                                                                                                                                                                                                                                               \n[34] \"와 너무 기쁘다! 이 시국에 정말 내 일같이 기쁘고 감사하다!!! 축하드려요 진심으로!!!\"                                                                                                                                                                                                   \n[35] \"진심...너무 자랑스럽습니다. ㅠㅠㅠㅠㅠ 너무 감격스러워요 ㅠㅠㅠ\"                                                                                                                                                                                                                      \n[36] \"진심 역사적인 쾌거입니다.\"                                                                                                                                                                                                                                                            \n[37] \"진심 동시대에 살고 있는 것만으로도 영광스럽다ㅠㅠㅠ\"                                                                                                                                                                                                                                  \n[38] \"봉준호 감독! 기생충! 송강호외 배우님들 대한민국 진심으로 축하합니다~\"                                                                                                                                                                                                                 \n[39] \"진심으로 축하하며 여러분들이 진정한 애국자네요\"                                                                                                                                                                                                                                       \n\ncomment_topic %>%\n  filter(topic == 1 & str_detect(reply, \"정치\")) %>%\n  head(5) %>%\n  pull(reply)\n\n[1] \"한국문화는 1등수준. 정치는 개돼지 3등수준. 대한민국 문화수준을 엎 그레이드 한 봉준호감독에게 존경을 표한다. 월드컵4강.봉준호감독 오스카수상. 한국국민들 절대로 잊혀지지않는 대사건이다. 정말 진심으로 축하드립니다.특히 CJ가 한국영화 산업에 큰 발전에 국민의 한 사람으로 감사드린다.\"\n[2] \"진심 축하드립니다. 대한민국 예술처럼 정치, 경제도 발전해서 살기좋은 나라가 되었으면 좋겠네요\"                                                                                                                                                                                         \n[3] \"좌좀세력, 태극기부대, 빨갱이, 일베애들과 이런것들을 이용애 먹는 정치인들만 없어지면 우리나라 엄청 발전할텐데...기업, 문화 모두 선진국인데 저런것들이 발목을 잡고 너무 깍아먹는다.\"                                                                                                    \n[4] \"정치충들 상은 봉준호가 받았는데 왜 정치얘기를하고있냐\"                                                                                                                                                                                                                                \n[5] \"문화는 일류 정치는 삼류에 개막장\"                                                                                                                                                                                                                                                     \n\n# 3. 토픽 이름 목록 만들기\nname_topic <- tibble(topic = 1:8,\n                     name = c(\"1. 작품상 수상 축하, 정치적 댓글 비판\",\n                              \"2. 수상 축하, 시상식 감상\",\n                              \"3. 조국 가족, 정치적 해석\",\n                              \"4. 새 역사 쓴 세계적인 영화\",\n                              \"5. 자랑스럽고 감사한 마음\",\n                              \"6. 놀라운 4관왕 수상\",\n                              \"7. 문화계 블랙리스트, 보수 정당 비판\",\n                              \"8. 한국의 세계적 위상\"))\n\n# 토픽 이름과 주요 단어 시각화하기\n# 토픽 이름 결합하기\ntop_term_topic_name <- top_term_topic %>%\n  left_join(name_topic, name_topic, by = \"topic\")\n\ntop_term_topic_name\n\n# A tibble: 83 × 4\n# Groups:   topic [8]\n   topic term        beta name                                 \n   <int> <chr>      <dbl> <chr>                                \n 1     1 작품     0.0299  1. 작품상 수상 축하, 정치적 댓글 비판\n 2     1 진심     0.0240  1. 작품상 수상 축하, 정치적 댓글 비판\n 3     1 정치     0.0192  1. 작품상 수상 축하, 정치적 댓글 비판\n 4     1 자랑     0.0181  1. 작품상 수상 축하, 정치적 댓글 비판\n 5     1 수상소감 0.0166  1. 작품상 수상 축하, 정치적 댓글 비판\n 6     1 댓글     0.0151  1. 작품상 수상 축하, 정치적 댓글 비판\n 7     1 외국     0.0122  1. 작품상 수상 축하, 정치적 댓글 비판\n 8     1 경사     0.0107  1. 작품상 수상 축하, 정치적 댓글 비판\n 9     1 훌륭     0.00998 1. 작품상 수상 축하, 정치적 댓글 비판\n10     1 좌파     0.00814 1. 작품상 수상 축하, 정치적 댓글 비판\n# … with 73 more rows\n\n# 막대 그래프 만들기\nggplot(top_term_topic_name,\n       aes(x = reorder_within(term, beta, name),\n           y = beta,\n           fill = factor(topic))) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ name, scales = \"free\", ncol = 2) +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"영화 기생충 아카데미상 수상 기사 댓글 토픽\",\n       subtitle = \"토픽별 주요 단어 Top 10\",\n       x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(text = element_text(family = \"nanumgothic\"),\n        title = element_text(size = 12),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n최적의 토픽 수 도출\n\n# 하이퍼파라미터 튜닝으로 토픽 수 정하기\n# 1. 토픽 수 바꿔가며 LDA 모델 여러 개 만들기\n# install.packages(\"ldatuning\") # 사양이 낮은 컴퓨터는 설치가 어려움, 에러 지속적으로 발생되는 것 확인\nlibrary(ldatuning)  # windows defender 알림 발생 시, '홈 네트워크, 회사 네트워크 등의 개인 네트워크'만 체크 후 '엑세스 허용' 선택\nmodels <- FindTopicsNumber(dtm = dtm_comment,  # windows defender 알림 발생 시, '홈 네트워크, 회사 네트워크 등의 개인 네트워크'만 체크 후 '엑세스 허용' 선택\n                           topics = 2:20,\n                           return_models = T,\n                           control = list(seed = 1234))\n\nmodels %>%\n  select(topics, Griffiths2004)\n\n   topics Griffiths2004\n1      20     -127213.1\n2      19     -127445.4\n3      18     -126984.0\n4      17     -127317.9\n5      16     -127139.2\n6      15     -126643.9\n7      14     -126742.4\n8      13     -126720.4\n9      12     -127429.4\n10     11     -126677.9\n11     10     -127039.5\n12      9     -127133.2\n13      8     -127234.1\n14      7     -128079.5\n15      6     -128948.9\n16      5     -129672.9\n17      4     -131006.8\n18      3     -133171.8\n19      2     -137154.4\n\n# 2. 최적 토픽 수 정하기\nFindTopicsNumber_plot(models)\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the ldatuning package.\n  Please report the issue at <https://github.com/nikita-moor/ldatuning/issues>.\n\n\n\n\n# 3. 모델 추출하기\n# 토픽 수가 8개인 모델 추출하기\noptimal_model <- models %>%\n  filter(topics == 8) %>%\n  pull(LDA_model) %>% # 모델 추출\n  .[[1]] # list 추출\n\n# optimal_model\ntidy(optimal_model, matrix = \"beta\")\n\n# A tibble: 47,960 × 3\n   topic term       beta\n   <int> <chr>     <dbl>\n 1     1 한국  0.000405 \n 2     2 한국  0.0000364\n 3     3 한국  0.0000353\n 4     4 한국  0.00295  \n 5     5 한국  0.0000353\n 6     6 한국  0.0000356\n 7     7 한국  0.00661  \n 8     8 한국  0.0593   \n 9     1 자랑  0.0181   \n10     2 자랑  0.00440  \n# … with 47,950 more rows\n\n# lda_model\ntidy(lda_model, matrix = \"beta\")\n\n# A tibble: 47,960 × 3\n   topic term       beta\n   <int> <chr>     <dbl>\n 1     1 한국  0.000405 \n 2     2 한국  0.0000364\n 3     3 한국  0.0000353\n 4     4 한국  0.00295  \n 5     5 한국  0.0000353\n 6     6 한국  0.0000356\n 7     7 한국  0.00661  \n 8     8 한국  0.0593   \n 9     1 자랑  0.0181   \n10     2 자랑  0.00440  \n# … with 47,950 more rows"
  },
  {
    "objectID": "teaching/media_ds/index.html",
    "href": "teaching/media_ds/index.html",
    "title": "Media & Data Science",
    "section": "",
    "text": "a <- \"미디어와\"\nb <- \"데이터사이언스\"\npaste0(\"Welcome to \", a,\" \",b, \" 수업\")\n\n[1] \"Welcome to 미디어와 데이터사이언스 수업\"\n\n\n\n\n\n\n\n\n\n\n\nNotice\n(공지)\n\n\n\nPre-class videos\n(주차별 선행 학습)\n\n\n\nSyllabus\n(주차별 수업 자료)"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html",
    "href": "teaching/media_ds/notice/index.html",
    "title": "Notice",
    "section": "",
    "text": "Pre-class\n\n학생들은 오프라인(또는 온라인 스트리밍 Zoom 수업)에 참석하기 전에 강사가 제공하는 녹화된 강의를 시청합니다.\n\n학생들은 자료를 독립적으로 공부함으로써 학습에 적극적인 역할을 하도록 권장됩니다.\n사전 학습 내용은 R과 머신러닝의 기본 이론과 개념을 다룹니다.\n\n\nIn-class\n\n수업 중에 강사는 수업 전 강의를 요약하고 다루는 개념에 대한 추가 설명을 제공합니다.\n학생들의 이해를 강화하기 위해 학생들은 실습에 참여합니다. 이를 통해 배운 지식을 적용하고 코딩 기술을 개발할 수 있는 기회를 얻을 수 있습니다.\n\nThe official in-class time is PM 14:00 ~ 16:00"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#textbooks",
    "href": "teaching/media_ds/notice/index.html#textbooks",
    "title": "Notice",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\n\n\n\n\n\n\n\n아래 페이지 방문하시면 R 기초와 기초 통계에 대한 정리가 너무 잘 되어 있는 사이트가 있습니다. 제 콘텐츠와 병행해서 보시면 많은 도움이 될 것 같아 공유합니다.\nhttps://kilhwan.github.io/rprogramming/"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#grading",
    "href": "teaching/media_ds/notice/index.html#grading",
    "title": "Notice",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nItems\nRatio (%)\n\n\n\n\nAttendance\n0\n\n\nPaper proposal\n100\n\n\nTotal\n100\n\n\n\n\n출석 점수 없지만 출석이 1/3 미만: F"
  },
  {
    "objectID": "teaching/media_ds/notice/index.html#communication",
    "href": "teaching/media_ds/notice/index.html#communication",
    "title": "Notice",
    "section": "Communication",
    "text": "Communication\n\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nhttps://open.kakao.com/o/gomSif7e\n\n\n\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s90SQocc\npw: hanyang\n\n\n\n\n\nInstall gadgets\n\nInstall R, R Studio, & Rtools\n\nR\n\nFor window https://cran.r-project.org/bin/windows/base/\nFor mac https://cran.r-project.org/bin/macosx/\n\nR Studio\n\nhttps://posit.co/download/rstudio-desktop/\n\nRtools (only for window user)\n\nhttps://cran.r-project.org/bin/windows/Rtools/\nTo install R packages containing C/C++ language for window users (no need for Linux and Mac users)\n\n\nThings you need to know\n\nDon’t use OneDrive.\n\nUse Github instead\nMany people get an error when installing becausedf OneDrive\n\nSet Windows user name to English\n\nIf Korean characters are mixed in the installation path, there is a high probability of error occurrence\n\n\nInstallation Order\n\nStep 1 ‑ Download the file\n\nDownload R, Rtools, Rstudio installation files\n\nStep 2 - Install R\n\nUnified installation path: All will be installed in the C:/R folder\nRun in administrator mode when running the R installation file\nAfter installing R, grant write permission to the R folder, Right-click and turn off read only\n\nStep 3 ‑ Install Rtools\n\nAdministrator mode execution installation and folder setting as C:\\R\\rtools40\nCreate environment variable RTOOLS40_HOME after installation: Value - C:\\R\\rtools40\\\nAdd %RTOOLS40_HOME%\\usr\\bin\\ to the Path variable.\n\nStep 4 - Install Rstudio\n\nRight-click and run as administrator - installation path C:\\R\\Rstudio\nCheck rtools connection with Sys.which(“make”) command after installation"
  },
  {
    "objectID": "teaching/media_ds/preclass/index.html",
    "href": "teaching/media_ds/preclass/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Week\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/ml101/about/index.html",
    "href": "teaching/ml101/about/index.html",
    "title": "About ML101",
    "section": "",
    "text": "Course description\n\n\n\n\n\n\nGoal\n\n\n\nThis course covers the fundamentals of the field, including supervised and unsupervised learning algorithms, regression, classification, and clustering. The course may also cover topics such as model evaluation, feature selection, and regularization.\nIn a supervised learning setting, students learn about linear regression and logistic regression, as well as more complex algorithms such as Naive Bayes, decision trees, random forests, and kNN. They learn how to train models on a labeled dataset and make predictions on new data.\nIn an unsupervised learning setting, students learn about clustering algorithms such as k-means and Apriori. They learn how to extract meaningful structure from unlabeled data.\nThe course may also cover advanced topics such as natural language processing. Students learn how to implement and use these algorithms in R.\nThroughout the course, students work on practical projects and assignments to apply the concepts they have learned. By the end of the course, students should have a solid understanding of the basics of machine learning and be able to apply these concepts to real-world problems.\n\n\n\n\nWeekly Design\n\n2023_ML101\n\n\n\n\n\n\n\n\n\n\n\nWeek\nThu\nTue\nPre-class\nDiscussion\nClass\nIC-PBL\n\n\n\n\n1\n03/02/2023\n03/07/2023\nInstall R & R Studio\n\nCourse intro\n\n\n\n2\n03/09/2023\n03/14/223\nAbout ML & Modelling\nD1\nPractice\n\n\n\n3\n03/16/2023\n03/21/2023\nClassification\n\nDecision Tree\n\nD2\nPractice\nProblem description\n\n\n4\n03/23/2023\n03/28/2023\n\nRandom Forest\n\nD3\nPractice\nData introduction\n\n\n5\n03/30/2023\n04/04/2023\n\nNaive Bayes\n\nD4\nPractice\nTeam arrangement\n\n\n6\n04/06/2023\n04/11/2023\n\nkNN\n\nD5\nPractice\nTeam meeting #1\n\n\n7\n04/13/2023\n04/18/2023\nRegression\n\nLinear regression\n\nD6\nPractice\nTeam meeting #2\n\n\n8\n\n\nQZ #1 - 04/19 (Wed) 13:00 ~ 15:00\n\n\nTeam meeting #3\n\n\n9\n04/27/2023\n05/02/2023\n\nNon-linear regression\n\nD7\nPractice\nTeam meeting #4\n\n\n10\n05/04/2023\n05/09/2023\nUnsupervised learning\n\nClustering\n\nD8\nPractice\nTeam meeting #5\n\n\n11\n05/11/2023\n05/16/2023\n\nApriori\n\nD9\nPractice\nTeam meeting #6\n\n\n12\n05/18/2023\n05/23/2023\nModel improvement\nD10\nPractice\nTeam meeting #7\n\n\n13\n05/25/2023\n05/30/2023\nText mining & other skills\n\nPractice\nTeam consulting #1\n\n\n14\n\n\nQZ #2: 05/31 (Wed)\n13:00 ~ 15:00\n\n\nTeam consulting #2\n\n\n15\n\n\n최종 프로젝트 발표일:\n6월 14일 수요일\n\n\nProject Presentation\n\n\n\n\n\nRules\n\nNo score for Attendance (Come to class if you want to learn something)\n\n\n\nPre-class: 100% Eng\nClass (Practice part): 50% Eng / 50% Kor\nDiscussion Submission: 100% Eng.\nQZ #1 & #2 : 100% Eng.\nIC-PBL project (100% Kor.)\n\nOnly for Domestic Students because the company that gives the problem for the class cannot proceed the project in English.\nICPBL 프로젝트는 한국 학생들로만 팀을 구성하여 진행\nExchange Students: I’ll give a different (virtual) problem set for you guys.\n\n\n\n\n\n수강 후기"
  },
  {
    "objectID": "teaching/ml101/icpbl/domestic.html",
    "href": "teaching/ml101/icpbl/domestic.html",
    "title": "IC-PBL",
    "section": "",
    "text": "Team Assignment\n\nRandom assignment (See the method)\n\n\nThursday classTuesday class\n\n\n\n\n\nTeam 1\nTeam 2\nTeam 3\n\n\n\n\n박승연\n김상경\n김재엽\n\n\n박지원\n박종현\n김찬우\n\n\n윤지성\n이윤진\n정재윤\n\n\n이유정\n정지윤\n정혜림\n\n\n\n\n\n\n\n\nTeam 1\nTeam 2\nTeam 3\n\n\n\n\n김숭기\n김민지\n김가영\n\n\n김정환\n김원\n박은서\n\n\n문하윤\n노솔\n이정헌\n\n\n조민석\n임예빈\n최지희\n\n\n\n\n\n\n\n\n시나리오\n독서어플 리더스에서 데이터를 제공 받아 주어진 문제를 풀게 됨.\n\n리더스 소개\n\n소개 블로그\n소개 기사\n\n리더스 다운로드\n\nAndroid\niOS\n\n\n\n\n\n\n\n\n문제\n\n\n\n독서 앱에서 사용하는 고객의 독서 관련 데이터를 받아 수업시간에 배운 기계학습 모델과 텍스트 분석 방법을 활용하여 새로운 서비스를 제안한다.\n\n\n\n\n\n지난 수업(2022-1) 결과물 모음(참고용)\n\n\n\n오전 1조\n오전 2조\n오전 3조\n오전 4조\n\n\n오후 1조\n오후 2조\n오후 3조\n오후 4조\n\n\n\n\n\n\n리더스 제공 데이터\n\n데이터 다운로드 [Here]\n\n한양대 계정으로 로그인해야 다운 받을 수 있습니다.\n\n\nR 에서 제공 데이터 불러오기\n\n#####\n# Import readers DB\n#####\n\n# Sys.setlocale(\"LC_ALL\", locale=\"Korean\")\n\nlibrary(readxl)\n\nuser <- read_excel(\"readers/01_user.xlsx\")\nuser_cat <- read_excel(\"readers/02_user_cat.xlsx\")\nfollow  <- read_excel(\"readers/03_follow.xlsx\")\nuser_book  <- read_excel(\"readers/04_user_book.xlsx\")\nbook  <- read_excel(\"readers/05_book.xlsx\")\nbook_cat  <- read_excel(\"readers/06_book_cat.xlsx\")\ncat  <- read_excel(\"readers/07_cat.xlsx\")\nscrap  <- read_excel(\"readers/08_scrap.xlsx\")\n\n\n\n\nEDA in brief\n\n\nuser: 유저 정보\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nbirth_year\ngender\ncreated_at\n직업\n직종\n\n\n\n\n10936\n1986\nM\n2019-07-20 11:16:15+00\n직장인\n디자인\n\n\n22042\n1988\nM\n2019-09-09 12:49:27+00\n직장인\n경영지원\n\n\n29446\n1987\nF\n2019-09-09 13:19:59+00\n직장인\n경영지원\n\n\n73870\n1985\nF\n2019-09-09 14:10:13+00\n직장인\n경영지원\n\n\n414454\n1991\nF\n2019-09-09 15:12:12+00\n직장인\n영업/고객상담\n\n\n532918\n1989\nF\n2019-09-09 15:44:42+00\n직장인\n디자인\n\n\n536620\n1985\nF\n2019-09-09 15:45:02+00\n직장인\n마케팅/광고/홍보\n\n\n592150\n1989\nF\n2019-09-09 15:50:59+00\n직장인\nNA\n\n\n629170\n1991\nF\n2019-09-09 15:54:22+00\n직장인\nNA\n\n\n917926\n1984\nF\n2019-09-24 12:54:10+00\n직장인\n의료\n\n\n\n\n\n\nuser_id: 유저 고유번호\nbirth_year: 생년\ngender: 성별\ncreated_at: 가입 날짜와 시간\n직업\n직종\n\n\n\nuser_cat: 유저의 관심 책 카테고리\n\n\n\n\n\n\nuser_id\ntitle\n\n\n\n\n10936\n언어\n\n\n10936\n에세이\n\n\n10936\n습관\n\n\n10936\n시\n\n\n10936\n다이어트\n\n\n10936\n부동산\n\n\n10936\n교육\n\n\n10936\n예술\n\n\n10936\n건강\n\n\n10936\n디자인\n\n\n\n\n\n\nuser_id: 유저 고유번호\ntitle: 해당 유저가 관심 있는 책의 카테고리\n\n\n한 유저가 복수의 카테고리를 선택 가능함. 위의 10936 유저는 언어, 에세이, 습관, 시, 다이어트, 부동산 카테고리 등에 관심이 있음.\n\n\n\nfollow: 유저의 팔로우 팔로워 관계\n\n\n\n\n\n\nuser_id\ntarget_user_id\n\n\n\n\n69249442\n13230778\n\n\n69249442\n48855124\n\n\n69249442\n21104932\n\n\n344337658\n48118426\n\n\n344337658\n34169290\n\n\n344337658\n13038274\n\n\n283176916\n48288718\n\n\n154550926\n423038476\n\n\n154550926\n271563742\n\n\n154550926\n71648338\n\n\n\n\n\n\nuser_id\ntarget_user_id: user_id의 유저가 팔로우 하는 유저의 고유 번호\n\n\n\nuser_book: 유저의 책장에 담긴 책 고유 번호와 현재 독서 상태\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nbook_id\nrate\nread_status\nmodified_at\n\n\n\n\n69249442\n2515\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:08:02.451517\n\n\n69249442\n39950\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:07:16.139095\n\n\n69249442\n35001\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:06:38.229688\n\n\n69249442\n25786\n0\nREAD_STATUS_DONE\n2022-03-20 11:05:44.171380\n\n\n69249442\n38623\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:05:23.039321\n\n\n69249442\n67901\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:05:03.298657\n\n\n69249442\n69673\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:04:39.875480\n\n\n69249442\n451816\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:03:50.586800\n\n\n69249442\n451815\nNA\nREAD_STATUS_PAUSE\n2022-03-20 11:03:35.242273\n\n\n69249442\n115635\n0\nREAD_STATUS_BEFORE\n2022-03-20 11:03:15.823352\n\n\n\n\n\n\nuser_id\nbook_id: 책의 고유 번호\nrate: 해당 책에 유저가 준 평점 (0~5)\nread_status: 최종 수정 시각의 독서 상태\n\nREAD_STATUS_BEFORE → 읽고 싶은 책 (하지만 아직 읽기 전)\nREAD_STATUS_DONE → 읽기 완료 (최종 수정 시각에)\nREAD_STATUS_ING → 읽는 중\nREAD_STATUS_PAUSE → 독서 (일시) 중단\nREAD_STATUS_STOP → 독서 중단\n\nmodified_at: read_status 최종 수정 시각: 이 수정 시각에 최종으로 읽기 상태를 수정한 것\n\n\n69249442 유저는 2515 책을 읽고 싶고(BEFORE) 이 레코드를 남긴 최종 시각은 2022년 3월 20일 11시 8분 2초경이다.\n\n\n\nbook: 책에 대한 정보\n\n\n\n# A tibble: 10 × 14\n   ...1       id title      sub_t…¹ isbn13 pub_d…² descr…³ publi…⁴ categ…⁵  page\n   <chr>   <dbl> <chr>      <chr>   <chr>  <chr>   <chr>   <chr>   <chr>   <dbl>\n 1 1     1124370 무엇이든 … <NA>    <NA>   2017-0… <NA>    예림당  국내도…    NA\n 2 2       86580 새벽 거리… <NA>    97889… 2011-0… 히가시… 재인    국내도…   420\n 3 3      262921 세계사의 … <NA>    97889… 2012-1… '가라…  비(도…  국내도…   477\n 4 4     2015776 모래그릇 1 <NA>    97889… 2017-1… 일본 …  문학동… eBook>…    NA\n 5 5       56903 가끔은 웅… 복잡다… 97911… 2018-0… 어린 …  빌리버… 국내도…   260\n 6 6       47114 머릿속 생… 그러니… 97889… 2018-0… 화려한… 시그마… 국내도…   324\n 7 7     1310946 세컨드 라… 인생을… 97889… 2019-0… 투자금… 문학동… 국내도…   344\n 8 8      289094 돈 카를로… <NA>    97889… 2014-0… '문학…  문학동… 국내도…   452\n 9 9      232305 즐거운 북… 현직 …  97889… 2010-1… 초등학… 우리책  국내도…   180\n10 10    1142843 만화로 읽… <NA>    97889… 2008-0… <NA>    태동출… 국내도…   120\n# … with 4 more variables: full_description <chr>, full_description_2 <chr>,\n#   toc <chr>, author <chr>, and abbreviated variable names ¹​sub_title,\n#   ²​pub_date, ³​description, ⁴​publisher, ⁵​category_name_aladin\n\n\n\nid: 책의 고유 번호 (주의: 위 테이블의 book_id와 동일)\ntitle: 책의 제목\nsub_title: 책의 부제목\nisbn13: 책의 isbn(출판 번호)\npub_date: 책이 출간된 날짜\ndescription: 책에 대한 간략한 설명\npublisher: 출판사\ncategory_name_aladin: 해당 책의 알라딘 카테고리\n\n첫 번째 책은 → 국내도서>전집/중고전집>창작동화\n\npage: 책의 총 페이지\nfull_description: 책에 대한 설명 (조금 더 긴 버전)\nfull_description_2: 다른 source 에서 가져온 책의 설명\ntoc: 책의 목차\n\n예를 들어, 4번째 책은 아래 목차\n\n\n\n1권 1장 토리스 바의 손님 2장 가메다 3장 누보 그룹 4장 미해결 5장 종이 날리는 여자  6장 방언 분포 7장 혈흔 8장 변사  2권  9장 모색 10장 에미코 11장 그녀의 죽음 12장 혼미 13장 실마리 14장 무성無聲 15장 항적 16장 어떤 호적 17장 방송   해설 | 일본 근대사회의 집합적 무의식, 그 터부를 비평하다 마쓰모토 세이초 연보\n\n\n\n\nauthor: 책의 저자\n\n\n\nbook_cat: 책과 카테고리의 연결 테이블\n\n\n\n\n\n\nbook_id\nbook_category_id\n\n\n\n\n796\n2326\n\n\n704\n4597\n\n\n704\n4723\n\n\n704\n4725\n\n\n704\n4653\n\n\n704\n4669\n\n\n866\n121\n\n\n866\n162\n\n\n866\n163\n\n\n866\n122\n\n\n\n\n\n\nbook_id: 책의 고유 번호\nbook_category_id: 카테고리의 고유 번호\n\n\n\ncat: 카테고리 고유 번호에 대한 상세 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbook_category_id\nname\ndepth_1\ndepth_2\ndepth_3\ndepth_4\ndepth_5\n\n\n\n\n8239\n산업연구\n일본 도서\n경제/경영\n경제학\n산업연구\nNA\n\n\n8403\n정치\n일본 도서\n인문/사회/논픽션\n정치/외교/국제관계\n정치\nNA\n\n\n8254\n문학/평론\n일본 도서\n문고/신서\n문학/평론\nNA\nNA\n\n\n8255\n일본문학\n일본 도서\n문고/신서\n문학/평론\n일본문학\nNA\n\n\n8252\n경제/경영\n일본 도서\n문고/신서\n경제/경영\nNA\nNA\n\n\n8253\n문고기타\n일본 도서\n문고/신서\n문고기타\nNA\nNA\n\n\n8250\n라이트노벨\n일본 도서\n라이트노벨\nNA\nNA\nNA\n\n\n8251\n문고/신서\n일본 도서\n문고/신서\nNA\nNA\nNA\n\n\n8248\n재테크/투자\n일본 도서\n경제/경영\n재테크/투자\nNA\nNA\n\n\n8249\nIT/e-commerce\n일본 도서\n경제/경영\nIT/e-commerce\nNA\nNA\n\n\n\n\n\n\nbook_category_id: 카테고리의 고유 번호\nname: 카테고리 이름\ndepth_1: 카테고리 대분류\ndepth_2: 카테고리 중분류\ndepth_3: 카테고리 소분류_1\ndepth_4: 카테고리 소분류_2\ndepth_5: 카테고리 소분류_3\n\n\n\nscrap: 유저가 책에서 스크랩한 문구\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\ncontent\nbook_id\npage\ncreated_at\n\n\n\n\n69249442\n분노가 가라앉으니 남는 건 부끄러움과 당혹스러움밖에 없었다.\n1511719\n408\n2021-09-01 12:47:40.171670\n\n\n69249442\n슬픔은 그 나름의 수명과 생명력을 가지게 마련이니까.\n1511719\n315\n2021-08-31 13:16:26.898560\n\n\n69249442\n세상에 완벽한 사람은 없어. 그런 사람하고만 친구가 되라는 법도 없고.\n1511719\n219\n2021-08-31 08:04:58.755703\n\n\n69249442\n다들 알아시피, 결말은 두려워하는 사람이 아니라 두려움을 유발하는 사람이 정하는 법이다.\n1560481\n288\n2021-08-29 11:54:53.155436\n\n\n69249442\n불은 아무것도 소유하지 않은 사람만 정화할 뿐이에요. 타오르는 것들 안에는 슬픔과 적막함이 깃들어 있어요.\n1560481\n267\n2021-08-29 11:38:52.528101\n\n\n69249442\n후회할 때가 아니야, 나 자신에게 말했다. 지나간 일은 지나간 일일 뿐이야.\n\n\n\n\n\n내 의무는 살아남는 것이었다.\n1560481\n247\n2021-08-29 10:41:01.534785\n\n\n\n69249442\n그 시간 동안 나는 현실이 언제나 확신을 깨부순다는 사실을 깨달았다.\n1560481\n178\n2021-08-28 16:26:24.889925\n\n\n69249442\n나라 전체가 그랬듯, 우리는 서로에게 남이 되는 형을 선고빋았다.\n1560481\n65\n2021-08-28 15:08:01.429808\n\n\n69249442\n나는 온몸으로 울었다.\n1560481\n35\n2021-08-27 12:31:43.595596\n\n\n69249442\n엄마의 묘비명을 작성하는 동안 첫 번째 죽음은 언어 안에서, 주어를 현재에서 끌어내 과거에 세워두는 과정에서 발생한다는 사실을 깨달았다.\n1560481\n12\n2021-08-26 12:52:51.500924\n\n\n\n\n\n\nuser_id: 유저 고유 번호\ncontent: 스크랩 문구\nbook_id: 스크랩한 책의 고유 번호\npage: 스크랩한 책의 페이지\ncreated_at: 스크랩한 날짜와 시간 (중요한점: 날짜와 시간은 UDT 기준입니다.)"
  },
  {
    "objectID": "teaching/ml101/icpbl/exchange.html",
    "href": "teaching/ml101/icpbl/exchange.html",
    "title": "Final projects",
    "section": "",
    "text": "There are 10 dataset you can choose\n\nSee introduction on the data\n\nAmong the dataset in the link, choose two (e.g. one for classification and the other for regression)\nDo the process below\n\nEDA\n\nData-preprocessing (wrangling)\nDeal with Missing values\nData visualization\nFactorization\nIf needed, standardization of the variables\n\nModeling\nPredicting\n\nData list\n\nNetflix show\nStudents performance\nMobile Price Classification\nDogs & Cats Images\nTrip Advisor Hotel Reviews\nMelbourne Housing Market\nChurn Modelling\nAmazon Best Seller Books\nMedial Cost Personal DB\nKepler Exoplanet Search Results\n\nFinal output\n\nSubmit a summary of your work, no more than 10 pages\nFile type\n\nIt can be in any form, such as a PDF, a link after uploading it to your blog, or a link after writing it on a Notion page, and so on."
  },
  {
    "objectID": "teaching/ml101/icpbl/index.html",
    "href": "teaching/ml101/icpbl/index.html",
    "title": "IC-PBL",
    "section": "",
    "text": "Domestic students (한국 학생)\n\n\n\nExchange students (외국인 학생, 교환 학생)"
  },
  {
    "objectID": "teaching/ml101/index.html",
    "href": "teaching/ml101/index.html",
    "title": "Machine Learning 101",
    "section": "",
    "text": "a <- \"Machine\"\nb <- \"Learning\"\npaste0(\"Welcome to \", a,\" \",b,\" \",100+1)\n\n[1] \"Welcome to Machine Learning 101\"\n\n\n\n\n\n\n\n\n\n\n\nAbout course\n(코스 소개)\n\n\n\nNotice\n(공지)\n\n\n\nWeekly design\n(주차별 학습)\n\n\n\nIC-PBL\n(Final project)"
  },
  {
    "objectID": "teaching/ml101/notice/index.html",
    "href": "teaching/ml101/notice/index.html",
    "title": "Notice",
    "section": "",
    "text": "Pre-class\n\nPrior to attending the offline (or online streaming Zoom class), students are expected to view the recorded lecture delivered by the lecturer.\n\nThey are encouraged to take an active role in their learning by studying the material independently.\nThe video covers the fundamental concepts of the Machine Learning algorithm, including its underlying theory when necessary.\nTo assess their level of understanding, students are required to submit discussion posts. This serves as a means for evaluating their comprehension of the material.\n\n\nIn-class\n\nDuring the class, the lecturer will summarize the pre-class lecture and provide additional clarification on the concepts covered.\nTo reinforce their understanding, students will engage in hands-on practice with more advanced code. This will provide them with the opportunity to apply their knowledge and develop their coding skills.\n\nThe official in-class time is AM 10:00 ~ 12:00"
  },
  {
    "objectID": "teaching/ml101/notice/index.html#textbooks",
    "href": "teaching/ml101/notice/index.html#textbooks",
    "title": "Notice",
    "section": "Textbooks",
    "text": "Textbooks"
  },
  {
    "objectID": "teaching/ml101/notice/index.html#grading",
    "href": "teaching/ml101/notice/index.html#grading",
    "title": "Notice",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nItems\nRatio (%)\n\n\n\n\nAttendance\n0\n\n\nDiscussion Submission\n20\n\n\nQZ #1 & QZ #2\n50\n\n\nIC-PBL Final Score\n30\n\n\nTotal\n100\n\n\n\n\n\nWhen attendance falls below 1/3 in a class, it may result in a grade of F.\n\nThis policy is in place to encourage regular attendance and ensure that all students have the opportunity to fully participate in the learning experience. It is the responsibility of the students to attend classes regularly and stay engaged in their education.\n\nThe Final Score for the ICPBL is calculated by multiplying the ICPBL Team Score with the Individual Peer Review Score.\nIndividual Peer Review Scores are obtained through an anonymous survey conducted within the group.\nIt is worth noting that the team assignment is only available for domestic students. On the other hand, exchange students are required to complete an individual project."
  },
  {
    "objectID": "teaching/ml101/notice/index.html#communication",
    "href": "teaching/ml101/notice/index.html#communication",
    "title": "Notice",
    "section": "Communication",
    "text": "Communication\n\n\nNotices & Questions\n\nPlease join Kakao open-chat room\n\nTHU_ML101_2023 (목요일반)\n\nhttps://open.kakao.com/o/gQnhw75e\n\nTUE_ML101_2023 (화요일반)\n\nhttps://open.kakao.com/o/gwiEw75e\n\n\n\n\n\n\nPersonal counsel (Scholarship, recommendation letter, etc.)\n\nCJ-counselling room (Anything but the class content)\n\nCJ상담실: https://open.kakao.com/o/s90SQocc\npw: hanyang"
  },
  {
    "objectID": "teaching/ml101/weekly/index.html",
    "href": "teaching/ml101/weekly/index.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\n1\n\n\nCourse Intro\n\n\n\n\n\n\n2\n\n\nAbout ML & Modelling\n\n\nEDA Review\n\n\n\n\n3\n\n\nClassification\n\n\nDecision Tree\n\n\n\n\n4\n\n\nClassification\n\n\nRandom Forest\n\n\n\n\n5\n\n\nClassification\n\n\nNaive Bayes\n\n\n\n\n6\n\n\nClassification\n\n\nK-Nearest Neighbors\n\n\n\n\n7\n\n\nRegression\n\n\nLinear Regression\n\n\n\n\n8\n\n\nQZ #1\n\n\n\n\n\n\n9\n\n\nRegression\n\n\nNon-linear Regression\n\n\n\n\n10\n\n\nUnsupervised Learning\n\n\nClustering\n\n\n\n\n11\n\n\nUnsupervised Learning\n\n\nPattern Finding\n\n\n\n\n12\n\n\nModel Improvement\n\n\n\n\n\n\n13\n\n\nNatural Language Process\n\n\n\n\n\n\n14\n\n\nQZ #2\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/01_week.html",
    "href": "teaching/ml101/weekly/posts/01_week.html",
    "title": "Course Intro",
    "section": "",
    "text": "Weekly design\n\n\nWeek 1 Class\n\nSlide\n\n\n\nInstall gadgets\n\nInstall R, R Studio, & Rtools\n\nR\n\nFor window https://cran.r-project.org/bin/windows/base/\nFor mac https://cran.r-project.org/bin/macosx/\n\nR Studio\n\nhttps://posit.co/download/rstudio-desktop/\n\nRtools (only for window user)\n\nhttps://cran.r-project.org/bin/windows/Rtools/\nTo install R packages containing C/C++ language for window users (no need for Linux and Mac users)\n\n\nThings you need to know\n\nDon’t use OneDrive.\n\nUse Github instead\nMany people get an error when installing becausedf OneDrive\n\nSet Windows user name to English\n\nIf Korean characters are mixed in the installation path, there is a high probability of error occurrence\n\n\nInstallation Order\n\nStep 1 ‑ Download the file\n\nDownload R, Rtools, Rstudio installation files\n\nStep 2 - Install R\n\nUnified installation path: All will be installed in the C:/R folder\nRun in administrator mode when running the R installation file\nAfter installing R, grant write permission to the R folder, Right-click and turn off read only\n\nStep 3 ‑ Install Rtools\n\nAdministrator mode execution installation and folder setting as C:\\R\\rtools40\nCreate environment variable RTOOLS40_HOME after installation: Value - C:\\R\\rtools40\\\nAdd %RTOOLS40_HOME%\\usr\\bin\\ to the Path variable.\n\nStep 4 - Install Rstudio\n\nRight-click and run as administrator - installation path C:\\R\\Rstudio\nCheck rtools connection with Sys.which(“make”) command after installation"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html",
    "href": "teaching/ml101/weekly/posts/02_week.html",
    "title": "About ML & Modelling",
    "section": "",
    "text": "Weekly design\nBefore attending class for Week 2, please complete the following tasks:"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#introduction",
    "href": "teaching/ml101/weekly/posts/02_week.html#introduction",
    "title": "About ML & Modelling",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nIt contains the analysis process of ‘Titanic’, one of the representative ‘Competition’ of ‘Kaggle’."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#prepare-to-work",
    "href": "teaching/ml101/weekly/posts/02_week.html#prepare-to-work",
    "title": "About ML & Modelling",
    "section": "2. Prepare to work",
    "text": "2. Prepare to work\n\n\n2.1 Packages\nsee “What is a package in R”\n\nThis is the process of loading (loading) the Packages I used for analysis, in addition to the representative Packages of R, such as tidyverse (including ggplot2 and dplyr).\n\n\n# Data input, assesment \nlibrary(titanic)\nlibrary(readr)           # Data input with readr::read_csv()\nlibrary(descr)           # descr::CrossTable() - Frequency by category, check with ratio figures\n\n# Visualization\nlibrary(VIM)             # Missing values assesment used by VIM::aggr()\nlibrary(RColorBrewer)    # Plot color setting\nlibrary(scales)          # plot setting - x, y axis\n\n# Feature engineering, Data Pre-processing\nlibrary(tidyverse)     # dplyr, ggplot2, purrr, etc..      # Feature Engineering & Data Pre-processing\nlibrary(ggpubr)\n\nlibrary(randomForest)\n# Model validation \nlibrary(caret)           # caret::confusionMatrix()\nlibrary(ROCR)            # Plotting ROC Curve\n\n\n\n\n2.2 Raw data import\n\nIn titanic competition, train data used to create Model and test data used for actual prediction (estimation) are separated.\nHere, we will load those two data and combine them into one. The reason for tying the separate data together is to work the same when feature engineering and pre-processing the input variables used in modeling.\nPlease see this link if you want to know about the story of Titanic.\n\n\ntitanic_train %>% glimpse\n\nRows: 891\nColumns: 12\n$ PassengerId <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    <int> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      <int> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        <chr> \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       <int> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       <int> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      <chr> \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       <chr> \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    <chr> \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\ntitanic_test %>% glimpse\n\nRows: 418\nColumns: 11\n$ PassengerId <int> 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      <int> 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        <chr> \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         <chr> \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         <dbl> 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       <int> 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       <int> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      <chr> \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        <dbl> 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"B45\", \"\",…\n$ Embarked    <chr> \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\ntrain <- titanic_train\ntest  <- titanic_test\n\nfull <- dplyr::bind_rows(train, test)\nfull %>% glimpse\n\nRows: 1,309\nColumns: 12\n$ PassengerId <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    <int> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      <int> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        <chr> \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       <int> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       <int> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      <chr> \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       <chr> \"\", \"C85\", \"\", \"C123\", \"\", \"\", \"E46\", \"\", \"\", \"\", \"G6\", \"C…\n$ Embarked    <chr> \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nThe reason why rbind() was not used even when combining the two data into full is that Survived, the dependent variable (target variable, Y) of Titanic competition, does not exist in test. Therefore, the dimensions (dimension) of the two data do not match, so they are not merged with rbind(). However, if you use dplyr::bind_rows(), Survived in test is treated as NA and merged into one.\n\n\n2.3 variable meaning explanation\n\n\n\n\n\n\n\n\nvariable name\nInterpretation (meaning)\nType\n\n\n\n\nPassengerID\nUnique ID number that identifies the passenger\nInt\n\n\nSurvived\nIndicates whether or not the passenger survived. Survival is 1 and death is 0.\nFactor\n\n\nPclass\nThe class of the cabin, with 3 categories from 1st class (1) to 3rd class (3).\nOrd.Factor\n\n\nName\nPassenger’s name\nFactor\n\n\nSex\nPassenger’s gender\nFactor\n\n\nAge\nAge of passenger\nNumeric\n\n\nSibSp\nVariable describing the number of siblings or spouses accompanying each passenger. It can range from 0 to 8.\nInteger\n\n\nParch\nVariable describing the number of parents or children accompanying each passenger, from 0 to 9.\nInteger\n\n\nTicket\nString variable for the ticket the passenger boarded\nFactor\n\n\nFare\nVariable for how much the passenger has paid for the trip so far\nNumeric\n\n\nCabin\nVariable that distinguishes each passenger’s cabin, with too many categories and missing values.\nFactor\n\n\nEmbarked\nIndicates the boarding port and departure port, and consists of three categories: C, Q, and S.\nFactor\n\n\n\n\n\n\n2.4 Change the variables type\n\nBefore the full-scale EDA and feature engineering, let’s transform some variable properties. For example, Pclass is treated as numeric, but actually 1, 2, 3 are factors representing 1st, 2nd, and 3rd grades.\n\n\nfull <- full %>%\n  dplyr::mutate(Survived = factor(Survived),\n                Pclass   = factor(Pclass, ordered = T),\n                Name     = factor(Name),\n                Sex      = factor(Sex),\n                Ticket   = factor(Ticket),\n                Cabin    = factor(Cabin),\n                Embarked = factor(Embarked))"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#eda-exploratory-data-analysis",
    "href": "teaching/ml101/weekly/posts/02_week.html#eda-exploratory-data-analysis",
    "title": "About ML & Modelling",
    "section": "3. EDA : Exploratory data analysis",
    "text": "3. EDA : Exploratory data analysis\n\nIt is the process of exploring and understanding raw data, such as how data is structured and whether there are missing values or outliers in it.\nWe will use various functions and visualizations here.\n\n\n3.1 Data confirmation using numerical values\nFirst of all, let’s check the data through the output of various functions such as head() and summary().\n\n\n3.1.1 head()\n\n\nhead(full, 10)\n\n   PassengerId Survived Pclass\n1            1        0      3\n2            2        1      1\n3            3        1      3\n4            4        1      1\n5            5        0      3\n6            6        0      3\n7            7        0      1\n8            8        0      3\n9            9        1      3\n10          10        1      2\n                                                  Name    Sex Age SibSp Parch\n1                              Braund, Mr. Owen Harris   male  22     1     0\n2  Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0\n3                               Heikkinen, Miss. Laina female  26     0     0\n4         Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0\n5                             Allen, Mr. William Henry   male  35     0     0\n6                                     Moran, Mr. James   male  NA     0     0\n7                              McCarthy, Mr. Timothy J   male  54     0     0\n8                       Palsson, Master. Gosta Leonard   male   2     3     1\n9    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female  27     0     2\n10                 Nasser, Mrs. Nicholas (Adele Achem) female  14     1     0\n             Ticket    Fare Cabin Embarked\n1         A/5 21171  7.2500              S\n2          PC 17599 71.2833   C85        C\n3  STON/O2. 3101282  7.9250              S\n4            113803 53.1000  C123        S\n5            373450  8.0500              S\n6            330877  8.4583              Q\n7             17463 51.8625   E46        S\n8            349909 21.0750              S\n9            347742 11.1333              S\n10           237736 30.0708              C\n\n\n\nLooking at the result of head(), we can see that there is a missing value (NA) in Age.\nIf so, is there only Age missing in the entire data?\nFor the answer, please refer to 3.2 Missing values.\n\n\n\n3.1.2 str()\n\n\nstr(full)\n\n'data.frame':   1309 obs. of  12 variables:\n $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass     : Ord.factor w/ 3 levels \"1\"<\"2\"<\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : Factor w/ 1307 levels \"Abbing, Mr. Anthony\",..: 156 287 531 430 23 826 775 922 613 855 ...\n $ Sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : Factor w/ 929 levels \"110152\",\"110413\",..: 721 817 915 66 650 374 110 542 478 175 ...\n $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 108 1 72 1 1 165 1 1 1 ...\n $ Embarked   : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 2 4 4 4 3 4 4 4 2 ...\n\n\n\nBy combining the train and test data, the total number of observations (record, row, row) is 1309 (train: 891, test: 418), and the number of variables (column, feature, variable, column) is 12.\nIn addition, you can find out what the attributes of each variable are and how many categories there are for variables that are factor attributes.\nIn addition, in head(), it can be seen that the missing value (NA), which was thought to exist only in Age, also exists in other variables including Cabin.\n\n\n\n3.1.3 summary()\n\n\nsummary(full)\n\n  PassengerId   Survived   Pclass                                Name     \n Min.   :   1   0   :549   1:323   Connolly, Miss. Kate            :   2  \n 1st Qu.: 328   1   :342   2:277   Kelly, Mr. James                :   2  \n Median : 655   NA's:418   3:709   Abbing, Mr. Anthony             :   1  \n Mean   : 655                      Abbott, Master. Eugene Joseph   :   1  \n 3rd Qu.: 982                      Abbott, Mr. Rossmore Edward     :   1  \n Max.   :1309                      Abbott, Mrs. Stanton (Rosa Hunt):   1  \n                                   (Other)                         :1301  \n     Sex           Age            SibSp            Parch            Ticket    \n female:466   Min.   : 0.17   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n male  :843   1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n              Median :28.00   Median :0.0000   Median :0.000   CA 2144 :   8  \n              Mean   :29.88   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n              3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n              Max.   :80.00   Max.   :8.0000   Max.   :9.000   347082  :   7  \n              NA's   :263                                      (Other) :1261  \n      Fare                     Cabin      Embarked\n Min.   :  0.000                  :1014    :  2   \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270   \n Median : 14.454   B57 B59 B63 B66:   5   Q:123   \n Mean   : 33.295   G6             :   5   S:914   \n 3rd Qu.: 31.275   B96 B98        :   4           \n Max.   :512.329   C22 C26        :   4           \n NA's   :1         (Other)        : 271           \n\n\n\nsummary() provides a lot of information about the data.\nThe representative values of quantitative variables (Integer, Numeric), the number of categories of categorical (Factor) variables, and the number of observations belonging to each category are all shown as numerical values.\nHere are the things to check and move on:\n\nSurvived: This is the target variable for this competition, and 418 missing values are due to the test data.\nPclass: There are three categories of 1st class, 2nd class, and 3rd class, and 3rd class passengers are the most.\nName: There are people with similar names. So you can see that some passengers are traveling alone, while others are traveling with their families.\nSex: There are almost twice as many males as females.\nAge: It ranges from 0.17 to 80 years old, but it seems necessary to check whether it is an outlier that incorrectly entered 17, and there are 263 missing values.\nSibSp: From 0 to 8, and the 3rd quartile is 1, so it can be seen that you boarded the Titanic with a couple or siblings.\nParch: It ranges from 0 to 9, but the fact that the 3rd quartile is 0 indicates that there are very few passengers with parents and children.\nBoth SibSp and Parch are variables representing family relationships. Through this, we will find out the total number of people in the family, although we do not know who was on board, and based on that, we will create a categorical derived variable called FamilySized that represents the size of the family.\nTicket: Looking at the result of 3.1.2 str(), you can see that some passengers have exactly the same ticket, some passengers have tickets overlapping only a certain part, and some passengers have completely different tickets. We plan to use this to create a derived variable called ticket.size.\nFare: 0 to 512, with 1 missing value. I care that the 3rd quartile is 31.275 and the max is 512.\nCabin: It has the most (1014) missing values among a total of 12 features. It’s a variable that represents the ship’s area, but if there’s no way to use it, I think it should be discarded.\nEmbarked: It consists of a total of 3 categories, S is the most, and there are 2 missing values.\n\nWhen performing a basic exploration of the data, please look at the outputs of various functions besides summary() and str() while comparing them.\n\n\n\n\n3.2 Missing values\n\nThis is the process of checking which variables have missing values mentioned above and how many of them exist.\nI’m going to check it numerically and visually at the same time using the dplyr, ggplot2, and VIM packages.\nYou don’t have to use all the code I’ve run, you can use only the parts you think you need or like as you read.\n\n\n3.2.1 VIM packages\n\n\nVIM::aggr(full, prop = FALSE, combined = TRUE, numbers = TRUE,\n          sortVars = TRUE, sortCombs = TRUE)\n\n\n\n\n\n Variables sorted by number of missings: \n    Variable Count\n    Survived   418\n         Age   263\n        Fare     1\n PassengerId     0\n      Pclass     0\n        Name     0\n         Sex     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n       Cabin     0\n    Embarked     0\n\n\n\n\n\n3.2.2 tidyverse packages\n\nIn addition to checking missing values at once using the VIM package, these are methods for checking missing values using various packages that exist in the tidyverse.\nFirst, find the proportion of missing values for each variable with dplyr.\n\n\nfull %>%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n\n\nThere is a way to check the proportion of missing values that exist in variables, but it can also be checked using visual data.\nPlease see the two bar plots below.\n\n\n# Calculate the missing value ratio of each feature -> Data Frame property but has a structure of 1 row and 12 columns.\nmissing_values <- full %>%\n  dplyr::summarize_all(funs(sum(is.na(.))/n()))\n\nmissing_values %>% head\n\n  PassengerId  Survived Pclass Name Sex       Age SibSp Parch Ticket\n1           0 0.3193277      0    0   0 0.2009167     0     0      0\n          Fare Cabin Embarked\n1 0.0007639419     0        0\n\n# Generate the missing_values obtained above as a 12X2 data frame\nmissing_values <- tidyr::gather(missing_values,\n                                key = \"feature\", value = \"missing_pct\")\n\nmissing_values %>% head(12)\n\n       feature  missing_pct\n1  PassengerId 0.0000000000\n2     Survived 0.3193277311\n3       Pclass 0.0000000000\n4         Name 0.0000000000\n5          Sex 0.0000000000\n6          Age 0.2009167303\n7        SibSp 0.0000000000\n8        Parch 0.0000000000\n9       Ticket 0.0000000000\n10        Fare 0.0007639419\n11       Cabin 0.0000000000\n12    Embarked 0.0000000000\n\n# Visualization with missing_values\nmissing_values %>% \n  # Aesthetic setting : missing_pct 내림차순으로 정렬  \n  ggplot(aes(x = reorder(feature, missing_pct), y = missing_pct)) +\n  # Bar plot \n  geom_bar(stat = \"identity\", fill = \"red\") +\n  # Title generation \n  ggtitle(\"Rate of missing values in each features\") +\n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = \"Feature names\", y = \"Rate\") + \n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nIf you look at the bar graph above, you can check the percentage of missing values for all features.\nHowever, what we are actually curious about is which variables have missing values and how many missing values exist in them.\nTherefore, after calculating the proportion of missing values using the purrr package, I extracted only the variables that had at least one and visualized them.\n\n\n# 변수별 결측치 비율 계산\nmiss_pct <- purrr::map_dbl(full, function(x){round((sum(is.na(x))/length(x)) * 100, 1) })\n\n# 결측치 비율이 0%보다 큰 변수들만 선택\nmiss_pct <- miss_pct[miss_pct > 0]\n\n# Data Frame 생성 \ndata.frame(miss = miss_pct, var = names(miss_pct), row.names = NULL) %>%\n  # Aesthetic setting : miss 내림차순으로 정렬 \n  ggplot(aes(x = reorder(var, miss), y = miss)) + \n  # Bar plot \n  geom_bar(stat = 'identity', fill = 'red') +\n  # Plot title setting \n  ggtitle(\"Rate of missing values\") + \n  # Title detail setting \n  theme(plot.title = element_text(face = \"bold\",    # 글씨체 \n                                  hjust = 0.5,      # Horizon(가로비율) = 0.5\n                                  size = 15, color = \"darkblue\")) +\n  # x, y axis label setting \n  labs(x = 'Feature names', y = 'Rate of missing values') +\n  # Plot의 x, y축 변환 \n  coord_flip()\n\n\n\n\n\nThrough this, only 4 variables out of a total of 12 variables have missing values (except Survived because it is due to test data), and there are many missing values in the order of Cabin, Age, Embarked, and Fare.\n\n\n\nNow, it is the process of analyzing and exploring feature through visualization.\n\n\n\n3.3 Age\n\n\nage.p1 <- full %>% \n  ggplot(aes(Age)) + \n  geom_histogram(breaks = seq(0, 80, by = 1), # interval setting \n                 col    = \"red\",              # bar border color\n                 fill   = \"green\",            # bar inner color\n                 alpha  = .5) +               # Bar Transparency = 50%\n  \n  # Plot title\n  ggtitle(\"All Titanic passengers age hitogram\") +\n  theme(plot.title = element_text(face = \"bold\",     \n                                  hjust = 0.5,      # Horizon (horizontal ratio) = 0.5\n                                  size = 15, color = \"darkblue\"))\n\nage.p2 <- full %>% \n# Exclude values where Survived == NA in the test dataset  \n  filter(!is.na(Survived)) %>% \n  ggplot(aes(Age, fill = Survived)) + \n  geom_density(alpha = .5) +\n  ggtitle(\"Titanic passengers age density plot\") + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5,\n                                  size = 15, color = \"darkblue\"))\n\n# Display the two graphs created above on one screen\nggarrange(age.p1, age.p2, ncol=2)\n\n\n\n\n\n\n\n3.4 Pclass\n\nLet’s visualize the frequency of passengers for each Pclass.\nAfter grouping (grouping) by Pclass using dplyr package, Data Frame representing frequency by category was created and visualized with ggplot.\n\n\nfull %>% \n  # Get Pclass frequencies using dplyr::group_by(), summarize()\n  group_by(Pclass) %>% \n  summarize(N = n()) %>% \n  # Aesthetic setting \n  ggplot(aes(Pclass, N)) +\n  geom_col() +\n  geom_text(aes(label = N),       \n            size = 5,             \n            vjust = 1.2,           \n            color = \"#FFFFFF\") +  \n  # Plot title \n  ggtitle(\"Number of each Pclass's passengers\") + \n  # Title setting \n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15)) +\n  # x, y axis name change  \n  labs(x = \"Pclass\", y = \"Count\")\n\n\n\n\n\nIt can be seen that the largest number of passengers boarded in the 3-class cabin.\n\n\n\n3.5 Fare\n\nThis is a visualization of the ‘Fare’ variable, which represents the amount paid by the passenger.\nTwo histograms and boxplots were used.\n\n# Histogram \nFare.p1 <- full %>%\n  ggplot(aes(Fare)) + \n  geom_histogram(col    = \"yellow\",\n                 fill   = \"blue\", \n                 alpha  = .5) +\n  ggtitle(\"Histogram of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\n# Boxplot \nFare.p2 <- full %>%\n  filter(!is.na(Survived)) %>% \n  ggplot(aes(Survived, Fare)) + \n  # Observations are drawn as gray dots, but overlapping areas are spread out.  \n  geom_jitter(col = \"gray\") + \n  # Boxplot: 50% transparency\n  geom_boxplot(alpha = .5) + \n  ggtitle(\"Boxplot of passengers Fare\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5, size = 15))\n\nggarrange(Fare.p1, Fare.p2, ncol=2)\n\n\n\n\n\nYou can see that the survivors have a higher ‘Fare’ than the deceased passengers, but not by much.\n\n\n\n3.6 Sex\nAre there differences in survival rates between men and women? See the plot below.\n\n\nsex.p1 <- full %>% \n  dplyr::group_by(Sex) %>% \n  summarize(N = n()) %>% \n  ggplot(aes(Sex, N)) +\n  geom_col() +\n  geom_text(aes(label = N), size = 5, vjust = 1.2, color = \"#FFFFFF\") + \n  ggtitle(\"Bar plot of Sex\") +\n  labs(x = \"Sex\", y = \"Count\")\n  \nsex.p2 <- full[1:891, ] %>% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  ggtitle(\"Survival Rate by Sex\") + \n  labs(x = \"Sex\", y = \"Rate\")\n\nggarrange(sex.p1, sex.p2, ncol = 2)\n\n\n\nmosaicplot(Survived ~ Sex,\n           data = full[1:891, ], col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\nIf you interpret the graph, you can see that the survival rate is higher for female passengers, while there are far more males than females."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#feature-engineering-data-pre-processing",
    "href": "teaching/ml101/weekly/posts/02_week.html#feature-engineering-data-pre-processing",
    "title": "About ML & Modelling",
    "section": "4. Feature engineering & Data Pre-processing",
    "text": "4. Feature engineering & Data Pre-processing\n\nThis is the process of filling in missing values (‘NA’) based on the contents of ‘Chapter 3 EDA’ and creating derived variables at the same time.\n\n\n4.1 Age -> Age.Group\n\n\nfull <- full %>%\n# The missing value (NA) is filled in first, and the average of the values excluding the missing value is filled.\n  mutate(Age = ifelse(is.na(Age), mean(full$Age, na.rm = TRUE), Age),\n# Create a categorical derived variable Age.Group based on Age values\n        Age.Group = case_when(Age < 13             ~ \"Age.0012\",\n                               Age >= 13 & Age < 18 ~ \"Age.1317\",\n                               Age >= 18 & Age < 60 ~ \"Age.1859\",\n                               Age >= 60            ~ \"Age.60inf\"),\n# Convert Chr attribute to Factor\n        Age.Group = factor(Age.Group))\n\n\n\n\n4.3 SibSp & Parch -> FamilySized\n\n\nfull <- full %>% \n # First create a derived variable called FamilySize by adding SibSp, Parch and 1 (self)\n  mutate(FamilySize = .$SibSp + .$Parch + 1,\n        # Create a categorical derived variable FamilySized according to the value of FamilySize\n         FamilySized = dplyr::case_when(FamilySize == 1 ~ \"Single\",\n                                        FamilySize >= 2 & FamilySize < 5 ~ \"Small\",\n                                        FamilySize >= 5 ~ \"Big\"),\n        # Convert the Chr property FamilySized to a factor\n        # Assign new levels according to the size of the group size\n         FamilySized = factor(FamilySized, levels = c(\"Single\", \"Small\", \"Big\")))\n\n\nCeated FamilySized using SibSp and Parch.\nReducing these two variables to one has the advantage of simplifying the model.\nA similar use case is to combine height and weight into a BMI index.\n\n\n\n4.4 Name & Sex -> title\n\nWhen looking at the results of ‘Chapter 3.6 Sex’, it was confirmed that the survival rate of women was higher than that of men.\nTherefore, in Name, “Wouldn’t it be useful to extract only names related to gender and categorize them?” I think it is.\nFirst, extract only the column vector named Name from full data and save it as title.\n\n\n# First, extract only the Name column vector and store it in the title vector\ntitle <- full$Name\ntitle %>% head(20)\n\n [1] Braund, Mr. Owen Harris                                \n [2] Cumings, Mrs. John Bradley (Florence Briggs Thayer)    \n [3] Heikkinen, Miss. Laina                                 \n [4] Futrelle, Mrs. Jacques Heath (Lily May Peel)           \n [5] Allen, Mr. William Henry                               \n [6] Moran, Mr. James                                       \n [7] McCarthy, Mr. Timothy J                                \n [8] Palsson, Master. Gosta Leonard                         \n [9] Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      \n[10] Nasser, Mrs. Nicholas (Adele Achem)                    \n[11] Sandstrom, Miss. Marguerite Rut                        \n[12] Bonnell, Miss. Elizabeth                               \n[13] Saundercock, Mr. William Henry                         \n[14] Andersson, Mr. Anders Johan                            \n[15] Vestrom, Miss. Hulda Amanda Adolfina                   \n[16] Hewlett, Mrs. (Mary D Kingcome)                        \n[17] Rice, Master. Eugene                                   \n[18] Williams, Mr. Charles Eugene                           \n[19] Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\n[20] Masselmani, Mrs. Fatima                                \n1307 Levels: Abbing, Mr. Anthony ... Zimmerman, Mr. Leo\n\n# Using regular expression and gsub(), extract only names that are highly related to gender and save them as title vectors\ntitle <- gsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title)\ntitle %>% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n# Save the title vector saved above to full again, but save it as a title derived variable\nfull$title <- title\n\nfull$title %>% head(20)\n\n [1] \"Mr\"     \"Mrs\"    \"Miss\"   \"Mrs\"    \"Mr\"     \"Mr\"     \"Mr\"     \"Master\"\n [9] \"Mrs\"    \"Mrs\"    \"Miss\"   \"Miss\"   \"Mr\"     \"Mr\"     \"Miss\"   \"Mrs\"   \n[17] \"Master\" \"Mr\"     \"Mrs\"    \"Mrs\"   \n\n\n\ngsub(\"^.*, (.*?)\\\\..*$\", \"\\\\1\", title) In short, this code uses gsub to search for and replace a pattern in the title string, where the pattern is defined by the regular expression \"^.*, (.*?)\\\\..*$\" and the replacement is defined by the string \"\\\\1\". If you want to understand more about regular expression. Please see my blog post: What are Regular Expressions and How to Use Them in R\nThen check what are the Unique titles.\n\n\nunique(full$title)\n\n [1] \"Mr\"           \"Mrs\"          \"Miss\"         \"Master\"       \"Don\"         \n [6] \"Rev\"          \"Dr\"           \"Mme\"          \"Ms\"           \"Major\"       \n[11] \"Lady\"         \"Sir\"          \"Mlle\"         \"Col\"          \"Capt\"        \n[16] \"the Countess\" \"Jonkheer\"     \"Dona\"        \n\n\n\nYou can see that there are 18 categories in total.\nIf you use this derived variable called ‘title’ as it is, the complexity of the model (especially the tree based model) increases considerably, so you need to reduce the category.\nBefore that, let’s check the frequency and rate for each category using the descr package.\n\n\n# Check frequency, ratio by category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|         Capt |          Col |          Don |         Dona |           Dr |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            4 |            1 |            1 |            8 |\n|        0.001 |        0.003 |        0.001 |        0.001 |        0.006 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|     Jonkheer |         Lady |        Major |       Master |         Miss |\n|--------------|--------------|--------------|--------------|--------------|\n|            1 |            1 |            2 |           61 |          260 |\n|        0.001 |        0.001 |        0.002 |        0.047 |        0.199 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|         Mlle |          Mme |           Mr |          Mrs |           Ms |\n|--------------|--------------|--------------|--------------|--------------|\n|            2 |            1 |          757 |          197 |            2 |\n|        0.002 |        0.001 |        0.578 |        0.150 |        0.002 |\n|--------------|--------------|--------------|--------------|--------------|\n\n|          Rev |          Sir | the Countess |\n|--------------|--------------|--------------|\n|            8 |            1 |            1 |\n|        0.006 |        0.001 |        0.001 |\n|--------------|--------------|--------------|\n\n\n\nThe frequencies and proportions of the 18 categories are very different.\nSo let’s narrow these down to a total of five categories.\n\n\n# Simplify into 5 categories\nfull <- full %>%\n# If you use \"==\" instead of \"%in%\", it won't work as you want because of Recyling Rule.\n  mutate(title = ifelse(title %in% c(\"Mlle\", \"Ms\", \"Lady\", \"Dona\"), \"Miss\", title),\n         title = ifelse(title == \"Mme\", \"Mrs\", title),\n         title = ifelse(title %in% c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\", \"Don\",\n                                     \"Sir\", \"the Countess\", \"Jonkheer\"), \"Officer\", title),\n         title = factor(title))\n\n# After creating the derived variable, check the frequency and ratio for each category\ndescr::CrossTable(full$title)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Row Total | \n|-------------------------|\n\n|  Master |    Miss |      Mr |     Mrs | Officer |\n|---------|---------|---------|---------|---------|\n|      61 |     266 |     757 |     198 |      27 |\n|   0.047 |   0.203 |   0.578 |   0.151 |   0.021 |\n|---------|---------|---------|---------|---------|\n\n\n\n\n\n4.5 Ticket -> ticket.size\n\nAs we saw in Chapter 3.1.3 Summary(), the number of passengers (train and test together) is 1309. However, all passengers’ tickets are not different.\nSee the results of summary() and unique() below.\n\n\n# We used length() to get only the number of unique categories.\nlength(unique(full$Ticket))\n\n[1] 929\n\n# Printing all of them was too messy, so only 10 were printed.\nhead(summary(full$Ticket), 10)\n\n    CA. 2343         1601      CA 2144      3101295       347077       347082 \n          11            8            8            7            7            7 \n    PC 17608 S.O.C. 14879       113781        19950 \n           7            7            6            6 \n\n\n\nWhy are there 929 unique tickets when there are no missing values in feature?\nEven the ticket is CA. There are 11 exactly the same number of people as 2343.\nLet’s see who the passengers are.\n\n\nfull %>% \n# Filter only 11 passengers with matching tickets\n  filter(Ticket == \"CA. 2343\") %>% \n  # We don't need to check for all variables, so we only want to look at the variables below.\n  select(Pclass, Name, Age, FamilySized)\n\n   Pclass                              Name      Age FamilySized\n1       3        Sage, Master. Thomas Henry 29.88114         Big\n2       3      Sage, Miss. Constance Gladys 29.88114         Big\n3       3               Sage, Mr. Frederick 29.88114         Big\n4       3          Sage, Mr. George John Jr 29.88114         Big\n5       3           Sage, Miss. Stella Anna 29.88114         Big\n6       3          Sage, Mr. Douglas Bullen 29.88114         Big\n7       3 Sage, Miss. Dorothy Edith \"Dolly\" 29.88114         Big\n8       3                   Sage, Miss. Ada 29.88114         Big\n9       3             Sage, Mr. John George 29.88114         Big\n10      3       Sage, Master. William Henry 14.50000         Big\n11      3    Sage, Mrs. John (Annie Bullen) 29.88114         Big\n\n\n\nYou can see that the 11 passengers above are all from the same family, brothers.\nWhile there are passengers whose tickets are exactly the same, there are also passengers whose tickets are partially matched.\nCreate a ticket.unique derived variable that represents the number of unique numbers (number of characters) of such a ticket.\nLet’s create a derived variable ticket.size with 3 categories based on ticket.unique.\n\n\n# First of all, ticket.unique is saved as all 0\nticket.unique <- rep(0, nrow(full))\n\n# Extract only the unique ones from ticket features and store them in the tickets vector\ntickets <- unique(full$Ticket)\n\n# After extracting only passengers with the same ticket by using overlapping loops, extract and store the length (number of characters) of each ticket.\n\nfor (i in 1:length(tickets)) {\n  current.ticket <- tickets[i]\n  party.indexes <- which(full$Ticket == current.ticket)\n    # For loop 중첩 \n    for (k in 1:length(party.indexes)) {\n    ticket.unique[party.indexes[k]] <- length(party.indexes)\n    }\n  }\n\n# Save ticket.unique calculated above as a derived variable\nfull$ticket.unique <- ticket.unique\n\n# Create ticket.size variable by dividing it into three categories according to ticket.unique\n\nfull <- full %>% \n  mutate(ticket.size = case_when(ticket.unique == 1 ~ 'Single',\n                                 ticket.unique < 5 & ticket.unique >= 2 ~ \"Small\",\n                                 ticket.unique >= 5 ~ \"Big\"),\n         ticket.size = factor(ticket.size,\n                              levels = c(\"Single\", \"Small\", \"Big\")))\n\n\n\n\n4.6 Embarked\n\nThis is feature with two missing values (NA). In the case of Embarked, replace it with S, which is the most frequent value among the three categories.\n\n\nfull$Embarked <- replace(full$Embarked,               # Specify Data$feature to replace\n                         which(is.na(full$Embarked)), # Find only missing values\n                         'S')                        # specify the value to replace\n\n\n\n\n4.7 Fare\n\nFor Fare, there was only one missing value.\nBased on the histogram seen above (Chapter 3.5 Fare), missing values are replaced with 0.\n\n\nfull$Fare <- replace(full$Fare, which(is.na(full$Fare)), 0)\n\n\nAt this point, data preprocessing is complete.\nThe following is the process of selecting the variables to be used for model creation while exploring the derived variables created so far.\nIn other words, Feature selection."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#relationship-to-target-feature-survived-feature-selection",
    "href": "teaching/ml101/weekly/posts/02_week.html#relationship-to-target-feature-survived-feature-selection",
    "title": "About ML & Modelling",
    "section": "5. Relationship to target feature Survived & Feature selection",
    "text": "5. Relationship to target feature Survived & Feature selection\n\nPrior to full-scale visualization, since the purpose here is to see how well each variable correlates with the survival rate, we did not use the entire full data, but only the train data set that can determine survival and death.\nAlso, please note that the plot used above may be duplicated as it is.\n\n\n5.0 Data set split\nFirst, use the code below to split preprocessed full data into train and test.\n\n# Before feature selection, select all variables first.\n\ntrain <- full[1:891, ]\n\ntest <- full[892:1309, ]\n\n\n\n\n5.1 Pclass\n\n\ntrain %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(position = \"fill\") +\n# Set plot theme: Converts to a more vivid color.\n  scale_fill_brewer(palette = \"Set1\") +\n  # Y axis setting \n  scale_y_continuous(labels = percent) +\n# Set x, y axis names and plot main title, sub title\n  labs(x = \"Pclass\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Pclass?\")\n\n\n\n\n\n\n\n5.2 Sex\n\nSame as Chapter 3.6 Sex.\n\n\nmosaicplot(Survived ~ Sex,\n           data = train, col = TRUE,\n           main = \"Survival rate by passengers gender\")\n\n\n\n\n\n\n\n5.3 Embarked\n\n\ntrain %>% \n  ggplot(aes(Embarked, fill = Survived)) +\n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Embarked\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"How many people survived in each Embarked?\")\n\n\n\n\n\n\n\n5.4 FamilySized\n\n\ntrain %>% \n  ggplot(aes(FamilySized, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"FamilySized\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by FamilySized\")\n\n\n\n\n\nIt can be seen that there is a difference in survival rate depending on the number of people on board, and that ‘FamilySized’ and ‘Survived’ have a non-linear relationship.\n\n\n5.5 Age.Group\n\n\ntrain %>% \n  ggplot(aes(Age.Group, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"Age group\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by Age group\")\n\n\n\n\n\n\n\n5.6 title\n\n\ntrain %>% \n  ggplot(aes(title, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"title\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by passengers title\")\n\n\n\n\n\n\n\n5.7 ticket.size\n\n\ntrain %>% \n  ggplot(aes(ticket.size, fill = Survived)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") + \n  scale_y_continuous(labels = percent) +\n  labs(x = \"ticket.size\", y = \"Rate\",\n       title = \"Bar plot\", subtitle = \"Survival rate by ticket.size\")\n\n\n\n\n\n\n\n5.8 Description of actual used features\n\nNow that all the derived variables created so far have been found to be useful, select and save only the variables you will actually use.\nThe table below is a brief description of the actual selected variables.\n\n\n\n\n\n\n\n\nvariable name\nType\nDescription\n\n\n\n\nSurvived\nfactor\nTarget feature, survival == 1, death == 0\n\n\nSex\nfactor\ngender, male or female\n\n\nPclass\nfactor\nCabin Class, First Class (1), Second Class (2), Third Class (3)\n\n\nEmbarked\nfactor\nPort of embarkation, Southampton (S), Cherbourg (C), Queenstown (Q)\n\n\nFamilySized\nfactor\nFamily size, a derived variable created using SibSp and Parch, with 3 categories\n\n\nAge.Group\nfactor\nAge group, a derived variable created using Age, with 4 categories\n\n\ntitle\nfactor\nA part of the name, a derived variable made using Name, and 5 categories\n\n\nticket.size\nfactor\nThe length of the unique part of the ticket, a derived variable created using ticket, with 3 categories\n\n\n\n\n\n# Excluding ID number, select and save 7 input variables and 1 target variable to actually use\n\ntrain <- train %>% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\", \"Survived\")\n\n# For Submit, extract the Id column vector and store it in ID\n\nID <- test$PassengerId\n\n# Select and save the remaining 6 variables except for Id and Survived\n\ntest <- test %>% \n  select(\"Pclass\", \"Sex\", \"Embarked\", \"FamilySized\",\n         \"Age.Group\", \"title\", \"ticket.size\")"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/02_week.html#machine-learning-model-generation",
    "href": "teaching/ml101/weekly/posts/02_week.html#machine-learning-model-generation",
    "title": "About ML & Modelling",
    "section": "6. Machine learning model generation",
    "text": "6. Machine learning model generation\n\nNow is the time to create a machine learning model using the train data set.\nOriginally, it is correct to create train, validation, test data sets first, create various models, and then select the final model through cross validation (CV, Cross Validation), but these processes are omitted here and RandomForest After creating only, we will predict (estimate) the test data and even create data to Submit to competition.\n\n\n6.1 Random Forest model generation\n\n\n# Set the seed number for reproducibility.\nset.seed(1901)\n\ntitanic.rf <- randomForest(Survived ~ ., data = train, importance = T, ntree = 2000)\n\n\n\n\n6.2 Feature importance check\n\n\nimportance(titanic.rf)\n\n                    0        1 MeanDecreaseAccuracy MeanDecreaseGini\nPclass      47.442449 53.94070             64.73724        36.807804\nSex         54.250630 37.30378             58.66109        57.223102\nEmbarked    -6.328112 38.10930             27.32587         9.632958\nFamilySized 32.430898 31.24383             50.13349        18.086894\nAge.Group   15.203313 26.72696             29.36321        10.201187\ntitle       48.228450 41.60124             57.02653        73.146999\nticket.size 39.544367 37.80849             59.59915        22.570142\n\nvarImpPlot(titanic.rf)\n\n\n\n\n\n\n6.3 Predict test data and create submit data\n\n\n# Prediction \npred.rf <- predict(object = titanic.rf, newdata = test, type = \"class\")\n\n# Data frame generation \nsubmit <- data.frame(PassengerID = ID, Survived = pred.rf)\n\n# Write the submit data frame to file : csv is created in the folder designated by setwd().\n\nwrite.csv(submit, file = './titanic_submit.csv', row.names = F)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/03_week.html",
    "href": "teaching/ml101/weekly/posts/03_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nLearning Types\n\n\n\nSupervised learning & Decision Tree\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #2\n\n<p>Loading…</p>\n\n\n\n\nClass\n\n\nWhy do we need various ML algorithms?\n\n“Each machine learning algorithm serves a specific purpose, much like how each individual serves a unique role in society. There is no one superior ML algorithm, as each is used for a different purpose.”\n\n\n\n\nDifferent types of data and problems: Different ML algorithms are suited to different types of data and problems. For example, linear regression is well-suited for continuous numerical data, while decision trees are well-suited for categorical data. Similarly, some algorithms are better suited for classification problems (predicting a categorical outcome), while others are better suited for regression problems (predicting a continuous outcome).\nPerformance trade-offs: Different ML algorithms can have different trade-offs in terms of performance and interpretability. For example, some algorithms, like k-nearest neighbors, are simple to understand and interpret, but may not perform as well as more complex algorithms like neural networks. On the other hand, some algorithms, like deep learning, can achieve very high performance, but may be more difficult to interpret.\nModel assumptions: Different ML algorithms make different assumptions about the relationship between the predictors and the outcome. For example, linear regression assumes a linear relationship, while decision trees do not make this assumption. Selecting the appropriate algorithm depends on the nature of the problem and the data, as well as the goals of the analysis.\nComputational resources: Some ML algorithms are more computationally intensive than others, and may require more resources (such as memory and processing power) to fit and use. In some cases, a more complex algorithm may be necessary to achieve the desired level of performance, but in other cases, a simpler algorithm may be sufficient.\nModel interpretability: Some ML algorithms are more interpretable than others, which can be important in certain applications. For example, in medical applications, it may be important to understand why a model is making a certain prediction, while in other applications, it may be more important to simply achieve high accuracy.\n\nThe choice of ML algorithm depends on the data, the problem, and the goals of the analysis, and it is important to consider these factors when selecting an algorithm.\n\n\n\nDecision Tree\n\nGenerates a set of rules that allow for accurate predictions using one explanatory variable at a time, resulting in a structure similar to an inverted tree: decision tree\n한번에 하나씩의 설명 변수를 사용하여 정확한 예측이 가능한 규칙들의 집합을 생성.\n\n\n\n\nTerminologies\n\nNode: A point in the decision tree where a decision or split is made.\nParent node: A node that has other nodes connected to it, called child nodes.\nChild node: A node that is connected to a parent node and splits off in a different direction in the tree.\nSplit criterion: The criterion used to determine the split at each node in the decision tree. This can be based on various measures such as information gain, Gini impurity, or chi-squared statistic.\nRoot node: The top node in the decision tree that represents the entire dataset.\nLeaf node: A node in the decision tree that represents a final prediction or classification, as it does not have any child nodes connected to it.\n\n\nThen, “Why is it called a ‘decision’ tree?”\n\nThis is because it presents results in the form of easily understandable rules,\nreduces the need for pre-processing data such as normalization or imputation,\nand considers both numeric and categorical variables as its independent variables (features).\n\n\n\nEssential ideas of decision tree\n\nRecursive Partitioning: The process of dividing the input variable space into two parts, with the goal of increasing the purity of each area after separation rather than before separation\nPruning the Tree: The process of consolidating areas that have been separated into overly detailed regions in order to prevent overfitting.”\n\n\n\n\nImpurity index 1: Gini index\n\n\n\\[\nI(A)=1-\\sum_{k = 1}^{c} p_k^2\n\\]\n\n\\(I(A)\\) is Gini index of A area in which there is c number of classes\n\\(P_k\\) is number of observations in k class\n\n\n\n\n\\[\nI(A)=1-\\sum_{k = 1}^{c} p_k^2\n=1-(\\frac{6}{16})^2-(\\frac{10}{16})^2 = 0.4688\n\\]\n\n\\(I(A)\\) is 0 when there are only one category in area c\n\\(I(A)\\) is 0.5 when there are half of one and half of another category in area c\n\n\n\n\nGini index when there are two or more areas\n\\[\nI(A)=\\sum_{i = 1}^{d} (R_i(1-\\sum_{k = 1}^{c} p_{ik}^2))\n\\]\n\nAfter splitting the area A, \\(I(A)\\) is Gini Index for two or more areas\n\\(R_i\\) is a ratio of i area in A\n\n\n\n\n\\[\nI(A)=\n\\frac{8}{16}\n\\times\n(1-(\\frac{7}{8})^2 -(\\frac{1}{8})^2)+\n\\frac{8}{16}\n\\times\n(1-(\\frac{3}{8})^2 -(\\frac{5}{8})^2)=0.3438\n\\]\n\nAfter splitting, the information gain is \\(0.4668 - 0.3438=0.1250\\)\n\n\n\n\nImpurity index 2: Deviance\n\\[\nD_i = -2 \\sum_{k}n_{ik} log(P_{ik})\n\\]\n\n\\(i\\) is node index\n\\(k\\) is class index\n\\(P_{ik}\\) is a probability of class k in node i\n\n\n\\[\nD_i = -2 \\times (10 \\times log (\\frac{10}{16})  + 6 \\times\n(\\frac{6}{16}))=21.17\n\\]\n\n\n\\[\nD_1 =\n-2 \\times (7 \\times log (\\frac{7}{8})  +\n1 \\times log(\\frac{1}{8}))=6.03\n\\]\n\\[\nD_2 =\n-2 \\times (3 \\times log (\\frac{3}{8})  +\n5 \\times log(\\frac{5}{8}))=10.59\n\\]\n\\[\nD_1 + D_2 = 16.62\n\\]\n\n\nAfter splitting, the information gain is \\(21.17 - 16.62=4.55\\)\n\n\n\n\nOver-fitting issue\n\nOver-fitting is a common issue in machine learning algorithms, including decision trees. It occurs when a model learns the training data too well and captures not only the underlying patterns but also the noise present in the data. As a result, the model becomes too complex and performs poorly on new, unseen data, even though it may have high accuracy on the training data.\nIn the context of decision trees, over-fitting can occur when the tree becomes too deep, with many levels and branches. A deep tree may split the training data into very specific and detailed regions, resulting in a model that is too tailored to the training data and not generalizable to new data.\n\n \n\nTo address over-fitting in decision trees, several techniques can be employed:\n\n\nPruning: This involves trimming the tree by removing branches that do not significantly improve the model’s performance. Pruning can be done in a top-down manner (pre-pruning), where the tree is stopped from growing further once a certain depth or node count is reached, or in a bottom-up manner (post-pruning), where the tree is fully grown and then branches are removed based on certain criteria.\nLimiting tree depth: By setting a maximum depth for the tree, the algorithm is forced to make fewer splits and create a simpler model, reducing the likelihood of over-fitting.\nUsing minimum node size: Setting a minimum number of samples required to create a split or leaf node can prevent the tree from making splits that are too specific to the training data.\nCross-validation: This technique involves splitting the data into multiple training and validation sets and averaging the model’s performance across these sets. Cross-validation can help in choosing the optimal tree depth and other hyper-parameters that lead to the best generalization performance.\n\n\n\n\nPruning\n \n\nA technique used to reduce over-fitting in decision tree models by removing branches that do not significantly improve model performance.\n\n\nGoal: To simplify the tree, improving generalizability and reducing complexity, while maintaining prediction accuracy.\nTwo main types of pruning:\n\nPre-pruning (Top-down): Stops tree growth once certain criteria are met, such as maximum depth or minimum node size.\nPost-pruning (Bottom-up): Allows the tree to fully grow, then iteratively removes branches based on specific criteria, such as error rate or information gain.\n\nCriteria for pruning: Can be based on various measures, such as error rate, information gain, or Gini impurity, and the cost of complexity (CC)\nCross-validation: Often used in conjunction with pruning to determine the optimal level of pruning, by comparing model performance on multiple training and validation sets.\nBenefits: Pruning can lead to better generalization performance, reduced over-fitting, and more interpretable models.\n\n\n\n\nCost of complexity\n\\[\nCC(T)=Err(T)+\\alpha L(T)\n\\]\n\n\\(CC(T)\\) is cost of complexity\n\\(Err(T)\\) is error rate of Tree with test dataset\n\\(L(T)\\) is the number of last nodes of Tree\n\\(\\alpha\\) is weight of combining Err(T) and L(T): depends on researcher’s decision.\n\n\n\nCost of complexity Example #1\n\n\n\nTree 1: Err(T) = 10%\nTree 2: Err(T) = 15%\nChoose Tree A is better as the same number of last nodes\n\n\n\n\nCost of complexity Example #2\n\n\n\nTree A: Err(T) = 15%\nTree B: Err(T) = 15%\nChoose Tree A is better as the same Err(T) but low number of last nodes\n\n\n\n\n\nPractice with data\n\n\nhead(iris, 10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\nUsing ‘rpart’ to train the model with the ‘iris’ dataset\n\nThe target (response) variable should be ‘factor’ in R\nOr, use rpart(…, method=‘class’) option\nOtherwise, rpart do regression instead of classification\nThen, let’s train the model with the data ‘iris’\n\n\nlibrary(rpart)\nr = rpart(Species ~ ., data = iris)\nprint(r)\n\nn= 150 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  \n  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  \n    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *\n    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *\n\n\n\nTree visualization\n\n\npar(mfrow = c(1,1), xpd = NA)\nplot(r)\ntext(r, use.n = T)\n\n\n\n\n\nThe question of the root node (node 0): [Petal.Length<2.45?]\n\nYes of 50 to the left,\nNo of 100 to the right among 150 sample\n50 on the left all belong to ‘setosa’ so STOP 🡪 Make a leaf and record setosa(50/0/0)\n\nThe child node on the right side of the root: [Petal.Width<1.75?]\n\nYes of 54 to the left,\nand No of 46 to the right among the 100\n\nOn the left 54, there are 49 versicolor and 5 virginica 🡪 To avoid overfitting the algorithm STOP and record versicolor(0/49/5).\nOn the right 46, there are 1 versicolor and 45 virginica 🡪 To avoid overfitting the algorithm STOP and record virginica(0/1/45).\n\n\n\nPrediction\n\nUse the function ‘prediction’\ntype=‘class’ option prints the class (the default is type=‘prob’ which prints probability of each class)\n\n\np = predict(r, iris, type = 'class')\nhead(p, 10)\n\n     1      2      3      4      5      6      7      8      9     10 \nsetosa setosa setosa setosa setosa setosa setosa setosa setosa setosa \nLevels: setosa versicolor virginica\n\n\n\nConfusion matrix\n\nShow the correct and the wrong classifications in detail\nSee below for example, among 50 versicolor, 49 are correct but one is wrong to virginica\n\n\ntable(p, iris$Species)\n\n\np            setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         49         5\n  virginica       0          1        45\n\n\n\n\nNew data is from train data but a little bit of change is given.\n\nnewd = data.frame(Sepal.Length = c(5.11, 7.01, 6.32),\n                  Sepal.Width = c(3.51, 3.2, 3.31),\n                  Petal.Length = c(1.4, 4.71, 6.02),\n                  Petal.Width = c(0.19, 1.4, 2.49))\nnewd\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1         5.11        3.51         1.40        0.19\n2         7.01        3.20         4.71        1.40\n3         6.32        3.31         6.02        2.49\n\npredict(r, newdata = newd)\n\n  setosa versicolor  virginica\n1      1 0.00000000 0.00000000\n2      0 0.90740741 0.09259259\n3      0 0.02173913 0.97826087\n\n\n\nLet’s read the result with the ‘function ’summary’\n\nsummary(r)\n\nCall:\nrpart(formula = Species ~ ., data = iris)\n  n= 150 \n\n    CP nsplit rel error xerror       xstd\n1 0.50      0      1.00   1.16 0.05127703\n2 0.44      1      0.50   0.59 0.05982753\n3 0.01      2      0.06   0.10 0.03055050\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          34           31           21           14 \n\nNode number 1: 150 observations,    complexity param=0.5\n  predicted class=setosa      expected loss=0.6666667  P(node) =1\n    class counts:    50    50    50\n   probabilities: 0.333 0.333 0.333 \n  left son=2 (50 obs) right son=3 (100 obs)\n  Primary splits:\n      Petal.Length < 2.45 to the left,  improve=50.00000, (0 missing)\n      Petal.Width  < 0.8  to the left,  improve=50.00000, (0 missing)\n      Sepal.Length < 5.45 to the left,  improve=34.16405, (0 missing)\n      Sepal.Width  < 3.35 to the right, improve=19.03851, (0 missing)\n  Surrogate splits:\n      Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.00, (0 split)\n      Sepal.Length < 5.45 to the left,  agree=0.920, adj=0.76, (0 split)\n      Sepal.Width  < 3.35 to the right, agree=0.833, adj=0.50, (0 split)\n\nNode number 2: 50 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3333333\n    class counts:    50     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 100 observations,    complexity param=0.44\n  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667\n    class counts:     0    50    50\n   probabilities: 0.000 0.500 0.500 \n  left son=6 (54 obs) right son=7 (46 obs)\n  Primary splits:\n      Petal.Width  < 1.75 to the left,  improve=38.969400, (0 missing)\n      Petal.Length < 4.75 to the left,  improve=37.353540, (0 missing)\n      Sepal.Length < 6.15 to the left,  improve=10.686870, (0 missing)\n      Sepal.Width  < 2.45 to the left,  improve= 3.555556, (0 missing)\n  Surrogate splits:\n      Petal.Length < 4.75 to the left,  agree=0.91, adj=0.804, (0 split)\n      Sepal.Length < 6.15 to the left,  agree=0.73, adj=0.413, (0 split)\n      Sepal.Width  < 2.95 to the left,  agree=0.67, adj=0.283, (0 split)\n\nNode number 6: 54 observations\n  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36\n    class counts:     0    49     5\n   probabilities: 0.000 0.907 0.093 \n\nNode number 7: 46 observations\n  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667\n    class counts:     0     1    45\n   probabilities: 0.000 0.022 0.978 \n\n\n\nVariable importance shows the order of the explainable (independent) variables which contributes to predicting Y\nWhen the model is trained, it chooses the features that are important at the same time\nThat’s why the first visualization tree used only two variables: Petal.Width and Length\n\n\nThe effective visualization helps to read the results of decision tree\n\nlibrary(rpart.plot)\nrpart.plot(r)\n\n\n\n\nYou can change a style of the graph\n\nrpart.plot(r, type = 4)\n\n\n\n\n\n\n\nPros & Cons of DT\n\nCon: the performance is not that good\nPros\n\nEasy interpretability\n(예를 들어, “꽃받침 길이가 2.54보다 크고, 꽃받침 너비가 1.75보다 작아 versicolor로 분류”했다는 해석을 내놓을 수 있음)\nFast prediction (몇 번의 비교 연산으로 분류)\nEnsemble techniques make DT the great again (e.g. Random Forest)\nDT accepts categorical variables well (e.g. gender, pclass)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/04_week.html",
    "href": "teaching/ml101/weekly/posts/04_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #3\n\n<p>Loading…</p>\n\n\n\n\nClass\n\nA large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n\n The Random Forest algorithm has become an essential tool in the world of machine learning and data science due to its remarkable performance in solving complex classification and regression problems. The popularity of this algorithm stems from its ability to create a multitude of decision trees, each contributing to the final output, thus providing a robust and accurate model. Let’s explore the ins and outs of the Random Forest algorithm, its benefits, and its applications in various industries.\n\n\nWhat is the Random Forest Algorithm?\nThe Random Forest algorithm is an ensemble learning method that combines multiple decision trees, each trained on different subsets of the dataset. The final prediction is generated by aggregating the results from each tree, typically through a majority vote for classification or averaging for regression problems.\n\n\n\nHow Does the Random Forest Algorithm Work?\n\nThe random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.\n\nThe Random Forest algorithm works through the following steps:\n\nBootstrapping: Random samples with replacement are drawn from the original dataset, creating several smaller datasets called bootstrap samples.\nDecision tree creation: A decision tree is built for each bootstrap sample. During the construction of each tree, a random subset of features is chosen to split the data at each node. This randomness ensures that each tree is diverse and less correlated with others.\nAggregation: Once all the decision trees are built, they are combined to make the final prediction. For classification tasks, this is done by taking the majority vote from all the trees, while for regression tasks, the average prediction is used.\n\n\n\n\nAdvantages of the Random Forest Algorithm\nThe Random Forest algorithm offers several benefits:\n\nHigh accuracy: By combining multiple decision trees, the algorithm minimizes the risk of overfitting and produces more accurate predictions than a single tree.\nHandling missing data: The algorithm can handle missing data efficiently, as it uses information from other trees when making predictions.\nFeature importance: The Random Forest algorithm can rank the importance of features, providing valuable insights into which variables contribute the most to the model’s predictive power.\nVersatility: The algorithm is applicable to both classification and regression tasks, making it a versatile choice for various problem types.\n\n\n\n\nReal-World Applications of Random Forest\n\nHealthcare: In medical diagnosis, the algorithm can predict diseases based on patient data, improving the accuracy and efficiency of healthcare professionals.\nFinance: The algorithm is used in credit scoring, fraud detection, and stock market predictions, enhancing the decision-making process for financial institutions.\nMarketing: The algorithm helps in customer segmentation, identifying potential customers, and predicting customer churn, allowing businesses to make informed marketing decisions.\nEnvironment: The Random Forest algorithm is used in remote sensing, climate modeling, and species distribution modeling, assisting researchers and policymakers in environmental conservation and management.\n\n\n\n\nHands-on practice\nFirst, make sure to install and load the randomForest package:\n\n# Install the package if you haven't already\n# if (!requireNamespace(\"randomForest\", quietly = TRUE)) {\n#   install.packages(\"randomForest\")\n# }\n\n# Load the package\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n Now, let’s create a random forest model using the iris dataset:\n\nSplitting the dataset\n\n\n# Load the iris dataset\ndata(iris)\n\n# Split the dataset into training (70%) and testing (30%) sets\nset.seed(42) # Set the seed for reproducibility\nsample_size <- floor(0.7 * nrow(iris))\ntrain_index <- sample(seq_len(nrow(iris)), \n                      size = sample_size)\ntrain_index\n\n  [1]  49  65  74 146 122 150 128  47  24  71 100  89 110  20 114 111 131  41\n [19] 139  27 109   5  84  34  92 104   3  58  97  42 142  30  43  15  22 123\n [37]   8  36  68  86  18 130 126  69   4  98  50  99  88  87 145  26   6 105\n [55]   2 124  21  96 115  10  40 129  33 140  73  29  76   9  35  16 107  93\n [73] 120 138  80  55  90  94  57 121  77  13  53  54  32  60  85  17  44  83\n [91]  72 135 118 149  48 136  64  38   1 144  14 132  61  81 103\n\niris_train <- iris[train_index, ]\niris_test <- iris[-train_index, ]\n\nhead(iris_train)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n49           5.3         3.7          1.5         0.2     setosa\n65           5.6         2.9          3.6         1.3 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n146          6.7         3.0          5.2         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\nhead(iris_test)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n7           4.6         3.4          1.4         0.3  setosa\n11          5.4         3.7          1.5         0.2  setosa\n12          4.8         3.4          1.6         0.2  setosa\n19          5.7         3.8          1.7         0.3  setosa\n23          4.6         3.6          1.0         0.2  setosa\n25          4.8         3.4          1.9         0.2  setosa\n\n\n\nBuilding a model\n\n\n# Create the random forest model\nrf_model <- randomForest(Species ~ ., data = iris_train, ntree = 500, mtry = 2, importance = TRUE)\n\n# Print the model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris_train, ntree = 500,      mtry = 2, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 5.71%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         38          0         0  0.00000000\nversicolor      0         33         2  0.05714286\nvirginica       0          4        28  0.12500000\n\n\nThis code creates a random forest model with 500 trees (ntree = 500) and 2 variables tried at each split (mtry = 2). The importance = TRUE argument calculates variable importance.\nPlease see the details below.\n\nCall: This is the function call that was used to create the random forest model. It shows the formula used (Species ~ .), the dataset (iris_train), the number of trees (ntree = 500), the number of variables tried at each split (mtry = 2), and whether variable importance was calculated (importance = TRUE).\nType of random forest: This indicates the type of problem the random forest model is built for. In this case, it’s a classification problem since we are predicting the species of iris flowers.\nNumber of trees: This is the number of decision trees that make up the random forest model. In this case, there are 500 trees.\nNo. of variables tried at each split: This is the number of variables (features) that are randomly selected at each node for splitting. Here, 2 variables are tried at each split.\nOOB estimate of error rate: The Out-of-Bag (OOB) error rate is an estimate of the model’s classification error based on the observations that were not included in the bootstrap sample (i.e., left out) for each tree during the training process. In this case, the OOB error rate is 5.71%, which means that the model misclassified about 5.71% of the samples in the training data.\nConfusion matrix: The confusion matrix shows the number of correct and incorrect predictions made by the random forest model for each class in the training data. The diagonal elements represent correct predictions, and the off-diagonal elements represent incorrect predictions. In this case, the model correctly predicted 38 setosa, 33 versicolor, and 28 virginica samples. It misclassified 2 versicolor samples as virginica and 4 virginica samples as versicolor.\nclass.error: This column displays the error rate for each class. The error rate for setosa is 0% (no misclassifications), 5.71% for versicolor (2 misclassifications), and 12.5% for virginica (4 misclassifications).\n\nIn a Random Forest algorithm, hyperparameters are adjustable settings that control the learning process and model’s behavior. Selecting the right hyperparameters is critical to achieving optimal performance. Here, we discuss some common hyperparameters in Random Forest and how to adjust them using the R programming language.\n\n# Train the Random Forest model with custom hyperparameters\nmodel <- randomForest(\n  Species ~ ., \n  data = iris_train,\n  ntree = 100,                # n_estimators\n  mtry = 6,                   # max_features\n  maxnodes = 30,              # max_depth (use maxnodes instead)\n  min.node.size = 10,         # min_samples_split\n  nodesize = 5,               # min_samples_leaf\n  replace = TRUE              # bootstrap\n)\n\nWarning in randomForest.default(m, y, ...): invalid mtry: reset to within valid\nrange\n\n\n\nn_estimators: The number of decision trees in the forest. A higher value typically results in better performance but increases computation time.\nmax_features: The maximum number of features considered at each split in a decision tree. Some common values are ‘auto’, ‘sqrt’, or a float representing a percentage of the total features.\nmax_depth: The maximum depth of each decision tree. A deeper tree captures more complex patterns but may also lead to overfitting.\nmin_samples_split: The minimum number of samples required to split an internal node. Higher values reduce overfitting by limiting the depth of the tree.\nmin_samples_leaf: The minimum number of samples required to be at a leaf node. This parameter prevents the creation of very small leaves, which can help avoid overfitting.\nbootstrap: A boolean value indicating whether bootstrap samples are used when building trees. If set to FALSE, the whole dataset is used to build each tree.\n\nTo find the best hyperparameters, you can perform a grid search or use other optimization techniques such as random search or Bayesian optimization. The caret package in R can help you with hyperparameter tuning. We will learn this later on.\n\nTo make predictions using the model and evaluate its accuracy, use the following code:\n\n# Make predictions using the testing dataset\npredictions <- predict(rf_model, iris_test)\npredictions\n\n         7         11         12         19         23         25         28 \n    setosa     setosa     setosa     setosa     setosa     setosa     setosa \n        31         37         39         45         46         51         52 \n    setosa     setosa     setosa     setosa     setosa versicolor versicolor \n        56         59         62         63         66         67         70 \nversicolor versicolor versicolor versicolor versicolor versicolor versicolor \n        75         78         79         82         91         95        101 \nversicolor  virginica versicolor versicolor versicolor versicolor  virginica \n       102        106        108        112        113        116        117 \n virginica  virginica  virginica  virginica  virginica  virginica  virginica \n       119        125        127        133        134        137        141 \n virginica  virginica  virginica  virginica versicolor  virginica  virginica \n       143        147        148 \n virginica  virginica  virginica \nLevels: setosa versicolor virginica\n\n# Calculate the accuracy\naccuracy <- sum(predictions == iris_test$Species) /  length(predictions)\ncat(\"Accuracy:\", accuracy)\n\nAccuracy: 0.9555556\n\n\n Finally, you can visualize the variable importance using the following code:\n\n# Plot variable importance\nimportance(rf_model)\n\n                setosa versicolor virginica MeanDecreaseAccuracy\nSepal.Length  7.496191  6.2012298  7.691861            10.430372\nSepal.Width   5.126713 -0.5597552  1.662978             3.209728\nPetal.Length 22.072402 28.0181030 28.358114            31.140032\nPetal.Width  21.082980 27.3286061 29.256384            29.796881\n             MeanDecreaseGini\nSepal.Length         7.081252\nSepal.Width          2.116113\nPetal.Length        30.519101\nPetal.Width         29.393582\n\nvarImpPlot(rf_model)\n\n\n\n\nThere are two measures of variable importance presented: Mean Decrease in Accuracy and Mean Decrease in Gini Impurity. Both measures provide an indication of how important a given feature is for the model’s performance.\n\nMean Decrease in Accuracy: This measure calculates the decrease in model accuracy when the values of a specific feature are randomly permuted (keeping all other features the same). A higher value indicates that the feature is more important for the model’s performance. In this case, the order of importance is: Petal.Length (31.14), Petal.Width (29.80), Sepal.Length (10.43), and Sepal.Width (3.21).\nMean Decrease in Gini Impurity: This measure calculates the average decrease in Gini impurity for a specific feature when it is used in trees of the random forest model. The Gini impurity is a measure of how “mixed” the classes are in a given node, with a lower impurity indicating better separation. A higher Mean Decrease in Gini indicates that the feature is more important for the model’s performance. In this case, the order of importance is: Petal.Length (30.52), Petal.Width (29.39), Sepal.Length (7.08), and Sepal.Width (2.12).\n\nBoth measures of variable importance agree on the ranking of the features. Petal.Length and Petal.Width are the most important features for predicting the species of iris flowers, while Sepal.Length and Sepal.Width are less important.\n\n\n\nAdvanced study\nThe AdaBoost algorithm is a type of ensemble learning algorithm that combines multiple \"weak\" classifiers to create a \"strong\" classifier. A weak classifier is one that performs only slightly better than random guessing (i.e., its accuracy is slightly better than 50%). In contrast, a strong classifier is one that performs well on the classification task. There are many similarities with Random Forest but different in the way of giving weights. If you want to know more about the adaboost algorithm then click (here)"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/05_week.html",
    "href": "teaching/ml101/weekly/posts/05_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #4\n\n<p>Loading…</p>\n\n\n\n\nClass\nWhat is Naive Bayes?\n\nNaive Bayes is a probabilistic classification algorithm that is based on Bayes’ theorem. It was first introduced by Thomas Bayes, an 18th-century statistician, who used it to develop a method for predicting the probability of an event based on prior knowledge of related events. The goal of the algorithm is to predict the probability of each class label given a set of observed features.\n\nMotivation\n\nThe main motivation behind Naive Bayes is its simplicity and efficiency. It is a fast and effective algorithm that can be used for both binary and multi-class classification problems. It is particularly useful in situations where the number of features is large compared to the number of observations, as it requires a relatively small amount of training data to produce accurate predictions.\n\nAssumption\n\nThe Naive Bayes algorithm assumes that the features are conditionally independent given the class label, which is where the “naive” in Naive Bayes comes from. This assumption allows the algorithm to simplify the calculations involved in determining the probability of each class label given the observed features.\n\nBayes’ Theorem\n\nBayes’ theorem is a fundamental theorem in probability theory that describes the relationship between the conditional probabilities of two events (here, A and B). It states that the probability of event A given event B is equal to the probability of event B given event A multiplied by the probability of event A, divided by the probability of event B. Mathematically, this can be written as:\n\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nwhere:\n\n\\(P(A|B)\\) is the probability of event A given event B (known as the posterior probability)\n\\(P(B|A)\\) is the probability of event B given event A (known as the likelihood)\n\\(P(A)\\) is the probability of event A (known as the prior probability)\n\\(P(B)\\) is the probability of event B (known as the evidence)\n\n\nThe Naive Bayes Algorithm\n\nThe Naive Bayes algorithm uses Bayes’ theorem to predict the probability of each class label given a set of observed features. The algorithm assumes that the features are conditionally independent given the class label, which allows the algorithm to simplify the calculations involved in determining the probability of each class label.\n\nLet \\(X = (X_1, X_2, ..., X_n)\\) represent the set of observed features, and let Y represent the class label. The goal is to predict the probability of each class label given X, i.e. \\(P(Y|X)\\). Using Bayes’ theorem, we can write:\n\\[\nP(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}\n\\]\nwhere:\n\n\\(P(Y|X)\\) is the posterior probability of Y given X\n\\(P(X|Y)\\) is the likelihood of X given Y\n\\(P(Y)\\) is the prior probability of Y\n\\(P(X)\\) is the evidence\n\n\nThe Naive Bayes algorithm assumes that the features \\(X_1, X_2, ..., X_n\\) are conditionally independent given Y, which means that:\n\\[\nP(X|Y) = P(X_1|Y) \\times P(X_2|Y) \\times \\ldots \\times P(X_n|Y)\n\\]\n\nUsing this assumption, we can rewrite the equation for \\(P(Y|X)\\) as:\n\\[\nP(Y|X) = \\frac{P(Y)P(X_1|Y)P(X_2|Y) \\cdots P(X_n|Y)}{P(X)}\n\\]\n\nThe evidence \\(P(X)\\) is a constant for a given set of features X, so we can ignore it for the purposes of classification. Therefore, we can simplify the equation to:\n\\[\nP(Y|X) \\propto P(Y) \\times P(X_1|Y) \\times P(X_2|Y) \\times \\ldots \\times P(X_n|Y)\n\\]\n\nThe Naive Bayes algorithm calculates the likelihoods \\(P(X_i|Y)\\) for each feature and class label from the training data, and uses these likelihoods to predict the probability of each class label given a new set of features. The algorithm selects the class label with the highest probability as the predicted class label.\n\nPros & Cons\n\nPros:\n\nSimple and easy to implement\nFast and efficient\nWorks well with high-dimensional data\nRobust to irrelevant features\nCan handle both binary and multi-class classification problems\n\nCons:\n\nAssumes independence between features, which may not always be the case\nCan be sensitive to outliers and imbalanced datasets\nMay not perform well if the training data is insufficient or unrepresentative of the population\n\n\n\nDespite its limiations..\nNaive Bayes remains a popular algorithm in the field of machine learning and is widely used in a variety of applications, including spam filtering, sentiment analysis, and medical diagnosis. Its simplicity, efficiency, and effectiveness make it a valuable tool in any machine learning practitioner’s toolkit.\n\nPop-up QZs\n\nWhat is Naive Bayes?\n\nA regression algorithm\nA clustering algorithm\nA probabilistic classification algorithm\nAn unsupervised learning algorithm\n\nWhat does the “naive” in Naive Bayes refer to?\n\nThe fact that the algorithm is simple and easy to implement\nThe assumption that the features are conditionally independent given the class label\nThe fact that the algorithm is fast and efficient\nThe fact that the algorithm works well with high-dimensional data\n\nWhat is the main motivation behind Naive Bayes?\n\nIts ability to handle imbalanced datasets\nIts ability to work well with irrelevant features\nIts simplicity and efficiency\nIts ability to handle both binary and multi-class classification problems\n\nWhat are some pros of Naive Bayes?\n\nIt is simple and easy to implement\nIt is fast and efficient\nIt works well with high-dimensional data\nAll of the above\n\nWhat are some cons of Naive Bayes?\n\nIt assumes independence between features, which may not always be the case\nIt can be sensitive to outliers and imbalanced datasets\nIt may not perform well if the training data is insufficient or unrepresentative of the population\nAll of the above\n\n\n\nHands-on Practice\nFor this example, we will be using the “Breast Cancer Wisconsin (Diagnostic)” dataset from the UCI Machine Learning Repository. The dataset contains information about breast cancer tumors, including various measurements such as radius, texture, perimeter, and area.\nHere is an overview of the steps we will follow:\n\nLoad and preprocess the data\nSplit the data into training and testing sets\nTrain the Naive Bayes model using the training set\nUse the trained model to predict the classes of the testing set\nEvaluate the performance of the model\n\nLet’s start by loading and preprocessing the data:\n\nlibrary(e1071) # Required library for Naive Bayes\n\n# Load the dataset\ndata <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header = FALSE)\n\n# Assign column names to the dataset\ncolnames(data) <- c(\"id\", \"diagnosis\", \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \"concave.points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \"concave.points_se\", \"symmetry_se\", \"fractal_dimension_se\", \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \"concave.points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\")\n\n# Convert the diagnosis column to a binary variable\ndata$diagnosis <- ifelse(data$diagnosis == \"M\", 1, 0)\n\n# Remove the ID column as it is not useful for modeling\ndata <- data[, -1]\n\nhead(data)\n\n  diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean\n1         1       17.99        10.38         122.80    1001.0         0.11840\n2         1       20.57        17.77         132.90    1326.0         0.08474\n3         1       19.69        21.25         130.00    1203.0         0.10960\n4         1       11.42        20.38          77.58     386.1         0.14250\n5         1       20.29        14.34         135.10    1297.0         0.10030\n6         1       12.45        15.70          82.57     477.1         0.12780\n  compactness_mean concavity_mean concave.points_mean symmetry_mean\n1          0.27760         0.3001             0.14710        0.2419\n2          0.07864         0.0869             0.07017        0.1812\n3          0.15990         0.1974             0.12790        0.2069\n4          0.28390         0.2414             0.10520        0.2597\n5          0.13280         0.1980             0.10430        0.1809\n6          0.17000         0.1578             0.08089        0.2087\n  fractal_dimension_mean radius_se texture_se perimeter_se area_se\n1                0.07871    1.0950     0.9053        8.589  153.40\n2                0.05667    0.5435     0.7339        3.398   74.08\n3                0.05999    0.7456     0.7869        4.585   94.03\n4                0.09744    0.4956     1.1560        3.445   27.23\n5                0.05883    0.7572     0.7813        5.438   94.44\n6                0.07613    0.3345     0.8902        2.217   27.19\n  smoothness_se compactness_se concavity_se concave.points_se symmetry_se\n1      0.006399        0.04904      0.05373           0.01587     0.03003\n2      0.005225        0.01308      0.01860           0.01340     0.01389\n3      0.006150        0.04006      0.03832           0.02058     0.02250\n4      0.009110        0.07458      0.05661           0.01867     0.05963\n5      0.011490        0.02461      0.05688           0.01885     0.01756\n6      0.007510        0.03345      0.03672           0.01137     0.02165\n  fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst\n1             0.006193        25.38         17.33          184.60     2019.0\n2             0.003532        24.99         23.41          158.80     1956.0\n3             0.004571        23.57         25.53          152.50     1709.0\n4             0.009208        14.91         26.50           98.87      567.7\n5             0.005115        22.54         16.67          152.20     1575.0\n6             0.005082        15.47         23.75          103.40      741.6\n  smoothness_worst compactness_worst concavity_worst concave.points_worst\n1           0.1622            0.6656          0.7119               0.2654\n2           0.1238            0.1866          0.2416               0.1860\n3           0.1444            0.4245          0.4504               0.2430\n4           0.2098            0.8663          0.6869               0.2575\n5           0.1374            0.2050          0.4000               0.1625\n6           0.1791            0.5249          0.5355               0.1741\n  symmetry_worst fractal_dimension_worst\n1         0.4601                 0.11890\n2         0.2750                 0.08902\n3         0.3613                 0.08758\n4         0.6638                 0.17300\n5         0.2364                 0.07678\n6         0.3985                 0.12440\n\n\nNext, we will split the data into training and testing sets. We will use 80% of the data for training and 20% for testing.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training and testing sets\ntrain.index <- sample(nrow(data), 0.8 * nrow(data))\ntrain <- data[train.index, ]\ntest <- data[-train.index, ]\n\nNow, we can train the Naive Bayes model using the training set. We will use the naiveBayes function from the e1071 library.\n\n# Train the Naive Bayes model\nmodel <- naiveBayes(diagnosis ~ ., data = train)\n\nNext, we can use the trained model to predict the classes of the testing set\n\n# Use the model to predict the classes of the testing set\npredictions <- predict(model, newdata = test)\n\nFinally, we can evaluate the performance of the model using various metrics such as accuracy, precision, and recall.\n\n# Calculate the accuracy of the model\naccuracy <- sum(predictions == test$diagnosis) / nrow(test)\ncat(\"Accuracy:\", round(accuracy, 2), \"\\n\")\n\nAccuracy: 0.93 \n\n# Calculate the precision and recall of the model\nTP <- sum(predictions == 1 & test$diagnosis == 1)\nFP <- sum(predictions == 1 & test$diagnosis == 0)\nFN <- sum(predictions == 0 & test$diagnosis == 1)\n\nprecision <- TP / (TP + FP)\nrecall <- TP / (TP + FN)\n\ncat(\"Precision:\", round(precision, 2), \"\\n\")\n\nPrecision: 0.94 \n\ncat(\"Recall:\", round(recall, 2), \"\\n\")\n\nRecall: 0.91 \n\n\nIn a binary classification problem, a confusion matrix consists of four categories:\n\n\n\n\nPositive (Actual)\nNegative (Actual)\n\n\n\n\nPositive (Predicted)\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nNegative (Predicted)\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nTrue Positive (TP): Number of (actual) positive cases that are correctly (True) classified\nFalse Positive (FP): Number of (actual) negative cases that are incorrectly classified as (predicted) positive\nTrue Negative (TN): Number of (actual) negative cases that are correctly classified\nFalse Negative (FN): Number of (actual) positive cases that are incorrectly classified as (predicted) negative\n\nPrecision & Recall\n\nThe precision of the model tells us how many of the positive cases that the model predicted were actually positive,\n\nThe accuracy of the positive predictions made by the model\n\\[\n\\frac{TP}{TP+FP}\n\\]\n\nwhile the recall tells us how many of the actual positive cases were correctly predicted by the model.\n\nThe ability of the model to correctly identify all positive instances\n\\[\n\\frac{TP}{TP+FN}\n\\]\n\nIn general, precision is more important when we want to avoid false positives. For example, in a spam classification task, we would want to have high precision to ensure that legitimate emails are not classified as spam. False positives can be costly and can lead to important messages being missed.\nOn the other hand, recall is more important when we want to avoid false negatives. For example, in a medical diagnosis task, we would want to have high recall to ensure that all instances of a disease are correctly identified, even if it means that some healthy individuals are identified as having the disease. False negatives can be costly and can lead to delayed treatment and potentially life-threatening consequences.\n\n\n\nIn summary, precision and recall are used differently depending on the task and the costs associated with false positives and false negatives."
  },
  {
    "objectID": "teaching/ml101/weekly/posts/06_week.html",
    "href": "teaching/ml101/weekly/posts/06_week.html",
    "title": "Classification",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #5\n\nLoading…\n\n\n\n\nClass\n\nCheck out this blog for the concepts of Precision(정밀도), Recall(재현율) and Accuracy(정확도)\n\nhttps://sumniya.tistory.com/26"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/07_week.html",
    "href": "teaching/ml101/weekly/posts/07_week.html",
    "title": "Regression",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #6\n\nLoading…\n\n\n\n\nClass"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/09_week.html",
    "href": "teaching/ml101/weekly/posts/09_week.html",
    "title": "Regression",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #7\n\nLoading…\n\n\n\n\nClass"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/10_week.html",
    "href": "teaching/ml101/weekly/posts/10_week.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #8\n\nLoading…\n\n\n\n\nClass"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/11_week.html",
    "href": "teaching/ml101/weekly/posts/11_week.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\nDiscussion\nDiscussion #9\n\nLoading…\n\n\n\n\nClass"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/12_week.html",
    "href": "teaching/ml101/weekly/posts/12_week.html",
    "title": "Model Improvement",
    "section": "",
    "text": "Weekly design\n\n\nPre-class video\n\nEng ver.\n\n\n\nKor ver.\n\n\n\nPre-class PPT pdf\n\n\n\n\n\n\n\n### Discussion\n\n\nDiscussion #10\n\n\nLoading…\n\n\n\n\n\nClass"
  },
  {
    "objectID": "teaching/ml101/weekly/posts/13_week.html",
    "href": "teaching/ml101/weekly/posts/13_week.html",
    "title": "Natural Language Process",
    "section": "",
    "text": "Weekly design\n\n\nClass"
  },
  {
    "objectID": "teaching/media_ds/about/preclass.html",
    "href": "teaching/media_ds/about/preclass.html",
    "title": "Pre-class R code",
    "section": "",
    "text": "Download data\n[data_for_class.zip]\n\n\n################################################\n# 1. Syntax \n################################################\n\n# 1. Iris Data\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# visualization\nplot(iris)\n\n\n\nplot(iris$Petal.Width, iris$Petal.Length, col=iris$Species)\n\n\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nplot(iris$Sepal.Length, iris$Sepal.Width)\n\n\n\n# 2. Tip data\n\ntips=read.csv('http://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\nstr(tips) \n\n'data.frame':   244 obs. of  7 variables:\n $ total_bill: num  17 10.3 21 23.7 24.6 ...\n $ tip       : num  1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ...\n $ sex       : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ smoker    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ day       : chr  \"Sun\" \"Sun\" \"Sun\" \"Sun\" ...\n $ time      : chr  \"Dinner\" \"Dinner\" \"Dinner\" \"Dinner\" ...\n $ size      : int  2 3 3 2 4 4 2 4 2 2 ...\n\nhead(tips, 7)\n\n  total_bill  tip    sex smoker day   time size\n1      16.99 1.01 Female     No Sun Dinner    2\n2      10.34 1.66   Male     No Sun Dinner    3\n3      21.01 3.50   Male     No Sun Dinner    3\n4      23.68 3.31   Male     No Sun Dinner    2\n5      24.59 3.61 Female     No Sun Dinner    4\n6      25.29 4.71   Male     No Sun Dinner    4\n7       8.77 2.00   Male     No Sun Dinner    2\n\ntail(tips, 7)\n\n    total_bill  tip    sex smoker  day   time size\n238      32.83 1.17   Male    Yes  Sat Dinner    2\n239      35.83 4.67 Female     No  Sat Dinner    3\n240      29.03 5.92   Male     No  Sat Dinner    3\n241      27.18 2.00 Female    Yes  Sat Dinner    2\n242      22.67 2.00   Male    Yes  Sat Dinner    2\n243      17.82 1.75   Male     No  Sat Dinner    2\n244      18.78 3.00 Female     No Thur Dinner    2\n\nsummary(tips)\n\n   total_bill         tip             sex               smoker         \n Min.   : 3.07   Min.   : 1.000   Length:244         Length:244        \n 1st Qu.:13.35   1st Qu.: 2.000   Class :character   Class :character  \n Median :17.80   Median : 2.900   Mode  :character   Mode  :character  \n Mean   :19.79   Mean   : 2.998                                        \n 3rd Qu.:24.13   3rd Qu.: 3.562                                        \n Max.   :50.81   Max.   :10.000                                        \n     day                time                size     \n Length:244         Length:244         Min.   :1.00  \n Class :character   Class :character   1st Qu.:2.00  \n Mode  :character   Mode  :character   Median :2.00  \n                                       Mean   :2.57  \n                                       3rd Qu.:3.00  \n                                       Max.   :6.00  \n\nhist(tips$total_bill)\n\n# visualization\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n\n\nhist(tips$size)\n\n\n\ntips %>% ggplot(aes(size)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ntips %>% ggplot(aes(total_bill, tip)) + geom_point()\n\n\n\ntips %>% ggplot(aes(total_bill, tip)) + geom_point(aes(col=day))\n\n\n\ntips %>% ggplot(aes(total_bill, tip)) + geom_point(aes(col=day, pch=sex), size=5)\n\n\n\ntips %>% ggplot(aes(total_bill, tip)) + \n  geom_point(aes(col=day)) +\n  geom_line()\n\n\n\ntips %>% ggplot(aes(total_bill, tip, col=day, pch=sex)) + geom_point(size=3)\n\n\n\n\n\n# 03 Data type #\n\nx = 5\ny = 2\nx/y\n\n[1] 2.5\n\nxi = 1 + 2i\nyi = 1 - 2i\nxi+yi\n\n[1] 2+0i\n\nstr = \"Hello, World!\"\nstr\n\n[1] \"Hello, World!\"\n\nblood.type = factor(c('A', 'B', 'O', 'AB'))\nblood.type\n\n[1] A  B  O  AB\nLevels: A AB B O\n\nT\n\n[1] TRUE\n\nF\n\n[1] FALSE\n\nxinf = Inf\nyinf = -Inf\nxinf/yinf\n\n[1] NaN\n\nx = 1       # x에 단순히 1을 넣은 경우 x는 숫자형\nx\n\n[1] 1\n\nis.integer(x)\n\n[1] FALSE\n\nx = 1L      # x에 1L을 입력한 경우 x는 정수형\nx\n\n[1] 1\n\nis.integer(x)\n\n[1] TRUE\n\nx = as.integer(1)   # x에 1을 as.integer 함수로 변환하여 입력한 경우 x는 정수형\nx\n\n[1] 1\n\nis.integer(x)\n\n[1] TRUE\n\n\n\n# 05 벡터 #\n1:7         # 1부터 7까지 1씩 증가시켜 요소가 7개인 벡터 생성\n\n[1] 1 2 3 4 5 6 7\n\n7:1         # 7부터 1까지 1씩 감소시켜 요소가 7개인 벡터 생성\n\n[1] 7 6 5 4 3 2 1\n\nvector(length = 5)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nc(1:5)      # 1~5 요소로 구성된 벡터 생성. 1:5와 동일\n\n[1] 1 2 3 4 5\n\nc(1, 2, 3, c(4:6))  # 1~3 요소와 4~6 요소를 결합한 1~6 요소로 구성된 벡터 생성\n\n[1] 1 2 3 4 5 6\n\nx = c(1, 2, 3)  # 1~3 요소로 구성된 벡터를 x에 저장\nx       # x 출력\n\n[1] 1 2 3\n\ny = c()         # y를 빈 벡터로 생성\ny = c(y, c(1:3))    # 기존 y 벡터에 c(1:3) 벡터를 추가해 생성\ny       # y 출력\n\n[1] 1 2 3\n\nseq(from = 1, to = 10, by = 2)  # 1부터 10까지 2씩 증가하는 벡터 생성\n\n[1] 1 3 5 7 9\n\nseq(1, 10, by = 2)          # 1부터 10까지 2씩 증가하는 벡터 생성\n\n[1] 1 3 5 7 9\n\nseq(0, 1, by = 0.1)             # 0부터 1까지 0.1씩 증가하는 요소가 11개인 벡터 생성\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nseq(0, 1, length.out = 11)      # 0부터 1까지 요소가 11개인 벡터 생성\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nrep(c(1:3), times = 2)      # (1, 2, 3) 벡터를 2번 반복한 벡터 생성\n\n[1] 1 2 3 1 2 3\n\nrep(c(1:3), each = 2)       # (1, 2, 3) 벡터의 개별 요소를 2번 반복한 벡터 생성\n\n[1] 1 1 2 2 3 3\n\nx = c(2, 4, 6, 8, 10)\nlength(x)       # x 벡터의 길이(크기)를 구함\n\n[1] 5\n\nx[1]        # x 벡터의 1번 요소 값을 구함\n\n[1] 2\n\n# x[1, 2, 3]        # x 벡터의 1, 2, 3번 요소를 구할 때 이렇게 입력하면 오류\nx[c(1, 2, 3)]   # x 벡터의 1, 2, 3번 요소를 구할 때는 벡터로 묶어야 함\n\n[1] 2 4 6\n\nx[-c(1, 2, 3)]  # x 벡터에서 1, 2, 3번 요소를 제외한 값 출력\n\n[1]  8 10\n\nx[c(1:3)]       # x 벡터에서 1번부터 3번 요소를 출력\n\n[1] 2 4 6\n\nx = c(1, 2, 3, 4)\ny = c(5, 6, 7, 8)\nz = c(3, 4)\nw = c(5, 6, 7)\nx+2         # x 벡터의 개별 요소에 2를 각각 더함\n\n[1] 3 4 5 6\n\nx + y       # x 벡터와 y 벡터의 크기가 동일하므로 각 요소별로 더함\n\n[1]  6  8 10 12\n\nx + z       # x 벡터가 z 벡터 크기의 정수배인 경우엔 작은 쪽 벡터 요소를 순환하며 더함\n\n[1] 4 6 6 8\n\nx + w       # x와 w의 크기가 정수배가 아니므로 연산 오류\n\nWarning in x + w: longer object length is not a multiple of shorter object\nlength\n\n\n[1]  6  8 10  9\n\nx >5        # x 벡터의 요소 값이 5보다 큰지 확인\n\n[1] FALSE FALSE FALSE FALSE\n\nall(x>5)        # x 벡터의 요소 값이 모두 5보다 큰지 확인\n\n[1] FALSE\n\nany(x>5)        # x 벡터의 요소 값 중 일부가 5보다 큰지 확인\n\n[1] FALSE\n\nx = 1:10\nhead(x)         # 데이터의 앞 6개 요소를 추출\n\n[1] 1 2 3 4 5 6\n\ntail(x)         # 데이터의 뒤 6개 요소를 추출\n\n[1]  5  6  7  8  9 10\n\nhead(x, 3)  # 데이터의 앞 3개 요소를 추출\n\n[1] 1 2 3\n\ntail(x, 3)      # 데이터의 뒤 3개 요소를 추출\n\n[1]  8  9 10\n\nx = c(1, 2, 3)\ny = c(3, 4, 5)\nz = c(3, 1, 2)\nunion(x, y)     # 합집합\n\n[1] 1 2 3 4 5\n\nintersect(x, y)     # 교집합\n\n[1] 3\n\nsetdiff(x, y)   # 차집합(x에서 y와 동일한 요소 제외)\n\n[1] 1 2\n\nsetdiff(y, x)   # 차집합(y에서 x와 동일 요소 제외)\n\n[1] 4 5\n\nsetequal(x, y)  # x와 y에 동일한 요소가 있는지 비교\n\n[1] FALSE\n\nsetequal(x, z)  # x와 z에 동일한 요소가 있는지 비교\n\n[1] TRUE\n\n\n\n# 06 행렬 #\n# N차원 배열 생성\nx = array(1:5, c(2, 4)) # 1~5 값을 2× 4 행렬에 할당\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    2\n[2,]    2    4    1    3\n\nx[1, ] # 1행 요소 값 출력\n\n[1] 1 3 5 2\n\nx[, 2] # 2열 요소 값 출력\n\n[1] 3 4\n\ndimnamex = list(c(\"1st\", \"2nd\"), c(\"1st\", \"2nd\", \"3rd\", \"4th\")) # 행과 열 이름 설정\nx = array(1:5, c(2, 4), dimnames = dimnamex)\nx\n\n    1st 2nd 3rd 4th\n1st   1   3   5   2\n2nd   2   4   1   3\n\nx[\"1st\", ]\n\n1st 2nd 3rd 4th \n  1   3   5   2 \n\nx[, \"4th\"]\n\n1st 2nd \n  2   3 \n\n# 2차원 배열 생성\nx = 1:12\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\nmatrix(x, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(x, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# 벡터를 묶어 배열 생성\nv1 = c(1, 2, 3, 4)\nv2 = c(5, 6, 7, 8)\nv3 = c(9, 10, 11, 12)\ncbind(v1, v2, v3) # 열 단위로 묶어 배열 생성\n\n     v1 v2 v3\n[1,]  1  5  9\n[2,]  2  6 10\n[3,]  3  7 11\n[4,]  4  8 12\n\nrbind(v1, v2, v3) # 행 단위로 묶어 배열 생성\n\n   [,1] [,2] [,3] [,4]\nv1    1    2    3    4\nv2    5    6    7    8\nv3    9   10   11   12\n\n# [표 3-7]의 연산자를 활용한 다양한 행렬 연산\n# 2×2 행렬 2개를 각각 x, y에 저장\nx = array(1:4, dim = c(2, 2))\ny = array(5:8, dim = c(2, 2))\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\ny\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nx + y\n\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n\nx - y\n\n     [,1] [,2]\n[1,]   -4   -4\n[2,]   -4   -4\n\nx * y # 각 열별 곱셈\n\n     [,1] [,2]\n[1,]    5   21\n[2,]   12   32\n\nx %*% y # 수학적인 행렬 곱셈\n\n     [,1] [,2]\n[1,]   23   31\n[2,]   34   46\n\nt(x) # x의 전치 행렬\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsolve(x) # x의 역행렬\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\ndet(x) # x의 행렬식\n\n[1] -2\n\nx = array(1:12, c(3, 4))\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\napply(x, 1, mean) # 가운데 값이 1이면 함수를 행별로 적용\n\n[1] 5.5 6.5 7.5\n\napply(x, 2, mean) # 가운데 값이 2이면 함수를 열별로 적용\n\n[1]  2  5  8 11\n\nx = array(1:12, c(3, 4))\ndim(x)\n\n[1] 3 4\n\nx = array(1:12, c(3, 4))\nsample(x) # 배열 요소를 임의로 섞어 추출\n\n [1]  8  1 11  9  4 10  6  7  5 12  2  3\n\nsample(x, 10) # 배열 요소 중 10개를 골라 추출\n\n [1]  8  3  1 12 11 10  4  5  2  9\n\nsample(x, 10, prob = c(1:12)/24) # 각 요소별 추출 확률을 달리할 수 있음\n\n [1]  8 11 10  9 12  7  4  6  2  3\n\nsample(10) # 단순히 숫자만 사용하여 샘플을 만들 수 있음\n\n [1]  8  3  9  6  1  7  5  4 10  2\n\n\n\n# 07 데이터 프레임 #\nname = c(\"철수\", \"춘향\", \"길동\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients = data.frame(name, age, gender, blood.type)\npatients\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\n# 다음과 같이 한 행으로 작성할 수도 있음\npatients1 = data.frame(name = c(\"철수\", \"춘향\", \"길동\"), age = c(22, 20, 25), gender = factor(c(\"M\", \"F\", \"M\")), blood.type = factor(c(\"A\", \"O\", \"B\")))\npatients1\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\npatients$name # name 속성 값 출력\n\n[1] \"철수\" \"춘향\" \"길동\"\n\npatients[1, ] # 1행 값 출력\n\n  name age gender blood.type\n1 철수  22      M          A\n\npatients[, 2] # 2열 값 출력\n\n[1] 22 20 25\n\npatients[3, 1] # 3행 1열 값 출력\n\n[1] \"길동\"\n\npatients[patients$name==\"철수\", ] # 환자 중 철수에 대한 정보 추출\n\n  name age gender blood.type\n1 철수  22      M          A\n\npatients[patients$name==\"철수\", c(\"name\", \"age\")] # 철수 이름과 나이 정보만 추출\n\n  name age\n1 철수  22\n\nhead(cars) # cars 데이터 셋 확인. head 함수의 기본 기능은 앞 6개 데이터를 추출함\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# speed\nattach(cars) # attach 함수를 통해 cars의 각 속성을 변수로 이용하게 함\n# speed # speed라는 변수명을 직접 이용할 수 있음.\ndetach(cars) # detach 함수를 통해 cars의 각 속성을 변수로 사용하는 것을 해제함\n\n\n# 데이터 속성을 이용해 함수 적용\nmean(cars$speed)\n\n[1] 15.4\n\nmax(cars$speed)\n\n[1] 25\n\n# with 함수를 이용해 함수 적용\nwith(cars, mean(speed))\n\n[1] 15.4\n\nwith(cars, max(speed))\n\n[1] 25\n\n# 속도가 20 초과인 데이터만 추출\nsubset(cars, speed > 20)\n\n   speed dist\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\n# 속도가 20 초과인 dist 데이터만 추출, 여러 열 선택은 c( ) 안을 ,로 구분\nsubset(cars, speed > 20, select = c(dist))\n\n   dist\n44   66\n45   54\n46   70\n47   92\n48   93\n49  120\n50   85\n\n# 속도가 20 초과인 데이터 중 dist를 제외한 데이터만 추출\nsubset(cars, speed > 20, select = -c(dist))\n\n   speed\n44    22\n45    23\n46    24\n47    24\n48    24\n49    24\n50    25\n\nhead(airquality) # airquality 데이터에는 NA가 포함되어 있음\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nhead(na.omit(airquality)) # NA가 포함된 값을 제외하여 추출함\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n7    23     299  8.6   65     5   7\n8    19      99 13.8   59     5   8\n\n# merge(x, y, by = intersect(names(x), names(y)), by.x = by, by.y = by, all = FALSE, all.x = all, all.y = all, sort = TRUE, suffixes = c(\".x\",\".y\"), incomparables = NULL, ...)\n\nname = c(\"철수\", \"춘향\", \"길동\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients1 = data.frame(name, age, gender)\npatients1\n\n  name age gender\n1 철수  22      M\n2 춘향  20      F\n3 길동  25      M\n\npatients2 = data.frame(name, blood.type)\npatients2\n\n  name blood.type\n1 철수          A\n2 춘향          O\n3 길동          B\n\npatients = merge(patients1, patients2, by = \"name\")\npatients\n\n  name age gender blood.type\n1 길동  25      M          B\n2 철수  22      M          A\n3 춘향  20      F          O\n\n# 이름이 같은 열 변수가 없다면, merge 함수의 by.x와 by.y에 합칠 때\n# 사용할 열의 속성명을 각각 기입해주어야 함\nname1 = c(\"철수\", \"춘향\", \"길동\")\nname2 = c(\"민수\", \"춘향\", \"길동\")\nage = c(22, 20, 25)\ngender = factor(c(\"M\", \"F\", \"M\"))\nblood.type = factor(c(\"A\", \"O\", \"B\"))\npatients1 = data.frame(name1, age, gender)\npatients1\n\n  name1 age gender\n1  철수  22      M\n2  춘향  20      F\n3  길동  25      M\n\npatients2 = data.frame(name2, blood.type)\npatients2\n\n  name2 blood.type\n1  민수          A\n2  춘향          O\n3  길동          B\n\npatients = merge(patients1, patients2, by.x = \"name1\", by.y = \"name2\")\npatients\n\n  name1 age gender blood.type\n1  길동  25      M          B\n2  춘향  20      F          O\n\npatients = merge(patients1, patients2, by.x = \"name1\", by.y = \"name2\", all = TRUE)\npatients\n\n  name1 age gender blood.type\n1  길동  25      M          B\n2  민수  NA   <NA>          A\n3  철수  22      M       <NA>\n4  춘향  20      F          O\n\nx = array(1:12, c(3, 4))\nis.data.frame(x) # 현재 x는 데이터 프레임이 아님\n\n[1] FALSE\n\nas.data.frame(x)\n\n  V1 V2 V3 V4\n1  1  4  7 10\n2  2  5  8 11\n3  3  6  9 12\n\n# is.data.frame 함수를 호출하는 것만으로 x가 데이터 프레임으로 바뀌지 않음\nis.data.frame(x)\n\n[1] FALSE\n\n# as.data.frame 함수로 x를 데이터 프레임 형식으로 변환\nx = as.data.frame(x)\nx\n\n  V1 V2 V3 V4\n1  1  4  7 10\n2  2  5  8 11\n3  3  6  9 12\n\n# x가 데이터 프레임 형식으로 변환되었음을 확인\nis.data.frame(x)\n\n[1] TRUE\n\n# 데이터 프레임으로 변환 시 자동 지정되는 열 이름을 names 함수로 재지정함\nnames(x) = c(\"1st\", \"2nd\", \"3rd\", \"4th\")\nx\n\n  1st 2nd 3rd 4th\n1   1   4   7  10\n2   2   5   8  11\n3   3   6   9  12\n\n\n\n# 08 리스트 #\npatients = data.frame(name = c(\"철수\", \"춘향\", \"길동\"), age = c(22, 20, 25), gender = factor(c(\"M\", \"F\", \"M\")), blood.type = factor(c(\"A\", \"O\", \"B\")))\nno.patients = data.frame(day = c(1:6), no = c(50, 60, 55, 52, 65, 58))\n\n\n# 데이터를 단순 추가\nlistPatients = list(patients, no.patients) \nlistPatients\n\n[[1]]\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\n[[2]]\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# 각 데이터에 이름을 부여하면서 추가 \nlistPatients = list(patients=patients, no.patients = no.patients) \nlistPatients\n\n$patients\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\n$no.patients\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\nlistPatients$patients       # 요소명 입력\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\nlistPatients[[1]]               # 인덱스 입력\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\nlistPatients[[\"patients\"]]          # 요소명을 \"\"에 입력\n\n  name age gender blood.type\n1 철수  22      M          A\n2 춘향  20      F          O\n3 길동  25      M          B\n\nlistPatients[[\"no.patients\"]]       # 요소명을 \"\"에 입력\n\n  day no\n1   1 50\n2   2 60\n3   3 55\n4   4 52\n5   5 65\n6   6 58\n\n# no.patients 요소의 평균을 구해줌\nlapply(listPatients$no.patients, mean) \n\n$day\n[1] 3.5\n\n$no\n[1] 56.66667\n\n# patients 요소의 평균을 구해줌. 숫자 형태가 아닌 것은 평균이 구해지지 않음\nlapply(listPatients$patients, mean) \n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(X[[i]], ...): argument is not numeric or logical:\nreturning NA\n\n\n$name\n[1] NA\n\n$age\n[1] 22.33333\n\n$gender\n[1] NA\n\n$blood.type\n[1] NA\n\nsapply(listPatients$no.patients, mean) \n\n     day       no \n 3.50000 56.66667 \n\n# sapply()의 simplify 옵션을 F로 하면 lapply() 결과와 동일한 결과를 반환함\nsapply(listPatients$no.patients, mean, simplify = F) \n\n$day\n[1] 3.5\n\n$no\n[1] 56.66667\n\n\n\n# 01 파일 읽고 쓰기 #\n\n# 파일 마지막 행에서 [Enter]를 누르지 않은 경우\nstudents = read.table(\"data_2/students1.txt\", header = T, fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\n# 파일 마지막 행에서 [Enter]를 누른 경우\nstudents = read.table(\"data_2/students2.txt\",  header = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \n\n# 읽은 파일의 구조 확인\nstr(students) \n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 파일을 있는 형태 그대로 읽음\nstudents = read.table(\"data_2/students1.txt\", header = T, as.is = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 파일을 읽을 때 문장을 요인으로 인식하지 않도록 설정\nstudents = read.table(\"data_2/students1.txt\", header = T, stringsAsFactors = F, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 구분 기호는 쉼표(,), 첫 행은 header로 인식하여 파일을 있는 그대로 읽어들이면 \n# NA로 인해 math 요소가 문장으로 인식됨\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : chr  \" 100\" \" 80\" \" 90\" \" NA\" ...\n\n# \"NA\" 문장을 결측값 NA로 처리하라고 해도 처리가 안됨. 정확한 문장은 NA 앞에 빈 칸이 있어야 하기 때문\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, na.strings = \"NA\", fileEncoding = \"CP949\", encoding = \"UTF-8\")  \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : chr  \" 100\" \" 80\" \" 90\" \" NA\" ...\n\n# \"NA\"로 정확하게 입력하자 결측값 NA로 처리되면서 math 요소가 모두 숫자로 인식됨\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, na.strings = \" NA\", fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 NA 100\n\n# strip.white에서 빈칸을 제거하면 na.string의 기본값이 \"NA\"로 설정되어 math 요소가 모두 숫자로 인식됨.\nstudents = read.table(\"data_2/students3.txt\", sep = \",\", header = T, as.is = T, strip.white = T, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 NA 100\n\n# 첫 행이 header이므로 header 옵션을 지정할 필요가 없음\nstudents = read.csv(\"data_2/students.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstudents\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     100   80\n3 박정원     90      95   90\n4 이상훈    100      85   95\n5 최건우     85     100  100\n\n# 읽은 파일의 구조 확인\nstr(students) \n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# name 속성을 요인에서 문장으로 변경\nstudents$name = as.character(students$name) \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 파일을 읽을 때 문장을 요인으로 인식하지 않도록 설정함\nstudents = read.csv(\"data_2/students.csv\", stringsAsFactors = FALSE, fileEncoding = \"CP949\", encoding = \"UTF-8\") \nstr(students)\n\n'data.frame':   5 obs. of  4 variables:\n $ name   : chr  \"강서준\" \"김도형\" \"박정원\" \"이상훈\" ...\n $ korean : int  100 90 90 100 85\n $ english: int  90 100 95 85 100\n $ math   : int  100 80 90 95 100\n\n# 문장에 큰따옴표가 표시됨.\nwrite.table(students, file = \"data_2/output.txt\") \n\n# 문장에 큰따옴표되지 않음.\nwrite.table(students, file = \"data_2/output.txt\", quote = F) \n\n\n# 02 데이터 정제를 위한 조건문과 반복문 #\n\ntest = c(15, 20, 30, NA, 45)    # 벡터인 경우\ntest[test<40]   # 값이 40 미만인 요소 추출\n\n[1] 15 20 30 NA\n\ntest[test%%3!= 0]   # 값이 3으로 나누어 떨어지지 않는 요소 추출\n\n[1] 20 NA\n\ntest[is.na(test)]   # NA인 요소 추출\n\n[1] NA\n\ntest[!is.na(test)]          # NA가 아닌 요소 추출\n\n[1] 15 20 30 45\n\ntest[test%%2==0 & !is.na(test)] # 2의 배수면서 NA가 아닌 요소 추출\n\n[1] 20 30\n\ncharacters = data.frame(name = c(\"길동\", \"춘향\", \"철수\"), \n                        age = c(30, 16, 21), \n                        gender = factor(c(\"M\", \"F\",\"M\")))  \n# 데이터 프레임인 경우\n\ncharacters\n\n  name age gender\n1 길동  30      M\n2 춘향  16      F\n3 철수  21      M\n\ncharacters[characters$gender ==\"F\",1]  # 성별이 여성인 행 추출\n\n[1] \"춘향\"\n\nlibrary(dplyr)\n\ncharacters %>% filter(gender==\"F\") %>% select(name)\n\n  name\n1 춘향\n\ncharacters[characters$age<30 & characters$gender ==\"M\",] \n\n  name age gender\n3 철수  21      M\n\n# 30살 미만의 남성 행 추출                    \ncharacters %>% filter(age<30 & gender==\"M\")\n\n  name age gender\n1 철수  21      M\n\nx = 5\nif(x %% 2 ==0) {\n  print('x는 짝수')    # 조건식이 참일 때 수행\n}   else {\n  print('x는 홀수')    # 조건식이 거짓일 때 수행\n}\n\n[1] \"x는 홀수\"\n\nx = 8\nif(x>0) {\n  print('x is a positive value.')   # x가 0보다 크면 출력\n} else if(x<0) {\n  print('x is a negative value.')   # 위 조건을 만족하지 않고 x가 0보다 작으면 출력\n} else {\n  print('x is zero.')       # 위 조건을 모두 만족하지 않으면 출력\n}\n\n[1] \"x is a positive value.\"\n\nx = c(-5:5)\noptions(digits = 3)     # 숫자 표현 시 유효자릿수를 3자리로 설정\nsqrt(x)\n\nWarning in sqrt(x): NaNs produced\n\n\n [1]  NaN  NaN  NaN  NaN  NaN 0.00 1.00 1.41 1.73 2.00 2.24\n\nsqrt(ifelse(x>=0, x, NA))   # NaN이 발생하지 않게 음수면 NA로 표시\n\n [1]   NA   NA   NA   NA   NA 0.00 1.00 1.41 1.73 2.00 2.24\n\nstudents = read.csv(\"data_2/students2.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\nstudents         # 데이터에 100 초과 값과 음수 값이 포함되어 있음.\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     120   80\n3 박정원     90      95   90\n4 이상훈    100      85 -100\n5 최건우     85     100  100\n\nstudents[, 2] = ifelse(students[, 2]>= 0 & students[, 2]<= 100, \n                       students[, 2], NA)\nstudents[, 3] = ifelse(students[, 3]>= 0 & students[, 3]<= 100, \n                       students[, 3], NA)\nstudents[, 4] = ifelse(students[, 4]>= 0 & students[, 4]<= 100, \n                       students[, 4], NA)\nstudents         # ifelse 문으로 2~4열 값 중 0~100 외의 값은 NA로 처리함.\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\n# repeat 문을 이용해 1부터 10까지 숫자 증가시키기\ni = 1                # i의 시작값은 1\nrepeat {\n  if(i>10) {         # i가 10을 넘으면 반복을 중단(break)함\n    break\n  } else {\n    print(i)\n    i = i+1           # i를 1 증가시킴.\n  }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\n# while 문을 이용해 1부터 10까지 숫자 증가시키기\ni = 1 # i의 시작값은 1임.\nwhile(i < 10){ # i가 10 이하인 동안에 반복함\n  print(i)\n  i = i+1 # i를 1 증가시킴.\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n# while 문을 이용해 구구단 2단 만들기\ni = 1\nwhile(i<10) {\n  print(paste(2, \"X\", i, \"=\", 2*i))\n  i = i+1\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n\n# for 문을 이용한 1부터 10까지 숫자 증가시키기\nfor(i in 1:10) {\n  print(i)\n}  \n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n# for 문을 이용해 구구단 2단 만들기\nfor(i in 1:9) {\n  print(paste(2, \"X\", i, \"=\", 2*i))\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n\n# for 문을 이용해 구구단 2~9단 만들기\nfor(i in 2:9) {\n  for(j in 1:9) {\n    print(paste(i, \"X\", j, \"=\", i*j))\n  }\n}\n\n[1] \"2 X 1 = 2\"\n[1] \"2 X 2 = 4\"\n[1] \"2 X 3 = 6\"\n[1] \"2 X 4 = 8\"\n[1] \"2 X 5 = 10\"\n[1] \"2 X 6 = 12\"\n[1] \"2 X 7 = 14\"\n[1] \"2 X 8 = 16\"\n[1] \"2 X 9 = 18\"\n[1] \"3 X 1 = 3\"\n[1] \"3 X 2 = 6\"\n[1] \"3 X 3 = 9\"\n[1] \"3 X 4 = 12\"\n[1] \"3 X 5 = 15\"\n[1] \"3 X 6 = 18\"\n[1] \"3 X 7 = 21\"\n[1] \"3 X 8 = 24\"\n[1] \"3 X 9 = 27\"\n[1] \"4 X 1 = 4\"\n[1] \"4 X 2 = 8\"\n[1] \"4 X 3 = 12\"\n[1] \"4 X 4 = 16\"\n[1] \"4 X 5 = 20\"\n[1] \"4 X 6 = 24\"\n[1] \"4 X 7 = 28\"\n[1] \"4 X 8 = 32\"\n[1] \"4 X 9 = 36\"\n[1] \"5 X 1 = 5\"\n[1] \"5 X 2 = 10\"\n[1] \"5 X 3 = 15\"\n[1] \"5 X 4 = 20\"\n[1] \"5 X 5 = 25\"\n[1] \"5 X 6 = 30\"\n[1] \"5 X 7 = 35\"\n[1] \"5 X 8 = 40\"\n[1] \"5 X 9 = 45\"\n[1] \"6 X 1 = 6\"\n[1] \"6 X 2 = 12\"\n[1] \"6 X 3 = 18\"\n[1] \"6 X 4 = 24\"\n[1] \"6 X 5 = 30\"\n[1] \"6 X 6 = 36\"\n[1] \"6 X 7 = 42\"\n[1] \"6 X 8 = 48\"\n[1] \"6 X 9 = 54\"\n[1] \"7 X 1 = 7\"\n[1] \"7 X 2 = 14\"\n[1] \"7 X 3 = 21\"\n[1] \"7 X 4 = 28\"\n[1] \"7 X 5 = 35\"\n[1] \"7 X 6 = 42\"\n[1] \"7 X 7 = 49\"\n[1] \"7 X 8 = 56\"\n[1] \"7 X 9 = 63\"\n[1] \"8 X 1 = 8\"\n[1] \"8 X 2 = 16\"\n[1] \"8 X 3 = 24\"\n[1] \"8 X 4 = 32\"\n[1] \"8 X 5 = 40\"\n[1] \"8 X 6 = 48\"\n[1] \"8 X 7 = 56\"\n[1] \"8 X 8 = 64\"\n[1] \"8 X 9 = 72\"\n[1] \"9 X 1 = 9\"\n[1] \"9 X 2 = 18\"\n[1] \"9 X 3 = 27\"\n[1] \"9 X 4 = 36\"\n[1] \"9 X 5 = 45\"\n[1] \"9 X 6 = 54\"\n[1] \"9 X 7 = 63\"\n[1] \"9 X 8 = 72\"\n[1] \"9 X 9 = 81\"\n\n# 1부터 10까지의 수 중 짝수만 출력하기\nfor(i in 1:10) {\n  if(i%%2 == 0) {\n    print(i)\n  }\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n# 1부터 10까지의 수 중 소수 출력하기\nfor(i in 1:10) {\n  check = 0\n  for(j in 1:i) {\n    if(i%%j ==0) {\n      check = check+1\n    }\n  }\n  if(check ==2) { \n    print(i)\n  }\n}\n\n[1] 2\n[1] 3\n[1] 5\n[1] 7\n\nstudents = read.csv(\"data_2/students2.csv\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\nstudents        # 데이터에 100 초과 값과 음수 값이 포함되어 있음\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90     120   80\n3 박정원     90      95   90\n4 이상훈    100      85 -100\n5 최건우     85     100  100\n\nfor(i in 2:4) {\n  students[, i] = ifelse(students[, i]>= 0 & students[, i]<= 100, \n                         students[, i], NA)\n}\n\n\nstudents        # ifelse 문으로 2~4열 값 중 0~100 외의 값은 NA로 처리함\n\n    name korean english math\n1 강서준    100      90  100\n2 김도형     90      NA   80\n3 박정원     90      95   90\n4 이상훈    100      85   NA\n5 최건우     85     100  100\n\n# 03 사용자 정의 함수 : 원하는 기능 묶기 # \nx=5\nfa = 1  # 계승값을 저장할 변수\nwhile(x>1) {  # x가 1보다 큰 동안 반복\n  \n  fa = fa*x   # x 값을 fa에 곱한 후 fa에 다시 저장\n  x = x-1  # x 값을 1 감소\n  x\n}  \nfa\n\n[1] 120\n\nfact = function(x) {   # 함수의 이름은 fact, 입력은 x\n  fa = 1  # 계승값을 저장할 변수\n  while(x>1) {  # x가 1보다 큰 동안 반복\n    fa = fa*x   # x 값을 fa에 곱한 후 fa에 다시 저장\n    x = x-1  # x 값을 1 감소\n  }  \n  return(fa)   # 최종 계산된 fa 반환\n}\nfact(5)   # 5!을 계산한 결과 출력\n\n[1] 120\n\nmy.is.na<-function(x) { # table(is.na()) 함수를 하나로 묶은 my.is.na 함수를 만듦\n  table(is.na(x))\n}\n\nmy.is.na(airquality)    # 이 결과는 table(is.na(airquality))와 같음.\n\n\nFALSE  TRUE \n  874    44 \n\ntable(is.na(airquality))\n\n\nFALSE  TRUE \n  874    44 \n\n\n\n# 04 데이터 정제 예제 1 : 결측값 처리 # \n\n# is.na 함수를 이용해 결측값 처리하기\nstr(airquality) # airquality 데이터의 구조를 살펴봄.\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n# airquality 데이터에서 NA인 것은 TRUE, 아니면 FALSE로 나타냄. 데이터가 많아 head 함수로 추려냄.\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nhead(is.na(airquality)) \n\n     Ozone Solar.R  Wind  Temp Month   Day\n[1,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE   FALSE FALSE FALSE FALSE FALSE\n[5,]  TRUE    TRUE FALSE FALSE FALSE FALSE\n[6,] FALSE    TRUE FALSE FALSE FALSE FALSE\n\ntable(is.na(airquality))    # NA가 총 44개 있음.\n\n\nFALSE  TRUE \n  874    44 \n\nsum(is.na(airquality))  # NA가 총 44개 있음.\n\n[1] 44\n\ntable(is.na(airquality$Temp))   # Temp에는 NA가 없음을 확인함.\n\n\nFALSE \n  153 \n\ntable(is.na(airquality$Ozone))  # Ozone에는 NA가 37개 발견됨.\n\n\nFALSE  TRUE \n  116    37 \n\nmean(airquality$Temp)       # NA가 없는 Temp는 평균이 구해짐.\n\n[1] 77.9\n\nmean(airquality$Ozone)      # NA가 있는 Ozone은 평균이 NA로 나옴.\n\n[1] NA\n\nair_narm = airquality[!is.na(airquality$Ozone), ] # Ozone 속성에서 NA가 없는 값만 추출함. \nair_narm\n\n    Ozone Solar.R Wind Temp Month Day\n1      41     190  7.4   67     5   1\n2      36     118  8.0   72     5   2\n3      12     149 12.6   74     5   3\n4      18     313 11.5   62     5   4\n6      28      NA 14.9   66     5   6\n7      23     299  8.6   65     5   7\n8      19      99 13.8   59     5   8\n9       8      19 20.1   61     5   9\n11      7      NA  6.9   74     5  11\n12     16     256  9.7   69     5  12\n13     11     290  9.2   66     5  13\n14     14     274 10.9   68     5  14\n15     18      65 13.2   58     5  15\n16     14     334 11.5   64     5  16\n17     34     307 12.0   66     5  17\n18      6      78 18.4   57     5  18\n19     30     322 11.5   68     5  19\n20     11      44  9.7   62     5  20\n21      1       8  9.7   59     5  21\n22     11     320 16.6   73     5  22\n23      4      25  9.7   61     5  23\n24     32      92 12.0   61     5  24\n28     23      13 12.0   67     5  28\n29     45     252 14.9   81     5  29\n30    115     223  5.7   79     5  30\n31     37     279  7.4   76     5  31\n38     29     127  9.7   82     6   7\n40     71     291 13.8   90     6   9\n41     39     323 11.5   87     6  10\n44     23     148  8.0   82     6  13\n47     21     191 14.9   77     6  16\n48     37     284 20.7   72     6  17\n49     20      37  9.2   65     6  18\n50     12     120 11.5   73     6  19\n51     13     137 10.3   76     6  20\n62    135     269  4.1   84     7   1\n63     49     248  9.2   85     7   2\n64     32     236  9.2   81     7   3\n66     64     175  4.6   83     7   5\n67     40     314 10.9   83     7   6\n68     77     276  5.1   88     7   7\n69     97     267  6.3   92     7   8\n70     97     272  5.7   92     7   9\n71     85     175  7.4   89     7  10\n73     10     264 14.3   73     7  12\n74     27     175 14.9   81     7  13\n76      7      48 14.3   80     7  15\n77     48     260  6.9   81     7  16\n78     35     274 10.3   82     7  17\n79     61     285  6.3   84     7  18\n80     79     187  5.1   87     7  19\n81     63     220 11.5   85     7  20\n82     16       7  6.9   74     7  21\n85     80     294  8.6   86     7  24\n86    108     223  8.0   85     7  25\n87     20      81  8.6   82     7  26\n88     52      82 12.0   86     7  27\n89     82     213  7.4   88     7  28\n90     50     275  7.4   86     7  29\n91     64     253  7.4   83     7  30\n92     59     254  9.2   81     7  31\n93     39      83  6.9   81     8   1\n94      9      24 13.8   81     8   2\n95     16      77  7.4   82     8   3\n96     78      NA  6.9   86     8   4\n97     35      NA  7.4   85     8   5\n98     66      NA  4.6   87     8   6\n99    122     255  4.0   89     8   7\n100    89     229 10.3   90     8   8\n101   110     207  8.0   90     8   9\n104    44     192 11.5   86     8  12\n105    28     273 11.5   82     8  13\n106    65     157  9.7   80     8  14\n108    22      71 10.3   77     8  16\n109    59      51  6.3   79     8  17\n110    23     115  7.4   76     8  18\n111    31     244 10.9   78     8  19\n112    44     190 10.3   78     8  20\n113    21     259 15.5   77     8  21\n114     9      36 14.3   72     8  22\n116    45     212  9.7   79     8  24\n117   168     238  3.4   81     8  25\n118    73     215  8.0   86     8  26\n120    76     203  9.7   97     8  28\n121   118     225  2.3   94     8  29\n122    84     237  6.3   96     8  30\n123    85     188  6.3   94     8  31\n124    96     167  6.9   91     9   1\n125    78     197  5.1   92     9   2\n126    73     183  2.8   93     9   3\n127    91     189  4.6   93     9   4\n128    47      95  7.4   87     9   5\n129    32      92 15.5   84     9   6\n130    20     252 10.9   80     9   7\n131    23     220 10.3   78     9   8\n132    21     230 10.9   75     9   9\n133    24     259  9.7   73     9  10\n134    44     236 14.9   81     9  11\n135    21     259 15.5   76     9  12\n136    28     238  6.3   77     9  13\n137     9      24 10.9   71     9  14\n138    13     112 11.5   71     9  15\n139    46     237  6.9   78     9  16\n140    18     224 13.8   67     9  17\n141    13      27 10.3   76     9  18\n142    24     238 10.3   68     9  19\n143    16     201  8.0   82     9  20\n144    13     238 12.6   64     9  21\n145    23      14  9.2   71     9  22\n146    36     139 10.3   81     9  23\n147     7      49 10.3   69     9  24\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\nmean(air_narm$Ozone)    # 결측값이 제거된 데이터에서는 mean 함수가 정상적으로 동작함.\n\n[1] 42.1\n\n# na.omit 함수를 이용해 결측값 처리하기\nair_narm1 = na.omit(airquality)\nmean(air_narm1$Ozone)\n\n[1] 42.1\n\n# 함수 속성인 na.rm을 이용해 결측값 처리하기\nmean(airquality$Ozone, na.rm = T)\n\n[1] 42.1\n\nmean(airquality$Ozone, na.rm = F)\n\n[1] NA\n\ntable(is.na(airquality))\n\n\nFALSE  TRUE \n  874    44 \n\ntable(is.na(airquality$Ozone))\n\n\nFALSE  TRUE \n  116    37 \n\ntable(is.na(airquality$Solar.R))\n\n\nFALSE  TRUE \n  146     7 \n\nair_narm = airquality[!is.na(airquality$Ozone) & !is.na(airquality$Solar.R), ]\nmean(air_narm$Ozone)\n\n[1] 42.1\n\n\n\n# 05 데이터 정제 예제 2 : 이상값 처리 # \n\n# 이상값이 포함된 환자 데이터\npatients = data.frame(name = c(\"환자1\", \"환자2\", \"환자3\", \"환자4\", \"환자5\"), age = c(22, 20, 25, 30, 27), gender=factor(c(\"M\", \"F\", \"M\", \"K\", \"F\")), blood.type = factor(c(\"A\", \"O\", \"B\", \"AB\", \"C\")))\npatients\n\n   name age gender blood.type\n1 환자1  22      M          A\n2 환자2  20      F          O\n3 환자3  25      M          B\n4 환자4  30      K         AB\n5 환자5  27      F          C\n\n# 성별에서 이상값 제거\npatients_outrm = patients[patients$gender==\"M\"|patients$gender==\"F\", ]\npatients_outrm  \n\n   name age gender blood.type\n1 환자1  22      M          A\n2 환자2  20      F          O\n3 환자3  25      M          B\n5 환자5  27      F          C\n\n# 성별과 혈액형에서 이상값 제거\npatients_outrm1 = patients[(patients$gender == \"M\"|patients$gender == \"F\") & \n                             (patients$blood.type == \"A\"|\n                                patients$blood.type == \"B\"|\n                                patients$blood.type == \"O\"|\n                                patients$blood.type == \"AB\"), ]\npatients_outrm1  \n\n   name age gender blood.type\n1 환자1  22      M          A\n2 환자2  20      F          O\n3 환자3  25      M          B\n\n# 이상값이 포함된 환자 데이터\npatients = data.frame(name = c(\"환자1\", \"환자2\", \"환자3\", \"환자4\", \"환자5\"), \n                      age = c(22, 20, 25, 30, 27), \n                      gender = c(1, 2, 1, 3, 2), \n                      blood.type = c(1, 3, 2, 4, 5))\npatients    \n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n4 환자4  30      3          4\n5 환자5  27      2          5\n\n# 성별에 있는 이상값을 결측값으로 변경\npatients$gender = ifelse((patients$gender<1|patients$gender>2), NA, patients$gender)\npatients    \n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n4 환자4  30     NA          4\n5 환자5  27      2          5\n\n# 형액형에 있는 이상값도 결측값으로 변경\npatients$blood.type = ifelse((patients$blood.type<1|patients$blood.type>4), NA, \n                             patients$blood.type)\npatients\n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n4 환자4  30     NA          4\n5 환자5  27      2         NA\n\n# 결측값을 모두 제거\npatients[!is.na(patients$gender)&!is.na(patients$blood.type), ]\n\n   name age gender blood.type\n1 환자1  22      1          1\n2 환자2  20      2          3\n3 환자3  25      1          2\n\nboxplot(airquality[, c(1:4)])    # Ozone, Solar.R, Wind, Temp에 대한 boxplot\n\n\n\nboxplot(airquality[, 1])$stats   # Ozone의 boxplot 통계값 계산\n\n\n\n\n      [,1]\n[1,]   1.0\n[2,]  18.0\n[3,]  31.5\n[4,]  63.5\n[5,] 122.0\n\nair = airquality                 # 임시 저장 변수로 airquality 데이터 복사\ntable(is.na(air$Ozone))          # Ozone의 현재 NA 개수 확인\n\n\nFALSE  TRUE \n  116    37 \n\n# 이상값을 NA로 변경\nair$Ozone = ifelse(air$Ozone<1|air$Ozone>122, NA, air$Ozone) \ntable(is.na(air$Ozone)) # 이상값 처리 후 NA 개수 확인(2개 증가)\n\n\nFALSE  TRUE \n  114    39 \n\n# NA 제거\nair_narm = air[!is.na(air$Ozone), ] \nmean(air_narm$Ozone) # 이상값 두 개 제거로 is.na 함수를 이용한 결과보다 값이 줄어듦\n\n[1] 40.2\n\n\n\n# 02 베이스 R을 이용한 데이터 가공 # \n\nlibrary(gapminder) \nlibrary(dplyr)\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   <dbl> 28.8, 30.3, 32.0, 34.0, 36.1, 38.4, 39.9, 40.8, 41.7, 41.8, …\n$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap <dbl> 779, 821, 853, 836, 740, 786, 978, 852, 649, 635, 727, 975, …\n\ngapminder[, c(\"country\", \"lifeExp\")]\n\n# A tibble: 1,704 × 2\n   country     lifeExp\n   <fct>         <dbl>\n 1 Afghanistan    28.8\n 2 Afghanistan    30.3\n 3 Afghanistan    32.0\n 4 Afghanistan    34.0\n 5 Afghanistan    36.1\n 6 Afghanistan    38.4\n 7 Afghanistan    39.9\n 8 Afghanistan    40.8\n 9 Afghanistan    41.7\n10 Afghanistan    41.8\n# … with 1,694 more rows\n\ngapminder[, c(\"country\", \"lifeExp\", \"year\")]\n\n# A tibble: 1,704 × 3\n   country     lifeExp  year\n   <fct>         <dbl> <int>\n 1 Afghanistan    28.8  1952\n 2 Afghanistan    30.3  1957\n 3 Afghanistan    32.0  1962\n 4 Afghanistan    34.0  1967\n 5 Afghanistan    36.1  1972\n 6 Afghanistan    38.4  1977\n 7 Afghanistan    39.9  1982\n 8 Afghanistan    40.8  1987\n 9 Afghanistan    41.7  1992\n10 Afghanistan    41.8  1997\n# … with 1,694 more rows\n\ngapminder[1:15, ]\n\n# A tibble: 15 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n11 Afghanistan Asia       2002    42.1 25268405      727.\n12 Afghanistan Asia       2007    43.8 31889923      975.\n13 Albania     Europe     1952    55.2  1282697     1601.\n14 Albania     Europe     1957    59.3  1476505     1942.\n15 Albania     Europe     1962    64.8  1728137     2313.\n\nlibrary(dplyr)\ngapminder %>% filter(country==\"Croatia\") %>% select(year, gdpPercap) %>% plot\n\n\n\ngapminder[gapminder$country == \"Croatia\", ]\n\n# A tibble: 12 × 6\n   country continent  year lifeExp     pop gdpPercap\n   <fct>   <fct>     <int>   <dbl>   <int>     <dbl>\n 1 Croatia Europe     1952    61.2 3882229     3119.\n 2 Croatia Europe     1957    64.8 3991242     4338.\n 3 Croatia Europe     1962    67.1 4076557     5478.\n 4 Croatia Europe     1967    68.5 4174366     6960.\n 5 Croatia Europe     1972    69.6 4225310     9164.\n 6 Croatia Europe     1977    70.6 4318673    11305.\n 7 Croatia Europe     1982    70.5 4413368    13222.\n 8 Croatia Europe     1987    71.5 4484310    13823.\n 9 Croatia Europe     1992    72.5 4494013     8448.\n10 Croatia Europe     1997    73.7 4444595     9876.\n11 Croatia Europe     2002    74.9 4481020    11628.\n12 Croatia Europe     2007    75.7 4493312    14619.\n\ngapminder[gapminder$country == \"Korea, Rep.\", ]\n\n# A tibble: 12 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Korea, Rep. Asia       1952    47.5 20947571     1031.\n 2 Korea, Rep. Asia       1957    52.7 22611552     1488.\n 3 Korea, Rep. Asia       1962    55.3 26420307     1536.\n 4 Korea, Rep. Asia       1967    57.7 30131000     2029.\n 5 Korea, Rep. Asia       1972    62.6 33505000     3031.\n 6 Korea, Rep. Asia       1977    64.8 36436000     4657.\n 7 Korea, Rep. Asia       1982    67.1 39326000     5623.\n 8 Korea, Rep. Asia       1987    69.8 41622000     8533.\n 9 Korea, Rep. Asia       1992    72.2 43805450    12104.\n10 Korea, Rep. Asia       1997    74.6 46173816    15994.\n11 Korea, Rep. Asia       2002    77.0 47969150    19234.\n12 Korea, Rep. Asia       2007    78.6 49044790    23348.\n\n\"Korea, Rep.\"\n\n[1] \"Korea, Rep.\"\n\nlevels(gapminder$country)\n\n  [1] \"Afghanistan\"              \"Albania\"                 \n  [3] \"Algeria\"                  \"Angola\"                  \n  [5] \"Argentina\"                \"Australia\"               \n  [7] \"Austria\"                  \"Bahrain\"                 \n  [9] \"Bangladesh\"               \"Belgium\"                 \n [11] \"Benin\"                    \"Bolivia\"                 \n [13] \"Bosnia and Herzegovina\"   \"Botswana\"                \n [15] \"Brazil\"                   \"Bulgaria\"                \n [17] \"Burkina Faso\"             \"Burundi\"                 \n [19] \"Cambodia\"                 \"Cameroon\"                \n [21] \"Canada\"                   \"Central African Republic\"\n [23] \"Chad\"                     \"Chile\"                   \n [25] \"China\"                    \"Colombia\"                \n [27] \"Comoros\"                  \"Congo, Dem. Rep.\"        \n [29] \"Congo, Rep.\"              \"Costa Rica\"              \n [31] \"Cote d'Ivoire\"            \"Croatia\"                 \n [33] \"Cuba\"                     \"Czech Republic\"          \n [35] \"Denmark\"                  \"Djibouti\"                \n [37] \"Dominican Republic\"       \"Ecuador\"                 \n [39] \"Egypt\"                    \"El Salvador\"             \n [41] \"Equatorial Guinea\"        \"Eritrea\"                 \n [43] \"Ethiopia\"                 \"Finland\"                 \n [45] \"France\"                   \"Gabon\"                   \n [47] \"Gambia\"                   \"Germany\"                 \n [49] \"Ghana\"                    \"Greece\"                  \n [51] \"Guatemala\"                \"Guinea\"                  \n [53] \"Guinea-Bissau\"            \"Haiti\"                   \n [55] \"Honduras\"                 \"Hong Kong, China\"        \n [57] \"Hungary\"                  \"Iceland\"                 \n [59] \"India\"                    \"Indonesia\"               \n [61] \"Iran\"                     \"Iraq\"                    \n [63] \"Ireland\"                  \"Israel\"                  \n [65] \"Italy\"                    \"Jamaica\"                 \n [67] \"Japan\"                    \"Jordan\"                  \n [69] \"Kenya\"                    \"Korea, Dem. Rep.\"        \n [71] \"Korea, Rep.\"              \"Kuwait\"                  \n [73] \"Lebanon\"                  \"Lesotho\"                 \n [75] \"Liberia\"                  \"Libya\"                   \n [77] \"Madagascar\"               \"Malawi\"                  \n [79] \"Malaysia\"                 \"Mali\"                    \n [81] \"Mauritania\"               \"Mauritius\"               \n [83] \"Mexico\"                   \"Mongolia\"                \n [85] \"Montenegro\"               \"Morocco\"                 \n [87] \"Mozambique\"               \"Myanmar\"                 \n [89] \"Namibia\"                  \"Nepal\"                   \n [91] \"Netherlands\"              \"New Zealand\"             \n [93] \"Nicaragua\"                \"Niger\"                   \n [95] \"Nigeria\"                  \"Norway\"                  \n [97] \"Oman\"                     \"Pakistan\"                \n [99] \"Panama\"                   \"Paraguay\"                \n[101] \"Peru\"                     \"Philippines\"             \n[103] \"Poland\"                   \"Portugal\"                \n[105] \"Puerto Rico\"              \"Reunion\"                 \n[107] \"Romania\"                  \"Rwanda\"                  \n[109] \"Sao Tome and Principe\"    \"Saudi Arabia\"            \n[111] \"Senegal\"                  \"Serbia\"                  \n[113] \"Sierra Leone\"             \"Singapore\"               \n[115] \"Slovak Republic\"          \"Slovenia\"                \n[117] \"Somalia\"                  \"South Africa\"            \n[119] \"Spain\"                    \"Sri Lanka\"               \n[121] \"Sudan\"                    \"Swaziland\"               \n[123] \"Sweden\"                   \"Switzerland\"             \n[125] \"Syria\"                    \"Taiwan\"                  \n[127] \"Tanzania\"                 \"Thailand\"                \n[129] \"Togo\"                     \"Trinidad and Tobago\"     \n[131] \"Tunisia\"                  \"Turkey\"                  \n[133] \"Uganda\"                   \"United Kingdom\"          \n[135] \"United States\"            \"Uruguay\"                 \n[137] \"Venezuela\"                \"Vietnam\"                 \n[139] \"West Bank and Gaza\"       \"Yemen, Rep.\"             \n[141] \"Zambia\"                   \"Zimbabwe\"                \n\ngapminder[gapminder$country == \"Croatia\", \"pop\"]\n\n# A tibble: 12 × 1\n       pop\n     <int>\n 1 3882229\n 2 3991242\n 3 4076557\n 4 4174366\n 5 4225310\n 6 4318673\n 7 4413368\n 8 4484310\n 9 4494013\n10 4444595\n11 4481020\n12 4493312\n\ngapminder[gapminder$country == \"Croatia\", c(\"lifeExp\",\"pop\")]\n\n# A tibble: 12 × 2\n   lifeExp     pop\n     <dbl>   <int>\n 1    61.2 3882229\n 2    64.8 3991242\n 3    67.1 4076557\n 4    68.5 4174366\n 5    69.6 4225310\n 6    70.6 4318673\n 7    70.5 4413368\n 8    71.5 4484310\n 9    72.5 4494013\n10    73.7 4444595\n11    74.9 4481020\n12    75.7 4493312\n\ngapminder[gapminder$country == \"Croatia\" & #Croatia extraction\n            gapminder$year > 1990, #1990 after\n          c(\"lifeExp\",\"pop\")] # those variables \n\n# A tibble: 4 × 2\n  lifeExp     pop\n    <dbl>   <int>\n1    72.5 4494013\n2    73.7 4444595\n3    74.9 4481020\n4    75.7 4493312\n\napply(gapminder[gapminder$country == \"Croatia\", \n                c(\"lifeExp\",\"pop\")], \n      2, mean)\n\n lifeExp      pop \n7.01e+01 4.29e+06 \n\napply(gapminder[gapminder$country == \"Korea, Rep.\", \n                c(\"lifeExp\",\"pop\")], \n      2, mean)\n\n lifeExp      pop \n      65 36499386 \n\n# 03 dplyr 라이브러리를 이용한 데이터 가공 # \nselect(gapminder, country, year, lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   <fct>       <int>   <dbl>\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# … with 1,694 more rows\n\nfilter(gapminder, country == \"Croatia\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp     pop gdpPercap\n   <fct>   <fct>     <int>   <dbl>   <int>     <dbl>\n 1 Croatia Europe     1952    61.2 3882229     3119.\n 2 Croatia Europe     1957    64.8 3991242     4338.\n 3 Croatia Europe     1962    67.1 4076557     5478.\n 4 Croatia Europe     1967    68.5 4174366     6960.\n 5 Croatia Europe     1972    69.6 4225310     9164.\n 6 Croatia Europe     1977    70.6 4318673    11305.\n 7 Croatia Europe     1982    70.5 4413368    13222.\n 8 Croatia Europe     1987    71.5 4484310    13823.\n 9 Croatia Europe     1992    72.5 4494013     8448.\n10 Croatia Europe     1997    73.7 4444595     9876.\n11 Croatia Europe     2002    74.9 4481020    11628.\n12 Croatia Europe     2007    75.7 4493312    14619.\n\nsummarise(gapminder, pop_avg = mean(pop))\n\n# A tibble: 1 × 1\n    pop_avg\n      <dbl>\n1 29601212.\n\nsummarise(group_by(gapminder, continent), pop_avg = mean(pop))\n\n# A tibble: 5 × 2\n  continent   pop_avg\n  <fct>         <dbl>\n1 Africa     9916003.\n2 Americas  24504795.\n3 Asia      77038722.\n4 Europe    17169765.\n5 Oceania    8874672.\n\nsummarise(group_by(gapminder, continent, country), pop_avg = mean(pop))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 142 × 3\n# Groups:   continent [5]\n   continent country                    pop_avg\n   <fct>     <fct>                        <dbl>\n 1 Africa    Algeria                  19875406.\n 2 Africa    Angola                    7309390.\n 3 Africa    Benin                     4017497.\n 4 Africa    Botswana                   971186.\n 5 Africa    Burkina Faso              7548677.\n 6 Africa    Burundi                   4651608.\n 7 Africa    Cameroon                  9816648.\n 8 Africa    Central African Republic  2560963 \n 9 Africa    Chad                      5329256.\n10 Africa    Comoros                    361684.\n# … with 132 more rows\n\ngapminder %>% \n  group_by(continent, country) %>% \n  summarise(pop_avg = mean(pop))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 142 × 3\n# Groups:   continent [5]\n   continent country                    pop_avg\n   <fct>     <fct>                        <dbl>\n 1 Africa    Algeria                  19875406.\n 2 Africa    Angola                    7309390.\n 3 Africa    Benin                     4017497.\n 4 Africa    Botswana                   971186.\n 5 Africa    Burkina Faso              7548677.\n 6 Africa    Burundi                   4651608.\n 7 Africa    Cameroon                  9816648.\n 8 Africa    Central African Republic  2560963 \n 9 Africa    Chad                      5329256.\n10 Africa    Comoros                    361684.\n# … with 132 more rows\n\ntemp1 = filter(gapminder, country == \"Croatia\")      \ntemp2 = select(temp1, country, year, lifeExp)  \ntemp3 = apply(temp2[ , c(\"lifeExp\")], 2, mean)\ntemp3\n\nlifeExp \n   70.1 \n\ngapminder %>% \n  filter(country == \"Croatia\") %>% \n  select(country, year, lifeExp) %>% \n  summarise(lifeExp_avg = mean(lifeExp))\n\n# A tibble: 1 × 1\n  lifeExp_avg\n        <dbl>\n1        70.1\n\n\n\n# 04 데이터 가공의 실제 # \navocado <- read.csv(\"data_2/avocado.csv\", header=TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\nstr(avocado)\n\n'data.frame':   18249 obs. of  14 variables:\n $ X           : int  0 1 2 3 4 5 6 7 8 9 ...\n $ Date        : chr  \"2015-12-27\" \"2015-12-20\" \"2015-12-13\" \"2015-12-06\" ...\n $ AveragePrice: num  1.33 1.35 0.93 1.08 1.28 1.26 0.99 0.98 1.02 1.07 ...\n $ Total.Volume: num  64237 54877 118220 78992 51040 ...\n $ X4046       : num  1037 674 795 1132 941 ...\n $ X4225       : num  54455 44639 109150 71976 43838 ...\n $ X4770       : num  48.2 58.3 130.5 72.6 75.8 ...\n $ Total.Bags  : num  8697 9506 8145 5811 6184 ...\n $ Small.Bags  : num  8604 9408 8042 5677 5986 ...\n $ Large.Bags  : num  93.2 97.5 103.1 133.8 197.7 ...\n $ XLarge.Bags : num  0 0 0 0 0 0 0 0 0 0 ...\n $ type        : chr  \"conventional\" \"conventional\" \"conventional\" \"conventional\" ...\n $ year        : int  2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 ...\n $ region      : chr  \"Albany\" \"Albany\" \"Albany\" \"Albany\" ...\n\n(x_avg = avocado %>% group_by(region) %>% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice)))\n\n# A tibble: 54 × 3\n   region                 V_avg P_avg\n   <chr>                  <dbl> <dbl>\n 1 Albany                47538.  1.56\n 2 Atlanta              262145.  1.34\n 3 BaltimoreWashington  398562.  1.53\n 4 Boise                 42643.  1.35\n 5 Boston               287793.  1.53\n 6 BuffaloRochester      67936.  1.52\n 7 California          3044324.  1.40\n 8 Charlotte            105194.  1.61\n 9 Chicago              395569.  1.56\n10 CincinnatiDayton     131722.  1.21\n# … with 44 more rows\n\n(x_avg = avocado %>% group_by(region, year) %>% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice)))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 216 × 4\n# Groups:   region [54]\n   region               year   V_avg P_avg\n   <chr>               <int>   <dbl> <dbl>\n 1 Albany               2015  38749.  1.54\n 2 Albany               2016  50619.  1.53\n 3 Albany               2017  49355.  1.64\n 4 Albany               2018  64249.  1.44\n 5 Atlanta              2015 223382.  1.38\n 6 Atlanta              2016 272374.  1.21\n 7 Atlanta              2017 271841.  1.43\n 8 Atlanta              2018 342976.  1.29\n 9 BaltimoreWashington  2015 390823.  1.37\n10 BaltimoreWashington  2016 393210.  1.59\n# … with 206 more rows\n\nx_avg = avocado %>% group_by(region, year, type) %>% summarize(V_avg = mean(Total.Volume), P_avg = mean(AveragePrice))\n\n`summarise()` has grouped output by 'region', 'year'. You can override using\nthe `.groups` argument.\n\navocado %>% \n  group_by(region, year, type) %>% \n  summarize(V_avg = mean(Total.Volume), \n            P_avg = mean(AveragePrice)) -> x_avg\n\n`summarise()` has grouped output by 'region', 'year'. You can override using\nthe `.groups` argument.\n\nx_avg %>% filter(region != \"TotalUS\") %>% ggplot(aes(year, V_avg, col = type)) + geom_line() + facet_wrap(~region)\n\n\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\narrange(x_avg, desc(V_avg))\n\n# A tibble: 432 × 5\n# Groups:   region, year [216]\n   region        year type             V_avg P_avg\n   <chr>        <int> <chr>            <dbl> <dbl>\n 1 TotalUS       2018 conventional 42125533. 1.06 \n 2 TotalUS       2016 conventional 34043450. 1.05 \n 3 TotalUS       2017 conventional 33995658. 1.22 \n 4 TotalUS       2015 conventional 31224729. 1.01 \n 5 SouthCentral  2018 conventional  7465557. 0.806\n 6 West          2018 conventional  7451445. 0.981\n 7 California    2018 conventional  6786962. 1.08 \n 8 West          2016 conventional  6404892. 0.916\n 9 West          2017 conventional  6279482. 1.10 \n10 California    2016 conventional  6105539. 1.05 \n# … with 422 more rows\n\nx_avg1 = x_avg %>% filter(region != \"TotalUS\")\n\n\nwine <- read.table(\"data_2/wine.data.txt\", header = TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\nhead(wine)\n\n  X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04 X3.92\n1  1   13.2  1.78  2.14  11.2  100 2.65  2.76 0.26  1.28  4.38  1.05  3.40\n2  1   13.2  2.36  2.67  18.6  101 2.80  3.24 0.30  2.81  5.68  1.03  3.17\n3  1   14.4  1.95  2.50  16.8  113 3.85  3.49 0.24  2.18  7.80  0.86  3.45\n4  1   13.2  2.59  2.87  21.0  118 2.80  2.69 0.39  1.82  4.32  1.04  2.93\n5  1   14.2  1.76  2.45  15.2  112 3.27  3.39 0.34  1.97  6.75  1.05  2.85\n6  1   14.4  1.87  2.45  14.6   96 2.50  2.52 0.30  1.98  5.25  1.02  3.58\n  X1065\n1  1050\n2  1185\n3  1480\n4   735\n5  1450\n6  1290\n\nn = readLines(\"data_2/wine.name.txt\")\nn\n\n [1] \"1) Alcohol\"                      \"2) Malic acid\"                  \n [3] \"3) Ash\"                          \"4) Alcalinity of ash\"           \n [5] \"5) Magnesium\"                    \"6) Total phenols\"               \n [7] \"7) Flavanoids\"                   \"8) Nonflavanoid phenols\"        \n [9] \"9) Proanthocyanins\"              \"10)Color intensity\"             \n[11] \"11)Hue\"                          \"12)OD280/OD315 of diluted wines\"\n[13] \"13)Proline\"                     \n\nnames(wine)[2:14] <- substr(n, 4, nchar(n))\nnames(wine)\n\n [1] \"X1\"                           \"Alcohol\"                     \n [3] \"Malic acid\"                   \"Ash\"                         \n [5] \"Alcalinity of ash\"            \"Magnesium\"                   \n [7] \"Total phenols\"                \"Flavanoids\"                  \n [9] \"Nonflavanoid phenols\"         \"Proanthocyanins\"             \n[11] \"Color intensity\"              \"Hue\"                         \n[13] \"OD280/OD315 of diluted wines\" \"Proline\"                     \n\ntrain_set = sample_frac(wine, 0.6)\nstr(train_set)\n\n'data.frame':   106 obs. of  14 variables:\n $ X1                          : int  1 2 1 3 3 1 2 2 3 3 ...\n $ Alcohol                     : num  13.1 12.7 14.1 14.3 13.4 ...\n $ Malic acid                  : num  1.73 1.75 1.63 1.68 3.91 1.73 1.39 1.01 4.6 2.51 ...\n $ Ash                         : num  2.04 2.28 2.28 2.7 2.48 2.27 2.5 1.7 2.86 2.48 ...\n $ Alcalinity of ash           : num  12.4 22.5 16 25 23 17.4 22.5 15 25 20 ...\n $ Magnesium                   : int  92 84 126 98 102 108 84 78 112 91 ...\n $ Total phenols               : num  2.72 1.38 3 2.8 1.8 2.88 2.56 2.98 1.98 1.68 ...\n $ Flavanoids                  : num  3.27 1.76 3.17 1.31 0.75 3.54 2.29 3.18 0.96 0.7 ...\n $ Nonflavanoid phenols        : num  0.17 0.48 0.24 0.53 0.43 0.32 0.43 0.26 0.27 0.44 ...\n $ Proanthocyanins             : num  2.91 1.63 2.1 2.7 1.41 2.08 1.04 2.28 1.11 1.24 ...\n $ Color intensity             : num  7.2 3.3 5.65 13 7.3 8.9 2.9 5.3 8.5 9.7 ...\n $ Hue                         : num  1.12 0.88 1.09 0.57 0.7 1.12 0.93 1.12 0.67 0.62 ...\n $ OD280/OD315 of diluted wines: num  2.91 2.42 3.71 1.96 1.56 3.1 3.19 3.18 1.92 1.71 ...\n $ Proline                     : int  1150 488 780 660 750 1260 385 502 630 660 ...\n\ntest_set = setdiff(wine, train_set)\nstr(test_set)\n\n'data.frame':   71 obs. of  14 variables:\n $ X1                          : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Alcohol                     : num  13.2 14.1 14.8 14.1 14.1 ...\n $ Malic acid                  : num  1.78 2.15 1.64 2.16 1.48 1.73 1.87 1.81 1.59 1.6 ...\n $ Ash                         : num  2.14 2.61 2.17 2.3 2.32 2.41 2.38 2.7 2.48 2.52 ...\n $ Alcalinity of ash           : num  11.2 17.6 14 18 16.8 16 12 17.2 16.5 17.8 ...\n $ Magnesium                   : int  100 121 97 105 95 89 102 112 108 95 ...\n $ Total phenols               : num  2.65 2.6 2.8 2.95 2.2 2.6 3.3 2.85 3.3 2.48 ...\n $ Flavanoids                  : num  2.76 2.51 2.98 3.32 2.43 2.76 3.64 2.91 3.93 2.37 ...\n $ Nonflavanoid phenols        : num  0.26 0.31 0.29 0.22 0.26 0.29 0.29 0.3 0.32 0.26 ...\n $ Proanthocyanins             : num  1.28 1.25 1.98 2.38 1.57 1.81 2.96 1.46 1.86 1.46 ...\n $ Color intensity             : num  4.38 5.05 5.2 5.75 5 5.6 7.5 7.3 8.7 3.93 ...\n $ Hue                         : num  1.05 1.06 1.08 1.25 1.17 1.15 1.2 1.28 1.23 1.09 ...\n $ OD280/OD315 of diluted wines: num  3.4 3.58 2.85 3.17 2.82 2.9 3 2.88 2.82 3.63 ...\n $ Proline                     : int  1050 1295 1045 1510 1280 1320 1547 1310 1680 1015 ...\n\nelec_gen = read.csv(\"data_2/electricity_generation_per_person.csv\", header = TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\n\nnames(elec_gen)\n\n [1] \"country\" \"X1985\"   \"X1986\"   \"X1987\"   \"X1988\"   \"X1989\"   \"X1990\"  \n [8] \"X1991\"   \"X1992\"   \"X1993\"   \"X1994\"   \"X1995\"   \"X1996\"   \"X1997\"  \n[15] \"X1998\"   \"X1999\"   \"X2000\"   \"X2001\"   \"X2002\"   \"X2003\"   \"X2004\"  \n[22] \"X2005\"   \"X2006\"   \"X2007\"   \"X2008\"   \"X2009\"   \"X2010\"   \"X2011\"  \n[29] \"X2012\"   \"X2013\"   \"X2014\"   \"X2015\"   \"X2016\"  \n\nnames(elec_gen) = substr(names(elec_gen), 2, nchar(names(elec_gen)))\nnames(elec_gen)[1]<-\"country\"\n\nnames(elec_gen)\n\n [1] \"country\" \"1985\"    \"1986\"    \"1987\"    \"1988\"    \"1989\"    \"1990\"   \n [8] \"1991\"    \"1992\"    \"1993\"    \"1994\"    \"1995\"    \"1996\"    \"1997\"   \n[15] \"1998\"    \"1999\"    \"2000\"    \"2001\"    \"2002\"    \"2003\"    \"2004\"   \n[22] \"2005\"    \"2006\"    \"2007\"    \"2008\"    \"2009\"    \"2010\"    \"2011\"   \n[29] \"2012\"    \"2013\"    \"2014\"    \"2015\"    \"2016\"   \n\nelec_use = read.csv(\"data_2/electricity_use_per_person.csv\", header = TRUE, sep = \",\", fileEncoding = \"CP949\", encoding = \"UTF-8\")\nnames(elec_use)[2:56] = substr(names(elec_use)[2:56], 2, nchar(names(elec_use)[2:56]))\n\n# install.packages(\"tidyr\")\nlibrary(tidyr)\nelec_gen_df = gather(elec_gen, -country, key = \"year\", value = \"ElectricityGeneration\")\nelec_use_df = gather(elec_use, -country, key = \"year\", value = \"ElectricityUse\")\n\nelec_gen_use = merge(elec_gen_df, elec_use_df)\n\n\n# Data Visualization\n\n# 평균\napply(anscombe, 1, mean)\n\n [1]  8.65  7.45 10.47  8.57  9.36 10.49  6.34  7.03  9.71  6.93  5.75\n\napply(anscombe, 2, mean)\n\n x1  x2  x3  x4  y1  y2  y3  y4 \n9.0 9.0 9.0 9.0 7.5 7.5 7.5 7.5 \n\n# 분산\napply(anscombe, 2, var)\n\n   x1    x2    x3    x4    y1    y2    y3    y4 \n11.00 11.00 11.00 11.00  4.13  4.13  4.12  4.12 \n\n# 상관관계(상관계수)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.816\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.816\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.816\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.817\n\nlibrary(gapminder)\nlibrary(dplyr)\ny <- gapminder %>% group_by(year, continent) %>% summarize(c_pop = sum(pop)) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nhead(y, 20)\n\n# A tibble: 20 × 3\n# Groups:   year [4]\n    year continent      c_pop\n   <int> <fct>          <dbl>\n 1  1952 Africa     237640501\n 2  1952 Americas   345152446\n 3  1952 Asia      1395357351\n 4  1952 Europe     418120846\n 5  1952 Oceania     10686006\n 6  1957 Africa     264837738\n 7  1957 Americas   386953916\n 8  1957 Asia      1562780599\n 9  1957 Europe     437890351\n10  1957 Oceania     11941976\n11  1962 Africa     296516865\n12  1962 Americas   433270254\n13  1962 Asia      1696357182\n14  1962 Europe     460355155\n15  1962 Oceania     13283518\n16  1967 Africa     335289489\n17  1967 Americas   480746623\n18  1967 Asia      1905662900\n19  1967 Europe     481178958\n20  1967 Oceania     14600414\n\nplot(y$year, y$c_pop)\n\n\n\nplot(y$year, y$c_pop, col = y$continent)\n\n\n\nplot(y$year, y$c_pop, col = y$continent, pch = c(1:5))\nplot(y$year, y$c_pop, col = y$continent, pch = c(1:length(levels(y$continent))))\n\n# 범례 개수를 숫자로 지정\nlegend(\"topright\", legend = levels((y$continent)), pch = c(1:5), col = c(1:5))\n\n# 범례 개수를 데이터 개수에 맞게 지정\nlegend(\"bottomleft\", legend = levels((y$continent)), pch = c(1:length(levels(y$continent))), col = c(1:length(levels(y$continent))))\n\n\n\n# 02 시각화의 기본 기능 #\nplot(gapminder$gdpPercap, gapminder$lifeExp, col = gapminder$continent)\nlegend(\"bottomright\", legend = levels((gapminder$continent)), \n       pch = c(1:length(levels(gapminder$continent))), \n       col = c(1:length(levels(y$continent))))\n\n\n\nplot(log10(gapminder$gdpPercap), gapminder$lifeExp, col = gapminder$continent)\nlegend(\"bottomright\", legend  = levels((gapminder$continent)), pch = c(1:length(levels(gapminder$continent))), col = c(1:length(levels(y$continent))))\n\n\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n# gapminder %>% ggplot(,aes())\n\nggplot(gapminder, aes(x =  gdpPercap, y = lifeExp, col = continent)) + \n  geom_point() + \n  scale_x_log10()\n\n\n\nggplot(gapminder, aes(x =  gdpPercap, y = lifeExp, col = continent, size = pop)) + \n  geom_point() + \n  scale_x_log10()\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent, size = pop)) + \n  geom_point(alpha = 0.5) + \n  scale_x_log10()\n\n\n\ntable(gapminder$year)\n\n\n1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 \n 142  142  142  142  142  142  142  142  142  142  142  142 \n\ngapminder %>% filter(year==1977) %>% \n  ggplot(., aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) + \n  geom_point(alpha=0.5) + \n  scale_x_log10() \n\n\n\ngapminder %>% filter(year==2007) %>% \n  ggplot(., aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) + \n  geom_point(alpha=0.5) + \n  scale_x_log10() \n\n\n\nggplot(gapminder, aes(x=gdpPercap, y=lifeExp, col=continent, size=pop)) + \n  geom_point(alpha=0.5) + \n  scale_x_log10() + \n  facet_wrap(~year)\n\n\n\ngapminder %>% \n  filter(year == 1952 & continent ==\"Asia\") %>% \n  ggplot(aes(reorder(country, pop), pop)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip()\n\n\n\ngapminder %>% filter(year==1952 & continent== \"Asia\") %>% ggplot(aes(reorder(country, pop), pop)) + geom_bar(stat  = \"identity\") + scale_y_log10() + coord_flip()\n\n\n\ngapminder %>% \n  filter(country == \"Korea, Rep.\") %>% \n  ggplot(aes(year, lifeExp, col = country)) + \n  geom_point() + \n  geom_line()\n\n\n\ngapminder %>% \n  filter(country == \"Korea, Rep.\") %>% \n  ggplot(aes(year, lifeExp, col = country)) + \n  # geom_point() + \n  geom_line()\n\n\n\ngapminder %>% \n  ggplot(aes(x = year, y = lifeExp, col = continent)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nx = filter(gapminder, year == 1952)\nhist(x$lifeExp, main = \"Histogram of lifeExp in 1952\")\n\n\n\nx %>% ggplot(aes(lifeExp)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nx %>% ggplot(aes(continent, lifeExp)) + geom_boxplot()\n\n\n\nplot(log10(gapminder$gdpPercap), gapminder$lifeExp)\n\n\n\n\n\n# 03 Visualization Tool\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n# type = \"p\"는 점 플롯, main = \"cars\"는 그래프의 제목\nplot(cars, type  = \"p\", main  = \"cars\")\n\n\n\nplot(cars, type = \"l\", main = \"cars\")       # type =\"l\"은 선을 사용한 플롯\n\n\n\nplot(cars, type=\"b\", main=\"cars\")   # type =\"b\"는 점과 선을 모두 사용한 플롯\n\n\n\nplot(cars, type = \"h\", main = \"cars\")  # type =\"h\"는 히스토그램과 같은 막대 그래프\n\n\n\nx = gapminder %>% filter(year == 1952 & continent == \"Asia\") %>% mutate(gdp = gdpPercap*pop) %>% select(country, gdp) %>% arrange(desc(gdp)) %>% head()\npie(x$gdp, x$country)\n\n\n\nbarplot(x$gdp, names.arg = x$country)\n\n\n\nx = gapminder %>% filter(year == 2007 & continent == \"Asia\") %>% mutate(gdp  = gdpPercap*pop) %>% select(country, gdp) %>% arrange(desc(gdp)) %>% head()\npie(x$gdp, x$country)\n\n\n\nbarplot(x$gdp, names.arg = x$country)\n\n\n\nmatplot(iris[, 1:4], type = \"l\")\nlegend(\"topleft\", names(iris)[1:4], lty = c(1, 2, 3, 4), col = c(1, 2, 3, 4))\n\n\n\nhist(cars$speed)\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) + geom_point(alpha = 0.2)\n\n\n\ngapminder %>% filter(lifeExp>70) %>% \n  group_by(continent) %>% \n  summarize(n = n_distinct(country)) %>% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\")\n\n\n\ngapminder %>% filter(year == 2007) %>% \n  ggplot(aes(lifeExp, col = continent)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ngapminder %>% filter(year == 2007) %>% \n  ggplot(aes(lifeExp, col = continent)) + \n  geom_histogram(position = \"dodge\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ngapminder %>% \n  filter(year == 2007) %>% \n  ggplot(aes(continent, lifeExp, col = continent)) + \n  geom_boxplot()\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) + \n  geom_point(alpha = 0.2)\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, col = continent)) +\n  geom_point(alpha = 0.2) + scale_x_log10() # 가로축을 로그 스케일로 변환함.\n\n\n\ngapminder %>% \n  filter(continent == \"Africa\") %>% \n  ggplot(aes(country, lifeExp)) + \n  geom_bar(stat  =  \"identity\")                  # [그림 6-35(a)]\n\n\n\ngapminder %>% \n  filter(continent == \"Africa\") %>% \n  ggplot(aes(country, lifeExp)) + \n  geom_bar(stat  =  \"identity\") + \n  coord_flip()    # [그림 6-35(b)] 플롯의 방향을 전환함. \n\n\n\n# install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\ndisplay.brewer.all()\n\n\n\n# [그림 6-37(a)] : 기본 팔레트를 적용한 그래프\ngapminder %>% filter(lifeExp>70) %>% \n  group_by(continent) %>% \n  summarize(n  = n_distinct(country)) %>% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\", aes(fill = continent)) \n\n\n\n# [그림 6-37(c)] Blues 팔레트를 적용한 그래프\ngapminder %>% \n  filter(lifeExp>70) %>% \n  group_by(continent) %>% \n  summarize(n = n_distinct(country)) %>% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\", aes(fill = continent)) + scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n# [그림 6-37(d)] Oranges 팔레트를 적용한 그래프\ngapminder %>% \n  filter(lifeExp>70) %>% \n  group_by(continent) %>% \n  summarize(n =  n_distinct(country)) %>% \n  ggplot(aes(x = continent, y = n)) + \n  geom_bar(stat = \"identity\", aes(fill =  continent)) + scale_fill_brewer(palette = \"Oranges\")\n\n\n\n\n\n# reorder(continent, -n)은 continent를 n을 기준으로 내림차 순으로 정렬하라는 의미\ngapminder %>% \n  filter(lifeExp >70) %>% \n  group_by(continent) %>% \n  summarize(n  =  n_distinct(country)) %>% \n  ggplot(aes(x = reorder(continent, -n), y =  n)) + \n  geom_bar(stat = \"identity\", aes(fill =  continent)) + \n  scale_fill_brewer(palette  = \"Blues\")\n\n\n\n# 실습!!\ngapminder %>%\n  filter(continent == \"Africa\", year==2007) %>%\n  ggplot(aes(reorder(country, lifeExp), lifeExp, fill=lifeExp)) +\n  geom_bar(stat  =  \"identity\") +\n  coord_flip()\n\n\n\n# \ngapminder %>%\n  filter(continent == \"Africa\", year==2007) %>%\n  ggplot(aes(reorder(country, lifeExp), lifeExp, fill=lifeExp)) +\n  geom_bar(stat  =  \"identity\") +\n  coord_flip()  + \n  scale_fill_distiller(palette = \"Oranges\", direction=1)\n\n\n\n\n\n# 04 시각화를 이용한 데이터 탐색 #\n\ngapminder %>% ggplot(aes(gdpPercap, lifeExp, col = continent)) + geom_point(alpha  =  0.2) + facet_wrap(~year) + scale_x_log10()\n\n\n\ngapminder %>% filter(year == 1952 & gdpPercap > 10000 & continent == \"Asia\") \n\n# A tibble: 1 × 6\n  country continent  year lifeExp    pop gdpPercap\n  <fct>   <fct>     <int>   <dbl>  <int>     <dbl>\n1 Kuwait  Asia       1952    55.6 160000   108382.\n\ngapminder %>% filter(country == \"Kuwait\") %>% ggplot(aes(year, gdpPercap)) + geom_point() + geom_line()             # [그림 6-40(a)]\n\n\n\ngapminder %>% filter(country == \"Kuwait\") %>% ggplot(aes(year, pop)) + geom_point() + geom_line()                   # [그림 6-40(b)]\n\n\n\ngapminder %>% filter(country == \"Korea, Rep.\") %>% ggplot(aes(year, gdpPercap)) + geom_point() + geom_line()        # [그림 6-41(a)]\n\n\n\ngapminder %>% filter(country == \"Korea, Rep.\") %>% ggplot(aes(year, pop)) + geom_point() + geom_line()              # [그림 6-41(b)]\n\n\n\ngapminder %>% filter(country == \"Kuwait\" | country == \"Korea, Rep.\") %>% mutate(gdp = gdpPercap*pop) %>% ggplot(aes(year, gdp, col = country)) + geom_point() + geom_line()\n\n\n\n# [그림 6-43(a)] gdpPercap의 변화 비교 \ngapminder %>% filter(country == \"Kuwait\"|country == \"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China\"|country == \"Japan\")  %>% ggplot(aes(year, gdpPercap, col = country)) + geom_point() + geom_line()\n\n\n\n# [그림 6-43(b)] pop의 변화 비교 \ngapminder %>% filter(country == \"Kuwait\"|country==\"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China\"|country == \"Japan\")  %>% ggplot(aes(year, pop, col=country)) + geom_point() + geom_line()\n\n\n\n# [그림 6-43(c)] gdp의 변화 비교 \ngapminder %>% filter(country == \"Kuwait\"|country == \"Saudi Arabia\"|country == \"Iraq\"|country == \"Iran\"|country == \"Korea, Rep.\"|country == \"China\"|country == \"Japan\")  %>% mutate(gdp=gdpPercap*pop) %>% ggplot(aes(year, gdp, col = country)) + geom_point() + geom_line() + scale_y_log10()"
  },
  {
    "objectID": "teaching/data_journalism/index.html",
    "href": "teaching/data_journalism/index.html",
    "title": "Data Journalism",
    "section": "",
    "text": "Introduction\nWelcome to our Data Journalism course, an exciting and comprehensive journey designed to equip you with the skills necessary to become a proficient data journalist. Over 15 weeks, you will learn about the fundamentals of journalism and data-driven storytelling, master data manipulation and visualization techniques using R and Tableau, and delve into ethical and legal considerations.\nThroughout the course, you’ll develop a strong foundation in identifying newsworthy stories, conducting interviews, and fact-checking information. You will also gain hands-on experience in data cleaning, preprocessing, exploratory analysis, and various data visualization techniques, including advanced chart types and interactivity.\nBy the end of this course, you will have completed a data journalism project incorporating data analysis, visualization, and journalistic storytelling. With a solid understanding of the concepts and tools covered, you’ll be well-prepared to apply your skills in the ever-evolving field of data journalism.\n\n\n\nSyllabus\nWeek 1: Introduction to Journalism and Data Journalism\n\nWhat is Journalism?\nFundamentals of news writing and reporting\nThe importance of data-driven storytelling\nWhat is Data Journalism?\nOverview of tools: R, Tableau, and others\n\nWeek 2: Finding and Evaluating News Stories\n\nIdentifying newsworthy stories\nGenerating story ideas\nEvaluating story angles and potential impact\nSourcing data for stories\nEvaluating data quality and credibility\n\nWeek 3: Interviewing and Fact-Checking\n\nPrinciples of journalistic interviewing\nPreparing for and conducting interviews\nFact-checking and verifying information\nEthical considerations in interviewing and reporting\n\nWeek 4: Data Cleaning and Preprocessing\n\nIntroduction to data manipulation in R (tidyverse)\nData cleaning, filtering, and aggregation\nData transformation and handling missing data\nData normalization and scaling\n\nWeek 5: Descriptive Statistics and Exploration\n\nDescriptive statistics in R\nExploratory data analysis (EDA) with R\nIdentifying trends, patterns, and outliers\nAsking the right questions\n\nWeek 6: Introduction to Data Visualization with R\n\nIntroduction to ggplot2\nGrammar of graphics with ggplot2\nCustomizing plots: themes, scales, labels, and titles\nDifferent types of plots and when to use them\n\nWeek 7: Advanced Data Visualization with R\n\nAdvanced ggplot2 techniques\nFaceting and multi-panel plots\nTime series and geospatial data visualization\nInteractive visualizations with plotly or ggplotly\n\nWeek 8: Introduction to Tableau\n\nTableau interface and basics\nConnecting Tableau to data sources\nCreating and customizing visualizations in Tableau\n\nWeek 9: Advanced Data Visualization with Tableau\n\nAdvanced chart types and techniques in Tableau\nCreating dashboards and stories in Tableau\nInteractive and dynamic visualizations in Tableau\nGeospatial data visualization in Tableau\n\nWeek 10: Combining R and Tableau\n\nExporting R data and visualizations to Tableau\nIntegrating R scripts within Tableau\nLeveraging the strengths of both tools for data journalism\n\nWeek 11: Crafting Compelling News Stories\n\nStructuring news articles and data-driven stories\nBalancing text and visualizations\nWriting clear and concise news copy\nIncorporating quotes and interview material\nEnsuring accuracy and transparency\n\nWeek 12: Digital Writing and Interactive Media with Shiny and Quarto\n\nIntroduction to Shiny and Quarto for interactive media\nCreating Shiny apps and Quarto websites for digital storytelling\nEmbedding data visualizations and interactive elements\nBest practices for designing engaging and accessible digital content\n\nWeek 13: Legal and Ethical Considerations\n\nData privacy and security\nCopyright and licensing issues\nResponsible data reporting and fact-checking\nAvoiding bias and promoting inclusivity in journalism\n\nWeek 14: Team project consultation\nWeek 15: Project presentation\n\nStudents present their data journalism projects\nProjects should incorporate data analysis, visualization, and journalistic storytelling\nFeedback and discussion on projects\nReflecting on the course and potential future applications in the field of data journalism"
  },
  {
    "objectID": "teaching/cul_tech/index.html",
    "href": "teaching/cul_tech/index.html",
    "title": "Culture & Technology",
    "section": "",
    "text": "Introduction\nCulture & Technology is an interdisciplinary course designed to explore the complex relationship between culture and technology, and the ways in which technological innovations have shaped and continue to shape our societies. The course will cover a wide range of topics, from the historical perspective of technology’s impact on culture to the contemporary issues raised by the rapid pace of technological change.\nThroughout the course, students will delve into the influences of technology on various aspects of culture, such as art, education, communication, identity, work, and the environment. The course will also address critical concerns like the digital divide, ethical considerations, and the role of regulation and policy in shaping technology.\nBy integrating a liberal arts perspective with technical insights, this course aims to foster a deeper understanding of the interplay between culture and technology, encouraging students to think critically about the role technology plays in our lives and to consider the ethical and societal implications of technological advancements.\nThroughout the course, students should engage in discussions, group projects, and individual assignments that explore the relationship between culture and technology. Encourage students to think critically about the role technology plays in shaping our cultural, social, and individual experiences, and to consider the ethical and societal implications\n\n\n\nSyllabus\nWeek 1: Introduction to Culture & Technology\n\nDefining culture and technology\nHistorical perspective on technology and its influence on culture\nThe role of technology in shaping human societies\n\nWeek 2: Technological Innovations and Cultural Shifts\n\nKey technological innovations throughout history\nThe impact of inventions on cultural, social, and economic changes\nThe role of technology in globalization\n\nWeek 3: The Internet and Digital Culture\n\nThe history of the internet and its impact on culture\nThe emergence of digital culture\nOnline communities, social media, and virtual worlds\n\nWeek 4: Communication Technologies and Media\n\nEvolution of communication technologies\nThe impact of media on culture (radio, television, and the internet)\nThe rise of new media and its effects on traditional media\n\nWeek 5: Art and Technology\n\nThe influence of technology on artistic expression\nDigital art forms and multimedia\nTechnology in the performing arts\n\nWeek 6: Technology and Education\n\nThe role of technology in education and learning\nThe impact of technology on traditional educational institutions\nOnline learning, MOOCs, and the future of education\n\nWeek 7: Technology and Language\n\nThe influence of technology on language and communication\nThe impact of technology on linguistic diversity\nLanguage technologies and natural language processing\n\nWeek 8: Midterm Exam Week\nWeek 9: Technology and Identity\n\nThe role of technology in shaping individual and collective identities\nOnline identity, anonymity, and privacy\nThe impact of technology on personal relationships and social interactions\n\nWeek 10: Artificial Intelligence and Society\n\nIntroduction to artificial intelligence (AI)\nThe societal impact of AI and automation\nEthical considerations in AI development and deployment\n\nWeek 11: Technology and Work\n\nThe influence of technology on work and employment\nTechnological innovations and the changing nature of work\nThe gig economy, remote work, and the future of work\n\nWeek 12: Technology and the Environment\n\nThe impact of technology on the environment and natural resources\nSustainable technologies and green innovations\nBalancing technological development with environmental responsibility\n\nWeek 13: Digital Divide and Inequality\n\nThe digital divide and its implications for social equality\nThe role of technology in perpetuating or alleviating social inequalities\nStrategies for fostering digital inclusion and equal access to technology\n\nWeek 14: Ethics, Regulation, and Policy in Technology\n\nEthical considerations in the development and use of technology\nThe role of regulation and policy in shaping technology\nBalancing innovation with the need for responsible technological development\n\nWeek 15: Final Exam Week"
  },
  {
    "objectID": "teaching/comvis/index.html",
    "href": "teaching/comvis/index.html",
    "title": "Computer Vision and Unstructured Data Analysis for Social Science Research",
    "section": "",
    "text": "Introduction\nIn today’s digital age, vast amounts of data are being generated in the form of text, sound, image, and video. Unstructured data, in particular, holds tremendous potential for social science research, as it can reveal insights that would be difficult or impossible to obtain through traditional methods. However, working with unstructured data requires specialized skills and techniques, which many social scientists may not possess.\nThis course, Unstructured Data Analysis for Social Science Research, is designed to equip students with the knowledge and skills needed to effectively analyze unstructured data. Throughout the 15-week course, students will learn how to collect, preprocess, and analyze text, sound, image, and video data, and how to integrate different forms of unstructured data to gain deeper insights. They will also explore ethical considerations related to using unstructured data and best practices for presenting findings.\nBy the end of this course, students will have a deep understanding of the potential of unstructured data in social science research and the skills needed to analyze it. They will be able to apply their knowledge to real-world problems, and they will have a portfolio of projects demonstrating their ability to work with unstructured data. The course is suitable for graduate students and researchers in social sciences who want to expand their research methods and explore new avenues for analysis, as well as professionals who work with data in a social science context.\n\n\n\nSyllabus\nWeek 1: Introduction to unstructured data analysis in social science research\n\nOverview of unstructured data and its relevance in social science research\nUnderstanding the different forms of unstructured data\nEthical considerations in using unstructured data\n\nWeek 2: Introduction to text data analysis\n\nCollecting and preprocessing text data\nText mining techniques for exploratory analysis\nSentiment analysis and topic modeling\n\nWeek 3: Advanced text data analysis\n\nNamed entity recognition and extraction\nText classification and clustering\nDeep learning for text analysis\n\nWeek 4: Introduction to sound data analysis\n\nCollecting and preprocessing sound data\nSound visualization and analysis\nFeature extraction techniques\n\nWeek 5: Advanced sound data analysis\n\nMusic information retrieval\nSpeech recognition and sentiment analysis\nDeep learning for sound analysis\n\nWeek 6: Introduction to image data analysis\n\nCollecting and preprocessing image data\nImage visualization and analysis\nFeature extraction techniques\n\nWeek 7: Advanced image data analysis\n\nObject detection and recognition\nImage classification and clustering\nDeep learning for image analysis\n\nWeek 8: Introduction to video data analysis\n\nCollecting and preprocessing video data\nVideo visualization and analysis\nFeature extraction techniques\n\nWeek 9: Advanced video data analysis\n\nAction recognition and detection\nVideo classification and clustering\nDeep learning for video analysis\n\nWeek 10: Data integration and fusion\n\nCombining different forms of unstructured data\nFusion techniques for unstructured data analysis\nCase studies on integrated data analysis\n\nWeek 11: Social media data analysis\n\nCollecting and preprocessing social media data\nSentiment analysis and opinion mining\nNetwork analysis and visualization\n\nWeek 12: Web data analysis\n\nWeb scraping and preprocessing\nWeb content analysis and classification\nWeb usage and access analysis\n\nWeek 13: Geospatial data analysis\n\nGeospatial data collection and preprocessing\nGeospatial data visualization and analysis\nGeospatial data fusion with other unstructured data\n\nWeek 14: Visualization and presentation of unstructured data analysis\n\nVisualization techniques for unstructured data\nStorytelling with unstructured data\nBest practices for presenting unstructured data analysis\n\nWeek 15: Project presentation and feedback\n\nStudents present their project work and receive feedback from the instructor and peers\nWrap-up and future directions in unstructured data analysis for social science research"
  },
  {
    "objectID": "teaching/network/index.html",
    "href": "teaching/network/index.html",
    "title": "Network Analysis in Social Science",
    "section": "",
    "text": "Introduction\nSocial scientists are increasingly turning to network analysis as a way to gain insights into the structure and dynamics of social systems. Network analysis allows researchers to examine how social actors are connected to one another, how information and resources flow through social networks, and how social networks shape individual behavior and collective outcomes. In this course, students will learn the fundamental concepts and techniques of network analysis and explore how they can be applied to social science research using the R programming language.\nHands-on practice with R will be an integral part of the course. Students will use the R programming language to manipulate, visualize, and analyze network data. R packages such as igraph, statnet, vizNetwork, and networkD3 will be used for network visualization, analysis, and modeling. Students will be provided with R code and examples to practice the concepts covered in class. Assignments and projects will require students to apply their knowledge of network analysis to real-world social science problems using R.\nThis course is designed for graduate students and researchers in social sciences who want to expand their research methods and explore new avenues for analysis using R. Prior experience with R is not required, but students should be comfortable with basic statistical analysis and have some familiarity with programming concepts. By the end of this course, students will have a solid understanding of network analysis in social science research and the ability to apply it using R. They will be able to use their new skills to explore social systems and gain insights into how social networks shape individual behavior and collective outcomes.\n\n\n\nSyllabus\nWeek 1: Introduction to network analysis in social science\n\nOverview of social network theory and concepts\nIntroduction to network data collection and analysis\nBasic network measures and visualization techniques using R\n\nWeek 2: Network data collection and preparation in R\n\nSurvey and interview techniques for network data collection\nData cleaning and preparation for network analysis in R\nEthical considerations in network data collection\n\nWeek 3: Network visualization and exploration in R\n\nNetwork visualization and layout techniques using R\nNetwork exploration and analysis using R software tools\nInteractive network visualization for exploration and presentation in R\n\nWeek 4: Measures of centrality and power in networks using R\n\nDegree centrality, betweenness centrality, and closeness centrality in R\nEigenvalue centrality and PageRank algorithm in R\nHubs and authorities and other measures of power in R\n\nWeek 5: Network clustering and community detection in R\n\nClustering algorithms and techniques for detecting communities in R\nModularity optimization and other community detection measures in R\nVisualization of network clusters and communities in R\n\nWeek 6: Network dynamics and change over time in R\n\nModels for network growth and evolution in R\nNetwork diffusion models and spread of influence in R\nLongitudinal network analysis and visualization in R\n\nWeek 7: Multiplex networks and multilevel analysis in R\n\nMultiplex networks and their analysis in R\nMultilevel network analysis and its applications in R\nNetwork-based models for social systems in R\n\nWeek 8: Network models for social contagion and influence in R\n\nDiffusion models and the spread of information and behavior in R\nContagion models and epidemics in social networks in R\nModels for social influence and persuasion in R\n\nWeek 9: Network models for social support and health in R\n\nSocial support and its measurement in network analysis in R\nSocial network analysis of health and illness in R\nNetwork models for health interventions and prevention in R\n\nWeek 10: Political networks and power relations in R\n\nNetwork analysis of power and influence in politics in R\nPolitical alliances and coalitions in networks in R\nNetwork models for predicting elections and voting behavior in R\n\nWeek 11: Economic networks and market dynamics in R\n\nNetwork analysis of economic systems and markets in R\nSocial networks and their influence on economic outcomes in R\nModels for network-based entrepreneurship and innovation in R\n\nWeek 12: Cultural networks and artistic production in R\n\nCultural networks and their analysis in R\nSocial networks in the creative industries in R\nNetwork models for artistic collaboration and production in R\n\nWeek 13: Social network interventions and applications in R\n\nNetwork-based interventions in social systems in R\nNetwork approaches to community building and development in R\nNetwork analysis and social policy in R\n\nWeek 14: Advanced network analysis and future directions in R\n\nAdvanced topics in network analysis in R\nFuture directions and trends in network analysis research in R\nStudent project presentations and feedback in R\n\nWeek 15: Final project and wrap-up\n\nStudents will work on a final project applying network analysis techniques to a social science research question using R.\nThe instructor will provide guidance and feedback to the students throughout the project.\nStudents will present their final project to the class and receive feedback from their peers.\nThe final class will be a wrap-up and future directions in network analysis research using R."
  },
  {
    "objectID": "proj/proj_01.html",
    "href": "proj/proj_01.html",
    "title": "한국연구재단 이공분야 생애첫연구",
    "section": "",
    "text": "개요\n\n본 과제는 전 세계 특허 데이터와 양자간 무역 데이터를 활용하여 한국의 미래 기술 혁신과 4차 산업 인재 양성 전략에 기여할 수 있는 정책 개발에 그 목적을 두고 있다.\n기존의 국가 기술 정책의 경우 특정 지역의 기술적 상황이나 지식 축적의 정도를 제대로 반영하지 못하는 한계점을 가지고 있어 지역 혁신 정책의 기반이 되는 단서를 제공하는 융복합 방법론 개발이 필요하다.\n따라서 이를 융복합 학문인 경제지리학과 데이터과학을 통해 지역 기술 혁신 정책을 근거 중심으로 체계화 하는 창의적 학술연구를 수행한다.\n나아가 현재 전 세계에서 일어나고 있는 무역 전쟁의 핵심인 기술 경쟁의 본질을 양자간 무역 데이터를 통해 동아시아의 기술의 흐름을 분석해 봄으로써 향후 정책 방향을 모색하는 도전적 학술연구를 수행한다.\n마지막으로 연구 과정에서 나오는 성과와 혜안을 공유하기 위한 웹 기반 플랫폼을 구축하고 홍보하여 정부 정책 입안 방향에 적용될 수 있도록 돕는다.\n\n\n\n연구 배경\n\n한국은 1960년대부터 전 세계에서 유례없는 성장률인 연 평균 7%를 기록하며 급속도로 성장해왔다. 그 결과 1960년대 158불(USD)이었던 일인당 국내 총생산이 2017년에는 약 3만불(29,742 USD)까지 증가했다. 현재 한국은 아시아에서는 4번째, 전 세계에서는 12번째로 큰 경제국이다. 호의적이었던 국제 시장 상황, 높은 비율의 숙련공, 노동력의 높은 교육 수준 등의 요소가 물론 주요했지만, 학계에서는 수출 주도의 산업 정책이 성공 요인으로 빈번하게 거론되어 왔다. 하지만 글로벌 경제 위기 이후 한국은 예전의 성장력을 회복하지 못하고 저성장의 길로 들어섰다. 한국은행과 국제통화기금(IMF)는 모두 한국경제의 잠재성장률을 2% 수준으로 보고 있으며 2020년대와 30년대는 1%대로 떨어질 것으로 예측하고 있다. 이는 물론 국제적인 불황과 노동 인구의 감소가 큰 축을 담당하고 있지만 그런 요인들은 현재 모든 선진국의 당면 과제들이다.\n\n유럽 연합(EU)은 이러한 문제점을 조기에 인식하고 국가를 중심으로 저성장, 실업문제를 해결할 지역정책 모색과 산업정책인 스마트 특성화 전략(Smart Specialization Strategy, S3)을 수행 중이며 하향식(Top-down)과 상향식(Bottom-up) 방법을 적절히 혼합하여 지역 혁신의 성과를 이루어 내고 있다. 유럽 내의 모든 국가와 지역에서 주력 산업 및 미래 성장동력산업 육성을 위해 획일적으로 적용되어 왔던 기술 정책들을 각 지역의 역량, 특성 및 잠재력에 기초하여 새로운 지역별 혁신 모델을 정착시키고 있다. 하지만, 한국은 달라진 기술 환경에 적응하지 못하고 기존의 혁신 모델에서 크게 벗어나고 있지 못하고 있다. 기존에 이루어진 많은 기술 정책들은 지역의 지식 역량을 고려하지 않은 채 바이오, 나노, 정보통신(IT) 등의 유사한 지식 클러스터를 특정 지역에 특화 시키려고 노력했으나 새로운 융합 기술이나 산업의 구조적 변화를 이끌어 내는 데는 한계가 있었다. 또한 이로 인해 지식기반이 획일화되고 국가 및 지역별 지식 구조의 독창성과 차별성을 상실했다는 비판도 면할 수 없었다. 이에, 지역 지식 역량을 고려한 전략과 투자를 차별화하여 불필요한 중복 투자를 막고 지역의 지식, 기술, 인적 자원을 최대한 활용한 지역 혁신정책의 필요성이 제기되고 있다.\n\n따라서 본 과제에서는 지역별 지식 구조를 분석하는 방법을 체계화하고 이를 근거로 지역의 기술 혁신 정책, 즉 한국형 스마트 특성화 전략(Korean Smart Specialization Strategy, KS3)을 개발하고자 한다. 구체적으로는 한국의 지식 구조의 진화 과정을 특허 데이터에 포함되어 있는 메타 정보를 활용하여 분석해보고 이를 지역별 지식 구조의 진화 과정과 비교해본다. 이를 통해 각 지역의 지식 역량을 기반으로 한 스마트 특성화 전략을 도출할 수 있다. 또한 특허 데이터를 통해 얻어진 지역의 지식 구조와 양국간 무역 데이터와의 연결을 통해 일본과 중국의 특성화된 기술에 비해 한국의 경쟁력이 낮은 기술들을 개발할 때 어떤 지역에서 해당 기술을 특화시켜야 하는지에 대한 전략도 수립해 보고자 한다.\n\n\n\n기대효과\n\n4차 산업 혁명을 대비해 많은 국가들이 인공지능, 로보틱스 등의 신산업을 육성하기 위해 기술 로드맵을 개발하고 있지만 정작 지역별 지식 공간을 분석하여 근거를 가지고 특정 지역을 특정 기술로 특화 시키거나 개발하는 노력은 부족해왔다.\n본 과제의 성공적 수행을 통해 첫째로 한국형 스마트 특성화 전략에 대한 학문적 근거를 마련할 수 있을 것으로 보인다. 둘째로 현재 이슈가 되고 있는 한국과 미국, 일본, 중국 등의 무역 갈등 기저에 있는 기술 갈등 역시 학문적 접근이 가능하다.\n이는 정책적으로 상당한 파급효과가 있을 것으로 예상된다. 학문적 성과 외에도 연구 결과가 정책 입안자들이 실용적으로 시의적절한 참고가 될 수 있도록 웹 기반 플랫폼을 구축할 예정이다. 이를 통해 연구 결과가 실제로 정책으로 이어지는 파급 효과가 예상된다.\n\n\n\n\n성과\n\n컨퍼런스 발표: 국내 4회, 국제 1회\n\n20200612: 사이버커뮤니케이션학회 춘계정기학술대회. 사이버커뮤니케이션학회\n\n미디어연관도 분석을 통한 미디어 수용자의 선택적 노출 측정과 정치적 양극화\n\n20200925: 제8회 한국미디어패널학술대회. 정보통신정책연구원.\n\n늦은 밤 OTT 시청이 수면에 끼치는 영향\n\n\n\n\n20201120: 정보통신정책학회 정기학술대회. 정보통신정책연구원.\n\n한국 AI 지식의 진화, 그리고 미래\n\n20210903: 제9회 한국미디어패널학술대회. 정보통신정책연구원.\n\n개인적 특성, 환경적 요인, 시간대와 요일효과를 고려한 OTT 선택 요인 분석: TV vs. OTT\n\n20221017: ERC TechEvo Workshop. European Research Council. @Vienna Complex Hub Lab, Austria\n\nNetwork Methods to Analyze Collective Knowledge\n\n\n\n\n논문 발간: S(S)CI 6편, KCI 1편\n\nShon, M., Lee, D. & Lee, C.(2022). Inward or Outward? Direction of Knowledge Flow and Firm Efficiency. International Journal of Technology Management. 90(1-2), 102-121. https://doi.org/10.1504/IJTM.2022.124617\nTóth, G., Elekes, Z., Whittle, A., Lee, C.*, & Kogler, D. F. (2022). Technology network structure conditions the economic resilience of regions. Economic Geography. 98(4), 1-24. https://doi.org/10.1080/00130095.2022.2035715\nKim, K., Lee, J., & Lee, C.(2022). Which innovation type is better for production efficiency? A comparison between product/service, process, organizational, and marketing innovations using stochastic frontier and meta-frontier analysis. Technology Analysis & Strategic Management. doi: https://doi.org/10.1080/09537325.2021.1965979\nRocchetta, S., Mina, A., Lee, C., & Kogler, F. D. (2022). Technological Knowledge Space and the Resilience of European Regions. Journal of Economic Geography. 22(1), 27-51.* doi: https://doi.org/10.1093/jeg/lbab001\nLee, C., Cho, H., & Lee, D.* (2021). The mechanism of innovation spill-over across sub-layers in the ICT industry. Asian Journal of Technology Innovation. 29(2), 159-179. doi:https://doi.org/10.1080/19761597.2020.1796725\nKim, K., Lee, J., & Lee, C.* (2021). Exploratory Analysis of Knowledge Structure and Evolutionary Trajectory in Korean Artificial Intelligence for Effective Technology Policy. Korean Innovation Study, 16(3). DOI:https://doi.org/10.46251/INNOS.2021.8.16.3.139\nLee, C., Lee, D., & Shon, M.* (2020). Effect of efficient triple-helix collaboration on organizations based on their stage of growth. Journal of Engineering and Technology Management. 58, 101604. https://doi.org/10.1016/j.jengtecman.2020.101604"
  },
  {
    "objectID": "proj/proj_02.html",
    "href": "proj/proj_02.html",
    "title": "한국연구재단 신진연구자지원사업(인문사회)",
    "section": "",
    "text": "연구 목표\n메타버스가 새로운 사회적 공간으로 급부상하면서, 그러한 가상세계에서 사용자를 대표하는 아바타 역시 제 2의 사회적 자아로 자리 잡고 있는 상황이다. 그러나 관련된 연구 현황은 변화한 사회 흐름을 반영하지 못한 채 기존의 온라인 플랫폼 속 아바타에 대한 분석에만 머무르고 있는 실정이다. 이에 본 연구는 새로운 가상세계인 메타버스 속 아바타가 어떠한 의도와 양상으로 표현되는지를 탐구하고자 한다. 특히 메타버스 사용자의 아바타 창조에 초점을 맞춰, 멀티 페르소나(다중 정체성) 지표를 개발하고 그에 따른 다양한 실험을 수행할 것이다. 또한 이를 통해 멀티 페르소나 성향이 메타버스 사용자의 감각적 몰입과 창의성에 어떤 영향을 끼치는지를 분석하고자 한다. 이는 국내외 연구에서 단 한 번도 시도된 적 없는 새로운 미디어-사용자 상호작용 프레임 워크의 개발로서, 기존 가상세계의 아바타와는 확연히 구분되는 결과를 제시할 것으로 예측한다.\n\n\n기대 효과\n본 연구의 결과물은 아바타 창조에 대한 단편적 분석 수준을 넘어 1) 사회적 측면에서 메타버스의 주요 사용 계층인 1020세대의 미디어 사용 현상과 경험에 관한 심층적 이해를 제시할 수 있을 것이다. 또한 이는 급변하는 미디어 환경에 적절히 대응하도록 하는 미디어 리터러시 교육에 대한 단초까지도 제공할 수 있을 것이라 기대한다. 2) 학문적 측면에서 역시 메타버스라는 새로운 플랫폼 사용자의 아바타 활용 성향에 창의적이고 도전적으로 접근함으로써, 뉴미디어 연구 분야에 다양하고 폭넓은 논의점을 제시할 수 있을 것이라 기대한다. 나아가, 설문과 실험 기법의 창의적인 융합은 복잡다난해지는 미디어 연구 영역에서 보다 과학적이고 체계적인 연구 방법을 제안하는 일이 될 것이다. 3) 연구인력 양성의 측면에서, 본 연구는 아직 충분히 연구되지 못한 메타버스 영역의 관련 서비스와 사용자에 대한 전문적인 연구 인력을 배출할 수 있을 것이다.\n\n\n연구 요약\n새롭게 등장한 가상세계 플랫폼인 메타버스는 사회 전반에 걸친 다양한 영역과 연계될 수 있다는 높은 가능성을 지니고 있음에도, 관련된 이해와 탐구는 여전히 미미한 수준이다. 특히 메타버스에서 사용자를 대신할 수 있는 아바타는 제2, 제3의 자아로 존재하며 사회적 상호작용과 가상세계로의 몰입에 중요한 매개체가 되어가고 있다. 이에 본 연구는 관련한 심층적 연구의 필요성을 인식하여 1) 메타버스 속 아바타 창조의도에 대한 탐구를 통해 사용자의 멀티 페르소나 지표를 개발하고 2) 메타버스 가상세계에서의 멀티 페르소나 특성이 사용자의 감각적 몰입에 끼치는 영향에 대해 분석하며 3) 메타버스 가상세계에서의 멀티 페르소나 특성이 사용자의 창의성에 끼치는 영향에 대해 분석하고자 한다. 그에 따라 구체적으로, 1차 년도에는 메타버스 가상공간 속 멀티 페르소나에 대한 이론적 프레임워크를 개발하여 이를 기반으로 멀티 페르소나 지수 측정 아이템을 고안한다. 이는 추후에 이어질 연구에서 사용자의 멀티 페르소나 성향을 제시하는 중요한 단초가 된다. 2차 년도에는 메타버스 속 감각적 몰입에 대한 정의와 측정 아이템을 개발한 후, 멀티 페르소나 성향과 감각적 몰입 간의 관계를 분석한다. 이를 위해, 100% 가상환경(제페토)과 가상-현실 혼합 환경(게더타운)을 중심으로 아바타에 발현되는 멀티 페르소나 성향과 감각적 몰입에 관한 창의적인 실험 환경을 구축할 예정이다. 마지막으로 3차 년도에는 메타버스 속 인지강화 경험 중 창의성에 대한 측정 아이템을 개발하고, 전 해에 구성된 창의적 실험 디자인을 활용하여 멀티 페르소나 성향과 창의성 간의 관계를 분석한다. 이와 같은 과정의 전반은 과거에 시도된 적 없는 매우 새롭고 창의적인 일이며, 미래에 이어질 후속연구에 유의미한 가치를 제공할 수 있을 것이다.\n\n\n\n1차 년도 실적(2022.05.01.~2023.04.30.)\n\n멀티페르소나 지수 측정 도구 개발 및 설문 실시\n본 연구는 1년차 연구 목표를 위해 메타버스 사용자의 멀티 페르소나 지수 측정 척도 개발을 수행하였으며, 특히 ’아바타 커스터마이징’을 중심으로 설문 도구를 제작하였다. 아바타 커스터마이징에 관한 기존의 연구들은 게임·디자인 영역에 한정되어, 디자인 요소 혹은 디지털 기술 향상에 관한 논의만이 주를 이뤄왔다. 본 연구는 그러한 한계점을 극복하고자 아바타 커스터마이징 항목을 미디어와 사회적 차원으로 가져와 학제 간 융합을 시도하였고, 메타버스 내 멀티 페르소나 지수 측정을 위한 척도 개발에 응용하였다. 구체적으로, 가상의 커스터마이징이 가능한 설문 문항을 통해 현실과 가상세계 아바타의 모습이 얼마나 다른지를 측정하였다. 설문에는 한양대 ERICA 언론정보대학 재학생 112명이 참여하였으며, 그에 따른 주요 연구 성과로서 사회자본이 메타버스 내 멀티 페르소나 지수와 연관이 있음을 도출하였다.\n500명 규모의 2차 설문 실시\n메타버스 사용자의 멀티 페르소나 특성을 측정한 첫 설문 이후, 분석 결과를 토대로 문항을 수정하여 2차 설문에 착수하였다. 본 연구에서는 약 500명 규모의 참여자를 대상으로, 1회의 파일럿(200명) 테스트와 1회의 본 조사(300명)를 계획하였다. 이를 위해, ㈜마크로밀엠브레인을 통해 2023년 2월 2일부터 파일럿 테스트를 실시하였으며, 그 결과 212명의 데이터를 확보하였다. 추가적으로, 파일럿 테스트에서 확보된 데이터의 심층적 검토를 거쳐 설문 도구를 최종 수정하여 300명 이상 규모의 본 조사를 실시할 예정이다.\n논문 투고\n본 연구에서는 1년차에 개발된 멀티 페르소나 척도를 메타버스에 관한 다양한 시각에 접목해, 사용자의 내적‧외적 측면을 깊이 있게 탐색할 수 있는 구체적 연구를 설계하였다. 특히 앞서 언급한 아바타 커스터마이징 행위를 통해 사용자의 자아 표현 양상을 다뤘을뿐만 아니라, 그것을 사회적 상호작용의 영역과 연계하였다. 이러한 연구 결과는 KCI급 저널에 투고될 예정이다.\nC&I studies 주관 특별 세미나 ‘메타버스 새로운 미래, 새로운 인간’ 개최\n본 연구팀은 2022년 10월 26일 한양대 ERICA 언론정보관 4층 MCN Open Studio에서 메타버스에 관한 특별 세미나를 실시하였다. 이는 메타버스와 아바타, 멀티페르소나에 대한 총 3개의 섹션(‘메타버스 트렌드와 전망, 수용자 특성의 주요 쟁점’, ‘가상세계 멀티 페르소나 성향과 사용자의 인지 강화’, ‘관계형성의 장으로서 메타버스에 관한 실증적 접근’)으로 나뉘어 진행되었다. 세미나를 통해, 관련 내용을 타 학자들과 논의함으로써 당해 연구 목표인 멀티 페르소나 지수 측정의 주요 요인들을 도출하였다. 또한 2차년도에 수행될 실험 연구를 위해, 보다 정교한 연구 방법(VR기기 활용 및 자체적 실험 환경 구축) 도입을 계획 및 논의 할 수 있는 교류의 장을 마련하였다.\n한국미디어경영학회, 프리드리히나우만재단 주관 특별 세미나 ‘메타버스 산업과 이용자, 그리고 디지털 리터러시’ 및 발표\n본 연구팀은 2022년 11월 8일 홍대 RYSE 호텔 SPACE에서 개최된 세미나에 참석하여, ’메타버스 사용자의 사회적 자본, 아바타 커스터마이징과 프레즌스’를 주제로 한 발표를 수행하였다(발표자: 이창준, 한양대학교 교수). 이를 통해 1년차 연구 목표인 멀티 페르소나 측정을 위한 아바타 커스터마이징 방법을 학계에 제시하였으며, 이러한 연구가 단순한 학술적 차원 이상의 실증적·실무적 차원까지 발전할 수 있음을 논의하였다. 또한 세미나의 주제인 진화하는 메타버스 환경에서 필요한 이용자 활동과 경험, 그에 따른 사회현상에 관해 타 학자들과의 활발한 교류를 진행하였다."
  },
  {
    "objectID": "teaching/test.html",
    "href": "teaching/test.html",
    "title": "Old Faithful",
    "section": "",
    "text": "Number of bins:"
  },
  {
    "objectID": "teaching/ml101/icpbl/team_assign.html",
    "href": "teaching/ml101/icpbl/team_assign.html",
    "title": "IC-PBL Team assignment",
    "section": "",
    "text": "Team assignemtn for Thursday class\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nset.seed(101101)\n# input students\nstudents<-c(\n\"이유정\",\n\"박승연\",\n\"이윤진\",\n\"정재윤\",\n\"김상경\",\n\"정혜림\",\n\"박지원\",\n\"정지윤\",\n\"김재엽\",\n\"김찬우\",\n\"박종현\",\n\"윤지성\")\n\nstudents\n\n [1] \"이유정\" \"박승연\" \"이윤진\" \"정재윤\" \"김상경\" \"정혜림\" \"박지원\" \"정지윤\"\n [9] \"김재엽\" \"김찬우\" \"박종현\" \"윤지성\"\n\n# No of people per a team\nteam_size <- 4\n\n# No of team we need\nno_team <- length(students) %/% team_size\nno_team\n\n[1] 3\n\n# No of remainders\nremainder <- length(students) %% team_size\nremainder\n\n[1] 0\n\n# Create a tibble (student_no, team)\n\nif (remainder == 0){\n  \n  tibble(\n    seat = 1:length(students),\n    team = rep(1:no_team, team_size)\n  ) -> seat_table\n  \n} else {\n  \n  tibble(\n    seat = 1:length(students),\n    team = c(rep(1:no_team, team_size),\n             1:remainder)\n  ) -> seat_table\n  \n}\n\nseat_table\n\n# A tibble: 12 × 2\n    seat  team\n   <int> <int>\n 1     1     1\n 2     2     2\n 3     3     3\n 4     4     1\n 5     5     2\n 6     6     3\n 7     7     1\n 8     8     2\n 9     9     3\n10    10     1\n11    11     2\n12    12     3\n\n# Give random number for the students\n\ntibble(\n  seat     = sample(1:length(students), \n                    length(students), \n                    replace = F),\n  students = students\n) -> given_seat\ngiven_seat\n\n# A tibble: 12 × 2\n    seat students\n   <int> <chr>   \n 1     4 이유정  \n 2     1 박승연  \n 3     5 이윤진  \n 4     6 정재윤  \n 5     8 김상경  \n 6     3 정혜림  \n 7     7 박지원  \n 8    11 정지윤  \n 9    12 김재엽  \n10     9 김찬우  \n11     2 박종현  \n12    10 윤지성  \n\n# Join the team - seat - students\n\nseat_table %>% \n  left_join(given_seat) %>% \n  arrange(team, students) %>% \n  select(-seat) -> team_table\n\nJoining with `by = join_by(seat)`\n\nknitr::kable(team_table)\n\n\n\n\nteam\nstudents\n\n\n\n\n1\n박승연\n\n\n1\n박지원\n\n\n1\n윤지성\n\n\n1\n이유정\n\n\n2\n김상경\n\n\n2\n박종현\n\n\n2\n이윤진\n\n\n2\n정지윤\n\n\n3\n김재엽\n\n\n3\n김찬우\n\n\n3\n정재윤\n\n\n3\n정혜림\n\n\n\n\n\n\n\nTeam assignemtn for Tuesday class\n\nlibrary(tidyverse)\nset.seed(101101)\n\n# input students\n\nstudents<-c(\n\"김정환\",\n\"김숭기\",\n\"김원\",\n\"이정헌\",\n\"노솔\",\n\"최지희\",\n\"조민석\",\n\"김민지\",\n\"김가영\",\n\"박은서\",\n\"임예빈\",\n\"문하윤\")\n\nstudents\n\n [1] \"김정환\" \"김숭기\" \"김원\"   \"이정헌\" \"노솔\"   \"최지희\" \"조민석\" \"김민지\"\n [9] \"김가영\" \"박은서\" \"임예빈\" \"문하윤\"\n\n# No of people per a team\nteam_size <- 4\n\n# No of team we need\nno_team <- length(students) %/% team_size\nno_team\n\n[1] 3\n\n# No of remainders\nremainder <- length(students) %% team_size\nremainder\n\n[1] 0\n\n# Create a tibble (student_no, team)\n\nif (remainder == 0){\n  \n  tibble(\n    seat = 1:length(students),\n    team = rep(1:no_team, team_size)\n  ) -> seat_table\n  \n} else {\n  \n  tibble(\n    seat = 1:length(students),\n    team = c(rep(1:no_team, team_size),\n             1:remainder)\n  ) -> seat_table\n  \n}\n\nseat_table\n\n# A tibble: 12 × 2\n    seat  team\n   <int> <int>\n 1     1     1\n 2     2     2\n 3     3     3\n 4     4     1\n 5     5     2\n 6     6     3\n 7     7     1\n 8     8     2\n 9     9     3\n10    10     1\n11    11     2\n12    12     3\n\n# Give random number for the students\n\ntibble(\n  seat     = sample(1:length(students), \n                    length(students), \n                    replace = F),\n  students = students\n) -> given_seat\ngiven_seat\n\n# A tibble: 12 × 2\n    seat students\n   <int> <chr>   \n 1     4 김정환  \n 2     1 김숭기  \n 3     5 김원    \n 4     6 이정헌  \n 5     8 노솔    \n 6     3 최지희  \n 7     7 조민석  \n 8    11 김민지  \n 9    12 김가영  \n10     9 박은서  \n11     2 임예빈  \n12    10 문하윤  \n\n# Join the team - seat - students\n\nseat_table %>% \n  left_join(given_seat) %>% \n  arrange(team, students) %>% \n  select(-seat) -> team_table\n\nJoining with `by = join_by(seat)`\n\nknitr::kable(team_table)\n\n\n\n\nteam\nstudents\n\n\n\n\n1\n김숭기\n\n\n1\n김정환\n\n\n1\n문하윤\n\n\n1\n조민석\n\n\n2\n김민지\n\n\n2\n김원\n\n\n2\n노솔\n\n\n2\n임예빈\n\n\n3\n김가영\n\n\n3\n박은서\n\n\n3\n이정헌\n\n\n3\n최지희"
  },
  {
    "objectID": "research/team.html",
    "href": "research/team.html",
    "title": "Research Team",
    "section": "",
    "text": "Creative & Interaction Research Institute\n창의성과인터랙션 연구소 \n\n\n\n\n\n\n\n\n\n연구책임자\n전임연구원\n선임연구원\n연구원\n\n\n\n\n\n\n\n\n\n\n이창준 교수\n최윤슬 연구교수\n이승경 박사과정\n박영주 석사과정\n\n\n\n\n\n\nActivities\n2022\n\nC&I studies 주관 특별 세미나 ‘메타버스 새로운 미래, 새로운 인간’"
  },
  {
    "objectID": "teaching/stat101/index.html",
    "href": "teaching/stat101/index.html",
    "title": "Statistics for Data Science",
    "section": "",
    "text": "Introduction\nWelcome to this course on STAT101 (a.k.a. statistical and data sciences via R)! In this course, we will learn the fundamentals of statistical inference and data analysis using the R programming language. Our textbook, “ModernDive”, will be our primary resource for learning the material.\nThe course is designed to be accessible to students with little to no prior experience in statistics or R. We will start by introducing you to the basics of R and data visualization using the ggplot2 package. We will then cover the fundamentals of data summarization, probability, distributions, and statistical inference.\nThroughout the course, you will have the opportunity to work with real-world datasets and apply the techniques you learn to answer interesting questions. You will also have the chance to develop your data analysis skills using R and the tidyverse packages (dplyr, ggplot2, tidyr).\nBy the end of the course, you should have a solid understanding of the fundamental concepts and tools of statistical inference and data analysis. You should also be comfortable using R to manipulate data, create visualizations, and perform statistical tests.\nI hope you will find this course engaging and rewarding, and that you will leave with a deeper understanding of the power and importance of statistical and data analysis in our modern world.\n\n\n\nSyllabus\n\nWeek 1: Introduction to data and R (Chapter 1)\nWeek 2: Visualization with ggplot2 (Chapter 2)\nWeek 3: Summarizing data with dplyr (Chapter 3)\nWeek 4: Probability (Chapter 4)\nWeek 5: Distributions (Chapter 5)\nWeek 6: Foundations for statistical inference: sampling distributions and the central limit theorem (Chapter 6)\nWeek 7: Mid-term exam\nWeek 8: Introduction to inference (Chapter 7)\nWeek 9: Confidence intervals (Chapter 8)\nWeek 10: Hypothesis testing (Chapter 9)\nWeek 11: Inference for numerical data (Chapter 10)\nWeek 12: Inference for categorical data (Chapter 11)\nWeek 13: Simple linear regression (Chapter 12)\nWeek 14: Multiple linear regression (Chapter 13)\nWeek 15: Final exam\n\n\n\n\n\nTextbooks for the course\n\nStatistical Inference via Data Science (Modern Dive) (written by Chester Ismay and Albert Y. Kim)\n\n“ModernDive: An Introduction to Statistical and Data Sciences via R” by Chester Ismay and Albert Y. Kim is a comprehensive textbook that provides an accessible and hands-on approach to learning the fundamental concepts of statistical inference and data analysis using the R programming language. The book covers a wide range of topics, including data visualization, data summarization, probability and distributions, statistical inference, hypothesis testing, and linear regression, and emphasizes the importance of data visualization, reproducibility, and statistical thinking. With its use of the tidyverse suite of packages, numerous exercises and examples, and guidance on how to use R and RStudio, “ModernDive” is a valuable resource for students and practitioners who want to develop their data analysis skills."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html",
    "href": "blogs/posts/5_adaboost.html",
    "title": "Understanding the AdaBoost",
    "section": "",
    "text": "The AdaBoost algorithm is a type of ensemble learning algorithm that combines multiple “weak” classifiers to create a “strong” classifier. A weak classifier is one that performs only slightly better than random guessing (i.e., its accuracy is slightly better than 50%). In contrast, a strong classifier is one that performs well on the classification task.\nThe basic idea behind AdaBoost is to iteratively train a sequence of weak classifiers, and then combine their predictions using a weighted majority vote to obtain a strong classifier. In each iteration, the algorithm assigns higher weights to the misclassified samples, so that the subsequent classifiers focus more on the difficult samples."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#algorithm-steps",
    "href": "blogs/posts/5_adaboost.html#algorithm-steps",
    "title": "Understanding the AdaBoost",
    "section": "Algorithm Steps",
    "text": "Algorithm Steps\nHere are the steps of the AdaBoost algorithm:\n\nInitialize the sample weights \\(w_i\\) to 1/n, where n is the number of samples.\nFor t in 1:T, where T is the number of iterations:\n\n\nTrain a weak classifier \\(h_t(x)\\) on the training data using the current weights.\nCalculate the error rate \\(ε_t\\) of \\(h_t(x)\\) on the training data. The error rate is defined as the weighted sum of the misclassified samples:\n\n\\[\nε_t = Σ_i w_i \\times I(y_i \\neq h_t(x_i))\n\\]\n\nwhere \\(y_i\\) is the true class label of sample i, \\(h_t(x_i)\\) is the predicted class label of \\(h_t(x)\\) for sample i, and \\(I()\\) is the indicator function that returns 1 if the argument is true and 0 otherwise.\n\n\nCalculate the weight \\(α_t\\) of \\(h_t(x)\\) as \\(α_t = \\frac{log((1 - ε_t)}{ε_t}\\). The weight \\(α_t\\) measures the “importance” of the weak classifier \\(h_t(x)\\) in the ensemble. The weight is larger for classifiers that perform well (i.e., have a low error rate) and smaller for classifiers that perform poorly (i.e., have a high error rate).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that \\(ε_t\\) must be strictly less than 0.5 to ensure that \\(α_t\\) is positive.\n\n\n\nUpdate the sample weights as \\(w_i = w_i \\times exp(α_t)\\). The weight update gives higher weight to the misclassified samples and lower weight to the correctly classified samples. The weight update is equivalent to:\n\n\nif y_i = h_t(x_i), then w_i = w_i * exp(-α_t)\nif y_i ≠ h_t(x_i), then w_i = w_i * exp(α_t)\n\n\nNormalize the weights so that they sum to 1. The normalization ensures that the weights are valid probability distributions.\n\n\nCombine the weak classifiers using the weighted majority vote rule to obtain the final prediction. The final prediction is given by:\n\n\\[\nH(x) = sign(Σ_t (α_t \\times h_t(x))),\n\\]\nwhere sign() is the sign function that returns -1 for negative values and 1 for positive values."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#summary",
    "href": "blogs/posts/5_adaboost.html#summary",
    "title": "Understanding the AdaBoost",
    "section": "Summary",
    "text": "Summary\nThe AdaBoost algorithm is a powerful ensemble learning algorithm that can improve the performance of weak classifiers by combining their predictions. It works by iteratively training a sequence of weak classifiers, and then combining their predictions using a weighted majority vote to obtain a strong classifier. The algorithm assigns higher weights to the misclassified samples in each iteration, so that the subsequent classifiers focus more on the difficult samples. The weight updates and normalization ensure that the subsequent classifiers focus more on the difficult samples, and the final prediction is based on the weighted majority vote rule, which gives more weight to the predictions of the strong classifiers.\nOverall, AdaBoost is a powerful and widely used algorithm in machine learning, particularly for classification problems. It is relatively simple to implement and can be applied to a wide range of classification tasks. Additionally, it has been shown to perform well even with noisy and unbalanced datasets."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#implementing-adaboost-in-r",
    "href": "blogs/posts/5_adaboost.html#implementing-adaboost-in-r",
    "title": "Understanding the AdaBoost",
    "section": "Implementing AdaBoost in R",
    "text": "Implementing AdaBoost in R\nTo implement AdaBoost in R, we can use the adabag package, which provides an implementation of the algorithm. Here’s an example of how to use adabag to train an AdaBoost classifier on the iris dataset:\n\nlibrary(adabag)\n\nWarning: package 'adabag' was built under R version 4.2.3\n\n\nLoading required package: rpart\n\n\nLoading required package: caret\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\nLoading required package: foreach\n\n\nLoading required package: doParallel\n\n\nWarning: package 'doParallel' was built under R version 4.2.3\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Load the iris dataset\ndata(iris)\n\n# Convert the species to a binary variable\niris$Species <- as.factor(ifelse(iris$Species == \"setosa\", 1, 0))\n\n# Split the dataset into training and testing sets\ntrain_idx <- sample(1:nrow(iris), size = 100, replace = FALSE)\ntrain_data <- iris[train_idx, ]\ntest_data <- iris[-train_idx, ]\n\n# Train an AdaBoost classifier with 50 iterations\nada_model <- boosting(Species ~ ., \n                      data = train_data, \n                      boos = TRUE, \n                      mfinal = 50)\n\n\n# Make predictions on the testing data\npred <- predict.boosting(ada_model, newdata = test_data)\n\n# Calculate the accuracy of the classifier\nacc <- sum(pred$class == test_data$Species) / nrow(test_data)\nprint(paste0(\"Accuracy: \", acc))\n\n[1] \"Accuracy: 1\"\n\n\nIn this example, we first load the adabag package and the iris dataset. We then convert the species variable to a binary variable (-1 for “setosa” and 1 for “versicolor” and “virginica”). We split the dataset into a training set (100 samples) and a testing set (50 samples).\nNext, we train an AdaBoost classifier with 50 iterations using the boosting() function from adabag. We specify the formula (Species ~ .) and the training data (train_data), and set the boos parameter to TRUE to enable AdaBoost.\nWe then make predictions on the testing data using the predict.boosting() function, and calculate the accuracy of the classifier by comparing the predicted class labels to the true class labels.\nYou can modify this example by changing the number of iterations (mfinal) or the dataset to fit your specific needs. Additionally, you can try using other weak classifiers, such as decision trees or logistic regression, and compare their performance to AdaBoost."
  },
  {
    "objectID": "blogs/posts/5_adaboost.html#similarities-difference-with-random-forest",
    "href": "blogs/posts/5_adaboost.html#similarities-difference-with-random-forest",
    "title": "Understanding the AdaBoost",
    "section": "Similarities & Difference with Random Forest",
    "text": "Similarities & Difference with Random Forest\nSimilarities\n\nBoth Random Forest and AdaBoost are ensemble learning algorithms that combine multiple “weak” models to create a “strong” model.\nBoth algorithms use a form of bootstrap sampling to generate multiple training sets, which helps to reduce overfitting and improve the generalization performance of the models.\nBoth algorithms are widely used in machine learning and can be applied to a wide range of classification and regression tasks.\n\nDifferences\n\nRandom Forest combines multiple decision trees, each trained on a different subset of the features and samples, and uses a majority vote to make predictions. In contrast, AdaBoost combines multiple weak models, with each model trained on the same dataset but with different weights assigned to the samples.\nRandom Forest places equal weight on all the samples, while AdaBoost assigns higher weights to the misclassified samples in each iteration, so that the subsequent models focus more on the difficult samples.\nRandom Forest uses a simple majority vote to make predictions, while AdaBoost combines the predictions of the weak models using weighted majority vote, with each model weighted by its importance in the ensemble.\nRandom Forest can handle a wide range of datasets and is less sensitive to outliers and noise, while AdaBoost is more sensitive to noisy and unbalanced datasets and may require more data preprocessing.\nRandom Forest typically performs well with large feature sets and high-dimensional data, while AdaBoost may require careful feature selection or dimensionality reduction to prevent overfitting and improve performance."
  }
]