---
title: NLP
subtitle: Topic model
---

This material is the contents of a seminar held at Sogang University on the subject of `Text mining in academic research`. \[[Link](https://changjunlee.com/blogs/posts/8_text_mining_talk.html)\]

<hr>

### LDA Model

```{r}
# 전처리하기
library(tidyverse)

raw_news_comment <- read_csv("data/news_comment_parasite.csv") %>%
  mutate(id = row_number())

raw_news_comment
```

기본적인 전처리

```{r}
# 기본적인 전처리
library(stringr)
library(textclean)
news_comment <- raw_news_comment %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply)) %>%
  distinct(reply, .keep_all = T) %>% # 중복 댓글 제거
  filter(str_count(reply, boundary("word")) >= 3) # 짧은 문서 제거, 3단어 이상 추출
news_comment
```

```{r}
# 2. 명사 추출하기
library(tidytext)
library(KoNLP)
# 명사 추출
comment <- news_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = extractNoun,
                drop = F) %>%
  filter(str_count(word) > 1) %>%
  group_by(id) %>% # 댓글 내 중복 단어 제거
  distinct(word, .keep_all = T) %>%
  ungroup() %>%
  select(id, word) 
comment
```

```{r}
# 3. 빈도 높은 단어 제거하기
count_word <- comment %>%
  add_count(word) %>%
  filter(n <= 200) %>%
  select(-n)
count_word
```

```{r}
# 4. 불용어 제거하기, 유의어 처리하기
# 불용어, 유의어 확인하기
count_word %>%
  count(word, sort = T) %>%
  print(n = 200)
```

이 부분은 상당히 context dependent 하면서 labour intensive 한 작업들..

```{r}
# 불용어 목록 만들기
stopword <- c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
              "해요", "이것", "니들", "하기", "하지", "한거", "해주",
              "그것", "어디", "여기", "까지", "이거", "하신", "만큼")
```

```{r}
# 불용어, 유의어 처리하기
count_word <- count_word %>%
  filter(!word %in% stopword) %>%
  mutate(word = recode(word,
                       "자랑스럽습니" = "자랑",
                       "자랑스럽" = "자랑",
                       "자한" = "자유한국당",
                       "문재" = "문재인",
                       "한국의" = "한국",
                       "그네" = "박근혜",
                       "추카" = "축하",
                       "정경" = "정경심",
                       "방탄" = "방탄소년단"))
```

```{r}
# ***불용어 목록을 파일로 만들어 활용하기
# tibble 구조로 불용어 목록 만들기
stopword <- tibble(word = c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
                            "해요", "이것", "니들", "하기", "하지", "한거", "해주",
                            "그것", "어디", "여기", "까지", "이거", "하신", "만큼"))
# 불용어 목록 저장하기
# readr::write_csv(stopword, "stopword.csv")

# 불용어 목록 불러오기
# stopword <- read_csv("stopword.csv")
```

아래 둘 중 하나 선택

(1) 불용어 제거 by using `filter()`

```{r}
# 불용어 제거하기 - filter()
count_word <- count_word %>%
  filter(!word %in% stopword$word)
```

(2) 불용어 제거 by using `anti_join()`

```{r}
# 불용어 제거하기 - dplyr::anti_join()
count_word <- count_word %>%
  anti_join(stopword, by = "word")
```

```{r}
# LDA 모델 만들기
# 1. DTM 만들기
# 문서별 단어 빈도 구하기
count_word_doc <- count_word %>%
  count(id, word, sort = T)
count_word_doc
```

### **DTM** : long form to wide

```{r}
# DTM 만들기
# install.packages("tm")
dtm_comment <- count_word_doc %>%
  cast_dtm(document = id, term = word, value = n)
dtm_comment
```

**LDA Model:** `LDA()` No. of topics: k

```{r}
# 2. LDA 모델 만들기
# install.packages("topicmodels")

# 토픽 모델 만들기
library(topicmodels)
lda_model <- LDA(dtm_comment,
                 k = 8,
                 method = "Gibbs",
                 control = list(seed = 1234))
```

```{r}
# 모델 내용 확인
glimpse(lda_model)
```

### 토픽별 주요 단어

```{r}
# 토픽별 단어 확률, beta 추출하기
term_topic <- tidy(lda_model, matrix = "beta")
term_topic
```

```{r}
# 토픽별 단어 수
term_topic %>%
  count(topic)
```

```{r}
# 토픽 1의 beta 합계
term_topic %>%
  filter(topic == 1) %>%
  summarise(sum_beta = sum(beta))
```

```{r}
# 특정 단어의 토픽별 확률 살펴보기
term_topic %>%
  filter(term == "작품상")
```

```{r}
# 특정 토픽에서 beta가 높은 단어 살펴보기
term_topic %>%
  filter(topic == 6) %>%
  arrange(-beta)
```

```{r}
# 모든 토픽의 주요 단어 살펴보기
terms(lda_model, 20) %>%
  data.frame()
```

```{r}
# 토픽별 주요 단어 시각화하기
# 1. 토픽별로 beta가 가장 높은 단어 추출하기
# 토픽별 beta 상위 10개 단어 추출
top_term_topic <- term_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10)
top_term_topic
```

```{r}
# 2. 막대 그래프 만들기
# install.packages("scales") # restart 알림 발생 시, '아니요' 선택
library(scales)
ggplot(top_term_topic,
       aes(x = reorder_within(term, beta, topic),
           y = beta,
           fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(n.breaks = 4,
                     labels = number_format(accuracy = .01)) +
  labs(x = NULL) 
```

### 문서를 토픽별로 분류

```{r}
# 문서별 토픽 확률 gamma 추출하기
doc_topic <- tidy(lda_model, matrix = "gamma")
doc_topic
```

```{r}
# gamma 살펴보기
doc_topic %>%
  count(topic)
```

```{r}
# 문서 1의 gamma 합계
doc_topic %>%
  filter(document == 1) %>%
  summarise(sum_gamma = sum(gamma))
```

```{r}
# 문서별 확률이 가장 높은 토픽으로 분류하기
# 1. 문서별로 확률이 가장 높은 토픽 추출하기
doc_class <- doc_topic %>%
  group_by(document) %>%
  slice_max(gamma, n = 1)
doc_class
```

```{r}
# 2. 원문에 확률이 가장 높은 토픽 번호 부여하기
# integer로 변환
doc_class$document <- as.integer(doc_class$document)
```

```{r}
# 원문에 토픽 번호 부여
news_comment_topic <- raw_news_comment %>%
  left_join(doc_class, by = c("id" = "document"))
```

```{r}
# 결합 확인
news_comment_topic %>%
  select(id, topic)
```

```{r}
# 3. 토픽별 문서 수 살펴보기
news_comment_topic %>%
  count(topic)
```

```{r}
# topic이 NA인 문서 제거
news_comment_topic <- news_comment_topic %>%
  na.omit()
```

```{r}
news_comment_topic %>%
  count(topic)
```

```{r}
# 토픽별 문서 수와 단어 시각화하기
# 1. 토픽별 주요 단어 목록 만들기
top_terms <- term_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 6, with_ties = F) %>%
  summarise(term = paste(term, collapse = ", "))
top_terms
```

```{r}
# 2. 토픽별 문서 빈도 구하기
count_topic <- news_comment_topic %>%
  count(topic)
count_topic
```

```{r}
# 3. 문서 빈도에 주요 단어 결합하기
count_topic_word <- count_topic %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic_name = paste("Topic", topic))
count_topic_word
```

```{r}
# 4. 토픽별 문서 수와 주요 단어로 막대 그래프 만들기
ggplot(count_topic_word,
       aes(x = reorder(topic_name, n),
           y = n,
           fill = topic_name)) +
  geom_col(show.legend = F) +
  coord_flip() +
  geom_text(aes(label = n) , # 문서 빈도 표시
            hjust = -0.2) + # 막대 밖에 표시
  geom_text(aes(label = term), # 주요 단어 표시
            hjust = 1.03, # 막대 안에 표시
            col = "white", # 색깔
            fontface = "bold", # 두껍게
            family = "nanumgothic") + # 폰트
  scale_y_continuous(expand = c(0, 0), # y축-막대 간격 줄이기
                     limits = c(0, 820)) + # y축 범위
  labs(x = NULL)
```

### 토픽 이름 짓기

```{r}
# 토픽별 주요 문서 살펴보고 토픽 이름 짓기
# 1. 원문을 읽기 편하게 전처리하기, gamma가 높은 순으로 정렬하기
comment_topic <- news_comment_topic %>%
  mutate(reply = str_squish(replace_html(reply))) %>%
  arrange(-gamma)

comment_topic %>%
  select(gamma, reply)
```

```{r}
# 2. 주요 단어가 사용된 문서 살펴보기
# 토픽 1 내용 살펴보기
comment_topic %>%
  filter(topic == 1 & str_detect(reply, "작품")) %>%
  head(50) %>%
  pull(reply)
```

```{r}
comment_topic %>%
  filter(topic == 1 & str_detect(reply, "진심")) %>%
  head(50) %>%
  pull(reply)
```

```{r}
comment_topic %>%
  filter(topic == 1 & str_detect(reply, "정치")) %>%
  head(5) %>%
  pull(reply)
```

```{r}
# 3. 토픽 이름 목록 만들기
name_topic <- tibble(topic = 1:8,
                     name = c("1. 작품상 수상 축하, 정치적 댓글 비판",
                              "2. 수상 축하, 시상식 감상",
                              "3. 조국 가족, 정치적 해석",
                              "4. 새 역사 쓴 세계적인 영화",
                              "5. 자랑스럽고 감사한 마음",
                              "6. 놀라운 4관왕 수상",
                              "7. 문화계 블랙리스트, 보수 정당 비판",
                              "8. 한국의 세계적 위상"))
```

```{r}
# 토픽 이름과 주요 단어 시각화하기
# 토픽 이름 결합하기
top_term_topic_name <- top_term_topic %>%
  left_join(name_topic, name_topic, by = "topic")

top_term_topic_name
```

```{r}
# 막대 그래프 만들기
ggplot(top_term_topic_name,
       aes(x = reorder_within(term, beta, name),
           y = beta,
           fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ name, scales = "free", ncol = 2) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "영화 기생충 아카데미상 수상 기사 댓글 토픽",
       subtitle = "토픽별 주요 단어 Top 10",
       x = NULL, y = NULL) +
  theme_minimal() +
  theme(text = element_text(family = "nanumgothic"),
        title = element_text(size = 12),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

### 최적의 토픽 수 도출

```{r}
# 하이퍼파라미터 튜닝으로 토픽 수 정하기
# 1. 토픽 수 바꿔가며 LDA 모델 여러 개 만들기
# install.packages("ldatuning") # 사양이 낮은 컴퓨터는 설치가 어려움, 에러 지속적으로 발생되는 것 확인
library(ldatuning)  # windows defender 알림 발생 시, '홈 네트워크, 회사 네트워크 등의 개인 네트워크'만 체크 후 '엑세스 허용' 선택
models <- FindTopicsNumber(dtm = dtm_comment,  # windows defender 알림 발생 시, '홈 네트워크, 회사 네트워크 등의 개인 네트워크'만 체크 후 '엑세스 허용' 선택
                           topics = 2:20,
                           return_models = T,
                           control = list(seed = 1234))

models %>%
  select(topics, Griffiths2004)
```

```{r}
# 2. 최적 토픽 수 정하기
FindTopicsNumber_plot(models)
```

```{r}
# 3. 모델 추출하기
# 토픽 수가 8개인 모델 추출하기
optimal_model <- models %>%
  filter(topics == 8) %>%
  pull(LDA_model) %>% # 모델 추출
  .[[1]] # list 추출
```

```{r}
# optimal_model
tidy(optimal_model, matrix = "beta")
```

```{r}
# lda_model
tidy(lda_model, matrix = "beta")
```
