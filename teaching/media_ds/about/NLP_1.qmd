---
title: NLP
subtitle: Text Pre-processing & Basic Analysis
---

## Download example data

1.  [speech_moon.txt](data/speech_moon.txt)
2.  [speech_park.txt](data/speech_park.txt)

## Install required packages

```{r}
# install.packages('stringr') 
# install.packages('magrittr')
# install.packages('glue')
# install.packages('stringi')
# install.packages('tidytext')

# stringr 설치가 안되면 아래 코드로 설치
# install.packages('https://cran.r-project.org/src/contrib/Archive/stringr/stringr_1.4.1.tar.gz', repos = NULL, type = 'source')

```

## Import required libraries

```{r}
#| output: false

library(tidyverse)
library(stringr)
library(tidytext)
```

## Text Pre-processing

Import speeches

```{r}
# Set to the path within the file where the current R script exists
raw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8')
head(raw_moon)

```

Remove unnecessary characters - `str_replace_all()`

```{r}
# Learn how it works with sample text
txt <- "치킨은!! 맛있다. xyz 정말 맛있다!@#"
txt
# string = 처리할 텍스트, 
# pattern = 규칙, 
# replacement = 바꿀 문자
str_replace_all(string = txt, pattern = '[^가-힣]', replacement = ' ')
```

```{r}
# raw_moon의 불필요한 문자 제거하기
moon <- raw_moon %>%
  str_replace_all('[^가-힣]', ' ')
head(moon)
```

Remove Consecutive Spaces

```{r}
txt <- "치킨은 맛있다 정말 맛있다 "
txt
str_squish(txt)
```

```{r}
# moon에 있는 연속된 공백 제거하기
moon <- moon %>% 
  str_squish()
head(moon)
```

Convert data to tibble structure - `as_tibble()`

```{r}
moon <- dplyr::as_tibble(moon)
moon
```

Pre-processing at once (feat. `%>%`)

```{r}
moon <- raw_moon %>% 
  str_replace_all('[^가-힣]', ' ') %>% # 한글만 남기기
  str_squish() %>% # 연속된 공백 제거
  as_tibble() # tibble로 변환
moon
```

## Tokenization - `unnest_tokens()`

Practice with sample data

```{r}
text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")
text
```

Sentence-based tokenization

```{r}
text %>% # 문장 기준 토큰화
  unnest_tokens(input = value, # 토큰화할 텍스트
                output = word, # 토큰을 담을 변수명
                token = 'sentences') # 문장 기준
```

Words-based Tokenization (Tokenization by spacing)

```{r}

text %>% # 띄어쓰기 기준 토큰화
  unnest_tokens(input = value,
                output = word,
                token = 'words')


```

Character-Based Tokenization

```{r}
text %>% # 문자 기준 토큰화
  unnest_tokens(input = value,
                output = word,
                token = 'characters')


```

Tokenizing speeches (Words-based)

```{r}
# 연설문 토큰화하기 
word_space <- moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = 'words')
word_space
```

## Word frequency visualization

Find word frequency - `count()`

```{r}
# 단어 빈도 구하기 - count()
temp_word_space <- word_space %>% 
  count(word, sort = T)
temp_word_space
```

Remove single-letter words - `filter(str_count())` `str_count()` = Count the number of characters in a string

```{r}
# 한 글자로 된 단어 제거하기 - filter(str_count())
# str_count = 문자열의 글자 수 구하기
str_count('배')
str_count('사과')
```

leave no more than two characters

```{r}
# 두 글자 이상만 남기기
temp_word_space <- temp_word_space %>% 
  filter(str_count(word) > 1)
temp_word_space
```

Let's work at once

```{r}
# 한 번에 작업하기
word_space <- word_space %>% 
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)
word_space
```

Extract frequently used words (Top 20)

```{r}
# 자주 사용된 단어 추출하기
top20 <- word_space %>% 
  head(20)
top20
```

Create a bar graph - `geom_col()`

```{r}
# 막대 그래프 만들기 - geom_col()
# mac 사용자, 그래프에 한글 지원폰트로 변경
# theme_set(theme_gray(base_family = "AppleGothic"))
ggplot2::ggplot(top20, aes(x = reorder(word, n), y = n)) + # 단어 빈도순 정렬
  geom_col() +
  coord_flip() # 회전
```

Graph Refinement

```{r}
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() + 
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(title = '문재인 대통령 출마 연설문 단어 빈도',
       x = NULL, y = NULL) +
  theme(title = element_text(size = 12))

```

Creating a word cloud - `geom_text_wordcloud()`

```{r}
# 워드 클라우드 만들기 - geom_text_wordcloud() 
# install.packages('ggwordcloud')
library(ggwordcloud)

ggplot(word_space, aes(label = word, size = n)) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(3, NA), # 최소, 최대 단어 빈도
               range = c(3, 30)) # 최소, 최대 글자 크기
```

Graph Refinement

```{r}
ggplot(word_space, 
       aes(label = word, 
           size = n,
           col = n)) + # 빈도에 따라 색깔 표현
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(3, NA),
               range = c(3, 30)) +
  scale_color_gradient(low = '#66aaf2', # 최소 빈도 색깔
                       high = '#004EA1') + # 최대 빈도 색깔
  theme_minimal() # 배경 없는 테마 적용

```

Change the graph font 1. Loading Google Fonts - `font_add_google()`

```{r, warning=F}

# 그래프 폰트 바꾸기
# 1. 구글 폰트 불러오기 - font_add_google()
# install.packages('showtext')
library(showtext)

# install.packages('jsonlite')
# install.packages('curl')

font_add_google(name = 'Nanum Gothic', family = 'nanumgothic')
showtext_auto()
```

2.  Assign fonts to graphs

> To prevent errors (or warnings), extrafont is installed and fonts in the operating system are imported into R with font_import. -\> Takes some time..

(시간이 매우 오래 걸리니 수업 후에 실행해줍니다, 한 시간 정도 걸림)

```{r}
# install.packages("extrafont")
library(extrafont)
# font_import(paths=NULL, recursive = TRUE, prompt=TRUE, pattern=NULL)
```

```{r}
#| warning: false

# 2. 그래프에 폰트 지정하기
ggplot(word_space,
       aes(label = word,
           size = n,
           col = n)) +
  geom_text_wordcloud(seed = 1234,
                      family = 'nanumgothic') + # 폰트 적용
  scale_radius(limits = c(3,NA),
               range = c(3,30)) +
  scale_color_gradient(low = '#66aaf2',
                       high = '#004EA1') +
  theme_minimal()
```

Font change (Black Gothick)

```{r}
#| warning: false


# '검은고딕' 폰트 적용
font_add_google(name = 'Black Han Sans', family = 'blackhansans')
showtext_auto()

ggplot(word_space,
       aes(label = word,
           size = n,
           col = n)) +
  geom_text_wordcloud(seed = 1234,
                      family = 'blackhansans') + # 폰트 적용
  scale_radius(limits = c(3,NA),
               range = c(3,30)) +
  scale_color_gradient(low = '#66aaf2',
                       high = '#004EA1') +
  theme_minimal()


```

Font change (gamjaflower family)

```{r}
# 3. ggplot2 패키지로 만든 그래프의 폰트 바꾸기
font_add_google(name = 'Gamja Flower', family = 'gamjaflower')
showtext_auto()

ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) + 
  labs(title = '문재인 대통령 출마 연설문 단어 빈도',
       x = NULL, y = NULL) +
  theme(title = element_text(size = 12), text = element_text(family = 'gamjaflower')) # 폰트 적용


```

Optional: If you don't' want to specify the font using theme() every time, set the default theme font of the `ggplot2` package like below.

```{r}
# ggplot2 기본 테마 폰트 변경하기 --------------------------------------------------------
# 매번 theme()를 이용해 폰트를 지정하는게 번거롭다면 ggplot2 패키지 기본 테마 폰트 설정
theme_set(theme_gray(base_family = 'nanumgothic'))
```

## Morphological analysis (형태소 단위 분석)

`[KoNLP]` Installing the Korean Morphological Analysis Package. The order of installation is important, so be sure to do it in that order.

1.  Install Java and rJava packages

```{r}
# install.packages('multilinguer')
library(multilinguer)
```

2.  After installing Amazon Corretto, close RStudio + restart

```{r}
# install_jdk()
```

3.  Installing KoNLP dependencies

```{r}
# install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = 'binary')
```

4.  Installing KoNLP dependencies

```{r}
# install.packages('remotes')
# remotes::install_github('haven-jeon/KoNLP',
#                         upgrade = 'never',
#                         INSTALL_opts = c('--no-multiarch'))


# 'scala-library-2.11.8.jar' 에러 발생 시, download.file 코드 실행

# download.file(url = "https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar",
#               destfile = paste0(.libPaths()[1], "/KoNLP/Java/scala-library-2.11.8.jar"))
```

```{r, warning=FALSE}
library(KoNLP) # Fail to locate 
# Checking user defined dictionary! <- This is not an error

# useNIADic() 
# 다운로드 항목 출력 시, 'All' 선택하여 다운로드
# When printing download items, select 'All' to download
```

<br>

Tokenize using a morpheme analyzer(형태소 분석기) - see how it works with noun extraction sample text

```{r, warning=FALSE}
text <- tibble(
  value = c("대한민국은 민주공화국이다.",
            "대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다."))
text
```

`extraNoun()`: Output nouns extracted from sentences in a list structure

```{r}
# extraNoun(): 문장에서 추출한 명사를 list 구조로 출력
extractNoun(text$value)


```

Extracting nouns using `unnest_tokens()`, outputting nouns in a tractable tibble structure

```{r}
# unnest_tokens()를 이용해 명사 추출하기, 다루기 쉬운 tibble 구조로 명사 출력
library(tidytext)

text %>% 
  unnest_tokens(input = value, # 분석 대상
                output = word, # 출력 변수명
                token = extractNoun) # 토큰화 함수
```

Let's compare with spacing-based extraction

```{r}
# 띄어쓰기 기준 추출과 비교해보자
text %>% 
  unnest_tokens(input = value,
                output = word,
                token = 'words')
```

Extracting **Nouns** from Speeches

```{r}
# 연설문에서 명사 추출하기
# 문재인 대통령 연설문 불러오기

raw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8')

library(stringr)
library(textclean)

moon <- raw_moon %>% 
  str_replace_all('[^가-힣]', ' ') %>% 
  str_squish() %>% 
  as_tibble()
moon

# 명사 기준 토큰화
word_noun <- moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)
word_noun
```

Find word frequency

```{r, warning=FALSE}
# 단어 빈도 구하기
word_noun <- word_noun %>% 
  count(word, sort = T) %>% # 단어 빈도 구해 내림차순 정렬
  filter(str_count(word) > 1) # 두 글자 이상만 남기기
word_noun
```

Comparison with the previous spacing-based extraction

```{r}
# 띄어쓰기 기준 추출과 비교
moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = 'words') %>% 
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)
```

Do at once (Noun extraction from the speech)

```{r}
# 명사 추출
moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = extractNoun) %>% 
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)
```

Extract Top 20 Nouns and create a bar graph

```{r}
# 상위 20개 단어 추출
top20 <- word_noun %>% 
  head(20)
top20

# 막대 그래프 만들기
library(showtext)
font_add_google(name = 'Nanum Gothic', family = 'nanumgothic')
showtext_auto()

ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(x = NULL) +
  theme(text = element_text(family = 'nanumgothic'))
```

Word Cloud of Nouns extracted from the speec

```{r}
#| warning: false

# 워드 클라우드 만들기
font_add_google(name = 'Black Han Sans', family = 'blackhansans')
showtext_auto()

library(ggwordcloud)
ggplot(word_noun, aes(label = word, size = n, col = n)) +
  geom_text_wordcloud(seed = 1234, family = 'blackhansans') +
  scale_radius(limits = c(3,NA),
               range = c(3,15)) +
  scale_color_gradient(low = '#66aaf2', high = '#004EA1') +
  theme_minimal()
```

## Extract sentences with specific words

1.  Tokenize by sentence

```{r}
# 문장 기준으로 토큰화하기
sentences_moon <- raw_moon %>% 
  str_squish() %>% 
  as_tibble() %>% 
  unnest_tokens(input = value,
                output = sentence,
                token = 'sentences')
sentences_moon
```

Extract sentences with specific words - `str_detect()`

```{r}
# 특정 단어가 사용된 문장 추출하기 - str_detect()
# 예시
str_detect('치킨은 맛있다', '치킨')
str_detect('치킨은 맛있다', '피자')
```

Example (1): Extract sentences with "국민"

```{r}
# 특정 단어가 사용된 문장 추출하기, '국민'
sentences_moon %>% 
  filter(str_detect(sentence, '국민'))
```

Example (2): Extract sentences with "일자리"

```{r}
# 특정 단어가 사용된 문장 추출하기, '일자리'
sentences_moon %>% 
  filter(str_detect(sentence, '일자리'))
```
