---
title: NLP
subtitle: 텍스트 전처리 및 분석
---

### Download example data

1.  [speech_moon.txt](data/speech_moon.txt)
2.  [speech_park.txt](data/speech_park.txt)

<br>

### 

Text Pre-processing

<br>

#### Install required packages

```{r}
# install.packages('strigr') # 최신 R 버전에서 다운로드 불가능, 아래 코드들을 실행하여 수동설치로 대체
# install.packages('magrittr') # 만약 반복해서 'Updating Loaded Packages' 알림 발생 시, '아니요' 선택
# install.packages('glue')
# install.packages('stringi')
# install.packages('https://cran.r-project.org/src/contrib/Archive/stringr/stringr_1.4.1.tar.gz', repos = NULL, type = 'source')
# install.packages('tidytext')
```

#### Import required libraries

```{r}
library(tidyverse)
library(stringr)
library(tidytext)
```

#### Text Pre-processing (텍스트 전처리)

Import speeches

```{r}
# Set to the path within the file where the current R script exists
raw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8')
head(raw_moon)

```

Remove unnecessary characters - str_replace_all

```{r}
# Learn how it works with sample text
txt <- "치킨은!! 맛있다. xyz 정말 맛있다!@#"
txt
# string = 처리할 텍스트, 
# pattern = 규칙, 
# replacement = 바꿀 문자
str_replace_all(string = txt, pattern = '[^가-힣]', replacement = ' ')
```

```{r}
# raw_moon의 불필요한 문자 제거하기
moon <- raw_moon %>%
  str_replace_all('[^가-힣]', ' ')
head(moon)
```

Remove Consecutive Spaces

```{r}
txt <- "치킨은 맛있다 정말 맛있다 "
txt
str_squish(txt)
```

```{r}
# moon에 있는 연속된 공백 제거하기
moon <- moon %>% 
  str_squish()
head(moon)
```

Convert data to tibble structure - as_tibble()

```{r}
moon <- dplyr::as_tibble(moon)
moon
```

Pre-processing at once (feat. %\>%)

```{r}
moon <- raw_moon %>% 
  str_replace_all('[^가-힣]', ' ') %>% # 한글만 남기기
  str_squish() %>% # 연속된 공백 제거
  as_tibble() # tibble로 변환
```

#### Tokenization - unnest_tokens()

Practice with sample data

```{r}

# 샘플 텍스트로 작동 원리 알아보기
text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")
text


text %>% # 문장 기준 토큰화
  unnest_tokens(input = value, # 토큰화할 텍스트
                output = word, # 토큰을 담을 변수명
                token = 'sentences') # 문장 기준

text %>% # 띄어쓰기 기준 토큰화
  unnest_tokens(input = value,
                output = word,
                token = 'words')

text %>% # 문자 기준 토큰화
  unnest_tokens(input = value,
                output = word,
                token = 'characters')

# 연설문 토큰화하기 
word_space <- moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = 'words')
word_space
```

#### Word frequency visualization

```{r}
# 단어 빈도 구하기 - count()
temp_word_space <- word_space %>% 
  count(word, sort = T)
temp_word_space

# 한 글자로 된 단어 제거하기 - filter(str_count())
# str_count = 문자열의 글자 수 구하기
str_count('배')
str_count('사과')

# 두 글자 이상만 남기기
temp_word_space <- temp_word_space %>% 
  filter(str_count(word) > 1)
temp_word_space

# 한 번에 작업하기
word_space <- word_space %>% 
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)
word_space

# 자주 사용된 단어 추출하기
top20 <- word_space %>% 
  head(20)
top20
```

```{r}
# 막대 그래프 만들기 - geom_col()
# mac 사용자, 그래프에 한글 지원폰트로 변경
# theme_set(theme_gray(base_family = "AppleGothic"))
ggplot2::ggplot(top20, aes(x = reorder(word, n), y = n)) + # 단어 빈도순 정렬
  geom_col() +
  coord_flip() # 회전
```

```{r}
# 그래프 다듬기
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() + 
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(title = '문재인 대통령 출마 연설문 단어 빈도',
       x = NULL, y = NULL) +
  theme(title = element_text(size = 12))

# 워드 클라우드 만들기 - geom_text_wordcloud() 
# install.packages('ggwordcloud')
library(ggwordcloud)

ggplot(word_space, aes(label = word, size = n)) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(3, NA), # 최소, 최대 단어 빈도
               range = c(3, 30)) # 최소, 최대 글자 크기

# 그래프 다듬기
ggplot(word_space, 
       aes(label = word, 
           size = n,
           col = n)) + # 빈도에 따라 색깔 표현
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(3, NA),
               range = c(3, 30)) +
  scale_color_gradient(low = '#66aaf2', # 최소 빈도 색깔
                       high = '#004EA1') + # 최대 빈도 색깔
  theme_minimal() # 배경 없는 테마 적용
```

### Morphological analysis

형태소 단위 분석

```{r, warning=F}

# 그래프 폰트 바꾸기
# 1. 구글 폰트 불러오기 - font_add_google()
# install.packages('showtext')
library(showtext)

# install.packages('jsonlite')
# install.packages('curl')

font_add_google(name = 'Nanum Gothic', family = 'nanumgothic')
showtext_auto()

# 2. 그래프에 폰트 지정하기
ggplot(word_space,
       aes(label = word,
           size = n,
           col = n)) +
  geom_text_wordcloud(seed = 1234,
                      family = 'nanumgothic') + # 폰트 적용
  scale_radius(limits = c(3,NA),
               range = c(3,30)) +
  scale_color_gradient(low = '#66aaf2',
                       high = '#004EA1') +
  theme_minimal()

# '검은고딕' 폰트 적용
font_add_google(name = 'Black Han Sans', family = 'blackhansans')
showtext_auto()

ggplot(word_space,
       aes(label = word,
           size = n,
           col = n)) +
  geom_text_wordcloud(seed = 1234,
                      family = 'blackhansans') + # 폰트 적용
  scale_radius(limits = c(3,NA),
               range = c(3,30)) +
  scale_color_gradient(low = '#66aaf2',
                       high = '#004EA1') +
  theme_minimal()

# 3. ggplot2 패키지로 만든 그래프의 폰트 바꾸기
font_add_google(name = 'Gamja Flower', family = 'gamjaflower')
showtext_auto()

ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) + 
  labs(title = '문재인 대통령 출마 연설문 단어 빈도',
       x = NULL, y = NULL) +
  theme(title = element_text(size = 12), text = element_text(family = 'gamjaflower')) # 폰트 적용

# ggplot2 기본 테마 폰트 변경하기 --------------------------------------------------------
# 매번 theme()를 이용해 폰트를 지정하는게 번거롭다면 ggplot2 패키지 기본 테마 폰트 설정
theme_set(theme_gray(base_family = 'nanumgothic'))

```

KoNLP 한글 형태소 분석 패키지 설치하기

<https://github.com/youngwoos/Doit_R/blob/master/FAQ/install_KoNLP.md>

```{r}
# 1. 자바와 rJava 패키지 설치하기
# install.packages('multilinguer')
library(multilinguer)
 
```

반드시 Amazon Corretto 설치 후, RStudio 종료 + 재시작하기

```{r}
# install_jdk()
```

KoNLP 의존성 패키지 설치하기

```{r}
# install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = 'binary')
```

KoNLP 패키지 설치하기

```{r}
# install.packages('remotes')
# remotes::install_github('haven-jeon/KoNLP',
#                         upgrade = 'never',
#                         INSTALL_opts = c('--no-multiarch'))


# 'scala-library-2.11.8.jar' 에러 발생 시, download.file 코드 실행

# download.file(url = "https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar",
#               destfile = paste0(.libPaths()[1], "/KoNLP/Java/scala-library-2.11.8.jar"))
```

```{r, warning=FALSE}
library(KoNLP) # Fail to locate 
# Checking user defined dictionary! <- 해당 문구는 에러가 아니고 아래 useNIADic() 실행하여 사전을 설정

# useNIADic() 
# 다운로드 항목 출력 시, 'All' 선택하여 다운로드
```

<br>

형태소 분석기를 이용해 토큰화하기 - 명사 추출 샘플 텍스트로 작동 원리 알아보기

```{r, warning=FALSE}
text <- tibble(
  value = c("대한민국은 민주공화국이다.",
            "대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다."))
text

# extraNoun(): 문장에서 추출한 명사를 list 구조로 출력
extractNoun(text$value)

# unnest_tokens()를 이용해 명사 추출하기, 다루기 쉬운 tibble 구조로 명사 출력
library(tidytext)

text %>% 
  unnest_tokens(input = value, # 분석 대상
                output = word, # 출력 변수명
                token = extractNoun) # 토큰화 함수

# 띄어쓰기 기준 추출
text %>% 
  unnest_tokens(input = value,
                output = word,
                token = 'words')

# 명사 추출
text %>% 
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

# 연설문에서 명사 추출하기
# 문재인 대통령 연설문 불러오기

raw_moon <- readLines('data/speech_moon.txt', encoding = 'UTF-8')

# 기본적인 전처리
library(stringr)
# install.packages('textclean')
library(textclean)

moon <- raw_moon %>% 
  str_replace_all('[^가-힣]', ' ') %>% 
  str_squish() %>% 
  as_tibble()
moon

# 명사 기준 토큰화
word_noun <- moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)
word_noun
```

```{r, warning=FALSE}
# 단어 빈도 구하기
word_noun <- word_noun %>% 
  count(word, sort = T) %>% # 단어 빈도 구해 내림차순 정렬
  filter(str_count(word) > 1) # 두 글자 이상만 남기기
word_noun

# 띄어쓰기 기준 추출
moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = 'words') %>% 
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)

# 명사 추출
moon %>% 
  unnest_tokens(input = value,
                output = word,
                token = extractNoun) %>% 
  count(word, sort = T) %>% 
  filter(str_count(word) > 1)

# 상위 20개 단어 추출
top20 <- word_noun %>% 
  head(20)
top20

# 막대 그래프 만들기
library(showtext)
font_add_google(name = 'Nanum Gothic', family = 'nanumgothic')
showtext_auto()

ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(x = NULL) +
  theme(text = element_text(family = 'nanumgothic'))

# 워드 클라우드 만들기
# 폰트 설정
font_add_google(name = 'Black Han Sans', family = 'blackhansans')
showtext_auto()

library(ggwordcloud)
ggplot(word_noun, aes(label = word, size = n, col = n)) +
  geom_text_wordcloud(seed = 1234, family = 'blackhansans') +
  scale_radius(limits = c(3,NA),
               range = c(3,15)) +
  scale_color_gradient(low = '#66aaf2', high = '#004EA1') +
  theme_minimal()
```

```{r}
# 문장 기준으로 토큰화하기
sentences_moon <- raw_moon %>% 
  str_squish() %>% 
  as_tibble() %>% 
  unnest_tokens(input = value,
                output = sentence,
                token = 'sentences')
sentences_moon

# 특정 단어가 사용된 문장 추출하기 - str_detect()
# 예시
str_detect('치킨은 맛있다', '치킨')
str_detect('치킨은 맛있다', '피자')

# 특정 단어가 사용된 문장 추출하기, '국민'
sentences_moon %>% 
  filter(str_detect(sentence, '국민'))

# 특정 단어가 사용된 문장 추출하기, '일자리'
sentences_moon %>% 
  filter(str_detect(sentence, '일자리'))
```
