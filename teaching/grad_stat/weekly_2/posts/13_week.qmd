---
title: "회귀 분석과 통계적 추론"
subtitle: "Linear & Non-linear Regression" 
Week: 13
format: html
slide-format: revealjs
editor: 
  markdown: 
    wrap: 72
---

[Weekly content](https://changjunlee.com/teaching/grad_stat/weekly_2/)

<br>

## Linear Regression

<br>

### Motivation

**Linear regression** is a foundational technique in statistical
analysis and machine learning that helps us understand and **quantify
relationships between variables**. As social scientists, we often aim to
analyze the effect of certain factors on an outcome of interest. Linear
regression provides us with a way to model these relationships, quantify
their effects, and make predictions based on our findings. By mastering
linear regression, social scientists can gain valuable insights into
various phenomena, test hypotheses, and make data-driven decisions.

<br>

### Usage and Importance

Linear regression is widely used in social science research for several
reasons:

-   **Simplicity**: Linear regression is relatively easy to understand
    and implement, making it an accessible method for researchers across
    disciplines. Despite its simplicity, it can often provide valuable
    insights and predictions.

-   **Interpretability**: The coefficients obtained from linear
    regression have a clear interpretation, allowing researchers to
    understand the effect of each independent variable on the dependent
    variable.

-   **Basis for Advanced Techniques**: Linear regression serves as a
    foundation for more advanced statistical and machine learning
    techniques. Gaining a deep understanding of linear regression helps
    social scientists better understand and apply these more advanced
    methods.

### Real-world Applications

Linear regression has a wide range of applications in social science
research. Some examples include:

-   **Economics**: Linear regression can be used to study the impact of
    various factors on economic indicators, such as GDP growth,
    unemployment rate, and inflation.

-   **Political Science**: Researchers can use linear regression to
    analyze the effects of political factors on election outcomes,
    public opinion, or policy decisions.

-   **Sociology**: Linear regression can help us understand the
    relationship between social variables, such as education level,
    income, and various social outcomes like crime rates, health status,
    and life satisfaction.

-   **Psychology**: Researchers can use linear regression to study the
    effects of different psychological factors on human behavior, mental
    health, and well-being.

-   **Education**: Linear regression can be used to analyze the impact
    of various factors on educational outcomes, such as standardized
    test scores, graduation rates, and college enrollment.

Overall, linear regression is a versatile and powerful tool for social
scientists, enabling them to gain insights into the relationships
between variables and make evidence-based predictions.

------------------------------------------------------------------------

![](images/clipboard-3806263144.png)

<br>

### Intuitive Explanation

![](images/clipboard-88603835.png)

<br>

![](images/clipboard-163481183.png)

<br>

![](images/clipboard-2601841393.png)

<br>

![](images/clipboard-1170280372.png)

<br>

![](images/clipboard-504319295.png)

<br>

![](images/clipboard-2579867685.png)

<br>

![](images/clipboard-4212948736.png)

<br>

![](images/clipboard-3654768598.png)

<br>

![](images/clipboard-2866183306.png)

<br>

![](images/clipboard-786289790.png)

<br>

![](images/clipboard-1146229628.png)

<br>

![](images/clipboard-3740575377.png)

<br>

![](images/clipboard-1783687065.png)

<br>

![](images/clipboard-155857060.png)

<Br>

![](images/clipboard-2101230765.png)

<br>

![](images/clipboard-2560427211.png)

<br>

![](images/clipboard-2484214359.png)

<br>

![](images/clipboard-2229676286.png)

<bR>

### Theory

**Simple Linear Regression**

> Simple linear regression is a statistical method that helps us
> understand the relationship between one dependent variable (y) and one
> independent variable (x). It models the relationship as a linear
> function.

![](http://cdn-0.r-statistics.co/screenshots/linear-regression-small.png)

**Equation**:

$$
y = β_1 + β_2x + ε
$$

-   $y$ : dependent variable (outcome)

-   $x$ : independent variable (predictor)

-   $β_1$ : intercept (value of y when x = 0)

-   $β_2$ : slope (change in y for a one-unit increase in x)

-   $ε$ : error term (difference between the predicted and observed
    values of y)

<br>

**Multiple Linear Regression**

> Multiple linear regression is an extension of simple linear regression
> that allows us to model the relationship between one dependent
> variable (y) and multiple independent variables (x₁, x₂, ..., xₙ). It
> is useful when we want to analyze the impact of several predictors on
> an outcome variable.

Equation

$$
y = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε 
$$

-   $y$ : dependent variable (outcome)

-   $x₁, x₂, …, xₙ$ : independent variables (predictors)

-   $β₀$ : intercept (value of y when all x's are 0)

-   $β₁, β₂, …, βₙ$ : coefficients (change in y for a one-unit increase
    in the corresponding x)

-   $ε$ : error term (difference between the predicted and observed
    values of y)

<br>

**Assumptions of Linear Regression**

-   `Linearity`: The relationship between the dependent variable and the
    independent variables is linear.

-   `Independence`: The observations in the dataset are independent of
    each other.

-   `Homoscedasticity`: The variance of the error term is constant for
    all values of the independent variables.

-   `Normality`: The error term follows a normal distribution.

-   `No multicollinearity`: The independent variables are not highly
    correlated with each other.

<br>

**Coefficient Estimation: Least Squares (LS) Method**

> Minimize the sum of the squared differences between the observed and
> predicted values of the dependent variable.

-   Formula:

    $$
    β = (X'X)^{-1}X'y
    $$where X is the matrix of independent variables, y is the dependent
    variable, and β is the vector of coefficients.

**Model Evaluation Metrics**

-   R-squared (Coefficient of Determination): Proportion of the variance
    in the dependent variable that can be explained by the independent
    variables. Ranges from 0 to 1.

    $$
    R^2 = 1- \frac{SSE}{SST} 
    $$

    $$
    SSE = \sum(y_i - \hat{y_i})^2
    $$

    $$
    SST=\sum(y_i - \bar{y_i})^2
    $$

    > where *SSE* is the sum of squared errors and *SST* is the sum of
    > squared total

-   Adjusted R-squared: R-squared adjusted for the number of predictors
    in the model. Useful for comparing models with different numbers of
    predictors.

    $$
    Adj.R^2=1-\frac{(1-R^2)(N-1)}{N-p-1}
    $$

    > where $R^2$ is sample R-squared, $N$ is Total Sample Size, and $p$
    > is the number of independent variables

-   Root Mean Squared Error (RMSE): The square root of the average
    squared differences between the observed and predicted values of the
    dependent variable. A measure of the model's prediction accuracy.

    $$
    RMSE = \sqrt{\frac{\sum_{i=1}^{N}{(y_i-\hat{y_i})^2}}{N}}
    $$

    > where N is the number of data points (observations)

<br>

------------------------------------------------------------------------

### Hands-on Practice

For this hands-on practice, we will use the `mtcars` dataset, which is
built into R. The dataset contains information about various car models,
including miles per gallon (mpg), number of cylinders (cyl), horsepower
(hp), and weight (wt). The goal is to predict miles per gallon based on
the number of cylinders, horsepower, and weight using linear regression.

```{r}
# Load necessary libraries
library(tidyverse)

# Load the 'mtcars' dataset
data(mtcars)

# View the first few rows of the dataset
head(mtcars)


```

**Exploratory Data Analysis**

```{r}
# Summary statistics
summary(mtcars)

# Visualize relationships between variables using scatterplots
pairs(mtcars[, c("mpg", "cyl", "hp", "wt")])

```

**Simple Linear Regression in R (Predicting mpg based on weight)**

```{r}
# Fit a simple linear regression model
simple_model <- lm(mpg ~ wt, data = mtcars)

# Model summary and interpretation
summary(simple_model)

# Model diagnostics (residuals vs. fitted values)
plot(simple_model, which = 1)

```

**Hypothesis Testing and Statistical Significance in Linear Regression**

T-statistics and p-values are essential concepts in statistical
hypothesis testing and linear regression analysis.

-   T-statistics

    > A t-statistic is a measure of how many standard deviations a
    > regression coefficient is from zero. It is used to test the null
    > hypothesis that there is no relationship between the independent
    > and dependent variables (i.e., the coefficient is zero). A higher
    > t-statistic value indicates a stronger relationship between the
    > variables.

    The t-statistic for a regression coefficient can be calculated as:

    $$
    t = \frac{\beta - H₀}{se(\beta)}
    $$

    > where $t$ is the t-statistic, $\beta$ is the estimated regression
    > coefficient, $H₀$ is the null hypothesis value (usually 0), and
    > $se(\beta)$ is the standard error of the estimated coefficient.

    See appendix (further study part) if you want to dig in more about
    the way of estimating $se(\beta)$.

<br>

::: callout-note
-   **t-통계량**은 **클 수록** 좋음: 분자는 클 수록, 분모는 작을 수록

-   분자가 크려면: 회귀 계수 (Beta의 추정값)이 커야함

-   분모가 작으려면: 회귀 계수의 표준 오차가 작아야함 (표준 오차
    추정법은 부록 참고)

-   회귀 계수의 표준 오차가 작으려면: $se(\beta)$ = MSE / (X의 표준편차
    \* 표본수) 이므로 MSE가 작아야 하고 표본수가 커야함.

-   **종합하면, 회귀 계수가 크고, MSE가 작고, 표본 수가 커질 수록
    t-통계량이 커진다**
:::

<br>

-   P-values

    > A p-value is the probability of obtaining a test statistic as
    > extreme as the observed value under the null hypothesis. It helps
    > us determine the statistical significance of a regression
    > coefficient. In general, a smaller p-value (typically ≤ 0.05)
    > indicates strong evidence against the null hypothesis, suggesting
    > that the coefficient is significantly different from zero.

    To calculate the p-value for a t-statistic, we use the cumulative
    distribution function (CDF) of the t-distribution with n - k degrees
    of freedom, where n is the number of observations and k is the
    number of estimated coefficients (including the intercept).

    $$
    P(T > |t|) = 1 - CDF(t, df = n - k)
    $$

<br>

**Multiple Linear Regression in R** (Adding number of cylinders and
horsepower as predictors)

```{r}
# Fit a multiple linear regression model
multiple_model <- lm(mpg ~ cyl + hp + wt, data = mtcars)

# Model summary and interpretation
summary(multiple_model)

# Model diagnostics (residuals vs. fitted values)
plot(multiple_model, which = 1)

```

**Model Evaluation and Comparison**

```{r}
# Calculate R-squared and adjusted R-squared for both models
simple_r_squared <- summary(simple_model)$r.squared
simple_adj_r_squared <- summary(simple_model)$adj.r.squared

multiple_r_squared <- summary(multiple_model)$r.squared
multiple_adj_r_squared <- summary(multiple_model)$adj.r.squared

# Compare R-squared and adjusted R-squared values
cat("Simple Model - R-squared:", simple_r_squared, "Adjusted R-squared:", simple_adj_r_squared, "\n")
cat("Multiple Model - R-squared:", multiple_r_squared, "Adjusted R-squared:", multiple_adj_r_squared, "\n")

```

**Model Predictions**

```{r}
# Make predictions using the multiple linear regression model
new_data <- data.frame(
  cyl = c(4, 6, 8),
  hp = c(100, 150, 200),
  wt = c(2.5, 3.0, 3.5)
)

predicted_mpg <- predict(multiple_model, newdata = new_data)

# View predicted mpg values
predicted_mpg

```

<br>

**Addressing Multi-collinearity**

```{r}
# Check for multicollinearity using the Variance Inflation Factor (VIF)
library(car)
vif(multiple_model)

```

**Variance Inflation Factor (VIF)** is a measure used to [detect the
presence and severity of multicollinearity]{.underline} in a multiple
linear regression model. **Multicollinearity** occurs when two or more
independent variables in the model are highly correlated, which can lead
to instability in the estimated regression coefficients and make it
difficult to interpret their individual effects on the dependent
variable.

> If VIF values are significantly greater than 1 (\> 5 or 10), consider
> removing or combining correlated predictors

VIF for the j-th independent variable can be calculated as:

$$
VIF(j) = \frac{1}{1 - R²(j)}
$$

Here, $R²(j)$ is the coefficient of determination (R-squared) of the
regression model [when the j-th independent variable is regressed on all
the other independent variables in the model]{.underline}. In other
words, $R²(j)$ measures the proportion of variance in the j-th
independent variable that can be explained by the other independent
variables.

If the VIF value for a particular independent variable is close to 1, it
means that there is no significant multicollinearity between that
variable and the other independent variables. As the VIF value
increases, it suggests a higher degree of multicollinearity.

The general interpretation of VIF values is as follows:

-   VIF = 1: No multicollinearity

-   VIF between 1 and 5: Moderate multicollinearity

-   VIF greater than 5 or 10: High multicollinearity (threshold values
    may vary depending on the field of study)

::: callout-note
If high multicollinearity is detected, it is often advisable to address
the issue by removing or combining correlated predictors, or by using
regularization techniques such as *Lasso*, *Ridge*, or *Elastic Net
regression*. This can help improve the stability and interpretability of
the regression coefficients.
:::

<br>

### **Optional:** Regularization techniques (Lasso, Ridge, and Elastic Net)

<br>

Lasso, Ridge, and Elastic Net are **regularization techniques** used in
linear regression models to address issues like *multicollinearity*,
*overfitting*, and *feature selection*. They work by adding a penalty
term to the linear regression's objective function, which helps to
shrink the coefficients towards zero and simplify the model. Here's a
brief explanation of each technique along with the relevant equations:

1.  **Lasso Regression** (Least Absolute Shrinkage and Selection
    Operator)

    Lasso regression adds an **L1** penalty term to the linear
    regression's objective function. The L1 penalty term is the sum of
    the absolute values of the coefficients. The objective function for
    Lasso regression is:

    $$
    Objective = RSS + λ Σ|β_j|
    $$

    where:

    -   $RSS$ is the residual sum of squares.

    -   $β_j$ represents the j-th coefficient in the model.

    -   $λ$ (lambda) is the regularization parameter that controls the
        strength of the L1 penalty. Higher values of λ result in more
        shrinkage and simpler models.

    Lasso regression can drive some coefficients to zero, effectively
    performing feature selection by excluding irrelevant variables from
    the model.

<br>

2.  **Ridge Regression**

    Ridge regression adds an **L2** penalty term to the linear
    regression's objective function. The L2 penalty term is the sum of
    the squares of the coefficients. The objective function for Ridge
    regression is:

    $$
    Objective = RSS + λ  Σ(β_j)^2
    $$

    where:

    -   $RSS$ is the residual sum of squares.

    -   $β_j$ represents the j-th coefficient in the model.

    -   $λ$ (lambda) is the regularization parameter that controls the
        strength of the L2 penalty. Higher values of λ result in more
        shrinkage and simpler models.

    > [Ridge regression doesn't drive coefficients to zero but can
    > shrink them close to zero]{.underline}, leading to a more
    > **stable** and **interpretable** model, especially when
    > multicollinearity is present.

<br>

3.  **Elastic Net Regression**

    Elastic Net regression combines both L1 and L2 penalty terms,
    effectively blending Lasso and Ridge regression ([진리의
    반반]{.underline}). The objective function for Elastic Net
    regression is:

    $$
    Objective = RSS + λ [(1 - α)  Σ(β_j)^2 + α  Σ|β_j|]
    $$

    where:

    -   $RSS$ is the residual sum of squares.

    -   $β_j$ represents the j-th coefficient in the model.

    -   $λ$ (lambda) is the regularization parameter that controls the
        overall strength of the penalty.

    -   $α$ (alpha) is the mixing parameter that determines the balance
        between L1 (Lasso) and L2 (Ridge) penalties.

        -   α = 1 results in Lasso regression,

        -   α = 0 results in Ridge regression,

        -   and values between 0 and 1 produce a mix of both.

    > Elastic Net regression can be useful when there are many
    > correlated predictors, as it can perform feature selection like
    > Lasso while maintaining the stability and robustness of Ridge
    > regression.

<br>

Let's learn how to code lasso, ridge, and elastic net regression.

```{r}
# Load necessary library
library(glmnet)

# Prepare data for regularization
x <- model.matrix(mpg ~ cyl + hp + wt, data = mtcars)[, -1]
y <- mtcars$mpg

# Fit Lasso, Ridge, and Elastic Net models
lasso_model <- glmnet(x, y, alpha = 1)
ridge_model <- glmnet(x, y, alpha = 0)
elastic_net_model <- glmnet(x, y, alpha = 0.5)

# Cross-validation to find the optimal lambda value
cv_lasso <- cv.glmnet(x, y, alpha = 1)
cv_ridge <- cv.glmnet(x, y, alpha = 0)
cv_elastic_net <- cv.glmnet(x, y, alpha = 0.5)

# Model summary and interpretation
cat("Lasso - Optimal Lambda:", cv_lasso$lambda.min, "\n")
cat("Ridge - Optimal Lambda:", cv_ridge$lambda.min, "\n")
cat("Elastic Net - Optimal Lambda:", cv_elastic_net$lambda.min, "\n")

# Make predictions using Lasso, Ridge, and Elastic Net models:
# Create new data for predictions
new_data <- data.frame(
  cyl = c(4, 6, 8),
  hp = c(100, 150, 200),
  wt = c(2.5, 3.0, 3.5)
)

# Prepare new data for predictions
new_data_x <- model.matrix(~ cyl + hp + wt, data = new_data)[, -1]

# Make predictions
lasso_predictions <- predict(cv_lasso, new_data_x, s = "lambda.min")
ridge_predictions <- predict(cv_ridge, new_data_x, s = "lambda.min")
elastic_net_predictions <- predict(cv_elastic_net, new_data_x, s = "lambda.min")

# View predictions
cat("Lasso Predictions:", lasso_predictions, "\n")
cat("Ridge Predictions:", ridge_predictions, "\n")
cat("Elastic Net Predictions:", elastic_net_predictions, "\n")


```

<br>

------------------------------------------------------------------------

## For your further study

### Derivation of the Coefficient: Least Squares (LS) Method

The goal of the Least Squares (LS) method is to minimize the sum of the
squared differences between the observed and predicted values of the
dependent variable. This method is commonly used in linear regression to
estimate the coefficients $\beta$.

<br>

#### 1. Objective Function

We aim to minimize the following objective function:

$$
\text{Objective: } \min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \min_{\beta} (y - X\beta)^T (y - X\beta)
$$

Where:

-    $y$ is the vector of observed dependent variable values.

-   $X$ is the matrix of independent variables.

-    $\beta$ is the vector of coefficients.

-    $\hat{y_i} = X\beta$ is the vector of predicted values.

<br>

#### 2. Expanding the Objective Function

Expand the objective function:

$$
(y - X\beta)^T (y - X\beta) = y^T y - 2 \beta^T X^T y + \beta^T X^T X \beta
$$ <br>

#### 3. Taking the Derivative

To minimize the objective function, take the derivative with respect to
$\beta$ and set it equal to 0:

$$
\frac{\partial}{\partial \beta} \left( y^T y - 2 \beta^T X^T y + \beta^T X^T X \beta \right) = 0
$$

This simplifies to:

$$
-2 X^T y + 2 X^T X \beta = 0
$$ <br>

#### 4. Solving for beta

Rearrange the equation to solve for $\beta$:

$$
X^T X \beta = X^T y
$$

<br>

Finally, solve for $\beta$:

$$
\beta = (X^T X)^{-1} X^T y
$$

<br>

### Steps to Estimate the Standard Error $\text{SE}(\hat{\beta})$

1.  **Residual Sum of Squares (RSS)**: First, compute the residuals (the
    difference between the observed values $y$ and the predicted values
    $\hat{y} = X\hat{\beta}$) and the residual sum of squares.

    $$
    \text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \hat{\epsilon}_i^2
    $$

    Where:

    -   $\hat{y}_i = X_i \hat{\beta}$ are the predicted values.
    -   $\hat{\epsilon}_i = y_i - X_i \hat{\beta}$ are the residuals.

2.  **Estimate of Variance of Errors** $\sigma^2$: The variance of the
    error terms $\sigma^2$ is estimated as:

    $$
    \hat{\sigma}^2 = \frac{\text{RSS}}{n - p}
    $$

    Where:

    -   $n$ is the number of observations.
    -   $p$ is the number of parameters in the model (including the
        intercept).
    -   $\text{RSS}$ is the residual sum of squares.

3.  **Variance-Covariance Matrix of** $\hat{\beta}$: The
    variance-covariance matrix of the estimated coefficients
    $\hat{\beta}$ is given by:

    $$
    \text{Var}(\hat{\beta}) = \hat{\sigma}^2 (X^T X)^{-1}
    $$

    Where:

    -   $X$ is the design matrix (the matrix of independent variables,
        including the intercept).
    -   $X^T X$ is the matrix product of $X$ transposed and $X$.
    -   $(X^T X)^{-1}$ is the inverse of the matrix $X^T X$.

4.  **Standard Error of** $\hat{\beta}$: The standard error of each
    coefficient $\hat{\beta}_j$ (where $j = 1, 2, ..., p$) is the square
    root of the diagonal elements of the variance-covariance matrix:

    $$
    \text{SE}(\hat{\beta}_j) = \sqrt{\text{Var}(\hat{\beta}_j)} = \sqrt{\hat{\sigma}^2 \cdot (X^T X)^{-1}_{jj}}
    $$

    Where:

    -   $(X^T X)^{-1}_{jj}$ is the $j$-th diagonal element of the matrix
        $(X^T X)^{-1}$.

<br>

**Practice in R**

```{r}
# Generate some example data
set.seed(123)  # For reproducibility

# Create a matrix of independent variables (X) with an intercept
n <- 100  # Number of observations
p <- 3    # Number of predictors

# Create a matrix X of independent variables with an intercept
X <- cbind(1, matrix(rnorm(n * (p - 1)), n, p - 1))  # X includes intercept

# Create a true beta (coefficients)
beta_true <- c(2, 1.5, -3)  # True coefficients for the model
# intercept: 2
# B1: 1.5
# B2: -3

# Generate the dependent variable (y) with some noise
y <- X %*% beta_true + rnorm(n)

# Compute beta using the formula: beta = (X'X)^(-1) X'y

# Step 1: Compute X'X (transpose of X times X)
XtX <- t(X) %*% X

# Step 2: Compute X'y (transpose of X times y)
Xty <- t(X) %*% y

# Step 3: Compute the inverse of X'X
XtX_inv <- solve(XtX)

# Step 4: Compute the estimated beta
beta_hat <- XtX_inv %*% Xty



# Print the estimated coefficients
print("Estimated beta coefficients:")
print(beta_hat)

# Compare the estimated beta with the true beta
print("True beta coefficients:")
print(beta_true)



################################################
# Check how well the model fits
y_hat <- X %*% beta_hat  # Predicted values

# Compute residuals
residuals <- y - y_hat

# Print residual sum of squares (RSS)
rss <- sum(residuals^2)
print(paste("Residual Sum of Squares (RSS):", rss))


# Step 5: Estimate variance of residuals (sigma^2)
sigma2_hat <- rss / (n - p)

# Step 6: Calculate standard errors of beta_hat
var_beta_hat <- sigma2_hat * solve(t(X) %*% X)
se_beta_hat <- sqrt(diag(var_beta_hat))


# --- Using lm() to fit the model ---

# Create a data frame to use with lm()
df <- data.frame(y = y, X1 = X[, 2], X2 = X[, 3])

# Fit the linear model using lm()
model_lm <- lm(y ~ X1 + X2, data = df)

# Print the estimated coefficients (lm())
print("Estimated beta coefficients (lm):")
print(coef(model_lm))

summary(model_lm)

```

<br>

### Maximum Likelihood Estimation

In linear regression, the method of least squares is commonly used to
estimate the coefficients of the regression model. However, there is
another estimation method called ***Maximum Likelihood Estimation
(MLE)*** that can be used as an alternative to least squares. In this
optional material, we will introduce the concept of MLE, explain how it
works, and discuss its advantages and disadvantages compared to least
squares.

<br>

**Maximum Likelihood Estimation**

> Maximum Likelihood Estimation is a statistical method used to estimate
> the parameters of a model by finding the values that maximize the
> likelihood function. The likelihood function measures how likely the
> observed data is, given the parameters of the model. In the context of
> linear regression, MLE seeks to find the values of the coefficients
> that maximize the likelihood of observing the data, assuming that the
> error terms follow a normal distribution

<br>

**MLE in Linear Regression**

Let's consider the linear regression model:

$$
y_i = β_0 + β_1 x_i + ε_i
$$

where $y_i$ is the dependent variable, $x_i$ is the independent
variable, $β_0$ and $β_1$ are the regression coefficients, and $ε_i$ is
the error term.

Assuming that the error terms $ε_i$ are normally distributed with mean 0
and constant variance $σ^2$, the probability density function
(***PDF***) of the normal distribution for a single observation is:

$$
f(y_i | x_i, β_0, β_1, σ^2) = \frac{1}{σ  \sqrt{2π}}  exp(\frac{-(y_i - (β_0 + β_1 x_i))^2}  {2  σ^2})
$$

The likelihood function is the product of the PDFs for all observations:

$$
L(β_0, β_1, σ^2) = Π f(y_i | x_i, β_0, β_1, σ^2)
$$

To make the optimization problem easier, we take the natural logarithm
of the likelihood function, which is called the log-likelihood function:

$$
logL(β_0, β_1, σ^2) = Σ log(f(y_i | x_i, β_0, β_1, σ^2))
$$

The goal of MLE is to find the values of $β_0$, $β_1$, and $σ^2$ that
maximize the log-likelihood function.

<br>

Practice in R

```{r}
# Generate some example data
set.seed(123)  # For reproducibility

# Create a matrix of independent variables (X) with an intercept
n <- 100  # Number of observations
p <- 3    # Number of predictors

# Create a matrix X of independent variables with an intercept
X <- cbind(1, matrix(rnorm(n * (p - 1)), n, p - 1))  # X includes intercept

# Create a true beta (coefficients)
beta_true <- c(2, 1.5, -3)  # True coefficients for the model

# Generate the dependent variable (y) with some noise
y <- X %*% beta_true + rnorm(n)

# --- Define the log-likelihood function for linear regression ---
log_likelihood <- function(params) {
  beta <- params[1:p]  # Extract beta parameters
  sigma2 <- params[p + 1]  # Extract variance (sigma^2)
  
  # Calculate predicted values
  y_hat <- X %*% beta
  
  # Calculate residuals
  residuals <- y - y_hat
  
  # Compute the log-likelihood
  log_like <- -(n / 2) * log(2 * pi * sigma2) - (1 / (2 * sigma2)) * sum(residuals^2)
  
  return(-log_like)  # Return negative log-likelihood for minimization
}

# --- Initial parameter guesses for beta and sigma^2 ---
init_params <- c(rep(0, p), var(y))  # Initial guesses: zeros for beta, var(y) for sigma^2

# --- Perform MLE using optim to minimize the negative log-likelihood ---
mle_results <- optim(par = init_params, fn = log_likelihood, method = "BFGS")

# Extract estimated betas and sigma^2
beta_mle <- mle_results$par[1:p]
sigma2_mle <- mle_results$par[p + 1]

# Print the results
print("Estimated beta coefficients (MLE):")
print(beta_mle)

print(paste("Estimated variance (sigma^2, MLE):", sigma2_mle))

# Compare the estimated betas (MLE) with the true betas
print("True beta coefficients:")
print(beta_true)

# Check how well the model fits using MLE
y_hat_mle <- X %*% beta_mle  # Predicted values using MLE betas

# Compute residual sum of squares (RSS) for MLE
rss_mle <- sum((y - y_hat_mle)^2)
print(paste("Residual Sum of Squares (RSS, MLE):", rss_mle))

```

<br>

The **`optim()`** function in R is a general-purpose optimization
function. Its role is to find the minimum or maximum of a given
objective function. In the case of MLE, we want to **maximize the
log-likelihood** (or equivalently **minimize the negative
log-likelihood**) to find the best estimates for our model parameters β
and $\sigma^2$.

The optimization problem is often framed as **minimizing** some
objective, so in MLE, we minimize the negative log-likelihood. The
`optim()` function adjusts the parameter values to reduce the negative
log-likelihood as much as possible, eventually finding the optimal
estimates.

-   **`par = init_params`**: This argument provides the **initial
    guesses** for the parameters β and $\sigma^2$ that we are trying to
    estimate. In our case:

    -   We initialize β with zeros (or some other sensible guess).

    -   We initialize $\sigma^2$ with the variance of y, since it's a
        reasonable starting point for the variance of the errors.

-   **`fn = log_likelihood`**: This is the **objective function** we are
    trying to minimize. In our case, it's the negative log-likelihood
    function (`log_likelihood`). The goal is to minimize this function
    by adjusting the parameters β and $\sigma^2$

-   **`method = "BFGS"`**: This specifies the optimization method.
    **BFGS** stands for **Broyden–Fletcher–Goldfarb–Shanno** algorithm,
    which is a popular method used for optimization when the objective
    function is smooth and differentiable (as is the case here). The
    BFGS algorithm is well-suited for problems like this because it
    approximates the second derivatives of the objective function
    (related to curvature), making it efficient for finding local
    minima.

#### What Happens Inside `optim()`?

<br>

1.  **Initial Step**:

    -   The function starts with the initial guesses for β and
        $\sigma^2$ provided in `init_params`.

2.  **Evaluate the Objective Function**:

    -   `optim()` calculates the **negative log-likelihood** for the
        current parameter values using the `log_likelihood()` function.

    -   It evaluates how far off the current guess is by computing the
        residuals $y - X\beta$ and using those residuals to calculate
        the likelihood.

3.  **Iterative Optimization**:

    -   `optim()` adjusts the parameter values iteratively. It evaluates
        the **gradient** (the direction in which the objective function
        decreases most quickly) and **step size** (how far to move in
        the direction of the gradient).

    -   With each iteration, the algorithm updates β and $\sigma^2$
        slightly and recalculates the negative log-likelihood.

    -   This process continues until the algorithm finds a set of
        parameters where the negative log-likelihood is minimized (i.e.,
        the most likely parameters given the data).

4.  **Convergence**:

    -   Once the negative log-likelihood cannot be significantly reduced
        by adjusting the parameters (i.e., when it has "converged" to a
        solution), `optim()` stops.

    -   At this point, the current values of β and $\sigma^2$ are
        considered the **Maximum Likelihood Estimates (MLEs)**.

#### The Log-Likelihood Function: What's Being Minimized?

The **log-likelihood function** describes the probability of observing
the data given the parameters $\beta$ and $\sigma^2$. For a linear
regression model with normally distributed errors, the log-likelihood
is:

$$
\mathcal{L}(\beta, \sigma^2 | X, y) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - X_i \beta)^2
$$

Our goal is to **maximize** this function. Since `optim()` minimizes
functions by default, we instead minimize the **negative
log-likelihood**:

$$
-\mathcal{L}(\beta, \sigma^2 | X, y) = \frac{n}{2} \log(2\pi\sigma^2) + \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - X_i \beta)^2
$$

The negative log-likelihood consists of two parts:

1.  The first part, $\frac{n}{2} \log(2\pi\sigma^2)$, depends on the
    variance $\sigma^2$.
2.  The second part,
    $\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - X_i \beta)^2$, depends on
    the residuals (differences between observed and predicted values)
    and both $\beta$ and $\sigma^2$.

The **optimizer** (`optim()`) will adjust the values of $\beta$ and
$\sigma^2$ to minimize this expression, thereby maximizing the
likelihood of the data given the model parameters.

<br>

**Advantages and Disadvantages of MLE**

Advantages:

-   MLE provides a general framework that can be applied to a wide range
    of statistical models, not just linear regression.

-   MLE is asymptotically unbiased and efficient, meaning that as the
    sample size increases, the estimates converge to the true parameter
    values, and the estimates have the smallest possible variance.

-   MLE allows for the estimation of additional parameters, such as the
    error variance $σ^2$ in linear regression.

Disadvantages:

-   MLE can be computationally intensive, especially for complex models
    with many parameters.

-   MLE relies on the assumption that the error terms follow a specific
    distribution (e.g., normal distribution in linear regression). If
    this assumption is not met, the estimates may be biased or
    inefficient.

Let's demonstrate the similarity of the estimates by fitting a linear
regression model using both LS and MLE, and then visualize the fitted
lines. To do this, we'll predict miles per gallon (mpg) based on the
weight (wt) of a car using the 'mtcars' dataset.

```{r}
# Load necessary libraries
library(tidyverse)
library(MASS)

# Load the 'mtcars' dataset
data(mtcars)

```

Fit the linear regression model using LS (lm function):

```{r}
# Fit the model using LS
ls_model <- lm(mpg ~ wt, data = mtcars)

summary(ls_model)
```

Fit the linear regression model using MLE (fit a normal linear model
with mle2 function):

```{r}
# Load necessary libraries
library(bbmle)

# Define the log-likelihood function for MLE
loglik_fn <- function(beta0, beta1, sigma) {
  y <- mtcars$mpg
  x <- mtcars$wt
  n <- length(y)
  
  mu <- beta0 + beta1 * x
  epsilon <- y - mu
  
  loglik <- -n/2 * log(2 * pi) - n/2 * log(sigma^2) - 1/(2 * sigma^2) * sum(epsilon^2)
  return(-loglik) # The optimization function will minimize the function, so we need to negate the log-likelihood
}


# Fit the model using MLE
mle_model <- mle2(loglik_fn, start = list(beta0 = coef(ls_model)[1], 
                                          beta1 = coef(ls_model)[2], 
                                          sigma = 1))
summary(mle_model)
```

Visualize the fitted lines:

```{r}
# Extract coefficients from the LS and MLE models
ls_coefs <- coef(ls_model)
mle_coefs <- coef(mle_model)

# Create a scatter plot of mpg vs. wt
mtcars_plot <- ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  xlim(c(1.5, 5.5)) +
  ylim(c(5, 40))

# Add the LS and MLE fitted lines to the plot
mtcars_plot +
  geom_abline(aes(intercept = ls_coefs[1], slope = ls_coefs[2], color = "LS", linetype = "LS"), size = 1, alpha=0.5) +
  geom_abline(aes(intercept = mle_coefs[1], slope = mle_coefs[2], color = "MLE", linetype = "MLE"), size = 1) +
  scale_color_manual("Model", values = c("LS" = "blue", "MLE" = "red")) +
  scale_linetype_manual("Model", values = c("LS" = "solid", "MLE" = "dashed")) +
  labs(title = "Linear Regression: LS vs. MLE", x = "Weight", y = "Miles per Gallon") +
  theme_minimal()


```

In the resulting plot, you'll notice that the LS and MLE fitted lines
are almost indistinguishable, which confirms that the estimates are the
same when the error terms follow a normal distribution.

<br>

------------------------------------------------------------------------

## Non-linear Regression

### **Motivation**

In many real-world applications, the relationship between the dependent
variable and independent variables [is not always linear.]{.underline}
**Non-linear regression** is [a versatile tool that can be used to model
complex relationships between variables]{.underline}, allowing for a
more accurate representation of the underlying processes.

### **Theory**

Non-linear regression seeks to find the best-fit curve or surface
through the data points by minimizing the sum of the squared residuals,
which represent the difference between the observed and predicted
values. The general form of a non-linear regression model can be written
as:

$$
y = f(x, β) + ε
$$

where

-   y is the dependent variable,

-   x is the independent variable,

-   β represents the vector of parameters to be estimated,

-   f(x, β) is the non-linear function, and

-   ε is the error term.

<br>

### **Generalized Linear Model (GLM)**

GLM stands for Generalized Linear Model in R. It is a flexible extension
of the ordinary linear regression that allows for response variables
with error distribution models other than the normal distribution, such
as the binomial or Poisson distributions. The GLM is used to model the
relationship between a response variable and one or more predictor
variables by combining a linear predictor function with a specified
probability distribution for the response variable.

The glm() function in R is used to fit generalized linear models, and
its general syntax is:

`glm(formula, data, family)`

where:

-   **formula**: A symbolic description of the model to be fitted, such
    as `y ~ x1 + x2`.

-   **data**: A data frame containing the variables in the model.

-   **family**: A description of the error distribution and link
    function to be used in the model. Common choices include binomial,
    poisson, and gaussian. The link function, which can be specified
    using the link argument within the family function, determines how
    the expected value of the response variable is related to the linear
    predictor function. Examples of link functions are *Logit* and
    *Probit*.

<br>

> The GLM can be applied to various types of regression problems,
> including linear regression, logistic regression, and Poisson
> regression, by specifying the appropriate distribution family and link
> function. This versatility makes the GLM a powerful and widely used
> tool for modeling relationships between variables in various fields.

-   Then, what is the difference btw GLM & LM? See the link below.

    [The Difference Between glm and lm in
    R](https://www.statology.org/glm-vs-lm-in-r/)

<br>

### Intuitive Explanation

![](images/clipboard-353478517.png)

<br>

![](images/clipboard-1839120999.png)

<Br>

![](images/clipboard-3066871532.png)

<br>

![](images/clipboard-3882029130.png)

<br>

![](images/clipboard-1037588417.png)

<br>

![](images/clipboard-543693814.png)

<br>

![](images/clipboard-3517613475.png)

<br>

![](images/clipboard-1261220122.png)

<br>

![](images/clipboard-975877763.png)

<br>

![](images/clipboard-3482915396.png)

<br>

![](images/clipboard-2242562025.png)

<br>

![](images/clipboard-8751192.png)

<br>

![](images/clipboard-3847778143.png)

<br>

### **Logit Model** (A representative model in GLM)

> Logistic regression, specifically the logit model, is a popular
> technique for handling non-linear dependent variables, allowing us to
> predict the probability of an event occurring given a set of input
> variables.

$$
P(Y=1) = \frac{1}{(1 + exp(-z))}
$$ where z is a linear function of the predictor variables: $$
z = β_0 + β_1X_1 + β_2X_2 + ... + β_kX_k
$$

The logit transformation, which is the log-odds of the probability, is
given by:

$$
logit(P(Y=1)) = \log{\frac {P(Y=1)}{P(Y=0)}} = z
$$ The coefficients $(β_0, β_1, ... β_k)$ are estimated using *Maximum
Likelihood Estimation (MLE)*, which seeks to maximize the likelihood of
observing the data given the logistic model.

Let's use R to fit a logit model to a simple dataset. First, we will
check if the required library is installed, and if not, install and load
it:

```{r}

# install.packages("glm2")
library(glm2)

```

Next, let's create a synthetic dataset for our example:

```{r}
set.seed(42)
x1 <- runif(100, 0, 10)
x2 <- runif(100, 0, 10)
z <- 0.5 + 0.7 * x1 - 0.3 * x2
p <- 1 / (1 + exp(-z))
y <- ifelse(p > 0.5, 1, 0)
data <- data.frame(x1, x2, y)
data

```

Here, we have generated 100 data points with two predictor variables, x1
and x2, and a binary outcome variable, y.

Now, let's fit the logit model using the `glm()` function:

```{r}
model <- glm(y ~ x1 + x2, data = data, family = binomial(link = "logit"))

```

To view the estimated coefficients, we can use the `summary()` function:

```{r}
summary(model)

```

To make predictions on new data, we can use the `predict()` function:

```{r}
new_data <- data.frame(x1 = c(5, 7), x2 = c(3, 9))
new_data

predicted_prob <- predict(model, newdata = new_data, 
                          type = "response")
predicted_prob

predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
predicted_class


```

<br>

### Second practice with another dataset

Let's use `haberman` dataset

```{r}
library(tidyverse)

haberman<-read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data", header=F)
names(haberman)<-c("age", "op_year", "no_nodes", "survival")

glimpse(haberman)
```

The **Haberman dataset**, also known as the Haberman's Survival dataset,
is a dataset containing cases from a study conducted between 1958 and
1970 at the University of Chicago's Billings Hospital on the survival of
patients who underwent surgery for breast cancer. The dataset is often
used for classification and data analysis tasks in machine learning and
statistics.

The Haberman dataset contains 306 instances (rows) and 4 attributes
(columns). The attributes are:

1.  **Age**: The patient's age at the time of the operation, represented
    as an integer.

2.  **Year**: The year of the operation, represented as an integer from
    58 (1958) to 69 (1969).

3.  **Nodes**: The number of positive axillary nodes detected,
    represented as an integer. A positive axillary node is a lymph node
    containing cancer cells. A higher number of positive axillary nodes
    generally indicates a more advanced stage of cancer.

4.  **Status**: The survival status of the patient, represented as an
    integer. A value of 1 indicates that the patient survived for 5
    years or longer after the surgery, while a value of 2 indicates that
    the patient died within 5 years of the surgery.

5.  **Response var**: Survival in 5 years

The goal of analyzing the Haberman dataset is usually to predict a
patient's survival status based on the other three attributes (age,
year, and nodes). This is typically treated as a binary classification
problem, with survival status as the dependent variable and the other
attributes as independent variables. Various machine learning
algorithms, including logistic regression, support vector machines, and
decision trees, can be applied to this dataset for predictive modeling
and analysis.

```{r}
table(haberman$survival)
prop.table(table(haberman$survival))

```

Adding a Binary Survival Indicator to the Haberman Dataset Using mutate
and ifelse

```{r}
haberman %>% 
  mutate(n_survival=ifelse(survival==2,1,0)) %>% 
  head

```

```{r}
haberman %>% 
  mutate(n_survival=ifelse(survival==2,1,0)) %>% 
  dplyr::select(-survival) -> haberman
summary(haberman)
```

Visualize the density of age, op_year, and no_nodes

```{r}
par(mfrow=c(1,3))
plot(density(haberman$age))
plot(density(haberman$op_year))
plot(density(haberman$no_nodes))
```

Make them box_plot as well

```{r}
par(mfrow=c(1,3))
boxplot(haberman$age)
boxplot(haberman$op_year)
boxplot(haberman$no_nodes)
```

Check correlation between vars in the data

```{r}
corr <- round(cor(haberman), 2)
corr
```

Make it cor_plot

```{r}
library(ggcorrplot)
ggcorrplot(corr, method = "circle")
```

See the relationship between Xs & Y

```{r}
par(mfrow=c(2,2))
plot(haberman$age, haberman$n_survival)
plot(haberman$op_year, haberman$n_survival)
plot(haberman$no_nodes, haberman$n_survival)
```

Age & Survival

```{r}
haberman %>% 
  ggplot(aes(x=age, y=n_survival)) + 
  geom_jitter(aes(col=factor(n_survival)), 
              height=0.1, width=0.1)
```

Op_year & Survival

```{r}
haberman %>% 
  ggplot(aes(x=op_year, y=n_survival)) + 
  geom_jitter(aes(col=factor(n_survival)), 
              height=0.1, width=0.1)
```

no_nodes & survival

```{r}
haberman %>% 
  ggplot(aes(x=no_nodes, y=n_survival)) + 
  geom_jitter(aes(col=factor(n_survival)), 
              height=0.1, width=0.1)
```

Fit the data to the simple linear model

```{r}
linear.model<-glm("n_survival~.", 
                  data=haberman)
summary(linear.model)

```

Fit the data to the generalized linear model

```{r}
logit.model<-glm("n_survival~.", 
                 data=haberman, 
                 family="binomial")
summary(logit.model)
```

**Odds ratio**

```{r}
exp(logit.model$coefficients)

exp(cbind(OR = coef(logit.model), confint(logit.model)))


```

Prediction

```{r}

newdata<-data.frame(age=c(10,20,30), 
                    op_year=c(40,50,60), 
                    no_nodes=c(1,3,5))
newdata

predict(linear.model, newdata)
```

Type of prediction is a predicted probability (type="response").

```{r}
predict(logit.model, newdata, type = "response")

pred_prob <- predict(logit.model, newdata, type = "response")

predicted_class <- ifelse(pred_prob > 0.5, 1, 0)
predicted_class
```
