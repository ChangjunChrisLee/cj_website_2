---
title: "논문 복기를 통한 실험 통계"
subtitle: "" 
Week: 10
format: html
slide-format: revealjs
---

[*Weekly content*](https://changjunlee.com/teaching/grad_stat/weekly_2/)

<br>

![](images/clipboard-571953391.png)

-   [*Paper*](https://www.pnas.org/doi/10.1073/pnas.2319112121?gad_source=1&gclid=CjwKCAjw9p24BhB_EiwA8ID5Boe0ZVgwd4np8lt0YNumarKotDFhqJlOMas3VGBnO8MHQZrB_QRjZhoCaccQAvD_BwE)

-   [*Data & R Code*](https://osf.io/8wnmr/files/osfstorage?view_only=30f6db3f57954e9eaa016457a29264b6)

<br>

## 좋은 논문의 조건

-   **명확한 연구 질문 및 목표**: 문제제기의 중요성

-   **좋은 포지셔닝**: 기존 연구와의 관련성, 차별점

-   **알맞은 연구 방법론의 선택**

-   **결과의 담백한 해석**

-   **간결하고 이해하기 쉬운 서술**: 문장 구조, 논리 구조, 시각화 활용

<br>

## Paper summary

#### 1. **연구 배경 및 목적**

-   **사람들의 기본 욕구**: 사람들은 자신이 이해받고, 확인받고, 가치있다고 느끼기를 원함.

-   **연구 질문**: AI가 이러한 감정을 제공할 수 있는가? 그리고 사람들이 그것이 AI로부터 온 것이라고 알 때 반응은 어떻게 달라지는가?

#### 2. **연구 방법**

-   **실험 설계**: 2 x 2 실험 설계.

    -   메시지 제공 출처: AI 대 인간

    -   라벨: AI 대 인간

-   참가자들은 복잡한 상황을 설명하고, AI 또는 인간 응답을 받음.

#### 3. **주요 결과**

-   **AI 응답의 강점**:

    -   AI가 감정을 정확하게 인식하고 더 높은 수준의 감정적 지원을 제공.

    -   AI 응답은 인간 응답보다 사람들을 더 많이 "들어줬다"고 느끼게 함.

-   **AI 라벨의 약점**:

    -   사람들이 응답이 AI에서 온 것임을 알게 되면, 해당 응답에 대해 부정적 반응을 보임.

    -   AI 라벨은 응답의 가치를 낮추는 효과가 있음.

#### 4. **추가 연구 발견**

-   **AI와 인간 응답의 차이**:

    -   AI는 감정적 지원을 더 잘 제공, 실질적 조언 제공은 부족.

    -   인간은 개인 경험을 더 자주 공유했으나, 감정적 지지를 제공하는데 AI에 비해 덜 효과적.

#### 5. **응용 및 시사점**

-   **AI와 인간 협력**:

    -   AI는 감정 인식과 감정적 지원 제공에 유리함.

    -   사람들은 AI가 아닌 인간과의 상호작용을 더 가치 있게 여길 수 있음.

    -   AI는 감정적 지원을 필요로 하는 상황에서 인간의 도움을 보완할 수 있음.

    -   AI 사용 시, AI 응답임을 투명하게 공개하는 것이 중요할 수 있음.

#### 6. **미래 연구 방향**

-   AI와 인간 협력에서 AI의 투명성과 신뢰 구축에 대한 연구가 필요함.

-   AI 응답이 인간 간 관계 개선에 어떤 역할을 할 수 있는지 추가 연구가 필요.

#### 7. **결론**

-   **AI의 가능성**: AI는 감정적 지원에서 중요한 역할을 할 수 있으며, 특히 사람들 간의 이해를 증진시킬 수 있는 잠재력을 가지고 있음.

-   **한계점**: AI 라벨이 부정적 영향을 미칠 수 있다는 점에서 인간 상호작용을 대체하기 어려움.

<br>

## Introduction 의 조건 (좋은 서론이란)

-   **흥미를 끄는 도입부 (연구 배경)**

    -   깔대기 구조, but 초반에 너무 거창하지 않게.

-   **명확한 문제 제기**

    -   해결하려는 **구체적인 문제**를 제시

    -   연구가 왜 필요한지에 대한 근거를 제시하는 역할

-   **연구 배경 및 문헌 검토**

    -   주로 2장의 한-두 단락 요약

    -   연구의 포지셔닝과 기여를 제시하기 위한 빌드업

```{=html}
<!-- -->
```
-   **연구 질문 및 가설 제시**

-   **연구의 목적과 중요성 명시**

    -   학문적 또는 실질적으로 어떤 의미를 가지는지, 왜 중요한지를 강조

-   **연구 방법에 대한 간단한 소개**

    -   독자가 연구 과정에 대한 큰 그림을 그릴 수 있도록

    -   서론에서는 연구의 접근 방식에 대한 개요를 제공 (너무 상세하지 않게)

-   **논문의 구조** 안내 (앞으로 펼쳐질 이야기)

<bR>

------------------------------------------------------------------------

## Introduction 

### Background

The rapid integration of artificial intelligence (AI) in various aspects of daily life has led to significant discussions around its potential and limitations in fulfilling fundamental human psychological needs. One crucial aspect of human interaction is the desire to “feel heard,” which involves perceiving oneself as understood, validated, and valued. This perception impacts both mental and physical health, as being heard is associated with better well-being.

The study titled *“AI Can Help People Feel Heard, but an AI Label Diminishes This Impact”* delves into whether AI can effectively simulate human-like empathy and whether people truly feel heard when they know the response is AI-generated.

### Key Questions

1.  **Can AI generate responses that make people feel heard?**

2.  **How do recipients react when they believe the response is generated by AI compared to a human?**

### Experiment Overview

To answer these questions, the study employed a between-subjects design, varying the source (AI or human) and the label (AI or human) provided to participants. Participants were asked to describe a complex personal situation and then received responses that were either AI- or human-generated. The responses were labeled as coming from either a human or an AI, allowing the researchers to disentangle the “response effect” and the “label effect.”

### Significance of Findings

Initial findings indicated that AI-generated responses were more effective at making recipients feel heard, demonstrating high empathic accuracy in understanding emotions. However, when recipients knew the response was from an AI, the sense of being heard was diminished. This reflects a bias toward AI and suggests that while AI can excel at creating emotionally supportive responses, human perceptions of AI's capabilities influence the impact of these responses.

The results also highlighted that AI-generated responses excelled at providing emotional support but did not engage in practical suggestions as much as human responses did. This aligns with the idea that emotional validation can be more effective for the feeling of being heard than practical advice.

### Methodological Note

The experiment used a 2x2 factorial design:

-   **Response Source**: Human vs. AI

-   **Response Label**: Human vs. AI

This setup allowed for a comprehensive examination of how actual and perceived sources influence participants' feelings of being heard, perceived accuracy, and connection to the responder.

<br>

### Data Explanation

The dataset used for this analysis contained responses from participants who described complex personal situations. These responses were paired with either AI- or human-generated replies, with each reply being assigned a label indicating its source. Key variables included:

-   **Feeling Heard Score**: A composite measure based on participants' ratings.

-   **Perceived Response Accuracy**: The degree to which participants believed the response accurately captured their sentiments.

-   **Connection Score**: A measure reflecting how connected participants felt to the responder.

The data was analyzed using ANOVAs to assess the main effects of the response source and label on these dependent variables.

<br>

## Methods

### Study Design

The study was conducted using a 2 (response source: human vs. AI) × 2 (response label: human vs. AI) between-subjects experimental design. This design enabled the researchers to evaluate the independent effects of both the actual source of the response and the label given to the participants.

#### Phases of the Study

The study was divided into three main phases:

1.  **Part 1: Initial Situation Description**

    -   Participants were recruited through Prolific, an online research platform, and asked to describe a complex personal situation they were currently dealing with. These descriptions were recorded as audio files and transcribed using Phonic AI.

    -   Participants then rated the intensity of six basic emotions (happiness, sadness, fear, anger, surprise, and disgust) they felt in that situation using a 7-point Likert scale (1 = not at all, 7 = very much).

2.  **Part 2: Response Generation**

    -   Participants from Part 1 were divided into two groups: those who would receive human-generated responses and those who would receive AI-generated responses.

    -   In the human response condition, participants were paired with another participant recruited to read and respond to the situation descriptions. These human responders were instructed to write replies that would make the original participant feel understood. The length of responses was standardized by using the median word count of previous responses.

    -   For the AI response condition, Bing Chat was used to generate replies. The AI was prompted with the transcribed situation and instructed to respond empathetically. The response length was adjusted to match the median length of human responses for consistency.

3.  **Part 3: Response Evaluation**

    -   The original participants from Part 1 were invited back to read the response they received. They were informed whether the response came from another human or Bing Chat, creating the label manipulation.

    -   Participants were asked to evaluate how much the response made them feel heard, the perceived accuracy of the response, and their connection to the responder. These measures included multiple items adapted from established scales:

        -   **Feeling Heard**: Measured using six items (e.g., “This response makes me feel understood”, “This response makes me feel affirmed”).

        -   **Response Accuracy**: Participants rated how accurately the response captured their sentiments (e.g., “The response accurately summarizes what I said”).

        -   **Connection to Responder**: Measured using questions like “How connected do you feel to the responder?”

### Sample Characteristics

-   A total of 540 participants completed Part 1, but 39 participants were excluded for insufficient descriptions (e.g., one-sentence responses), resulting in a final sample of 501.

-   In Part 2, 233 participants received human-generated responses, and 250 received AI-generated responses.

-   456 participants completed Part 3, providing evaluations of the responses they received.

-   Demographics included diverse age and gender distributions, with participants primarily from the United States.

### Statistical Analysis

-   The data were analyzed using analysis of variance (ANOVA) to explore the main effects and interactions between response source and label on dependent variables such as feeling heard, perceived accuracy, and connection.

-   Moderator analyses were conducted to examine if attitudes toward AI or perceived agency of the AI influenced the effects.

<Br>

## *Hands-on Practice*

<br>

### Table 1

```{r}
library(tidyverse)
library(ggpubr)
library(gridExtra)
library(plotrix)
library(ggplot2)
library(lsr)

###############################################

all = read.csv('data/Being Heard by AI OSF.csv')

all %>% glimpse

```

-   **`tidyverse`**: A collection of R packages for data manipulation, visualization, and analysis.

-   **`ggpubr`**: Used for creating publication-ready plots.

-   **`gridExtra`**: Provides functions to arrange multiple grid-based plots.

-   **`plotrix`**: Contains various plotting functions, including those for error bars.

-   **`ggplot2`**: Part of the `tidyverse`, specifically for creating data visualizations.

-   **`lsr`**: Used for calculating effect sizes, including eta-squared.

<br>

```{r}

etaSquared(aov(feelheard~responseR*labelR,all), type = 2, anova = T) %>% 
  as_tibble %>% round(.,3)

etaSquared(aov(accuracy~responseR*labelR,all), type = 2, anova = T) %>% 
  as_tibble %>% round(.,3)

etaSquared(aov(understoodme~responseR*labelR,all), type = 2, anova = T) %>% 
  as_tibble %>% round(.,3)

etaSquared(aov(connection~responseR*labelR,all), type = 2, anova = T) %>% 
  as_tibble %>% round(.,3)
```

<br>

![](images/clipboard-1149830399.png)

<br>

-   **`aov()` Function**: Performs an ANOVA to test the main and interaction effects of the factors `responseR` (the source of the response) and `labelR` (how the response was labeled) on each dependent variable (`feelheard`, `accuracy`, `understoodme`, and `connection`).

-   **`etaSquared()` Function**: Calculates the effect size (eta-squared) for the ANOVA results. The `type = 2` argument specifies the use of Type II sum of squares, which is appropriate for balanced designs or when you do not have a clear nesting order.

    -   The `etaSquared()` function provides an indication of how much of the variance in each dependent variable is explained by the independent variables (`responseR`, `labelR`, and their interaction). This helps in understanding the practical significance of the findings.

    -   The `anova = T` argument ensures that the output includes ANOVA results alongside the effect size.

### Purpose of the Analysis

-   **`feelheard`**: Measures how much participants felt heard based on the response.

-   **`accuracy`**: Evaluates the perceived accuracy of the response.

-   **`understoodme`**: Assesses how well participants felt understood.

-   **`connection`**: Captures the perceived connection to the responder.

### Explanation of the Output

1.  **Eta Squared (η²)**:

    -   **`eta.sq`**: The proportion of total variance explained by each factor (overall effect size). For instance, `responseR` has an eta squared value of approximately 0.0439, indicating that it accounts for about 4.39% of the variance in the "feeling heard" scores.

    -   **`eta.sq.part`**: The partial eta squared, which represents the proportion of variance explained by each factor, excluding other variables. This is often used for interpretation as it isolates the effect of each factor. [**(like marginal effect)**]{.underline}

2.  **Sum of Squares (SS)**:

    -   **`SS`**: The variability attributed to each factor. Higher SS values mean that the factor contributes more to the variability in the dependent variable.

3.  **Degrees of Freedom (df)**:

    -   Represents the number of values that are free to vary for each factor. For `responseR` and `labelR`, `df = 1` because they are categorical variables with two levels (e.g., AI vs. human).

4.  **Mean Squares (MS)**:

    -   The average variability per degree of freedom. Calculated as `SS/df`.

5.  **F-statistic (F)**:

    -   Indicates the ratio of the variance explained by the factor to the variance within groups (residual variance). A higher F value signifies a more substantial effect.

    -   For `responseR`, the F-statistic is approximately 22.00, which is highly significant with a **p-value of 3.61e-06**, showing that the source of the response significantly impacts how participants feel heard.

<br>

![](images/clipboard-1149830399.png)

```{r}

all %>% group_by(labelR) %>% 
  summarise(feelheard = mean(feelheard,na.rm=T),
            accuracy = mean(accuracy,na.rm=T),
            understoodme = mean(understoodme,na.rm=T),
            connection = mean(connection,na.rm=T)) %>%
  mutate_at(vars(feelheard:connection), round, 3)

all %>% group_by(labelR) %>% 
  summarise(feelheard = sd(feelheard,na.rm=T),
            accuracy = sd(accuracy,na.rm=T),
            understoodme = sd(understoodme,na.rm=T),
            connection = sd(connection,na.rm=T)) %>%
  mutate_at(vars(feelheard:connection), round, 3)


```

<Br>

```{r}


all %>% group_by(responseR) %>% 
  summarise(feelheard = mean(feelheard,na.rm=T),
            accuracy = mean(accuracy,na.rm=T),
            understoodme = mean(understoodme,na.rm=T),
            connection = mean(connection,na.rm=T)) %>%
  mutate_at(vars(feelheard:connection), round, 3)



all %>% group_by(responseR) %>% 
  summarise(feelheard = sd(feelheard,na.rm=T),
            accuracy = sd(accuracy,na.rm=T),
            understoodme = sd(understoodme,na.rm=T),
            connection = sd(connection,na.rm=T)) %>%
  mutate_at(vars(feelheard:connection), round, 3)


```

**The end of the TABLE 1**

<Br>

### Figure 1

```{r}


all$labelRR = ifelse(all$labelR=='ai label',"AI label","human label")
all$responseRR = ifelse(all$responseR=='ai response',"AI response","human response")

dodge = position_dodge(width=0.9)

apatheme=theme_bw()+theme(panel.grid.major=element_blank(),
                          panel.grid.minor=element_blank(),
                          panel.border=element_blank(),
                          axis.line=element_line(),
                          text=element_text(family='Helvetica',size=15),
                          axis.text.x = element_text(color="black"))

```

<br>

```{r}

diff.heard.label = independentSamplesTTest(feelheard~labelR,all)
delta.heard.label= diff.heard.label$mean[1]-diff.heard.label$mean[2]
conf.heard.label = diff.heard.label$conf.int

diff.heard.label
delta.heard.label
conf.heard.label

```

```{r}
diff.accuracy.label =independentSamplesTTest(accuracy~labelR,all)
delta.accuracy.label= diff.accuracy.label $mean[1]-diff.accuracy.label $mean[2]
conf.accuracy.label= diff.accuracy.label$conf.int

```

```{r}
diff.understoodme.label = independentSamplesTTest(understoodme~labelR,all)
delta.understoodme.label= diff.understoodme.label$mean[1]-diff.understoodme.label$mean[2]
conf.understoodme.label = diff.understoodme.label$conf.int

```

```{r}
diff.connection.label = independentSamplesTTest(connection~labelR,all)
delta.connection.label = diff.connection.label$mean[1]-diff.connection.label$mean[2]
conf.connection.label = diff.connection.label$conf.int

```

```{r}
diff.heard.response = independentSamplesTTest(feelheard~responseR,all)
delta.heard.response = diff.heard.response$mean[1]-diff.heard.response$mean[2]
conf.heard.response = diff.heard.response$conf.int

```

```{r}
diff.accuracy.response = independentSamplesTTest(accuracy~responseR,all)
delta.accuracy.response = diff.accuracy.response $mean[1]-diff.accuracy.response $mean[2]
conf.accuracy.response = diff.accuracy.response$conf.int

```

```{r}
diff.understoodme.response = independentSamplesTTest(understoodme~responseR,all)
delta.understoodme.response = diff.understoodme.response$mean[1]-diff.understoodme.response$mean[2]
conf.understoodme.response = diff.understoodme.response$conf.int

```

```{r}
diff.connection.response = independentSamplesTTest(connection~responseR,all)
delta.connection.response = diff.connection.response$mean[1]-diff.connection.response$mean[2]
conf.connection.response = diff.connection.response$conf.int

```

```{r}
deltaplot = data.frame(effect=c('Label','Label','Label','Label','Response','Response','Response','Response'),
                       dv = c('Feeling\nheard',"Response\naccuracy","Responder\nunderstood me",
                              "Connection\nto responder",'Feeling\nheard',"Response\naccuracy","Responder\nunderstood me",
                              "Connection\nto responder"),
                       delta =c(delta.heard.label,delta.accuracy.label,delta.understoodme.label,delta.connection.label,
                                delta.heard.response,delta.accuracy.response,delta.understoodme.response,delta.connection.response),
                       lower = c(conf.heard.label[1],conf.accuracy.label[1],conf.understoodme.label[1],conf.connection.label[1],
                                 conf.heard.response[1],conf.accuracy.response[1],conf.understoodme.response[1],conf.connection.response[1]),
                       high=c(conf.heard.label[2],conf.accuracy.label[2],conf.understoodme.label[2],conf.connection.label[2],
                              conf.heard.response[2],conf.accuracy.response[2],conf.understoodme.response[2],conf.connection.response[2]))

deltaplot$dv= factor(deltaplot$dv, levels = c("Feeling\nheard", "Response\naccuracy", "Responder\nunderstood me","Connection\nto responder"))

deltaplot
```

```{r}

apatheme=theme_bw()+theme(panel.grid.major=element_blank(),
                          panel.grid.minor=element_blank(),
                          panel.border=element_blank(),
                          axis.line=element_line(),
                          text=element_text(family='Helvetica',size=12),
                          axis.text.x = element_text(color="black"))

```

```{r}
figure1=deltaplot  %>%
  ggplot(aes(x = dv, y = delta, fill = effect))+
  geom_bar(stat='identity', position=dodge)+
  geom_errorbar(aes(ymin= lower, ymax = high), 
                position = dodge,width = 0.1)+
  ylab('Delta (AI - Human)')+xlab('')+
  apatheme+
  scale_fill_manual(values = c("darkred", "lightgreen"),name="Manipulations")

figure1

```

-   **Findings Highlighted**:

    -   **AI response with a human label** often yielded higher ratings compared to when labeled as AI. This suggests that participants felt more positively about responses when they were believed to be human, despite being generated by AI.

    -   **Human response with an AI label** showed lower ratings compared to when labeled as human, demonstrating a consistent bias where an AI label diminished the perceived quality of responses.

    -   The **interaction between response source and label** was not significant, as shown by overlapping error bars for some conditions. This supports the independent effects of response and label without an interactive component.

### Interpretation:

-   The figure underscores that while AI-generated responses can effectively make participants feel heard, an AI label diminishes this perception due to potential biases. Conversely, human responses labeled as AI also suffer reduced ratings, highlighting a broader skepticism toward AI in empathetic communication

<br>

### Multi group contrast

```{r}

########multi group contrast###############

temp1 = subset(all,responseR=='ai response'&labelR=='ai label')
temp2 = subset(all,responseR=='human response'&labelR=='human label')
temp3 = subset(all,responseR=='human response'&labelR=='ai label')
temp4 = subset(all,responseR=='ai response'&labelR=='human label')

temp1$cond = 'ai response ai label'
temp2$cond = 'human response human label'
temp3$cond = 'human response ai label'
temp4$cond = 'ai response human label'

all.long = rbind(temp1,temp2,temp3,temp4)
all.long$cond= factor(all.long$cond, levels = c("ai response human label", "ai response ai label", "human response human label","human response ai label"))

all.long %>% head(10)

```

```{r}

library(rempsyc)
table.stats1 <- nice_contrasts(
  response = "feelheard",
  group = "cond",
  data = all.long
)

(my_table1 <- nice_table(table.stats1))

```

```{r}


table.stats2 <- nice_contrasts(
  response = "accuracy",
  group = "cond",
  data = all.long
)

(my_table2 <- nice_table(table.stats2))

```

```{r}

table.stats3 <- nice_contrasts(
  response = "understoodme",
  group = "cond",
  data = all.long
)


(my_table3 <- nice_table(table.stats3))


```

```{r}


table.stats4 <- nice_contrasts(
  response = "connection",
  group = "cond",
  data = all.long
)

(my_table4 <- nice_table(table.stats4))


```

```{r}

# print(my_table1,my_table2,my_table3,my_table4, preview ="docx")
# flextable::save_as_docx(my_table1,my_table2,my_table3,my_table4, path = "contrasts.docx")
```

<br>

### Figure 2

```{r}

## four condition plots

all.long=all.long%>%
  mutate(cond = recode(cond, 
                       "ai response human label" = "AI Response\nHuman Label",
                       "ai response ai label" = "AI Response\nAI Label",
                       "human response human label" = "Human Response\nHuman Label",
                       "human response ai label" = "Human Response\nAI Label"))

```

```{r}
apatheme=theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border=element_blank(),
        axis.line=element_line(),
        text=element_text(family='Helvetica',size=10,colour='black'),
        axis.text.x = element_text(color="black"))

```

```{r}

heard = all.long %>%
  group_by(cond)%>%
  summarize(ratings = mean(feelheard), se.ratings = std.error(feelheard))%>%
  ggplot(aes(x = cond, y = ratings,fill=cond))+
  geom_bar(stat='identity', position=dodge,color="black",linewidth=1)+
  geom_errorbar(aes(ymin= ratings - se.ratings, ymax = ratings + se.ratings), 
                position = dodge,width = 0.1)+coord_cartesian(ylim = c(1,7))+
  scale_y_continuous(breaks=seq(1,7,1))+
  ylab('Feeling Heard')+xlab('')+  
  scale_fill_manual(values = c("black", "darkgrey","grey","white"))+
  apatheme
heard

```

```{r}
accuracy = all.long %>%
  group_by(cond)%>%
  summarize(ratings = mean(accuracy), se.ratings = std.error(accuracy))%>%
  ggplot(aes(x = cond, y = ratings,fill=cond))+
  geom_bar(stat='identity', position=dodge,color="black",linewidth=1)+
  geom_errorbar(aes(ymin= ratings - se.ratings, ymax = ratings + se.ratings), 
                position = dodge,width = 0.1)+coord_cartesian(ylim = c(1,7))+
  scale_y_continuous(breaks=seq(1,7,1))+
  ylab('Response Accuracy')+xlab('')+  
  scale_fill_manual(values = c("black", "darkgrey","grey","white"))+
  apatheme
accuracy

```

```{r}
understood = all.long %>%
  group_by(cond)%>%
  summarize(ratings = mean(understoodme), se.ratings = std.error(understoodme))%>%
  ggplot(aes(x = cond, y = ratings,fill=cond))+
  geom_bar(stat='identity', position=dodge,color="black",linewidth=1)+
  geom_errorbar(aes(ymin= ratings - se.ratings, ymax = ratings + se.ratings), 
                position = dodge,width = 0.1)+coord_cartesian(ylim = c(1,7))+
  scale_y_continuous(breaks=seq(1,7,1))+
  ylab('Responder Understood Me')+xlab('')+  
  scale_fill_manual(values = c("black", "darkgrey","grey","white"))+
  apatheme
understood

```

```{r}
connection = all.long %>%
  group_by(cond)%>%
  summarize(ratings = mean(connection), se.ratings = std.error(connection))%>%
  ggplot(aes(x = cond, y = ratings,fill=cond))+
  geom_bar(stat='identity', position=dodge,color="black",linewidth=1)+
  geom_errorbar(aes(ymin= ratings - se.ratings, ymax = ratings + se.ratings), 
                position = dodge,width = 0.1)+coord_cartesian(ylim = c(1,7))+
  scale_y_continuous(breaks=seq(1,7,1))+
  ylab('Connnection to Responder')+xlab('')+  
  scale_fill_manual(values = c("black", "darkgrey","grey","white"))+
  apatheme
connection

```

```{r}
figure2=ggarrange(heard, accuracy,understood,connection, ncol=2, nrow=2, common.legend = TRUE, legend="none")
figure2

```

<br>

### Understanding Moderators in Statistical Analysis

Moderators are variables that affect the strength or direction of the relationship between independent and dependent variables. Including moderators in your analysis allows you to explore complex interactions and understand under what conditions certain effects hold true.

### How to Include Moderators in R

In R, moderators can be incorporated into linear models to examine their interaction with independent variables. The general approach involves specifying the interaction term in the model formula.

### Example Code for Moderator Analysis

Below is an example of how to perform moderator analysis in R:

```{r}
# Perform ANOVA with moderator interaction
anova(lm(feelheard ~ labelR * atti_bing, all))
anova(lm(accuracy ~ labelR * atti_bing, all))
anova(lm(understoodme ~ labelR * atti_bing, all))
anova(lm(connection ~ labelR * atti_bing, all))
```

#### Explanation:

-   **`lm()`**: Creates a linear model to analyze the relationship between dependent and independent variables.

-   **`labelR * atti_bing`**: Specifies that the interaction between `labelR` (the label of the response) and `atti_bing` (participants' attitudes toward Bing) should be included.

-   **`anova()`**: Performs ANOVA to determine the significance of the main and interaction effects.

### Steps for Conducting Moderator Analysis

1.  **Fit the Model**: Use `lm()` to fit a linear model with the interaction term.

2.  **Run ANOVA**: Use `anova()` to evaluate the significance of the interaction effect.

3.  **Interpret the Results**:

    -   Check the p-value for the interaction term to see if it is statistically significant.

    -   If significant, this indicates that the effect of `labelR` on the dependent variable varies depending on `atti_bing`.

4.  **Report the Findings**:

    -   Include the F-statistic, degrees of freedom, and p-values to show the significance of the interaction.

    -   Discuss how the moderator influences the relationship between the independent and dependent variables.

<br>

### Example Interpretation:

If the interaction term `labelR:atti_bing` is significant, this suggests that participants' attitudes toward Bing moderate the impact of response labeling on feeling heard. For instance, participants with more positive attitudes may be less affected by an AI label compared to those with neutral or negative attitudes.

### Main Findings

The analysis revealed several key outcomes regarding the effectiveness of AI-generated versus human-generated responses:

1.  **Feeling Heard**: AI-generated responses scored significantly higher on participants' feeling heard (mean = 5.74) compared to human responses (mean = 5.17). This demonstrates that, in terms of content quality, AI responses had a superior effect on making recipients feel heard.

2.  **Perceived Accuracy and Understanding**: Participants rated AI-generated responses as more accurate (mean = 5.92) and understanding (mean = 5.79) compared to human responses (mean = 5.24 for accuracy and 5.07 for understanding). This suggests that AI responses were perceived as better at capturing the essence of the participants' emotions.

3.  **Connection to the Responder**: While AI responses were effective at making participants feel heard, they fell short in fostering a connection when labeled as AI-generated. Participants reported a lower sense of connection to AI responders (mean = 4.71) compared to human responders (mean = 4.06), indicating that knowing the response was from an AI reduced perceived personal closeness.

4.  **Label Effects**: The analysis showed that labeling responses as AI reduced the perceived impact across all measures, including feeling heard, response accuracy, and connection. The “AI label” effect demonstrated a consistent decrease in ratings by approximately 0.68 points for feeling heard and 0.31 points for response accuracy.

<br>

Let's see the real code:

### Figure 3

```{r}

###################Moderators####################
summary(lm(feelheard~labelR*atti_bing,all))
anova(lm(feelheard~labelR*atti_bing,all))

anova(lm(accuracy~labelR*atti_bing,all))

anova(lm(understoodme~labelR*atti_bing,all))

anova(lm(connection~labelR*atti_bing,all))

```

```{r}
anova(lm(feelheard~labelR*experience,all))
anova(lm(accuracy~labelR*experience,all))
anova(lm(understoodme~labelR*experience,all))
anova(lm(connection~labelR*experience,all))

```

```{r}
anova(lm(feelheard~labelR*agency,all))
anova(lm(accuracy~labelR*agency,all))
anova(lm(understoodme~labelR*agency,all))
anova(lm(connection~labelR*agency,all))

```

```{r}
library(interactions)
library(jtools)
library(ggplot2)
m1=lm(feelheard~labelRR*atti_bing,all)
f1=interact_plot(m1, pred = atti_bing, modx = labelRR,interval=T,y.label = "Feel Heard",x.label = 'Attitude towards Bing Chat',legend.main = c("Label")) + theme_apa()

```

```{r}
m2=lm(accuracy~labelRR*atti_bing,all)
f2=interact_plot(m2, pred = atti_bing, modx = labelRR,interval=T,y.label = "Response Accuracy",x.label = 'Attitude towards Bing Chat',legend.main='Label') + theme_apa()
f2

```

```{r}
m3=lm(understoodme~labelRR*atti_bing,all)
f3=interact_plot(m3, pred = atti_bing, modx = labelRR,interval=T,y.label = "Responder Understood Me",x.label = 'Attitude towards Bing Chat',legend.main='Label') + theme_apa()
f3

```

```{r}
m4=lm(connection~labelRR*atti_bing,all)
f4=interact_plot(m4, pred = atti_bing, modx = labelRR,interval=T,y.label = "Connection to Responder",x.label = 'Attitude towards Bing Chat',legend.main='Label') + theme_apa()
f4

```

```{r}
m5=lm(connection~labelRR*agency,all)
f5=interact_plot(m5, pred = agency, modx = labelRR,interval=T,y.label = "Connection to Responder",x.label = 'Mind Perception of Bing Chat \n- Agency',legend.main='Label') + theme_apa()
f5

```

```{r}
m6=lm(connection~labelRR*experience,all)
f6=interact_plot(m6, pred = experience, modx = labelRR,interval=T,y.label = "Connection to Responder",x.label = 'Mind Perception of Bing Chat \n- Experience',legend.main='Label') + theme_apa()
f6

```

```{r}

library(grid)
library(gridExtra)

figure3=ggarrange(f1, f2,f3,f4,f5,f6,nrow=2,ncol=3,common.legend = TRUE,legend='bottom')
figure3

```

The end of the Figure 3

<br>

### Empathic Accuracy

```{r}

######################empathic accuracy########################


all$happiness_ai.d = abs(all$happiness_ai-all$happiness_d)
all$sadness_ai.d = abs(all$sadness_ai-all$sadness_d)
all$fear_ai.d = abs(all$fear_ai-all$fear_d)
all$anger_ai.d = abs(all$anger_ai-all$anger_d)
all$surprise_ai.d = abs(all$surprise_ai-all$surprise_d)
all$disgust_ai.d = abs(all$disgust_ai-all$disgust_d)

```

```{r}

all$happiness_r.d = abs(all$happiness_r-all$happiness_d)
all$sadness_r.d = abs(all$sadness_r-all$sadness_d)
all$fear_r.d = abs(all$fear_r-all$fear_d)
all$anger_r.d = abs(all$anger_r-all$anger_d)
all$surprise_r.d = abs(all$surprise_r-all$surprise_d)
all$disgust_r.d = abs(all$disgust_r-all$disgust_d)

```

```{r}
pairedSamplesTTest( formula= ~happiness_ai.d + happiness_r.d, data=all )
pairedSamplesTTest( formula= ~sadness_ai.d + sadness_r.d, data=all )
pairedSamplesTTest( formula= ~fear_ai.d + fear_r.d, data=all )
pairedSamplesTTest( formula= ~disgust_ai.d + disgust_r.d, data=all )
pairedSamplesTTest( formula= ~surprise_ai.d + surprise_r.d, data=all )
pairedSamplesTTest( formula= ~anger_ai.d + anger_r.d, data=all )

```

<br>

### Follow up study

```{r}
######################follow up study####################################
library(dplyr)
library(lsr)
fu = read.csv('data/Followup Study OSF.csv')
fu$id = fu$OriginalDiscloserID
all = left_join(all,fu,by='id')

```

```{r}
independentSamplesTTest(m_emotional ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_practical ~aiorhuman.response,fu,var.equal = T)

```

```{r}
independentSamplesTTest(m_specifics_1 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_2 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_3 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_4 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_5 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_6 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_7 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_8 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_9 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_10 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_11 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_12 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_13 ~aiorhuman.response,fu,var.equal = T)
independentSamplesTTest(m_specifics_14 ~aiorhuman.response,fu,var.equal = T)

```

```{r}
independentSamplesTTest(m_motivation ~aiorhuman.response,fu,var.equal = T)
fu$heard = with(fu,apply(data.frame(m_understood,m_validated,m_affirmed,m_seen,m_accepted,m_caredfor),1,mean,na.rm=T))
independentSamplesTTest(heard ~aiorhuman.response,fu,var.equal = T)

```

```{r}
######correlation table###########
library(apaTables)
apa.cor.table(all[,c(107,141:156)], filename = "table.all.doc", table.number = 1)

```

```{r}
apa.cor.table(all[all$labelR=='ai label',c(107,141:156)], filename = "table.ai label.doc", table.number = 2)
apa.cor.table(all[all$labelR=='human label',c(107,141:156)], filename = "table.human label.doc", table.number = 3)

```

```{r}
#no sig moderation by label condition
anova(lm(feelheard~m_emotional*labelR,all))
anova(lm(feelheard~m_practical*labelR,all))
anova(lm(feelheard~m_specifics_1*labelR,all))
anova(lm(feelheard~m_specifics_2*labelR,all))
anova(lm(feelheard~m_specifics_3*labelR,all))
anova(lm(feelheard~m_specifics_4*labelR,all))
anova(lm(feelheard~m_specifics_5*labelR,all))
anova(lm(feelheard~m_specifics_6*labelR,all))
anova(lm(feelheard~m_specifics_7*labelR,all))
anova(lm(feelheard~m_specifics_8*labelR,all))
anova(lm(feelheard~m_specifics_9*labelR,all))
anova(lm(feelheard~m_specifics_10*labelR,all))
anova(lm(feelheard~m_specifics_11*labelR,all))
anova(lm(feelheard~m_specifics_12*labelR,all))
anova(lm(feelheard~m_specifics_13*labelR,all))
anova(lm(feelheard~m_specifics_14*labelR,all))

```

```{r}

############### EMOTIONS#####################
etaSquared(aov(hope~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(distress~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(uncomfortable~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(creeped~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(ambivalent~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(happy~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(shame~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(excitement~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(fear~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(surprised~responseR*labelR,all), type = 2, anova = T)
etaSquared(aov(statelonely~responseR*labelR,all), type = 2, anova = T)
```

<br>

## Discussion

The discussion section typically synthesizes the study's findings and places them in the context of existing research. It also addresses the implications, limitations, and future research directions. Here’s what should be explained:

1.  **Interpretation of Findings**:

    -   Summarize how the results support or challenge previous assumptions or findings in the field. For instance, the study found that while AI responses were effective at making participants feel heard, labeling them as AI reduced their perceived effectiveness. This finding is significant as it highlights human biases against AI, even when the content is supportive.

2.  **Implications**:

    -   Discuss the real-world applications of these findings. In practical terms, this could mean that organizations using AI for customer support or therapy might consider ways to mitigate the negative impact of an AI label or design interfaces where AI interactions appear more human-like to foster connection.

3.  **Limitations**:

    -   Acknowledge any limitations of the study, such as the representativeness of the participant sample, potential biases in response interpretation, or limitations in generalizing findings across different AI platforms or interaction types.

4.  **Future Directions**:

    -   Suggest areas for further investigation. For example, future research could explore how different types of AI, like chatbots with varying levels of anthropomorphism or transparency about their AI nature, impact perceptions. Additionally, testing these dynamics in more diverse and real-world settings could provide more insights.

5.  **Theoretical Contributions**:

    -   Detail how the study contributes to theoretical frameworks, such as media richness theory or models of human-computer interaction. It supports the idea that emotional connection and perceived understanding can be influenced by how AI is presented and perceived by users.

6.  **Practical Recommendations**:

    -   Offer actionable strategies, such as training AI to mimic human-like empathy cues more convincingly or developing hybrid models where human agents complement AI responses to maintain high levels of perceived connection.

<br>

Now, see the Discussion of the paper,

-   **Integration of Findings with Existing Literature**: The discussion starts by positioning the findings within the broader context of existing literature on human-AI interaction and empathy. The study confirms that AI can be highly effective in creating responses that make individuals feel heard, aligning with prior research on the capability of natural language processing systems to simulate human-like empathy. However, the discussion emphasizes the new insight that AI labels reduce the perceived quality of these responses, showcasing a bias against AI that persists despite its high performance.

-   **Interpretation of the AI Label Effect**: The section delves into why labeling a response as AI might negatively impact its reception. It theorizes that individuals may associate AI with a lack of genuine emotional capacity or authenticity, which diminishes their emotional connection. This bias can lead participants to undervalue the empathetic quality of AI responses, even when the content itself is indistinguishable from that of a human.

-   **Implications for AI Design and Human-AI Collaboration**: The findings have important implications for designing AI systems, particularly in fields like customer service, mental health support, and personal assistance. Developers and stakeholders are advised to consider the framing and transparency of AI interactions. The study suggests that reducing the salience of the AI label or integrating AI with human oversight might help mitigate biases and enhance the perceived quality of interaction.

-   **The Role of Emotional Validation vs. Practical Support**: A notable aspect of the discussion highlights that AI responses focused more on emotional validation rather than practical advice, which proved effective for making participants feel heard. This reinforces the idea that validation of emotions is a critical component of empathy and supportive communication. Future development of AI systems should prioritize emotional attunement when the goal is to enhance perceived empathy.

-   **Moderating Factors**: The study identifies that individual differences, such as attitudes toward AI, play a moderating role. Participants with more favorable views of AI were less influenced by the negative labeling effect. This insight prompts further research into how user characteristics, like trust in technology or familiarity with AI, affect the reception of AI-generated content.

-   **Limitations and Future Research Directions**: The authors acknowledge the limitations, such as the demographic homogeneity of the sample (e.g., primarily U.S.-based participants) and the artificial nature of the experimental setup compared to real-world applications. They suggest that future studies should include more diverse populations and real-world scenarios to enhance external validity. Additionally, examining other types of AI (e.g., those with more advanced emotional intelligence features) and varying levels of anthropomorphism could provide more comprehensive insights.

-   **Theoretical Contributions**: The paper contributes to theories of media richness and human-computer interaction by demonstrating that the perceived empathy of a response depends not just on the content but also on the framing and perceived source of the response. It underscores that biases can shape user experiences in ways that could limit the effectiveness of even highly capable AI systems.

-   **Concluding Thoughts**: The discussion wraps up by emphasizing the dual-edged nature of AI in supportive roles: it is capable of understanding and validating emotions effectively, but societal biases and perceptions may limit its impact. Addressing these biases is crucial for maximizing the potential of AI in roles traditionally reserved for humans.

<Br>
