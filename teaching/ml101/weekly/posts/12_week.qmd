---
title: "Model Improvement"
subtitle: "Enhancing Predictive Performance with Model Evaluation"
Week: 12
slide-format: revealjs
editor: 
  markdown: 
    wrap: 72
---

[Weekly design](https://changjunlee.com/teaching/ml101/weekly/)

<br>

### Pre-class video

-   Eng ver.

{{< video https://youtu.be/L-_mGMoII3I >}}

-   Kor ver.

{{< video https://youtu.be/I3y5yEk5YH0 >}}

-   Pre-class PPT [pdf](content/ML_pre_11.pdf)

### Discussion

Discussion #10

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLScmmosKUF9iLZAiJqZ-IDM-EcbBI2urfn_1ybc6vd-6xmgpdg/viewform?embedded=true" width="640" height="2800" frameborder="0" marginheight="0" marginwidth="0">

Loading...

</iframe>

### Class

#### Training data to fit diverse models using `caret` package

-   Regardless of the technique or algorithm used in ML, [the common
    process required is **learning**]{.underline}.

-   The object of learning is the training dataset, and there are
    numerous ways to learn from the training data.

    -   Each method has distinct principles, characteristics, and
        nuances, and there are various approaches. Knowing all
        algorithms has its limits.

-   When faced with a practical problem, one must consider [what to
    adopt and determine **the appropriate parameter values** to create a
    suitable model]{.underline}.

-   The `caret` package offers convenient functions for training data to
    develop predictive models.

    -   A standardized interface allows for testing around 300 machine
        learning algorithms.

    -   Easy tuning is possible by configuring different parameter
        scenarios and measuring variable importance.

    -   Through convenient training data learning, you can receive
        assistance in making an informed algorithm selection decision.

#### `caret` package: Classification And REgression Training

-   `Classification` if the dependent variable (predictor) is a nominal
    (categorical) variable

-   `Regression` if it is a continuous variable

```{r}
library(tidyverse) # for tidy tools (pipe operation, tibble, etc..)
library(caret)
```

-   Data: ***Sonar: Mines Vs. Rocks***

![](https://storage.googleapis.com/kaggle-datasets-images/1662635/2727659/3493b9309a1cf4f0c07aa6175b820060/dataset-cover.jpg?t=2021-11-13-14-25-59)

> This is the data set used by Gorman and Sejnowski in their study of
> the classification of sonar signals using a neural network. The task
> is to train a network to discriminate between sonar signals bounced
> off a metal cylinder and a roughly cylindrical rock. [Each pattern is
> a set of 60 numbers in the range 0.0 to 1.0.]{.underline} Each number
> represents the energy within a particular frequency band, integrated
> over a certain period of time. The label associated with each record
> contains the letter ***R*** if the object is a rock and ***M*** if it
> is a mine (metal cylinder). The numbers in the labels are in
> increasing order of aspect angle, but they do not encode the angle
> directly.

```{r}
# install.packages("mlbench")

set.seed(1234) # for reproducibility

data(Sonar, package = "mlbench")

Sonar %>% glimpse
```

```{r}
table(Sonar$Class)
```

**Scatterplot Matrix**

A scatterplot matrix shows a grid of scatterplots where each attribute
is plotted against all other attributes. It can be read by column or
row, and each plot appears twice, allowing you to consider the spatial
relationships from two perspectives. An improvement of just plotting the
scatterplots, is to further include class information. This is commonly
done by coloring dots in each scatterplot by their class value.

```{r}

fig <- function(width, heigth){
     options(repr.plot.width = width, 
             repr.plot.height = heigth)
}

fig(10,10)

featurePlot(x=Sonar[,1:4], 
            y=Sonar[,61], 
            plot="pairs", 
            auto.key=list(columns=2))

```

For example, in Iris dataset,

```{r}

fig(10,10)
featurePlot(x=iris[,1:4], 
            y=iris[,5], 
            plot="pairs",
            auto.key=list(columns=3))

```

**Density Plots**

***Density estimation plots (density plots for short)*** summarize the
distribution of the data. Like a histogram, the relationship between the
attribute values and number of observations is summarized, but rather
than a frequency, the relationship is summarized as a continuous
probability density function (PDF). This is the probability that a given
observation has a given value. The density plots can further be improved
by separating each attribute by their class value for the observation.
This can be useful to understand the single-attribute relationship with
the class values and highlight useful structures like linear
separability of attribute values into classes.

```{r}
fig(10, 5)
featurePlot(x=Sonar[,1:4], 
            y=Sonar[,61], 
            plot="density", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=2))
```

For example, in Iris dataset,

```{r}
fig(10, 5)
caret::featurePlot(x=iris[,1:4], 
            y=iris[,5], 
            plot="density", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3))
```

**Hold out method:** (7:3 rule)

```{r}
# Without package
indexTrain <- sample(1:nrow(Sonar), 
                     round(nrow(Sonar) * .7))
training <- Sonar[ indexTrain, ]
testing  <- Sonar[-indexTrain, ]
```

```{r}
table(training$Class)
table(testing$Class)
```

Partitioning using `createDataPartition()`

-   There is a factor, making it convenient for partitioning based on a
    specific ratio.

    -   p = .7 means 70% training 30% test

-   When using the `sample()` function, it performs complete random
    sampling, disregarding the factor ratio of the dependent variable.
    However, when using the `createDataPartition()` function, it
    supports ***stratified random sampling*** based on the factor ratio
    of the dependent variable, which is more effective.

-   By default, the returned type is a list. If the list argument value
    is set to FALSE, a vector is output.

```{r}
indexTrain <- createDataPartition(Sonar$Class, p = .7, list = F)
training <- Sonar[ indexTrain, ]
testing  <- Sonar[-indexTrain, ]
```

```{r}
table(training$Class)
table(testing$Class)
```

For the best parameter tuning

-   The parameters must be set accordingly, such as k in KNN, mtry and
    ntre in RF

-   With `caret`, the "Tuning parameters" feature helps identify optimal
    parameters based on data using methods like LOOCV, K-fold
    cross-validation, and more.

-   The number of tuning parameters varies for each algorithm. For
    instance, in the case of the number of parameters is $p$, candidate
    models are tested by searching a grid of $3^p$.

-   For example, in KNN, which has a single parameter K, three K values
    ($3^1 = 3$) are used as candidates, and the models are compared.

-   For a model with two parameters, there are 9 combinations
    ($3^2 = 9$) of parameter values used as candidates, and the models
    are compared.

-   If K-fold cross-validation is chosen as the comparison method,
    determining the number of folds (K) is also necessary.

-   The `trainControl()` function helps evaluate by consistently
    applying a uniform comparison method to each candidate.

The following code creates a configuration that repeats 10-fold
cross-validation five times to find the best candidate's parameter grid.
The ***fitControl*** object, which contains information on how to
compare models, is later utilized in the learning process.

```{r}
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

```

Now let's train with the training data set!

The standardized interface for learning is the `train()` function: You
can implement the desired learning model algorithm by changing the
method argument in the function

```{r}
rf_fit <- train(Class ~ ., 
                data = training, 
                method = "rf", 
                trControl = fitControl, 
                verbose = F)
rf_fit
```

-   `mtry` candidates are automatically set to 2, 31, and 60
-   Of these, `mtry = 2` was finally selected based on Kappa statistics
    and accuracy.
-   If you want to see the selection process in detail, set
    `verbose = F` and run

```{r}
plot(rf_fit)

```

Let's summarize various performance evaluation indicators

-   **Accuracy**: Overall correctness of the model's predictions.
    Measures the proportion of correct predictions.

    $$ \frac{TP + TN}{TP + TN + FP + FN} $$

<hr>

![](img/bey_acc_2.png)

<hr>

-   **Precision**: Proportion of true positive predictions among all
    positive predictions. High precision means fewer false positives.

    $$ \frac{TP}{TP + FP} $$

-   **Recall**: Proportion of true positive predictions among all actual
    positive instances. Also known as sensitivity or true positive rate.

    $$ \frac{TP}{TP + FN} $$

<hr>

![](img/bey_acc_1.png)

<hr>

-   **Sensitivity** is the same index with **Recall**

-   **Specificity**: Proportion of true negative predictions among all
    actual negative instances. Also known as true negative rate.

    $$ \frac{TN}{TN + FP} $$

-   **FP Rate**: Proportion of false positive predictions among all
    actual negative instances. Also known as false positive rate.

    $$ \frac{FP}{FP + TN} $$

-   **F1 Score**: Harmonic mean of precision and recall. Provides a
    balanced measure between the two.

    $$ \frac{2 \cdot (Precision \cdot Recall)}{Precision + Recall} $$

-   **Kappa**: Measures the agreement between the model's predictions
    and the expected outcomes, taking into account the possibility of
    agreement by chance.

    $$ \frac{Accuracy - RandomAccuracy}{1 - RandomAccuracy} $$

**Model Validation**

Let's compete fairly among the models using various indicators

```{r}
modelLookup() %>% head(10)

```

```{r}
modelLookup("rpart")

```

Let's train the same data fitting to four different models (DT, RF, KNN,
NB)

```{r}
dt_fit <- train(Class ~ ., 
                data = training, 
                method = "rpart", 
                trControl = fitControl)

rf_fit <- train(Class ~ ., 
                data = training, 
                method = "rf", 
                trControl = fitControl)

knn_fit <- train(Class ~ ., 
                 data = training, 
                 method = "knn", 
                 trControl = fitControl)

nb_fit <- train(Class ~ ., 
                data = training, 
                method = "nb", 
                trControl = fitControl)
```

```{r}
resamp=resamples(list(DecisionTree=dt_fit,
                      RandomForest=rf_fit, 
                      kNN=knn_fit, 
                      NaiveBayes=nb_fit))

```

`dotplot()` shows Accuracy and Kappa for all models included in
`resamples()`

```{r}
dotplot(resamp)

```

Use `predict()` which is a generic function of Testing models

```{r}
predict(rf_fit, newdata = testing)

```

```{r}
table(predict(rf_fit, newdata = testing),
      testing$Class)

```

If you add `confusionMatrix()` of the caret package, you can output
various statistics including confusion matrix and accuracy.

```{r}
predict(rf_fit, newdata = testing) %>% 
  confusionMatrix(testing$Class)

```

***Custom search grid***: Grid adjustment of tuning parameters

-   When selecting the optimal parameters, the search range and grid can
    be manually adjusted

-   The candidates for mtry, which are automatically determined by the
    3P formula: 2, 31, and 60

-   If you want to compare with more candidates, you set the candidates
    yourself

-   The code below changes the mtry candidates to 1, 2, 3, 4, 5, 6, 7,
    8, 9, and 10 and sets them up.

```{r}
customGrid <- expand.grid(mtry = 1:10)

rf_fit2 <- train(Class ~ ., 
                 data = training, 
                 method = "rf", 
                 trControl = fitControl, 
                 tuneGrid = customGrid, 
                 verbose = F)

rf_fit2
```

Generic `plot()` function shows the Accuracy change depending on `mtry`
grid

```{r}
plot(rf_fit2)

```

***Random Search Grid***: random selection of tuning parameter
combinations

-   As the number of tuning parameters increases, the number of search
    grids increases exponentially, and the search process may become
    inefficient due to the grid configuration with equal intervals

-   Let's train through RDA (Regularized Discriminant Analysis) with two
    tuning parameters

```{r}
rda_fit <- train(Class ~ ., 
                 data = training, 
                 method = "rda", 
                 trControl = fitControl, 
                 verbose = F)

rda_fit

```

-   You can see a total of 9 parameter combinations being compared

-   The user search grid introduced just above can also be searched for
    equally spaced grids using the `expand.grid()` function, but this
    time, let's configure a parameter combination that is not equally
    spaced using a random search grid.

-   `search = "random"` in the `trainControl()` function, change the
    search type to random.

```{r}
plot(rda_fit)

```

```{r}
fitControl <- trainControl(method = "repeatedcv", 
                           number = 10, 
                           repeats = 5, 
                           search = "random")

```

```{r}
rda_fit2 <- train(Class ~ ., data = training, 
                  method = "rda", 
                  trControl = fitControl, 
                  verbose = F)
rda_fit2
```

-   There are two tuning parameters, `gamma` and `lambda`, but the
    moment you change the search type to random, you can see that the
    candidate group is not set with the 3P formula.

-   To manually increase the number of tuning parameter combinations,
    use the `tuneLength` argument of the `train()` function.

```{r}
rda_fit2 <- train(Class ~ ., 
                  data = training, 
                  method = "rda", 
                  trControl = fitControl, 
                  tuneLength = 50, 
                  verbose = F)
rda_fit2
```

-   By randomly setting 50 parameters, we consider the optimal parameter
    tuning method that is slightly more flexible.

-   In comparison, it can be seen that the value of the adopted
    parameter has more decimal points and has naturally become more
    precise.

```{r}
plot(rda_fit2)
```
